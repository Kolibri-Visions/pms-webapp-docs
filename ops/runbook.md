# PMS Operations Runbook

**Purpose**: Practical troubleshooting guide for common production issues.

**Audience**: Ops engineers, DevOps, on-call responders.

**Last Updated**: 2025-12-26

---

## Quick Reference

| Issue | Symptom | Section |
|-------|---------|---------|
| API returns 503 after deploy | "Service degraded" or "Database unavailable" | [DB DNS / Degraded Mode](#db-dns--degraded-mode) |
| JWT auth fails | 401 Unauthorized despite valid token | [Token Validation](#token-validation-apikey-header) |
| API returns 503 with schema error | "Schema not installed/out of date" | [Schema Drift](#schema-drift) |
| Booking detail returns 500 | ResponseValidationError on status field | [Booking Status Validation](#booking-status-validation-error-500) |
| Smoke script fails | Empty TOKEN/PID, bash errors | [Smoke Script Pitfalls](#smoke-script-pitfalls) |

---

## Daily Ops Checklist (5–10 minutes)

**Purpose**: Quick daily health check for production PMS system. Catches common issues before they escalate.

**When to Run**: Start of shift, after deployments, or when investigating user reports.

---

### Step 1: Verify Containers + Deployed Commit

**WHERE:** HOST-SERVER-TERMINAL

```bash
# Check all PMS containers are running
docker ps --format 'table {{.Names}}\t{{.Status}}\t{{.Image}}' | grep -E 'pms-backend|pms-admin|pms-worker'

# Expected output:
# pms-backend       Up X hours   ghcr.io/.../pms-backend:main
# pms-admin         Up X hours   ghcr.io/.../pms-admin:main
# pms-worker-v2     Up X hours   ghcr.io/.../pms-worker-v2:main

# Verify deployed commit matches latest main
docker exec pms-backend env | grep SOURCE_COMMIT
# Cross-check with: git log -1 --oneline (on your local main branch)
```

**Red Flags:**
- Any container missing or in "Restarting" status
- pms-worker (old) still running alongside pms-worker-v2
- SOURCE_COMMIT doesn't match expected deploy

---

### Step 2: Health Checks

**WHERE:** HOST-SERVER-TERMINAL

```bash
# Quick HEAD checks (status-only, no body)
curl -I https://api.fewo.kolibri-visions.de/health
# Expected: HTTP/2 200

curl -I https://api.fewo.kolibri-visions.de/health/ready
# Expected: HTTP/2 200

# Full readiness check (JSON body for component details)
curl -s https://api.fewo.kolibri-visions.de/health/ready | jq .
```

**Expected Response:**
```json
{
  "status": "up",
  "components": {
    "db": {"status": "up"},
    "redis": {"status": "up"},
    "celery": {
      "status": "up",
      "details": {"workers": ["celery@pms-worker-v2-..."]}
    }
  },
  "checked_at": "2026-01-01T12:00:00Z"
}
```

**Red Flags:**
- `/health/ready` returns 503
- `db: "down"` → Check [DB DNS / Degraded Mode](#db-dns--degraded-mode)
- `redis: "down"` → Verify Redis container running
- `celery: "down"` → Check worker status (see Step 3)

---

### Step 3: Worker Singleton Enforcement

**WHERE:** HOST-SERVER-TERMINAL

```bash
# Verify exactly ONE Celery worker is running
docker ps --format 'table {{.Names}}\t{{.Status}}' | grep pms-worker

# Expected: Only pms-worker-v2 listed (NOT pms-worker)

# Cross-check with health endpoint
curl -s https://api.fewo.kolibri-visions.de/health/ready | jq '.components.celery'
```

**Expected Health Response:**
```json
{
  "status": "up",
  "details": {
    "workers": ["celery@pms-worker-v2-abc123"]
  }
}
```

**Red Flags:**
- Multiple workers detected (e.g., pms-worker AND pms-worker-v2)
- Health check shows `celery: "down"` with error: "Expected exactly 1 worker, found 2"

**Fix (if multiple workers found):**
```bash
# Stop old worker (HOST-SERVER-TERMINAL)
docker stop pms-worker  # or whichever is the old container

# Verify only one remains
docker ps | grep pms-worker
# Should only show pms-worker-v2

# Verify health check passes
curl -s https://api.fewo.kolibri-visions.de/health/ready | jq '.components.celery.status'
# Expected: "up"
```

**Why This Matters:**
- Multiple workers cause duplicate sync tasks (same operation executed 2-3x)
- Race conditions when updating sync log status
- Inconsistent batch status in Admin UI

See [Single-Worker Enforcement](#get-healthready) for automatic detection details.

---

### Step 4: Channel Manager Sanity Check

**WHERE:** Browser (Admin UI) + HOST-SERVER-TERMINAL (optional curl)

**Admin UI (Quick Visual Check):**

1. Navigate to: `https://admin.fewo.kolibri-visions.de/connections`
2. Click any connection to view details
3. Check **Sync History** section:
   - Recent batches should show within last 24h
   - Filter by "Failed" status → Expect 0 or minimal failed batches
   - If many failures: investigate sync logs for error patterns

**Optional: curl API Check (HOST-SERVER-TERMINAL):**

```bash
# Get admin token (if not cached)
TOKEN=$(curl -sX POST "$SB_URL/auth/v1/token?grant_type=password" \
  -H "apikey: $ANON_KEY" \
  -H "Content-Type: application/json" \
  -d '{"email":"admin@example.com","password":"password"}' \
  | jq -r '.access_token')

# List recent sync batches for a connection
CID="your-connection-uuid-here"
curl -s "https://api.fewo.kolibri-visions.de/api/v1/channel-connections/$CID/sync-batches?limit=10&status=any" \
  -H "Authorization: Bearer $TOKEN" \
  | jq '.items[] | {batch_id, batch_status, created_at_min}'

# Get batch detail for failed batch (if any)
BATCH_ID="batch-uuid-from-above"
curl -s "https://api.fewo.kolibri-visions.de/api/v1/channel-connections/$CID/sync-batches/$BATCH_ID" \
  -H "Authorization: Bearer $TOKEN" \
  | jq '{batch_status, status_counts, operations: .operations[] | {operation_type, status}}'
```

**Red Flags:**
- High failed batch rate (>10% in last 24h)
- All batches stuck in "running" status (worker may be down)
- Recent batches show `status_counts.other > 0` (unexpected status values)

---

### Step 5: Quick Issue Triage (If Red Flags Found)

**DB Degraded Mode:**

```bash
# WHERE: HOST-SERVER-TERMINAL
# Check DNS resolution
docker exec pms-backend getent hosts supabase-db
# Empty output = DNS failure → See [DB DNS / Degraded Mode](#db-dns--degraded-mode)

# Check network attachment
docker inspect pms-backend | jq '.[0].NetworkSettings.Networks | keys'
# Must include: "bccg4gs4o4kgsowocw08wkw4" (Supabase network)
```

**Redis/Celery Issues:**

```bash
# WHERE: HOST-SERVER-TERMINAL
# Check Redis container
docker ps | grep redis
# Expected: supabase-redis running

# Check worker logs for errors
docker logs pms-worker-v2 --tail 50
# Look for: connection errors, task failures, retry exhaustion
```

**Where to Look:**
- Backend logs: `docker logs pms-backend --tail 100`
- Worker logs: `docker logs pms-worker-v2 --tail 100`
- Admin logs: `docker logs pms-admin --tail 50`
- Health endpoint: `/health/ready` component details

---

### Step 6: Retention Reminder (Weekly/Monthly)

**WHERE:** Supabase SQL Editor (Dashboard → SQL Editor → New Query)

```sql
-- Quick check: row count and date range
SELECT COUNT(*) AS total_logs,
       MIN(created_at) AS oldest_log,
       MAX(created_at) AS newest_log,
       COUNT(*) FILTER (WHERE created_at < NOW() - INTERVAL '90 days') AS logs_older_than_90d
FROM public.channel_sync_logs;

-- If logs_older_than_90d > 0, consider cleanup
-- See [Channel Manager — Sync Log Retention & Cleanup](#channel-manager--sync-log-retention--cleanup)
```

**Retention Guidelines:**
- Test/Staging: 90 days
- Production: 180-365 days (depending on compliance)

**Cleanup (when needed):**
```sql
-- Preview deletion count first (dry-run)
SELECT COUNT(*) AS logs_to_delete
FROM public.channel_sync_logs
WHERE created_at < NOW() - INTERVAL '90 days';

-- Execute cleanup (after verifying count above)
DELETE FROM public.channel_sync_logs
WHERE created_at < NOW() - INTERVAL '90 days';
```

See [Channel Manager — Sync Log Retention & Cleanup](#channel-manager--sync-log-retention--cleanup) for full cleanup procedures and safety notes.

**Monthly Code Quality Check:**

Run regression guard to verify no browser popups (alert/confirm/prompt) were reintroduced:

```bash
# WHERE: LOCAL-DEV-TERMINAL (in repo root)
bash frontend/scripts/check_no_browser_popups.sh
```

**Expected Output:**
```
✅ OK: No browser popups (alert/confirm/prompt) found in frontend/
```

If popups detected, see [Regression Guard](#regression-guard) section for remediation steps.

---

**Checklist Complete** ✅

- If all green: System healthy, no action needed
- If red flags found: Investigate using sections linked above
- Document any issues/fixes in ops log for trending analysis

**Related:**
- [Quick Smoke (5 minutes)](../scripts/README.md#quick-smoke-5-minutes) - Automated smoke test script
- [Health Monitoring (HEAD vs GET)](#head-vs-get-for-health-monitoring) - Detailed health check guide
- [Top 5 Failure Modes](#top-5-failure-modes-and-fixes) - Common production issues and fixes

---

## Top 5 Failure Modes (and Fixes)

This section provides quick, actionable fixes for the most common production failures. Each includes symptoms, root cause, fix steps, and verification commands with explicit WHERE labels.

---

### 1. DB DNS / Network Disconnect → Degraded Mode

**Symptoms:**
```
socket.gaierror: [Errno -2] Name or service not known: 'supabase-db'
GET /health/ready → 503 {"status":"unhealthy","db":"down"}
Logs: "Database connection pool creation FAILED"
```

**Root Cause:**
pms-backend or pms-worker-v2 container not attached to Supabase network (`bccg4gs4o4kgsowocw08wkw4`).

**Fix Steps:**

1. Verify DNS resolution fails

WHERE: HOST-SERVER-TERMINAL
```bash
docker exec pms-backend getent hosts supabase-db
# Empty output = DNS failure
```

2. Attach container to Supabase network

WHERE: HOST-SERVER-TERMINAL
```bash
docker network connect bccg4gs4o4kgsowocw08wkw4 pms-backend
docker network connect bccg4gs4o4kgsowocw08wkw4 pms-worker-v2
```

3. Restart containers

WHERE: HOST-SERVER-TERMINAL
```bash
docker restart pms-backend
docker restart pms-worker-v2
```

**Verification:**

WHERE: HOST-SERVER-TERMINAL
```bash
# 1. DNS resolves
docker exec pms-backend getent hosts supabase-db
# Expected: 172.20.0.X supabase-db

# 2. DB connection works
docker exec pms-backend python -c "import asyncpg; import asyncio; asyncio.run(asyncpg.connect('postgresql://postgres:$PASSWORD@supabase-db:5432/postgres').execute('SELECT 1'))"

# 3. Health endpoint passes
curl https://api.fewo.kolibri-visions.de/health/ready
# Expected: {"status":"healthy","db":"up","redis":"up","celery":"up"}
```

**See Also:** [DB DNS / Degraded Mode](#db-dns--degraded-mode) (detailed section below)

---

### 2. ImportError at Startup → 503 No Available Server

**Symptoms:**
```
Traefik returns: 503 Service Unavailable - "no available server"
Container status: Restarting
Logs: ImportError: cannot import name 'X' from 'app.Y'
Backend never reaches healthy state
```

**Root Cause:**
Python import-time error in application startup, often due to:
- Importing non-existent function/symbol from a module
- Circular import dependency
- Missing dependency (typo in import statement)

**Common Example (Public Booking Router):**
```python
# WRONG - get_db_pool doesn't exist in app.api.deps
from app.api.deps import get_db_pool

# CORRECT - use canonical DB dependency
from app.api.deps import get_db
```

**Fix Steps:**

1. Check container logs for ImportError

WHERE: HOST-SERVER-TERMINAL
```bash
docker logs pms-backend --tail 50 | grep -A 5 "ImportError"
# Look for: "ImportError: cannot import name 'X' from 'Y'"
```

2. Identify the failing import

WHERE: HOST-SERVER-TERMINAL
```bash
# Check which module/file triggered the error
docker logs pms-backend | grep -B 10 "ImportError"
# Note the file path (e.g., /app/app/api/routes/public_booking.py)
```

3. Fix the import (use correct symbol name)

WHERE: YOUR-WORKSTATION
```bash
# Example: Replace invalid get_db_pool with canonical get_db
# Edit backend/app/api/routes/public_booking.py:
# - from app.api.deps import get_db_pool  # REMOVE
# + from app.api.deps import get_db       # ADD
```

4. Commit and redeploy

WHERE: YOUR-WORKSTATION
```bash
git add backend/app/api/routes/public_booking.py
git commit -m "fix: use canonical get_db dependency"
git push origin main
```

**Verification:**

WHERE: HOST-SERVER-TERMINAL
```bash
# 1. Container stays running (not restarting)
docker ps | grep pms-backend
# Expected: "Up X seconds" (not "Restarting")

# 2. Health endpoint responds
curl https://api.fewo.kolibri-visions.de/health
# Expected: 200 OK

# 3. No ImportError in logs
docker logs pms-backend --tail 50 | grep ImportError
# Expected: No output
```

**Prevention:**
- Use canonical dependencies defined in `app/api/deps.py` (__all__ exports)
- Verify imports exist before deploying
- Run `python3 -m py_compile <file>` to catch import errors early

---

### 3. JWT/Auth Failures (401 Invalid Token / 403 Not Authenticated)

**Symptoms:**
```
POST /api/v1/bookings → 401 {"detail":"Invalid authentication token"}
Logs: "JWT token validation failed"
Missing TOKEN env var or TOKEN=""
Wrong JWT_SECRET (token signature verification fails)
Kong /auth/v1/user returns 401 without apikey header
```

**Root Cause:**
- Missing/empty TOKEN environment variable
- JWT_SECRET mismatch between backend and GoTrue
- Missing `apikey` header when calling Kong-protected /auth/v1/user

**Fix Steps:**

1. Fetch valid JWT token

WHERE: HOST-SERVER-TERMINAL
```bash
# Login and extract token
curl -X POST https://sb-pms.kolibri-visions.de/auth/v1/token?grant_type=password \
  -H "apikey: $ANON_KEY" \
  -H "Content-Type: application/json" \
  -d '{"email":"$EMAIL","password":"$PASSWORD"}' | jq -r '.access_token'

# Save to variable
export TOKEN="eyJhb..."
```

**Note**: If SB_URL or ANON_KEY are not available in your environment, retrieve them from the Supabase Kong container:

WHERE: HOST-SERVER-TERMINAL
```bash
# Get Supabase container name
SUPABASE_KONG_CONTAINER=$(docker ps --filter "name=supabase-kong" --format "{{.Names}}" | head -n1)

# Extract environment variables from Kong container
export SB_URL=$(docker inspect $SUPABASE_KONG_CONTAINER | jq -r '.[0].Config.Env[]' | grep '^SUPABASE_URL=' | cut -d'=' -f2-)
export ANON_KEY=$(docker inspect $SUPABASE_KONG_CONTAINER | jq -r '.[0].Config.Env[]' | grep '^SUPABASE_ANON_KEY=' | cut -d'=' -f2-)

# Verify
echo "SB_URL: $SB_URL"
echo "ANON_KEY: ${ANON_KEY:0:20}..." # Show first 20 chars only
```

2. Verify token structure

WHERE: HOST-SERVER-TERMINAL
```bash
# Check token length (should be ~500+ chars)
echo ${#TOKEN}

# Check JWT parts (should have 3 parts: header.payload.signature)
echo $TOKEN | tr '.' '\n' | wc -l
# Expected: 3
```

3. Verify JWT_SECRET matches GoTrue

WHERE: Coolify Dashboard > pms-backend > Environment Variables
```
JWT_SECRET=your-jwt-secret-here
SUPABASE_JWT_SECRET=your-jwt-secret-here
```

WHERE: Supabase SQL Editor
```sql
-- Get GoTrue JWT secret
SELECT decrypted_secret
FROM vault.decrypted_secrets
WHERE name = 'jwt_secret';
```

4. Test auth endpoint with apikey header

WHERE: HOST-SERVER-TERMINAL
```bash
# Kong requires apikey header
curl https://sb-pms.kolibri-visions.de/auth/v1/user \
  -H "apikey: $ANON_KEY" \
  -H "Authorization: Bearer $TOKEN"

# Expected: {"id":"...","email":"...","role":"authenticated"}
```

**Verification:**

WHERE: HOST-SERVER-TERMINAL
```bash
# Test authenticated endpoint
curl https://api.fewo.kolibri-visions.de/api/v1/properties \
  -H "Authorization: Bearer $TOKEN"

# Expected: 200 with property list (not 401)
```

**See Also:** [Token Validation](#token-validation-apikey-header) (detailed section below)

---

### 3. Worker/Celery/Redis Misconfig (Connection Refused / Tasks Not Updating Logs)

**Symptoms:**
```
Celery: ConnectionRefusedError [Errno 111] Connection refused
GET /health/ready → 503 {"celery":"down"}
Sync logs stuck in "triggered" or "running" (never "success"/"failed")
Worker logs: "Cannot connect to redis://localhost:6379"
```

**Root Cause:**
- Missing/wrong REDIS_URL, CELERY_BROKER_URL, CELERY_RESULT_BACKEND
- Redis container not reachable from worker
- Health check not enabled in celery.py

**Fix Steps:**

1. Verify Redis is reachable

WHERE: Coolify Terminal (pms-worker-v2 container)
```bash
# Check Redis connection
redis-cli -u $CELERY_BROKER_URL ping
# Expected: PONG
```

2. Set correct environment variables

WHERE: Coolify Dashboard > pms-worker-v2 > Environment Variables
```
REDIS_URL=redis://coolify-redis:6379/0
CELERY_BROKER_URL=redis://coolify-redis:6379/0
CELERY_RESULT_BACKEND=redis://coolify-redis:6379/1
```

3. Verify Celery worker is running

WHERE: Coolify Terminal (pms-worker-v2 container)
```bash
# Check Celery status
celery -A app.worker.celery_app inspect ping
# Expected: {"celery@...": {"ok": "pong"}}

# Check active tasks
celery -A app.worker.celery_app inspect active
```

4. Check worker logs

WHERE: Coolify Dashboard > pms-worker-v2 > Logs
```
# Should see:
[INFO/MainProcess] Connected to redis://coolify-redis:6379/0
[INFO/MainProcess] celery@worker ready
```

**Verification:**

WHERE: HOST-SERVER-TERMINAL
```bash
# 1. Health check passes
curl https://api.fewo.kolibri-visions.de/health/ready
# Expected: {"celery":"up","redis":"up"}

# 2. Trigger availability sync
curl -X POST https://api.fewo.kolibri-visions.de/api/v1/channel-sync/trigger \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"property_id":"$PROPERTY_ID","sync_type":"availability"}'

# 3. Check sync log status updates
curl https://api.fewo.kolibri-visions.de/api/v1/channel-sync/logs/$LOG_ID \
  -H "Authorization: Bearer $TOKEN"
# Expected: status transitions from "triggered" → "running" → "success"
```

**See Also:** Worker troubleshooting sections below

---

### 4. Schema Drift / Missing Migrations (UndefinedTable/UndefinedColumn → 503)

**Symptoms:**
```
GET /api/v1/properties → 503 {"detail":"Database schema not installed or out of date..."}
Logs: asyncpg.exceptions.UndefinedTableError: relation "properties" does not exist
Logs: asyncpg.exceptions.UndefinedColumnError: column "agency_id" does not exist
```

**Root Cause:**
Deployed database schema doesn't match migration files in repo. Migrations not applied after schema changes.

**Fix Steps:**

1. Check what tables exist

WHERE: Supabase SQL Editor
```sql
SELECT tablename
FROM pg_tables
WHERE schemaname = 'public'
ORDER BY tablename;

-- Expected: agencies, properties, bookings, inventory_ranges, etc.
```

2. Check for missing columns

WHERE: Supabase SQL Editor
```sql
SELECT column_name, data_type
FROM information_schema.columns
WHERE table_schema = 'public'
  AND table_name = 'properties'
ORDER BY ordinal_position;

-- Verify key columns exist: id, agency_id, name, created_at, etc.
```

3. List migration files in repo

WHERE: HOST-SERVER-TERMINAL
```bash
ls -1 supabase/migrations/*.sql | tail -5
# See latest migration files
```

4. Apply migrations

WHERE: Supabase SQL Editor
```sql
-- Copy/paste migration SQL from supabase/migrations/*.sql
-- Execute each migration in order (oldest to newest)
-- Use DO $$ blocks for idempotent DDL if needed:

DO $$
BEGIN
  IF NOT EXISTS (
    SELECT 1 FROM information_schema.tables
    WHERE table_schema = 'public' AND table_name = 'properties'
  ) THEN
    CREATE TABLE properties (...);
  END IF;
END $$;
```

**Verification:**

WHERE: HOST-SERVER-TERMINAL
```bash
# 1. Check endpoint returns 200
curl https://api.fewo.kolibri-visions.de/api/v1/properties \
  -H "Authorization: Bearer $TOKEN"
# Expected: 200 with data (not 503)

# 2. Verify health check passes
curl https://api.fewo.kolibri-visions.de/health/ready
# Expected: {"status":"healthy","db":"up"}
```

WHERE: Supabase SQL Editor
```sql
-- Sample query to verify schema
SELECT p.id, p.name, p.agency_id
FROM properties p
LIMIT 1;

-- Should return data without errors
```

**See Also:**
- [Schema Drift](#schema-drift) (detailed section below)
- [Migrations Guide - Schema Drift SOP](../database/migrations-guide.md#schema-drift-sop) (step-by-step SOP)

---

### 5. Bash Smoke Script Pitfalls (TOKEN/PID Empty, set -u Unbound Variable, 307 Redirect)

**Symptoms:**
```bash
smoke.sh: line 42: PID: unbound variable
smoke.sh: line 55: TOKEN: unbound variable
curl: Expecting value: line 1 column 1 (char 0)  # Empty response
curl: 307 Temporary Redirect (without -L flag)
```

**Root Cause:**
- `set -u` causes script to exit if PID or TOKEN unset/empty
- Missing `-L` flag on curl (doesn't follow redirects)
- Invalid JSON response (HTML redirect page instead of JSON)

**Fix Steps:**

1. Export required variables before running script

WHERE: HOST-SERVER-TERMINAL
```bash
# Set variables
export BACKEND_URL="https://api.fewo.kolibri-visions.de"
export ANON_KEY="your-anon-key"
export EMAIL="admin@example.com"
export PASSWORD="your-password"
export PID="some-property-id"

# Verify variables are set
echo "BACKEND_URL: $BACKEND_URL"
echo "PID length: ${#PID}"
```

2. Fetch and validate TOKEN

WHERE: HOST-SERVER-TERMINAL
```bash
# Login and extract token
TOKEN=$(curl -X POST https://sb-pms.kolibri-visions.de/auth/v1/token?grant_type=password \
  -H "apikey: $ANON_KEY" \
  -H "Content-Type: application/json" \
  -d "{\"email\":\"$EMAIL\",\"password\":\"$PASSWORD\"}" \
  | jq -r '.access_token')

# Validate token
if [ -z "$TOKEN" ] || [ "$TOKEN" = "null" ]; then
  echo "ERROR: Failed to fetch token"
  exit 1
fi

# Check token structure
echo "Token length: ${#TOKEN}"
echo "Token parts: $(echo $TOKEN | tr '.' '\n' | wc -l)"  # Should be 3
```

3. Use -L flag for curl redirects

WHERE: HOST-SERVER-TERMINAL
```bash
# Without -L: may return 307 redirect
curl https://api.fewo.kolibri-visions.de/health

# With -L: follows redirect to final destination
curl -L https://api.fewo.kolibri-visions.de/health
# Expected: {"status":"ok","service":"pms-backend"}
```

4. Quick smoke checks

WHERE: HOST-SERVER-TERMINAL
```bash
# Health check
curl -L $BACKEND_URL/health
# Expected: {"status":"ok"}

# Readiness check
curl -L $BACKEND_URL/health/ready
# Expected: {"status":"healthy","db":"up","redis":"up"}

# Authenticated endpoint
curl -L $BACKEND_URL/api/v1/properties \
  -H "Authorization: Bearer $TOKEN"
# Expected: 200 with JSON array
```

**Verification:**

WHERE: HOST-SERVER-TERMINAL
```bash
# Run smoke script with all variables set
./smoke.sh
# Expected: All checks pass, no "unbound variable" errors
```

**See Also:** [Smoke Script Pitfalls](#smoke-script-pitfalls) (detailed section below)

---

## DB DNS / Degraded Mode

### Symptom

After deployment or container restart:
- API returns `503 Service Unavailable`
- Logs show: `"Database connection pool creation FAILED"` or `"Service degraded"`
- Health endpoint (`/health`) returns 200, but `/health/ready` returns 503

### Root Cause

PMS backend container cannot resolve Supabase database DNS (`supabase-db`) due to missing Docker network attachment.

**Why this happens:**
- Coolify creates a dedicated network for Supabase stack: `bccg4gs4o4kgsowocw08wkw4`
- Coolify creates a default network for PMS app: `coolify`
- Container needs to be attached to **both** networks to resolve `supabase-db` hostname

**Common scenario:**
- Database/network may be temporarily unavailable during container startup (race condition)
- DNS resolution may fail transiently before network is fully ready
- Backend now retries connection for up to 60s before entering degraded mode

### New Behavior: Startup Retry + Background Self-Heal (2026-01-04)

**Startup Retry:**
- Backend now retries DB connection for up to `DB_STARTUP_MAX_WAIT_SECONDS` (default: 60s)
- Sleeps `DB_STARTUP_RETRY_INTERVAL_SECONDS` (default: 2s) between attempts
- Logs progress: elapsed time, error type, next retry
- If connection succeeds within timeout: starts normally (no degraded mode warning)
- If still failing after timeout: enters degraded mode and starts background reconnection

**Background Self-Heal:**
- If startup fails, backend starts a background task to periodically retry DB connection
- Retries every `DB_BACKGROUND_RECONNECT_INTERVAL_SECONDS` (default: 30s)
- When DB becomes available: pool is created automatically (no restart needed)
- Logs success: "Background reconnection: SUCCESS. App is now in NORMAL MODE"
- Disable with `DB_BACKGROUND_RECONNECT_ENABLED=false` if needed

**Expected Logs:**
```
# Startup (first attempt fails, retrying)
Database connection attempt 1 failed (gaierror: Temporary failure in name resolution).
Retrying in 2.0s (elapsed: 0.2s, max: 60s)...

# Startup (success after retries)
Database connection pool created successfully (PID=1, host=supabase-db, attempt=3, elapsed=4.5s)

# Startup (all retries exhausted, entering degraded mode)
Database connection failed after 30 attempts (60.1s).
Last error: gaierror: Temporary failure in name resolution.
App will start in DEGRADED MODE (DB unavailable).
Background reconnection will retry every 30s.

# Background reconnection (attempting)
Background reconnection: attempting to create DB pool...

# Background reconnection (success)
Background reconnection: SUCCESS. App is now in NORMAL MODE (DB available).
```

**Impact:**
- **Before:** Immediate degraded mode on any startup DNS failure (required restart)
- **After:** Retries for 60s, then self-heals via background task (no restart needed)
- **Readiness:** `/health/ready` will return 503 for up to 60s after deploy if DB is slow to start
- **Operators:** If you see 503 immediately after deploy, wait up to 60s before investigating

### Single-Instance Pool Initialization (2026-01-04 Updated)

**Symptom:**
- Backend logs show repeated "Database connection pool created successfully" messages with different attempt numbers
- Multiple pool creation log lines appear (e.g., "attempt=12", then "attempt=1", then "attempt=1")
- Indicates multiple init call sites creating new pools instead of reusing shared instance
- Logs may show: pool created ... generation=1, then generation=2, generation=3 (generation increments)

**Root Cause (Updated Analysis):**
- Previous fix added lock but didn't prevent sequential retry loops
- Multiple call sites (startup, request-time ensure, health checks) each started own retry loop
- Lock only prevented concurrent execution, not duplicate initialization attempts
- First caller starts retry loop (attempt=1...12), completes; second caller starts new retry (attempt=1)
- Risk: leaked pools/connections, wasted DB retry attempts, noisy logs

**Fix Applied (Init Task Tracking):**
- Added `_init_task` to track ongoing pool initialization (prevents duplicate init)
- Added `_pool_generation` counter (increments on each pool creation for verification)
- `ensure_pool()` now uses init task pattern:
  - If pool exists: returns immediately (fast path, no lock)
  - If init task exists: awaits it (reuses ongoing initialization)
  - If neither exists: creates init task, awaits it (single retry loop)
- All call sites use `ensure_pool()` (startup, requests, health, background task)
- Retry loop runs exactly once per pool creation (not once per caller)

**Expected Behavior After Fix:**
```
# Startup (single pool creation, single retry loop)
Creating database connection pool (PID=1, host=supabase-db, max_wait=60s, retry_interval=2s)...
Database connection pool created successfully (PID=1, host=supabase-db, generation=1, attempt=3, elapsed=4.5s, pool_id=140...)
Database connected: PostgreSQL 15.1...

# Subsequent calls (pool already exists, reuse, NO logs)
[Silent - pool is reused via fast path]

# If pool creation fails and background reconnect succeeds
Background reconnection: attempting to create DB pool...
Creating database connection pool (PID=1, host=supabase-db, max_wait=60s, retry_interval=2s)...
Database connection pool created successfully (PID=1, host=supabase-db, generation=2, attempt=1, elapsed=0.5s, pool_id=140...)
Background reconnection: SUCCESS. App is now in NORMAL MODE (DB available).
```

**Verification Steps:**
```bash
# 1. Restart backend container
docker restart $(docker ps -q -f name=pms-backend)

# 2. Count pool creation events (should be exactly 1)
docker logs $(docker ps -q -f name=pms-backend) 2>&1 | grep "pool created successfully" | wc -l
# Expected: 1

# 3. Check generation counter (should be 1 for first pool)
docker logs $(docker ps -q -f name=pms-backend) 2>&1 | grep "pool created successfully"
# Expected: Single line with "generation=1"

# 4. Check pool_id stays constant (proves same pool reused)
docker logs $(docker ps -q -f name=pms-backend) 2>&1 | grep "pool created successfully" | grep -o "pool_id=[0-9]*"
# Expected: Single pool_id value

# 5. Make multiple concurrent /health/ready requests (should NOT create new pools)
for i in {1..20}; do curl -s https://api.fewo.kolibri-visions.de/health/ready & done
wait

# 6. Verify still only one pool creation
docker logs $(docker ps -q -f name=pms-backend) 2>&1 | grep "pool created successfully" | wc -l
# Expected: Still 1 (not 2, 3, 10, etc.)

# 7. Check for multiple attempt sequences (regression symptom)
docker logs $(docker ps -q -f name=pms-backend) 2>&1 | grep "Database connection attempt" | grep -o "attempt=[0-9]*"
# Expected: Single sequence (e.g., attempt=1, attempt=2, attempt=3)
# NOT multiple sequences starting from attempt=1
```

**Operator Notes:**
- **Healthy:** Exactly one "pool created" log with generation=1, single attempt sequence
- **Regression:** Multiple "pool created" logs, or generation>1 without background reconnect
- **Forked processes:** Celery workers create own pool (generation increments per worker - expected)
- **Pool ID:** Should remain constant across all requests (proves reuse)

### Multiple Pools Created in Same PID (pool_id changes) - 2026-01-04

**Symptom:**
- Same PID creates multiple pools with DIFFERENT pool_id values
- Example logs from PID=1:
  - pool_id=128315581080752, generation=1
  - pool_id=136539789892368, generation=1
  - pool_id=140137983023888, generation=1
- Generation stays at 1 (should increment to 2, 3, 4)
- Indicates true pool recreation, not just duplicate logging

**Root Causes (Investigated):**

**A) Multiple module instances (singleton not shared):**
- Multiple imports of database module create separate _pool variables
- Check: `module_id` should be identical across all "pool created" logs
- If module_id changes: multiple module instances exist (reimport issue)

**B) Code path closes/resets pool incorrectly:**
- Some code calls `_pool = None` or `pool.close()` outside shutdown
- Check: grep logs for "Resetting pool state" or "Closing database connection pool"
- If appears during normal operation: incorrect reset/close call

**C) Celery worker reset without clearing init task:**
- `reset_pool_state()` must reset `_pool`, `_pool_pid`, AND `_init_task`
- If `_init_task` not reset: child process may await parent's task (wrong event loop)
- Fix: reset_pool_state() now clears _init_task (2026-01-04)

**Fix Applied (2026-01-04):**
- Updated `reset_pool_state()` to also reset `_init_task` (critical for fork safety)
- Added comprehensive diagnostics to pool creation log:
  - `module_file`: Path to database.py file (proves same module)
  - `module_id`: ID of module object (proves singleton)
  - `pool_id`: ID of pool instance (changes on recreation)
  - `generation`: Counter (must increment on each creation)

**Verification Commands:**
```bash
# 1. Check if pool_id changes (regression symptom)
docker logs $(docker ps -q -f name=pms-backend) 2>&1 | grep "pool created successfully" | grep -o "pool_id=[0-9]*"
# Expected: Single pool_id value
# Bad: Multiple different pool_id values

# 2. Check if generation increments correctly
docker logs $(docker ps -q -f name=pms-backend) 2>&1 | grep "pool created successfully" | grep -o "generation=[0-9]*"
# Expected: generation=1 (or generation=1, generation=2 if background reconnect)
# Bad: Multiple generation=1 entries

# 3. Check module_id stays constant (proves singleton)
docker logs $(docker ps -q -f name=pms-backend) 2>&1 | grep "pool created successfully" | grep -o "module_id=[0-9]*"
# Expected: Single module_id value
# Bad: Multiple different module_id values (indicates multiple module instances)

# 4. Check module_file stays constant
docker logs $(docker ps -q -f name=pms-backend) 2>&1 | grep "pool created successfully" | grep -o "module_file=[^ ]*"
# Expected: Same file path for all entries
# Bad: Different file paths (indicates import from different locations)

# 5. Full diagnostic line
docker logs $(docker ps -q -f name=pms-backend) 2>&1 | grep "pool created successfully"
# Analyze: PID, generation, pool_id, module_id, module_file should be consistent
```

**Diagnosis Decision Tree:**
1. **pool_id changes, generation stays 1, module_id changes:**
   - Cause: Multiple module instances (reimport or circular import)
   - Action: Check import paths, remove circular imports

2. **pool_id changes, generation stays 1, module_id constant:**
   - Cause: Pool is being closed/reset incorrectly during operation
   - Action: Check for unauthorized pool.close() or _pool=None assignments

3. **pool_id changes, generation increments, module_id constant:**
   - Cause: Background reconnect working correctly (not a bug)
   - Action: Verify degraded mode preceded this (expected behavior)

### Pool Churn Diagnostics - Python Helpers (2026-01-04)

**Purpose:** Programmatic diagnostic functions for detecting pool churn at runtime.

**Function: `pool_debug_state()`**

Returns comprehensive pool state for debugging:

```python
from app.core.database import pool_debug_state
import json

# In endpoint (for debugging)
@router.get("/debug/pool-state")
async def get_pool_state():
    return pool_debug_state()

# In logs
logger.info(f"Pool state: {json.dumps(pool_debug_state(), indent=2)}")
```

**Returns:**
```json
{
  "pool_exists": true,
  "pool_id": 140137983023888,
  "pool_pid": 1,
  "current_pid": 1,
  "pool_generation": 1,
  "init_task_running": false,
  "background_reconnect_running": false,
  "module_info": {
    "duplicates_found": false,
    "module_keys": ["app.core.database"],
    "module_id": 140137982456320,
    "module_file": "/app/app/core/database.py"
  }
}
```

**Function: `detect_duplicate_module_import()`**

Detects if database module is imported multiple times (root cause of pool churn):

```python
from app.core.database import detect_duplicate_module_import

result = detect_duplicate_module_import()
if result["duplicates_found"]:
    logger.error(f"Duplicate module imports detected: {result['module_keys']}")
    # Example: ["app.core.database", "core.database", "database"]
```

**Common Causes of Duplicate Imports:**
- Import path variation: `app.core.database` vs `core.database` vs `database`
- Symlinks: `/app/core/database.py` vs `/app-symlink/core/database.py`
- sys.path manipulation: module loaded from different paths
- Circular imports: Python imports module twice to break cycle

**Action on Detection:**
1. Review all import statements: grep for `from.*database import` or `import.*database`
2. Standardize to single import path: `from app.core.database import ...`
3. Remove circular imports (use TYPE_CHECKING for type hints)
4. Verify sys.path doesn't contain duplicate entries

### Singleflight Pool Creation - Race Condition Fix (2026-01-04)

**Symptom:**
- Multiple "Database connection pool created successfully" logs within single container lifetime
- Different pool_id values despite same PID=1, RestartCount=0
- Occurs during startup + concurrent requests (health checks, tenant resolution)
- Race window: startup creates pool while concurrent requests also call ensure_pool()

**Root Cause:**
- **Before Fix:** ensure_pool() had bug where existing init task was awaited INSIDE lock
- Lock held while waiting → concurrent callers blocked from joining same task
- Result: Sequential callers each created own init task → multiple asyncpg.create_pool() calls
- Manifests as: pool_id=123, then pool_id=456, then pool_id=789 (all PID=1, generation=1)

**Fix Applied (2026-01-04 Singleflight):**

Changed ensure_pool() to implement true singleflight pattern:
1. **Fast path (line 262):** If pool exists, return immediately (zero lock contention, 99.9% case)
2. **Slow path (lines 269-311):**
   - Acquire lock to atomically check/create init task
   - If init task in progress: store reference, **release lock**, await outside lock
   - If no init task: create one, store reference, **release lock**, await outside lock
   - All concurrent callers await the SAME task → asyncpg.create_pool() called exactly once

**Key Change:**
```python
# BEFORE (BUG): Await inside lock - blocks other callers
async with lock:
    if _init_task is not None and not _init_task.done():
        await _init_task  # ❌ Lock held while waiting!
        return _pool

# AFTER (FIXED): Store reference, await outside lock - allows concurrent joins
async with lock:
    if _init_task is not None and not _init_task.done():
        task_to_await = _init_task  # ✅ Store reference
# Lock released here
await task_to_await  # ✅ Await outside lock
```

**Verification After Deployment:**

```bash
# 1. Check pool created only once per container lifetime
docker logs $(docker ps -q -f name=pms-backend) 2>&1 | grep "pool created successfully" | wc -l
# Expected: 1 (unless background reconnect triggered after degraded mode)

# 2. Verify pool_id stays constant
docker logs $(docker ps -q -f name=pms-backend) 2>&1 | grep "pool created successfully" | grep -o "pool_id=[0-9]*" | sort -u
# Expected: Single pool_id value

# 3. Check for singleflight log messages
docker logs $(docker ps -q -f name=pms-backend) 2>&1 | grep "joining existing task (singleflight)"
# If present: Confirms concurrent callers joined existing init task (correct behavior)

# 4. Check generation counter
docker logs $(docker ps -q -f name=pms-backend) 2>&1 | grep "pool created successfully" | grep -o "generation=[0-9]*"
# Expected: generation=1 (first pool creation after startup)
```

**Expected Behavior:**
- Startup calls ensure_pool() → creates init task, starts pool creation
- Concurrent health check calls ensure_pool() → sees init task, awaits same task
- Concurrent tenant resolution calls ensure_pool() → sees init task, awaits same task
- Result: Single "pool created successfully" log, single pool_id, all callers get same pool

**Regression Test:**
- Unit tests added: `tests/unit/test_database_singleflight.py`
- Test spawns 20 concurrent ensure_pool() calls, asserts asyncpg.create_pool() called exactly once
- Test simulates realistic scenario: startup + health check + tenant resolution concurrently

### Multiple pool_id Within One Runtime - Detection & Debug (2026-01-04)

**Symptom:**
- Same container (RestartCount=0, PID=1) shows multiple "pool created successfully" logs
- Different pool_id values within single runtime (no container restart)
- Server startup sequence appears twice in logs:
  - "Started server process [1]"
  - "Waiting for application startup"
  - "Application startup complete"
  - (repeats above sequence again)

**Root Cause Analysis:**

Multiple pool creations within same runtime can be caused by:
1. **Application startup running twice** (reload, worker restart, double-init)
2. **Concurrent requests racing** during startup (health checks, tenant resolution)
3. **Background reconnect** after degraded mode (expected, generation increments)

**Detection Commands:**

```bash
# 1. Check container hasn't restarted
docker inspect $(docker ps -q -f name=pms-backend) --format '{{.Name}}: Started={{.State.StartedAt}} RestartCount={{.RestartCount}}'
# Expected: RestartCount=0 (no restarts)
# If symptom persists with RestartCount=0: multiple pools in same runtime confirmed

# 2. Count pool creations in current runtime
docker logs $(docker ps -q -f name=pms-backend) 2>&1 | grep "pool created successfully" | wc -l
# Expected: 1 (unless background reconnect triggered)
# Bad: 2+ within same runtime

# 3. List unique pool_id values
docker logs $(docker ps -q -f name=pms-backend) 2>&1 | grep "pool created successfully" | grep -o "pool_id=[0-9]*" | sort -u
# Expected: Single pool_id
# Bad: Multiple different pool_id values

# 4. Check generation counter progression
docker logs $(docker ps -q -f name=pms-backend) 2>&1 | grep "pool created successfully" | grep -o "generation=[0-9]*"
# Expected: generation=1 (or 1,2 if background reconnect)
# Bad: Multiple generation=1 entries (indicates duplicate init, not progression)

# 5. Look for startup sequence duplication
docker logs $(docker ps -q -f name=pms-backend) 2>&1 | grep -c "Started server process"
# Expected: 1
# Bad: 2+ (indicates application lifecycle running twice)
```

**Debug Mode (DB_POOL_DEBUG=true):**

Enable detailed pool initialization logging to diagnose which code path triggers duplicate creation:

```bash
# Set via environment variable
docker exec pms-backend env | grep DB_POOL_DEBUG
# Or restart with DB_POOL_DEBUG=true

# Check debug logs after enabling
docker logs $(docker ps -q -f name=pms-backend) 2>&1 | grep "\[DB_POOL_DEBUG\]"
# Shows: entry point, PID, generation, reason, callsite (file:line/function)
```

**Debug Log Example:**
```
[DB_POOL_DEBUG] ensure_pool entry: PID=1, gen=0, pool_exists=False, callsite=main.py:77/lifespan
[DB_POOL_DEBUG] Creating pool init task (PID=1, reason=no_pool, callsite=main.py:77/lifespan)
Database connection pool created successfully (PID=1, host=db.supabase.co, generation=1, ...)
[DB_POOL_DEBUG] ensure_pool entry: PID=1, gen=1, pool_exists=True, callsite=health.py:15/health_check
# Fast path returns immediately (no duplicate creation)
```

**Mitigation Applied (2026-01-04):**
- Singleflight pattern ensures only ONE pool creation per runtime
- DB_POOL_DEBUG flag for detailed diagnostics (silent by default)
- Defensive check: warns + closes old pool if duplicate detected (shouldn't happen)

**Expected Behavior:**
- Within single runtime (RestartCount=0): exactly 1 pool creation
- pool_id stays constant throughout runtime
- generation=1 for first pool (or increments on reconnect after degraded mode)
- DB_POOL_DEBUG shows fast path returns after first init

### External Stop-Start Causing Duplicate Startup (2026-01-04)

**Symptom:**
- Container logs show duplicate startup signatures:
  - "Started server process [1]" appears twice
  - "Application startup complete" appears twice
  - CancelledError between the two startup sequences
- Multiple "pool created successfully" logs with different pool_id values
- RestartCount remains 0 (no increment)
- Container appears to have been stopped and started without a "restart"

**How to Prove It's External Stop-Start (Not In-Process Issue):**

```bash
# 1. Check container state for FinishedAt timestamp
docker inspect $(docker ps -q -f name=pms-backend) --format '{{.State.FinishedAt}}'
# If non-zero: container was stopped at some point
# Compare with StartedAt to see if stop happened during current runtime

# 2. Compare container StartedAt vs log timestamps
docker inspect $(docker ps -q -f name=pms-backend) --format 'StartedAt={{.State.StartedAt}}'
docker logs $(docker ps -q -f name=pms-backend) 2>&1 | grep "Started server process" | head -2
# If first "Started server process" timestamp is BEFORE StartedAt: old log from previous run
# If both timestamps are AFTER StartedAt: duplicate startup in current run (different issue)

# 3. Check container state for ExitCode
docker inspect $(docker ps -q -f name=pms-backend) --format 'ExitCode={{.State.ExitCode}} RestartCount={{.RestartCount}}'
# ExitCode=0 + RestartCount=0 suggests clean stop (not crash/restart)

# 4. Check daemon journal for manual stop events
journalctl -u docker.service --since "1 hour ago" | grep "container.*stop"
journalctl -u docker.service --since "1 hour ago" | grep "hasBeenManuallyStopped=true"
# Look for stop events matching container ID at the same time as FinishedAt

# 5. Search host for network connectivity automation
find /usr/local/bin /etc/systemd/system -name "*network*" -o -name "*ensure*" 2>/dev/null
systemctl list-timers --all | grep -i network
systemctl list-timers --all | grep -i ensure
# Look for timers or services that might restart containers

# 6. Inspect the automation script for restart behavior
cat /usr/local/bin/pms-ensure-*-network.sh  # or similar
# Check if script contains "docker restart" command
```

**Root Cause:**
Host-level automation designed to "ensure network connectivity" was:
- Running on a timer (e.g., every 30 seconds)
- Executing `docker network connect <network> <container>`
- **Also executing `docker restart <container>`** (unnecessary and harmful)

This caused the container to stop cleanly (ExitCode=0), then start again, without incrementing RestartCount (because it's a manual stop-start, not an automatic restart).

**Fix Applied (on Host):**
Updated the host network connectivity script to:
1. **NEVER restart the container**
2. Only attach the network if it's missing (idempotent `docker network connect`)
3. Only operate on running containers (check container state first)
4. Optionally adjust timer frequency if 30s is too aggressive

**Example Fixed Script Pattern:**
```bash
#!/bin/bash
# Good: Idempotent network attach WITHOUT restart

CONTAINER_NAME="pms-backend"
NETWORK_NAME="your-network"

# Only proceed if container is running
if [ "$(docker inspect -f '{{.State.Running}}' $CONTAINER_NAME 2>/dev/null)" != "true" ]; then
    exit 0
fi

# Idempotent network connect (fails silently if already connected)
docker network connect $NETWORK_NAME $CONTAINER_NAME 2>/dev/null || true

# NO docker restart command!
```

**Verification Steps After Fix:**

```bash
# 1. Verify StartedAt doesn't change over time
docker inspect $(docker ps -q -f name=pms-backend) --format 'StartedAt={{.State.StartedAt}}'
# Wait 5 minutes, check again - should be unchanged

# 2. Monitor daemon journal for stop/start events
journalctl -u docker.service -f | grep "container.*pms-backend"
# Should see network connect events, but NO stop/start events

# 3. Check for new startup signatures in logs
docker logs --since 5m $(docker ps -q -f name=pms-backend) 2>&1 | grep "Started server process"
# Should be empty (no new startup sequences)

# 4. Verify timer still runs but doesn't restart
systemctl list-timers --all | grep ensure
journalctl -u your-ensure-timer.service --since "5m ago"
# Timer should run, but logs should show only network operations, no restarts

# 5. Verify single pool_id persists
docker logs $(docker ps -q -f name=pms-backend) 2>&1 | grep "pool created successfully" | tail -1
# Wait 5 minutes, check again - should show same pool_id
```

**Expected Behavior After Fix:**
- Container StartedAt timestamp remains constant
- No stop/start events in daemon journal
- No new "Started server process" signatures in logs
- Single pool_id throughout entire runtime
- Network connectivity maintained without container disruption

### Duplicate Startup Signatures + Multiple pool_id: Distinguish External Causes (2026-01-04)

**Overview:**
When you observe duplicate "Started server process [1]" + "Application startup complete" logs with CancelledError between them, and multiple different pool_id values while RestartCount=0, this is NOT an in-process application bug (e.g., uvicorn reload or multiple workers). It is caused by external container lifecycle events.

**Proven Evidence (Not In-Process):**
```bash
# CONTAINER: Check process tree shows single process
docker exec pms-backend ps aux
# Expected: Single /opt/venv/bin/python ... uvicorn app.main:app --host 0.0.0.0 --port 8000

# CONTAINER: Verify no reload or workers flags
docker exec pms-backend cat /proc/1/cmdline | tr '\0' ' '
# Expected: NO --reload or --workers flags
```

**Two Distinct External Causes:**

---

#### **CASE A: Container Replace (Deploy/Recreate)**

**What Happens:**
- Deployment manager (orchestrator) creates NEW container with updated image tag
- Old container stops/removed, new container starts
- Container ID changes (new container created)
- RestartCount may stay 0 (new container, not a daemon restart loop)
- Each container has its own startup sequence + pool_id

**How to Prove (HOST):**

```bash
# 1. Check for container ID change
docker ps -a --no-trunc --filter name=pms-backend | head -5
# If multiple containers with different IDs: replacement occurred

# 2. Check container creation/start times
docker inspect pms-backend --format 'Id={{.Id}}
Created={{.Created}}
StartedAt={{.State.StartedAt}}
FinishedAt={{.State.FinishedAt}}
RestartCount={{.RestartCount}}
ExitCode={{.State.ExitCode}}'
# Created timestamp shows when container was created (replacement creates new)

# 3. Check rendered deployment files modification time
# (Use your deployment manager's rendered directory path)
stat $DEPLOY_APP_DIR/docker-compose.yaml
stat $DEPLOY_APP_DIR/.env
# If mtimes align with container Created time: deployment recreated container

# 4. Check image tag change in rendered compose
diff <previous-compose-backup> $DEPLOY_APP_DIR/docker-compose.yaml
# Look for image: tag changes (e.g., old-hash -> new-hash)

# 5. Check logs show TWO separate container lifecycles
docker logs --timestamps pms-backend 2>&1 | grep -E 'Started server process|Application startup complete|pool created' | head -20
# First startup from old container (before Created time)
# Second startup from new container (after Created time)
```

**Expected Behavior (Case A):**
- Two container IDs exist (old removed, new running)
- Two startup sequences in logs (one per container)
- Each container has different pool_id (expected, separate processes)
- RestartCount=0 in new container (not a restart, it's a new container)

**Safe Pattern (Case A):**
- This is normal deployment behavior (recreate strategy)
- Use SOURCE_COMMIT or image tag + compose/env mtime to correlate deployments
- Monitor for exactly ONE startup sequence in new container
- Verify old container is stopped/removed (not running simultaneously)

---

#### **CASE B: Host Automation Restart (Network Connectivity)**

**What Happens:**
- Host-level script/timer monitors network connectivity
- Script executes `docker network connect <network> <container>` (correct, idempotent)
- Script ALSO executes `docker restart <container>` (incorrect, harmful)
- Same container ID, but stop/start cycle produces duplicate startups
- RestartCount stays 0 (manual stop/start, not daemon restart)

**How to Prove (HOST):**

```bash
# 1. Check container ID stays SAME across time
docker ps -a --no-trunc --filter name=pms-backend
# Same container ID but FinishedAt set: stop/start within same container

# 2. Check container state shows stop event
docker inspect pms-backend --format 'FinishedAt={{.State.FinishedAt}}
ExitCode={{.State.ExitCode}}
RestartCount={{.RestartCount}}'
# FinishedAt non-zero + ExitCode=0 + RestartCount=0: manual stop (not crash)

# 3. Check daemon journal for manual stop events
journalctl -u docker.service --since "1 hour ago" | grep -E "container.*stop|hasBeenManuallyStopped=true"
# Look for stop events matching container ID

# 4. Search host for network connectivity automation
systemctl list-timers --all | grep -iE 'network|ensure|connectivity'
find /usr/local/bin /etc/systemd/system -type f -name "*network*" -o -name "*ensure*" 2>/dev/null

# 5. Audit automation script for restart behavior
systemctl cat <timer-service-name>
cat /usr/local/bin/<ensure-network-script>
# Look for "docker restart" command (HARMFUL)

# 6. Monitor for stop/start events during timer execution
journalctl -u docker.service -f | grep "container.*pms-backend"
# While timer runs, watch for stop/start events
```

**Expected Behavior Before Fix (Case B):**
- Same container ID persists
- FinishedAt timestamp appears (container was stopped)
- Duplicate startup signatures every timer interval (e.g., 30s)
- Multiple pool_id values within same container name

**Safe Fix Pattern (Case B):**
```bash
#!/bin/bash
# CORRECT: Idempotent network attach WITHOUT restart

CONTAINER_NAME="pms-backend"
NETWORK_NAME="database-network"  # Generic placeholder

# Only proceed if container is running
if [ "$(docker inspect -f '{{.State.Running}}' $CONTAINER_NAME 2>/dev/null)" != "true" ]; then
    exit 0
fi

# Idempotent network connect (fails silently if already connected)
docker network connect $NETWORK_NAME $CONTAINER_NAME 2>/dev/null || true

# NO docker restart command!
# NO docker stop/start commands!
```

**Verification After Fix (Case B):**
```bash
# 1. Verify StartedAt doesn't change
docker inspect pms-backend --format 'StartedAt={{.State.StartedAt}}'
# Wait through several timer cycles, check again - should be unchanged

# 2. Monitor daemon journal shows NO stop/start
journalctl -u docker.service -f | grep "container.*pms-backend"
# Should see network connect events, but NO stop/start

# 3. Check logs show NO new startup signatures
docker logs --since 10m pms-backend 2>&1 | grep "Started server process"
# Should be empty (no new startups)

# 4. Verify single pool_id persists
docker logs pms-backend 2>&1 | grep "pool created successfully" | tail -5
# Same pool_id across all recent logs
```

---

**Decision Tree: Which Case?**

| Observation | Indicates |
|-------------|-----------|
| Container ID changed between startups | **CASE A** (Deploy Replace) |
| Container ID same, FinishedAt set | **CASE B** (Host Automation) |
| Rendered compose/env mtime matches startup | **CASE A** (Deploy Replace) |
| Image tag changed in compose | **CASE A** (Deploy Replace) |
| Daemon journal shows "manually stopped" | **CASE B** (Host Automation) |
| Host timer service found with restart | **CASE B** (Host Automation) |
| Two container IDs in `docker ps -a` | **CASE A** (Deploy Replace) |
| StartedAt changes regularly (e.g., every 30s) | **CASE B** (Host Automation) |

**Verification Checklist:**

After Host Automation Fix (Case B):
- [ ] Container StartedAt stable across timer triggers
- [ ] No stop/start events in daemon journal
- [ ] No new startup signatures in logs
- [ ] Single pool_id throughout runtime
- [ ] Network connectivity maintained without disruption

After Deploy Replace (Case A):
- [ ] Exactly one startup sequence in new container
- [ ] Old container stopped/removed (not running)
- [ ] Process tree shows single uvicorn process
- [ ] No --reload or --workers flags in /proc/1/cmdline
- [ ] New container ID correlates with compose/env mtime

### Verify

```bash
# SSH to host server
ssh root@your-host

# Check container networks
docker inspect $(docker ps -q -f name=pms) | grep -A 10 Networks

# Expected: both "coolify" and "bccg4gs4o4kgsowocw08wkw4" networks

# Test DNS resolution inside container
docker exec $(docker ps -q -f name=pms) getent hosts supabase-db

# Expected: IP address (e.g., 172.20.0.2)
# If empty/error: DNS resolution failed
```

### Fix

**Option 1: Via Coolify Dashboard (Recommended)**

1. Open Coolify dashboard
2. Navigate to: **Applications > PMS-Webapp > Networks**
3. Ensure both networks are selected:
   - `coolify` (default)
   - `bccg4gs4o4kgsowocw08wkw4` (Supabase network)
4. Click **Save** and **Restart** application

**Option 2: Manual Docker Network Attachment**

```bash
# Attach container to Supabase network
docker network connect bccg4gs4o4kgsowocw08wkw4 $(docker ps -q -f name=pms)

# Verify
docker exec $(docker ps -q -f name=pms) getent hosts supabase-db
# Should now return IP address

# Restart container to recreate pool
docker restart $(docker ps -q -f name=pms)
```

**Option 3: Update DATABASE_URL to use public DNS**

If network attachment fails, use Supabase public URL:

```bash
# In Coolify: Applications > PMS-Webapp > Environment Variables
DATABASE_URL=postgresql://postgres:your-password@your-project.supabase.co:5432/postgres

# Note: This bypasses pgBouncer connection pooling (less efficient)
```

### Prevention

- Ensure Coolify deployment config includes both networks
- Add network check to CI/CD health checks
- Monitor `/health/ready` endpoint (should return 200 within 10s of deploy)

### Auto-Heal Supabase Network Attachment (Host Cron)

**Problem:** Coolify redeploys may drop the extra network attachment, causing DB DNS resolution failures and "Database temporarily unavailable" errors even when the network is configured in Coolify UI.

**Solution:** Use a host-side cron job to automatically reconnect containers to the Supabase network if they become detached.

---

#### The Auto-Heal Script

**Location (Production):** `/usr/local/bin/pms_ensure_supabase_net.sh`

**Source:** `backend/scripts/ops/pms_ensure_supabase_net.sh` (in repo)

**What It Does:**

The script performs automatic network attachment healing for PMS containers:

1. **Checks target containers** (`pms-backend`, `pms-worker-v2`) every 2 minutes
2. **Inspects Docker networks** for each container
3. **Detects missing attachment** to Supabase network (`bccg4gs4o4kgsowocw08wkw4`)
4. **Auto-attaches + restarts** if detached (logs action to `/var/log/pms_ensure_supabase_net.log`)
5. **Silent no-op** if already attached (logs daily "OK" heartbeat to `/var/log/pms_ensure_supabase_net.ok`)

**Behavior Summary:**
- **Intentionally quiet:** No output when everything is OK (except daily heartbeat)
- **Logs only when fixing:** Writes to `.log` when it reconnects a container
- **Daily heartbeat:** Writes single "OK" message to `.ok` file once per day (confirms cron is alive)
- **Restart required:** Container must restart after network attachment for DNS to work

**Key Features:**
- Idempotent (safe to run every 2 minutes)
- No-op if containers don't exist or are stopped
- Automatic restart after network reattachment
- Timestamped logs (UTC)

---

#### Cron Schedule

**Location:** `/etc/cron.d/pms_ensure_supabase_net` (production)

**Schedule:** Every 2 minutes

**Content:**
```bash
# PMS Supabase Network Auto-Heal
# Runs every 2 minutes to ensure pms-backend and pms-worker-v2
# are always attached to the Supabase network (bccg4gs4o4kgsowocw08wkw4)
*/2 * * * * root /usr/local/bin/pms_ensure_supabase_net.sh >> /var/log/pms_ensure_supabase_net.log 2>&1
```

**Alternative (Root crontab):**
```bash
# Edit root crontab
crontab -e

# Add this line:
*/2 * * * * /usr/local/bin/pms_ensure_supabase_net.sh >> /var/log/pms_ensure_supabase_net.log 2>&1
```

---

#### Log Files

**Primary Log:** `/var/log/pms_ensure_supabase_net.log`

Contains timestamped entries when script takes action (attaches network, restarts container):

```
[2025-12-29 14:32:01 UTC] ====== PMS Supabase Network Auto-Attach Check ======
[2025-12-29 14:32:01 UTC] FIXING: pms-backend not attached to bccg4gs4o4kgsowocw08wkw4
[2025-12-29 14:32:01 UTC] ACTION: Connecting pms-backend to bccg4gs4o4kgsowocw08wkw4...
[2025-12-29 14:32:02 UTC] SUCCESS: Connected pms-backend to bccg4gs4o4kgsowocw08wkw4
[2025-12-29 14:32:02 UTC] ACTION: Restarting pms-backend...
[2025-12-29 14:32:05 UTC] SUCCESS: Restarted pms-backend
[2025-12-29 14:32:05 UTC] OK: pms-worker-v2 already attached to bccg4gs4o4kgsowocw08wkw4
[2025-12-29 14:32:05 UTC] ====== Check Complete ======
```

**Heartbeat Log:** `/var/log/pms_ensure_supabase_net.ok`

Contains daily heartbeat (single "OK" message per day, confirms cron is running):

```
[2025-12-29 00:00:01 UTC] OK: Daily heartbeat - pms-backend and pms-worker-v2 healthy
[2025-12-30 00:00:01 UTC] OK: Daily heartbeat - pms-backend and pms-worker-v2 healthy
```

**Why separate .ok file?**
- Keeps primary log clean (only shows actual fixes)
- Easy grep for "all is well" vs "something was fixed"
- Prevents log bloat (1 heartbeat/day vs 720 checks/day)

---

#### Verification Commands

**Check cron is configured:**
```bash
# Option 1: Check /etc/cron.d/
ls -la /etc/cron.d/pms_ensure_supabase_net

# Option 2: Check root crontab
crontab -l | grep pms_ensure_supabase_net
```

**Check cron daemon is running:**
```bash
# SystemD (most modern distros)
systemctl status cron

# OR
systemctl status crond

# Expected: "active (running)"
```

**Check script exists and is executable:**
```bash
ls -la /usr/local/bin/pms_ensure_supabase_net.sh

# Expected: -rwxr-xr-x (executable)
```

**Check recent logs:**
```bash
# Last 20 lines of action log (shows fixes)
tail -20 /var/log/pms_ensure_supabase_net.log

# Last heartbeat (daily OK)
tail -5 /var/log/pms_ensure_supabase_net.ok

# Live tail (watch for fixes in real-time)
tail -f /var/log/pms_ensure_supabase_net.log
```

**Verify network attachment:**
```bash
# Check pms-backend networks
docker inspect pms-backend --format '{{range $k, $v := .NetworkSettings.Networks}}{{$k}} {{end}}'

# Expected: coolify bccg4gs4o4kgsowocw08wkw4

# Check pms-worker-v2 networks
docker inspect pms-worker-v2 --format '{{range $k, $v := .NetworkSettings.Networks}}{{$k}} {{end}}'

# Expected: coolify bccg4gs4o4kgsowocw08wkw4
```

**Manual test (trigger script immediately):**
```bash
# Run script manually (see output)
/usr/local/bin/pms_ensure_supabase_net.sh

# Expected output (if already attached):
# [2025-12-29 14:45:01 UTC] ====== PMS Supabase Network Auto-Attach Check ======
# [2025-12-29 14:45:01 UTC] OK: pms-backend already attached to bccg4gs4o4kgsowocw08wkw4
# [2025-12-29 14:45:01 UTC] OK: pms-worker-v2 already attached to bccg4gs4o4kgsowocw08wkw4
# [2025-12-29 14:45:01 UTC] ====== Check Complete ======
```

---

#### Setup Instructions (HOST-SERVER-TERMINAL)

**Step 1: Install script to production path**

```bash
# SSH to host server
ssh root@your-server.com

# Option A: Copy from cloned repo
cd /root
git clone https://github.com/Kolibri-Visions/PMS-Webapp.git
cp PMS-Webapp/backend/scripts/ops/pms_ensure_supabase_net.sh /usr/local/bin/
chmod +x /usr/local/bin/pms_ensure_supabase_net.sh

# Option B: Download directly (if repo is public)
curl -o /usr/local/bin/pms_ensure_supabase_net.sh \
  https://raw.githubusercontent.com/Kolibri-Visions/PMS-Webapp/main/backend/scripts/ops/pms_ensure_supabase_net.sh
chmod +x /usr/local/bin/pms_ensure_supabase_net.sh
```

**Step 2: Create cron.d entry (Recommended)**

```bash
# Create /etc/cron.d/pms_ensure_supabase_net
cat > /etc/cron.d/pms_ensure_supabase_net <<'EOF'
# PMS Supabase Network Auto-Heal
# Runs every 2 minutes to ensure containers stay attached to Supabase network
*/2 * * * * root /usr/local/bin/pms_ensure_supabase_net.sh >> /var/log/pms_ensure_supabase_net.log 2>&1
EOF

# Set correct permissions
chmod 644 /etc/cron.d/pms_ensure_supabase_net

# Reload cron
systemctl reload cron || systemctl reload crond
```

**Step 3: Verify cron is active**

```bash
# Check cron daemon
systemctl status cron

# Wait 2 minutes, then check logs
tail -f /var/log/pms_ensure_supabase_net.log

# Expected (if all OK):
# [2025-12-29 14:50:01 UTC] ====== PMS Supabase Network Auto-Attach Check ======
# [2025-12-29 14:50:01 UTC] OK: pms-backend already attached to bccg4gs4o4kgsowocw08wkw4
# [2025-12-29 14:50:01 UTC] OK: pms-worker-v2 already attached to bccg4gs4o4kgsowocw08wkw4
# [2025-12-29 14:50:01 UTC] ====== Check Complete ======
```

**Step 4: Verify network attachment persists**

```bash
# Check backend networks
docker inspect pms-backend | grep -A 10 '"Networks"'

# Check worker networks
docker inspect pms-worker-v2 | grep -A 10 '"Networks"'

# Expected: Both "coolify" and "bccg4gs4o4kgsowocw08wkw4" networks
```

---

#### Customization

**Find your Supabase network ID:**

```bash
# Method 1: List all networks
docker network ls | grep supabase

# Method 2: Inspect Supabase DB container
docker inspect supabase-db | grep -A 5 '"Networks"'

# Update script with your network ID:
# Edit SUPABASE_NETWORK="your-network-id" in script
```

**Add more containers to monitor:**

```bash
# Edit /usr/local/bin/pms_ensure_supabase_net.sh
# Change CONTAINERS array:
CONTAINERS=("pms-backend" "pms-worker-v2" "your-other-service")
```

---

#### Troubleshooting

**Script not running:**
```bash
# Check cron daemon
systemctl status cron || systemctl status crond

# Check cron.d file permissions (must be 644)
ls -la /etc/cron.d/pms_ensure_supabase_net

# Check script permissions (must be executable)
ls -la /usr/local/bin/pms_ensure_supabase_net.sh

# Check cron logs (SystemD journal)
journalctl -u cron -f
```

**No log output:**
```bash
# Ensure log directory is writable
touch /var/log/pms_ensure_supabase_net.log
chmod 644 /var/log/pms_ensure_supabase_net.log

# Run script manually to see output
/usr/local/bin/pms_ensure_supabase_net.sh
```

**Container keeps getting detached:**
```bash
# Verify Coolify network config
# Coolify Dashboard → pms-backend → Settings → Networks
# Ensure "bccg4gs4o4kgsowocw08wkw4" is selected

# Check if Coolify is fighting with cron (check timestamps)
tail -f /var/log/pms_ensure_supabase_net.log
# If you see "FIXING" messages every 2 minutes → Coolify may be removing network
```

---

#### Security Note

**Caution:** This script requires root access on the host server and uses the Docker socket. This is acceptable for single-server ops environments but should be carefully reviewed for multi-tenant or high-security deployments.

**Alternative approaches for more secure environments:**
- Use Coolify API to trigger redeploy with network config
- Use Docker Swarm or Kubernetes for network management
- Implement network checks in application health probes
- Use Docker events API to trigger network reattachment (reactive instead of polling)

---

### Optional: Cleanup Stale Sync Logs

**Problem:** Sync logs may remain in `running` status after previous outages or worker crashes.

**Solution:** Run this SQL snippet to mark stale logs as `failed`:

```sql
-- Mark sync logs stuck in "running" status for > 1 hour as failed
UPDATE channel_sync_logs
SET
    status = 'failed',
    error = 'Task timed out or worker crashed (auto-cleaned)',
    updated_at = NOW()
WHERE
    status = 'running'
    AND created_at < NOW() - INTERVAL '1 hour';

-- Check affected rows
SELECT COUNT(*)
FROM channel_sync_logs
WHERE status = 'failed' AND error LIKE '%auto-cleaned%';
```

**When to run:**
- After resolving worker outages
- Before investigating sync issues (to clear old noise)
- As part of regular maintenance (monthly)

**Caution:** Review logs before cleaning to ensure you're not cancelling legitimate long-running tasks.

---

## Deploy Gating (Docs-Only Change Detection)

### Overview

**Problem:** Docs-only commits (e.g., `*.md`, `docs/**`) currently trigger full container rebuild + redeploy, causing unnecessary downtime and duplicate startup signatures.

**Solution:** Use `backend/scripts/ops/deploy_should_run.sh` to classify changes as docs-only vs code/config, enabling CI/CD to skip deploys for non-functional changes.

**Ticket:** See `backend/docs/ops/tickets/2026-01-04_deploy_gating_docs_only.md`

---

### How It Works

```bash
# Script classifies git changes
./scripts/ops/deploy_should_run.sh HEAD~1..HEAD

# Exit codes:
#   0 = Needs deploy (code/config changes detected)
#   1 = Skip deploy (docs-only changes detected)
#   2 = Error (invalid git range, not a git repo)
```

**Classification Rules:**

**Docs-only paths** (skip deploy):
- `*.md` (Markdown files)
- `docs/**` (Documentation directories)
- `*.txt` (Text files, EXCEPT `requirements.txt`)
- `.gitignore`, `LICENSE`
- `scripts/ops/deploy_should_run.sh` (Deploy classifier itself - tooling)
- `scripts/ops/deploy_gate.sh` (Deploy wrapper - tooling)

**Always deploy paths** (proceed with deploy):
- `app/**` (Python application code)
- `requirements.txt` (Python dependencies)
- `Dockerfile`, `docker-compose*.yml` (Container config)
- `alembic/**` (Database migrations)
- `tests/**` (Test code)
- `.env*` (Environment config)
- `scripts/**` (Operational scripts, EXCEPT `deploy_should_run.sh` and `deploy_gate.sh`)
- Any other files not in docs-only category

**Note:** Changes to `ops/deploy_should_run.sh` and `ops/deploy_gate.sh` are treated as tooling and do not require app deploy.

---

### CI/CD Integration Examples

#### CI Pipeline Example 1

```yaml
jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Need full history for git diff

      - name: Check if deploy needed
        id: deploy_gate
        run: |
          if ./backend/scripts/ops/deploy_should_run.sh ${{ github.event.before }}..${{ github.sha }}; then
            echo "should_deploy=true" >> $GITHUB_OUTPUT
          else
            echo "should_deploy=false" >> $GITHUB_OUTPUT
          fi

      - name: Build and deploy
        if: steps.deploy_gate.outputs.should_deploy == 'true'
        run: |
          docker build -t myapp backend/
          docker push myapp
```

#### CI Pipeline Example 2

```yaml
deploy:
  stage: deploy
  script:
    - ./backend/scripts/ops/deploy_should_run.sh $CI_COMMIT_BEFORE_SHA..$CI_COMMIT_SHA
    - docker build -t myapp backend/
    - docker push myapp
  rules:
    - if: '$CI_COMMIT_BRANCH == "main"'
  only:
    - main
```

---

### Manual Testing

```bash
# Test with last commit
cd backend
./scripts/ops/deploy_should_run.sh HEAD~1..HEAD

# Test with specific commit range
./scripts/ops/deploy_should_run.sh abc123..def456

# Example output (docs-only):
# Changed files in range HEAD~1..HEAD:
#   - docs/ops/runbook.md
#
#   [DOCS] docs/ops/runbook.md
#
# ✅ Classification: DOCS-ONLY
# Action: Skip deploy (no container rebuild needed)
# Exit code: 1

# Example output (needs deploy):
# Changed files in range HEAD~1..HEAD:
#   - app/core/database.py
#   - docs/ops/runbook.md
#
#   [DEPLOY] app/core/database.py (Application code)
#   [DOCS] docs/ops/runbook.md
#
# 🚀 Classification: NEEDS DEPLOY
# Action: Proceed with deploy (code/config changes detected)
# Exit code: 0
```

---

### Benefits

1. **Reduced downtime**: Documentation changes no longer trigger container replacement
2. **Fewer duplicate startup signatures**: Avoid Case A (container replace) for docs commits
3. **Faster feedback**: Docs contributors see changes merged without waiting for deploy
4. **Cost savings**: Skip unnecessary image builds/pushes/pulls

---

### Deployment Runner Wrapper (Enforcement)

**Execution Location:** HOST-SERVER-TERMINAL (deployment automation context)

**Script:** `backend/scripts/ops/deploy_gate.sh`

**Purpose:** Vendor-neutral wrapper for deployment runners to gate container rebuild/recreate operations based on change classification.

**Usage:**
```bash
# Explicit commit range
./scripts/ops/deploy_gate.sh OLD_COMMIT NEW_COMMIT

# Auto-infer from SOURCE_COMMIT env var
SOURCE_COMMIT=abc123 ./scripts/ops/deploy_gate.sh

# Auto-infer from HEAD~1..HEAD (last commit)
./scripts/ops/deploy_gate.sh
```

**Output Format (Machine-Readable):**
```
DEPLOY=1 reason="code/config changes detected" old=abc123 new=def456
DEPLOY=0 reason="docs-only changes" old=abc123 new=def456
DEPLOY=1 reason="error: ... (fail-open mode)" old=unknown new=unknown
```

**Exit Codes:**
- `0` = Proceed with deploy (code/config changes OR fail-open error)
- `1` = Skip deploy (docs-only changes OR fail-closed error)
- `2` = Critical error (git unavailable, not a repo)

**Fail-Safe Behavior:**

Use `DEPLOY_GATE_FAIL_MODE` environment variable to control error handling:

- **`open`** (default, recommended): On error, proceed with deploy (exit 0)
  - Rationale: Transient issues shouldn't block legitimate deployments
  - Use case: Production environments where availability > optimization

- **`closed`** (strict): On error, skip deploy (exit 1)
  - Rationale: Maximum protection against unnecessary deployments
  - Use case: Staging environments, cost-sensitive workloads

**Integration Pattern:**

```bash
# Deployment runner pseudocode
if ./backend/scripts/ops/deploy_gate.sh "$OLD" "$NEW"; then
  echo "Proceeding with container rebuild"
  # Build image
  # Push to registry
  # Replace/recreate container
else
  echo "Skipping deployment (docs-only changes)"
  # No container operations
fi
```

**Auto-Inference Logic:**

1. If `OLD_COMMIT` and `NEW_COMMIT` arguments provided: Use them directly
2. If `SOURCE_COMMIT` env var is set: Use `SOURCE_COMMIT..HEAD`
3. Otherwise: Use `HEAD~1..HEAD` (last commit only)

**Example Scenarios:**

```bash
# Scenario 1: Explicit range (deployment platform provides commit range)
./scripts/ops/deploy_gate.sh f936bda 12cbe9f
# Output: DEPLOY=1 reason="code/config changes detected" old=f936bda new=12cbe9f
# Exit: 0 (proceed)

# Scenario 2: SOURCE_COMMIT env var (platform sets this)
SOURCE_COMMIT=f936bda ./scripts/ops/deploy_gate.sh
# Output: DEPLOY=0 reason="docs-only changes" old=f936bda new=HEAD
# Exit: 1 (skip)

# Scenario 3: Fail-closed mode (strict)
DEPLOY_GATE_FAIL_MODE=closed ./scripts/ops/deploy_gate.sh invalid_ref HEAD
# Output: DEPLOY=0 reason="error: old commit not found: invalid_ref (fail-closed mode)" old=unknown new=unknown
# Exit: 1 (skip on error)

# Scenario 4: Fail-open mode (default)
./scripts/ops/deploy_gate.sh invalid_ref HEAD
# Output: DEPLOY=1 reason="error: old commit not found: invalid_ref (fail-open mode)" old=unknown new=unknown
# Exit: 0 (proceed on error)
```

**Safety Notes:**

- This script is **read-only** (only classifies changes)
- **NEVER** triggers container operations directly
- Safe to run multiple times (idempotent)
- No side effects on repository or containers

**Recommended Default:** Fail-open mode (`DEPLOY_GATE_FAIL_MODE=open`) to avoid blocking deployments due to transient errors.

---

### Phase-1 vs Phase-2

**Phase-1** (Current):
- Helper script exists (`deploy_should_run.sh`)
- Enforcement wrapper exists (`deploy_gate.sh`)
- Documentation provided
- CI/CD integration examples available
- No enforcement in actual pipelines yet (manual opt-in)

**Phase-2** (Future):
- Integrate script into actual CI/CD pipeline
- Add force-deploy override flag
- Monitor deployment frequency reduction
- Add metrics (deploy count, docs-only commit %)

---

## Network Attachment at Create-Time (Docker)

### Overview

**Problem:** PMS backend container currently lacks network connectivity at create-time, requiring a post-create restart by host automation to establish connectivity. This causes duplicate startup signatures (Case B).

**Solution:** Attach network at `docker run` time using `--network` flag (or equivalent in Docker Compose, Kubernetes, etc.), eliminating the need for post-create restarts.

**Ticket:** See `backend/docs/ops/tickets/2026-01-04_network_attach_create_time.md`

---

### Current vs Desired Behavior

**Current** (Network attached AFTER create):
```bash
docker run --name pms-backend ghcr.io/org/pms-backend:latest
  → Container starts (PID=1, generation=1)
  → No network connectivity
  → DB connection fails (DNS timeout)
  → App enters degraded mode
  → Host timer detects missing network
  → docker restart pms-backend  # ← Creates duplicate startup
  → Network now available
  → DB connection succeeds (new PID=1, generation=1 again)
```

**Desired** (Network attached AT create):
```bash
docker run --name pms-backend --network pms-network ghcr.io/org/pms-backend:latest
  → Container starts (PID=1, generation=1)
  → Network connectivity ALREADY available
  → DB connection succeeds immediately
  → App enters ready mode (no degraded mode)
  → Host timer becomes optional safety net (no restart)
```

---

### Implementation Examples

#### Docker CLI

**Before:**
```bash
docker run -d \
  --name pms-backend \
  --env-file .env \
  ghcr.io/org/pms-backend:latest
```

**After:**
```bash
docker run -d \
  --name pms-backend \
  --network pms-network \
  --env-file .env \
  ghcr.io/org/pms-backend:latest
```

#### Docker Compose

**Before:**
```yaml
services:
  backend:
    image: ghcr.io/org/pms-backend:latest
    env_file: .env
    # No networks section
```

**After:**
```yaml
services:
  backend:
    image: ghcr.io/org/pms-backend:latest
    env_file: .env
    networks:
      - pms-network

networks:
  pms-network:
    external: true  # Assumes network already exists
    # OR
    # driver: bridge  # Creates network if not exists
```

#### Kubernetes

Kubernetes automatically attaches Pod network at create-time (no action needed):

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pms-backend
spec:
  containers:
  - name: backend
    image: ghcr.io/org/pms-backend:latest
  # Network attachment is automatic
```

---

### Update Host Timer Script (Safety Net)

**Before** (restart-based):
```bash
# Host timer script (HARMFUL - causes duplicate startups)
if ! docker exec pms-backend ping -c1 database 2>/dev/null; then
  echo "No connectivity, restarting container"
  docker restart pms-backend  # ← Creates duplicate startup
fi
```

**After** (attach-only):
```bash
# Host timer script (SAFE - attach without restart)
NETWORK_ATTACHED=$(docker inspect pms-backend \
  --format '{{range $k,$v := .NetworkSettings.Networks}}{{$k}}{{end}}' \
  | grep -q pms-network && echo yes || echo no)

if [ "$NETWORK_ATTACHED" != "yes" ]; then
  echo "Network not attached, attaching now (no restart)"
  docker network connect pms-network pms-backend
else
  echo "Network already attached, no action needed"
fi
```

**Why no restart?** Docker supports live network attachment (`docker network connect`) without container restart. DNS resolution will work immediately after attachment.

---

### Benefits

1. **Single startup signature**: Eliminates Case B duplicate startups (host automation restart)
2. **Faster application readiness**: DB connection succeeds on first attempt (no degraded mode)
3. **Simpler operations**: Host timer becomes attach-only safety net, not primary mechanism
4. **Cleaner logs**: No false-positive "duplicate pool creation" alerts

---

### Verification

**Before fix** (duplicate startup):
```bash
# Check logs for multiple startups
docker logs pms-backend 2>&1 | grep -E "Started server process|pool created successfully"

# Expected (BEFORE):
# [Startup #1] Started server process [1]
# [Startup #1] Database connection pool created successfully ... pool_id=12345
# [Startup #1] Database connection pool initialization FAILED ... Degraded mode
# [Startup #2] Started server process [1]  ← Duplicate!
# [Startup #2] Database connection pool created successfully ... pool_id=67890
# [Startup #2] ✅ Database connection pool initialized
```

**After fix** (single startup):
```bash
# Check logs for single startup
docker logs pms-backend 2>&1 | grep -E "Started server process|pool created successfully"

# Expected (AFTER):
# [Startup #1] Started server process [1]
# [Startup #1] Database connection pool created successfully ... pool_id=12345
# [Startup #1] ✅ Database connection pool initialized
# [Startup #1] 🚀 PMS Backend API started successfully
```

---

### Phase-1 vs Phase-2

**Phase-1** (Current - Ticket Created):
- Ticket documents problem + solution
- Examples provided for Docker CLI, Compose, Kubernetes
- Host timer script update pattern documented
- No infrastructure changes yet

**Phase-2** (Future):
- Update actual deployment configs to include `--network` flag
- Patch host timer script to attach-only (no restart)
- Validate single startup signature in production logs
- Monitor reduction in duplicate startup events

---

## Token Validation (apikey Header)

### Symptom

- JWT token fetched successfully (200 from Supabase auth)
- API requests with `Authorization: Bearer <token>` return `401 Unauthorized`
- Logs show: `"Invalid JWT"` or `"Unauthorized"`

### Root Cause

**IMPORTANT: apikey header scope depends on which service you're calling:**

1. **Supabase Kong endpoints** (e.g., `https://sb-pms.../auth/v1/...`) require **two headers**:
   - `Authorization: Bearer <jwt>`
   - `apikey: <anon_key>`

2. **PMS Backend API** (e.g., `https://api.fewo.../api/v1/...`) requires **only**:
   - `Authorization: Bearer <jwt>`
   - **NO apikey header needed**

**Why does Supabase Kong require both?**
- `Authorization` header contains user's JWT (specific user identity)
- `apikey` header contains project's anon key (project identification for rate limiting/routing)

**Why doesn't PMS Backend need apikey?**
- PMS Backend validates JWT directly using the JWT_SECRET
- No Kong gateway in front of PMS Backend
- apikey is only needed for Supabase services

### Verify

```bash
# Test Supabase Kong endpoint (requires both headers)
# Example: Fetch current user profile
curl -X GET "https://sb-pms.kolibri-visions.de/auth/v1/user" \
  -H "Authorization: Bearer $TOKEN" \
  -H "apikey: $ANON_KEY"
# Returns: 200 OK (user profile)

# Test without apikey (FAILS on Kong)
curl -X GET "https://sb-pms.kolibri-visions.de/auth/v1/user" \
  -H "Authorization: Bearer $TOKEN"
# Returns: 401 Unauthorized

# Test PMS Backend API (only Authorization needed)
curl -X GET "https://api.fewo.kolibri-visions.de/api/v1/properties?limit=1" \
  -H "Authorization: Bearer $TOKEN"
# Returns: 200 OK (no apikey needed)
```

### Fix

**For Supabase Kong API Clients:**

When calling Supabase services (auth, storage, etc.), include both headers:

```python
# Python example - Supabase Kong endpoint
headers = {
    "Authorization": f"Bearer {jwt_token}",
    "apikey": anon_key,  # Required for Kong
    "Content-Type": "application/json"
}
response = requests.get(f"{supabase_url}/auth/v1/user", headers=headers)
```

**For PMS Backend API Clients:**

When calling PMS Backend, only Authorization header is needed:

```python
# Python example - PMS Backend API
headers = {
    "Authorization": f"Bearer {jwt_token}",
    # NO apikey needed
    "Content-Type": "application/json"
}
response = requests.get(f"{api_url}/api/v1/properties", headers=headers)
```

**For Smoke Scripts:**

Our smoke scripts call both:
- Supabase Kong for auth (`fetch_token()` includes apikey)
- PMS Backend API for tests (only Authorization header)

### Prevention

- Document apikey requirement in API docs
- Add example requests showing both headers
- Test authentication flow in CI/CD

---

## Fresh JWT (Supabase)

### Quick Token Generation

Use the `get_fresh_token.sh` helper script to obtain Supabase JWT tokens for testing, debugging, or manual API calls:

```bash
# Get token to variable
TOKEN="$(./backend/scripts/get_fresh_token.sh)"

# Export to environment
source <(./backend/scripts/get_fresh_token.sh --export)

# Verify token metadata
./backend/scripts/get_fresh_token.sh --check

# Test token against /auth/v1/user
./backend/scripts/get_fresh_token.sh --user
```

**Required environment variables:**
- `SUPABASE_ANON_KEY` - Supabase anonymous key
- `SB_EMAIL` - User email for authentication
- `SB_PASSWORD` - User password for authentication
- `SB_URL` (optional) - Supabase URL (defaults to https://sb-pms.kolibri-visions.de)

**Exit codes:**
- `0` - Success
- `1` - Missing required environment variables
- `2` - Authentication failed (invalid credentials)
- `3` - Empty or invalid JSON response
- `4` - Token verification failed (--user flag)

For detailed usage, integration examples, and troubleshooting, see **[backend/scripts/README.md - Get Fresh JWT Token](../scripts/README.md#get-fresh-jwt-token-get_fresh_tokensh)**.

---

## CORS Errors (Admin Console Blocked)

### Symptom

- Admin UI at `https://admin.fewo.kolibri-visions.de` shows CORS error in browser console
- Browser error: `Access to fetch at 'https://api.fewo.kolibri-visions.de/...' has been blocked by CORS policy`
- Preflight request (OPTIONS) fails with missing `Access-Control-Allow-Origin` header
- API returns 403 or connection refused for cross-origin requests

### Root Cause

CORS (Cross-Origin Resource Sharing) middleware not configured to allow admin console origin:
- Default CORS origins only include localhost (development)
- Production domains not added to `ALLOWED_ORIGINS` environment variable
- Missing `Authorization` header in allowed headers (rare, default allows all)

### Verify

Check current CORS configuration:

```bash
# Check environment variable on backend container
docker exec pms-backend env | grep ALLOWED_ORIGINS
# Should show: ALLOWED_ORIGINS=https://admin.fewo.kolibri-visions.de,https://fewo.kolibri-visions.de,...

# Or check backend logs on startup for CORS origins
docker logs pms-backend | grep -i cors
```

Test CORS preflight manually:

```bash
# Send OPTIONS preflight request
curl -X OPTIONS https://api.fewo.kolibri-visions.de/api/v1/properties \
  -H "Origin: https://admin.fewo.kolibri-visions.de" \
  -H "Access-Control-Request-Method: GET" \
  -H "Access-Control-Request-Headers: Authorization" \
  -v

# Should return:
# Access-Control-Allow-Origin: https://admin.fewo.kolibri-visions.de
# Access-Control-Allow-Methods: GET, POST, PUT, DELETE, PATCH
# Access-Control-Allow-Headers: *
```

### Fix

**Option 1: Set Environment Variable** (Recommended)

Add `ALLOWED_ORIGINS` to backend environment:

```bash
# In Coolify or deployment config
ALLOWED_ORIGINS=https://admin.fewo.kolibri-visions.de,https://fewo.kolibri-visions.de,http://localhost:3000
```

Restart backend:

```bash
docker restart pms-backend
```

**Option 2: Update Default in Code** (Already Done)

The default in `backend/app/core/config.py` now includes:
- `https://admin.fewo.kolibri-visions.de` (admin console)
- `https://fewo.kolibri-visions.de` (public site)
- `http://localhost:3000` (local dev)

If env var is not set, these defaults will be used.

### Prevention

- Always include admin and frontend origins in `ALLOWED_ORIGINS`
- Test CORS with `curl -X OPTIONS` before deploying frontend changes
- Document required origins in deployment checklist

---

## Booking Status Validation Error (500)

### Symptom

- Admin UI shows "Failed to fetch" when viewing booking details
- Browser console shows: `GET /api/v1/bookings/{id}` returns HTTP 500
- Backend logs show: `ResponseValidationError` with `literal_error` on `response.status`
- Error message: `Input should be 'inquiry', 'pending', 'confirmed', ... (value='requested')`
- CORS headers missing on 500 response (FastAPI behavior)

### Root Cause

Database contains booking status values (e.g., `'requested'`, `'under_review'`) that are not in the `BookingStatus` Literal type definition in `backend/app/schemas/bookings.py`.

**Why this happens:**
- Booking request workflow creates bookings with status `'requested'`
- Schema Literal was not updated when new statuses were added to database
- Pydantic validation fails when serializing response, causing 500 before CORS middleware runs

### Verify

Check backend logs for validation error:

```bash
docker logs pms-backend | grep -A 5 "ResponseValidationError"
# Look for: "literal_error" on "response.status"
# Input value causing error: 'requested' or 'under_review'
```

Test booking detail endpoint:

```bash
export API_BASE_URL="https://api.fewo.kolibri-visions.de"
export TOKEN="..."  # valid JWT token
export BOOKING_ID="..."  # booking ID with 'requested' status

curl -k -sS -i -H "Authorization: Bearer $TOKEN" "$API_BASE_URL/api/v1/bookings/$BOOKING_ID" | head -20
# Expected before fix: HTTP 500, no CORS headers
# Expected after fix: HTTP 200, booking details with status='requested'
```

Check database for affected bookings:

```bash
# Connect to database
docker exec -it <supabase-db-container> psql -U postgres

# Find bookings with non-standard statuses
SELECT id, status, created_at FROM bookings WHERE status NOT IN ('inquiry', 'pending', 'confirmed', 'checked_in', 'checked_out', 'cancelled', 'declined', 'no_show');
```

### Fix

**Solution:** Extend `BookingStatus` Literal in `backend/app/schemas/bookings.py` to include all valid database statuses.

**Code Change:**

```python
# backend/app/schemas/bookings.py (line ~35)

# Before:
BookingStatus = Literal[
    "inquiry", "pending", "confirmed", "checked_in",
    "checked_out", "cancelled", "declined", "no_show"
]

# After:
BookingStatus = Literal[
    "requested", "under_review",  # Booking request lifecycle
    "inquiry", "pending", "confirmed", "checked_in",
    "checked_out", "cancelled", "declined", "no_show"
]
```

**Deploy:**

```bash
# Commit fix
git add backend/app/schemas/bookings.py backend/tests/unit/test_booking_schemas.py
git commit -m "fix: allow booking status 'requested' in API responses"
git push origin main

# Verify deploy
curl -s "https://api.fewo.kolibri-visions.de/api/v1/ops/version" | jq -r .source_commit
```

### Test

After deploy, verify booking detail endpoint returns 200:

```bash
export API_BASE_URL="https://api.fewo.kolibri-visions.de"
export TOKEN="..."
export BOOKING_ID="..."

curl -k -sS -i -H "Authorization: Bearer $TOKEN" "$API_BASE_URL/api/v1/bookings/$BOOKING_ID" | head -30
# Expected: HTTP 200, body includes "status": "requested"
```

### Prevention

- When adding new booking statuses to database, update `BookingStatus` Literal in schemas
- Add unit tests for new status values in `backend/tests/unit/test_booking_schemas.py`
- Use database migrations to document valid status values with CHECK constraints

### PROD Verified (2026-01-07)

**Deployed Commit:** cb8da7f18b4fb19f9d68908afcaf52c8f8ca4645

**Verification Evidence:**
```bash
# HOST-SERVER-TERMINAL
export API_BASE_URL="https://api.fewo.kolibri-visions.de"

# Verify deployed commit
curl -s "$API_BASE_URL/api/v1/ops/version" | jq -r .source_commit
# Output: cb8da7f18b4fb19f9d68908afcaf52c8f8ca4645

# Verify booking with status='requested' returns 200
curl -k -sS -i -H "Authorization: Bearer $TOKEN" \
  "$API_BASE_URL/api/v1/bookings/de5aac06-486e-4c22-a6cf-0c7708d603d1" | head -20
# Output: HTTP/2 200
# Body includes: "status":"requested"

# Verify CORS headers present
curl -sS -i -H "Origin: https://admin.fewo.kolibri-visions.de" \
  "$API_BASE_URL/api/v1/branding" | grep -i access-control-allow-origin
# Output: access-control-allow-origin: https://admin.fewo.kolibri-visions.de
```

**Backend Started At:** 2026-01-07T17:49:04.742363+00:00

**Result:** ✅ Fix verified in production - booking detail endpoint returns 200 for status='requested'

---

## Schema Drift

### Symptom

- API returns `503 Service Unavailable`
- Logs show: `"Schema not installed"` or `"Schema out of date"` or `"Relation does not exist"`
- Database is reachable (DNS resolves, pool created), but queries fail

### Root Cause

Database schema is out of sync with application code:
- Migrations not applied after deploy
- Database restored from old backup
- Manual schema changes not tracked in migrations

### Verify

```bash
# SSH to host server
ssh root@your-host

# Check migration status (if using Alembic)
docker exec $(docker ps -q -f name=pms) alembic current
# Expected: Latest migration hash

# Check if tables exist
docker exec $(docker ps -q -f name=pms) psql "$DATABASE_URL" -c "\dt"
# Expected: List of tables (properties, bookings, availability_blocks, etc.)

# Check Supabase migrations (if using Supabase CLI)
docker exec $(docker ps -q -f name=supabase) supabase migration list
# Expected: All migrations marked as applied
```

### Fix

**Option 1: Apply Missing Migrations (Recommended)**

```bash
# If using Alembic
docker exec $(docker ps -q -f name=pms) alembic upgrade head

# If using Supabase CLI
docker exec $(docker ps -q -f name=supabase) supabase migration up

# Restart application
docker restart $(docker ps -q -f name=pms)
```

**Option 2: Manual Schema Inspection**

```bash
# Connect to database
docker exec -it $(docker ps -q -f name=supabase) psql -U postgres -d postgres

# Check if critical tables exist
\dt properties
\dt bookings
\dt availability_blocks

# If missing, check migration files in /app/migrations/ or supabase/migrations/
```

**Option 3: Full Database Reset (DESTRUCTIVE - Dev/Staging Only)**

```bash
# DANGER: This deletes all data
docker exec $(docker ps -q -f name=supabase) supabase db reset

# Re-apply migrations
docker exec $(docker ps -q -f name=pms) alembic upgrade head
```

### Prevention

- Include migration check in deployment pipeline
- Version migrations with semantic versioning
- Test migrations on staging before production
- Document schema changes in migration files

### Channel Manager Sync Logs Migration

**Migration File**: `supabase/migrations/20251227000000_create_channel_sync_logs.sql`

**When to Apply**: Required for Channel Manager sync log persistence (replaces stub/dummy data)

**Symptom if Missing**:
- GET `/api/v1/channel-connections/{id}/sync-logs` returns `503 Service Unavailable`
- Error message: `"Channel sync logs schema not installed (missing table: channel_sync_logs)"`
- Logs show: `UndefinedTableError: relation "channel_sync_logs" does not exist`

**Apply Migration**:

```bash
# Option 1: Using Supabase CLI
cd supabase/migrations
docker exec $(docker ps -q -f name=supabase) supabase migration up

# Option 2: Manual SQL execution
docker exec -it $(docker ps -q -f name=supabase) psql -U postgres -d postgres \
  < supabase/migrations/20251227000000_create_channel_sync_logs.sql

# Option 3: Via Supabase Dashboard
# Navigate to: SQL Editor > Paste migration content > Run
```

**Verify Installation**:

```bash
# Check table exists
docker exec $(docker ps -q -f name=supabase) psql -U postgres -d postgres \
  -c "\dt channel_sync_logs"

# Expected output:
#  Schema |        Name          | Type  | Owner
# --------+----------------------+-------+-------
#  public | channel_sync_logs    | table | postgres

# Check indexes
docker exec $(docker ps -q -f name=supabase) psql -U postgres -d postgres \
  -c "\d channel_sync_logs"

# Expected: 4 indexes (connection_id, task_id, tenant_id, status)
```

**What This Migration Does**:
- Creates `channel_sync_logs` table with JSONB details column
- Adds indexes for fast queries (connection_id, task_id, tenant_id, status)
- Sets up CHECK constraints for operation_type, direction, status
- Conditionally adds FK to `channel_connections` (if table exists)
- Enables persistent tracking of Channel Manager sync operations

**Rollback** (if needed):

```bash
docker exec -it $(docker ps -q -f name=supabase) psql -U postgres -d postgres \
  -c "DROP TABLE IF EXISTS public.channel_sync_logs CASCADE;"
```

### Guests Metrics Columns Migration

**Migration File**: `supabase/migrations/20260103120000_ensure_guests_metrics_columns.sql`

**When to Apply**: Required when existing guests table is missing metrics columns

**Symptom if Missing**:
- API returns `503 Service Unavailable` on guest operations
- Error message: `"column total_bookings of relation guests does not exist"`
- Logs show: `UndefinedColumn: column "total_bookings" does not exist`

**Apply Migration**:

```bash
# Using Supabase CLI
docker exec $(docker ps -q -f name=supabase) supabase migration up

# Or manual SQL execution
docker exec -it $(docker ps -q -f name=supabase) psql -U postgres -d postgres \
  < supabase/migrations/20260103120000_ensure_guests_metrics_columns.sql
```

**Verify Installation**:

```bash
# Check columns exist
docker exec $(docker ps -q -f name=supabase) psql -U postgres -d postgres \
  -c "\d guests"

# Expected columns: total_bookings, total_spent, last_booking_at
```

**What This Migration Does**:
- Adds `total_bookings` column (INTEGER, default 0) if missing
- Adds `total_spent` column (NUMERIC(12,2), default 0) if missing
- Adds `last_booking_at` column (TIMESTAMPTZ NULL) if missing
- Creates index on `last_booking_at` for recent guest activity queries
- Idempotent: safe to run multiple times (uses information_schema checks)

### Guests Booking Timeline Columns Migration

**Migration File**: `supabase/migrations/20260103123000_ensure_guests_booking_timeline_columns.sql`

**When to Apply**: Required when existing guests table is missing booking timeline columns

**Symptom if Missing**:
- API returns `503 Service Unavailable` on guest operations or Phase 20 smoke tests
- Error message: `"column first_booking_at of relation guests does not exist"`
- Logs show: `UndefinedColumn: column "first_booking_at" does not exist`

**Apply Migration**:

```bash
# Using Supabase CLI
docker exec $(docker ps -q -f name=supabase) supabase migration up

# Or manual SQL execution (Supabase SQL Editor or psql)
docker exec -it $(docker ps -q -f name=supabase) psql -U postgres -d postgres \
  < supabase/migrations/20260103123000_ensure_guests_booking_timeline_columns.sql
```

**Verify Installation**:

```bash
# Check columns exist
docker exec $(docker ps -q -f name=supabase) psql -U postgres -d postgres \
  -c "\d guests"

# Expected columns: first_booking_at, average_rating, updated_at, deleted_at
```

**What This Migration Does**:
- Adds `first_booking_at` column (TIMESTAMPTZ NULL) if missing
- Adds `average_rating` column (NUMERIC(3,2) NULL) if missing
- Adds `updated_at` column (TIMESTAMPTZ NOT NULL DEFAULT now()) if missing
- Adds `deleted_at` column (TIMESTAMPTZ NULL) for soft delete support
- Creates indexes on `first_booking_at` and `deleted_at` for performance
- Idempotent: safe to run multiple times (uses information_schema checks)

---

## Data Hygiene

### Legacy bookings.cancelled_by Strings (host/guest)

**Symptom**: The `public.bookings.cancelled_by` column contains non-UUID string values like `'host'` or `'guest'` instead of proper user UUIDs.

**Root Cause**: Early legacy data stored the cancellation actor as a string descriptor (`'host'` or `'guest'`) before the schema standardized to UUIDs referencing `auth.users.id`.

**Impact**: 
- Data inconsistency warnings in logs
- Potential validation errors when querying cancelled bookings
- Foreign key or JOIN operations may fail if code expects UUIDs

**Verification (before cleanup)**:
```sql
-- Count non-UUID values in cancelled_by column
SELECT count(*) FILTER (
  WHERE cancelled_by IS NOT NULL
    AND substring(cancelled_by from '^[0-9a-f]{8}-[0-9a-f]{4}-[1-5][0-9a-f]{3}-[89ab][0-9a-f]{3}-[0-9a-f]{12}') IS NULL
) AS cancelled_by_non_uuid
FROM public.bookings;

-- List actual non-UUID values
SELECT DISTINCT cancelled_by
FROM public.bookings
WHERE cancelled_by IS NOT NULL
  AND substring(cancelled_by from '^[0-9a-f]{8}-[0-9a-f]{4}-[1-5][0-9a-f]{3}-[89ab][0-9a-f]{3}-[0-9a-f]{12}') IS NULL;
```

**Fix (transaction-safe)**:
```sql
BEGIN;

-- Normalize legacy string values to NULL
-- We do NOT attempt to invent user identities for legacy data
UPDATE public.bookings
  SET cancelled_by = NULL
  WHERE cancelled_by IN ('host', 'guest');

COMMIT;
```

**Verification (after cleanup)**:
```sql
-- Should return 0
SELECT count(*) FILTER (
  WHERE cancelled_by IS NOT NULL
    AND substring(cancelled_by from '^[0-9a-f]{8}-[0-9a-f]{4}-[1-5][0-9a-f]{3}-[89ab][0-9a-f]{3}-[0-9a-f]{12}') IS NULL
) AS cancelled_by_non_uuid
FROM public.bookings;
```

**PROD Remediation (2026-01-12)**:
- **Before**: 27 rows with legacy values (`host`=26, `guest`=1)
- **After**: 0 rows with non-UUID values
- **Action**: Applied UPDATE transaction, set legacy values to NULL
- **Impact**: No functional impact; cancelled_by is nullable and optional metadata

**Note**: This cleanup does not affect booking status or other fields. The `cancelled_at` timestamp and `cancellation_reason` fields (if populated) remain intact and provide cancellation context.

---

## DB Migrations (Production)

**Date Added:** 2026-01-03 (Phase 21A)

**Purpose:** Apply Supabase SQL migrations to production database in a safe, idempotent manner.

### Migration Runner Script

**Location:** `backend/scripts/ops/apply_supabase_migrations.sh`

**Features:**
- Tracks applied migrations in `public.pms_schema_migrations` table
- Only applies pending migrations (idempotent)
- Dry-run mode (preview without applying)
- Status mode (show applied/pending summary)
- Transaction-based (each migration runs in a transaction)
- Production safety guard (requires explicit confirmation)

### Common Use Cases

#### 1. Check Migration Status

```bash
# SSH to host server
ssh root@your-host

# Navigate to repo
cd /data/repos/pms-webapp

# Check status
bash backend/scripts/ops/apply_supabase_migrations.sh --status
```

**Expected Output:**
```
INFO: Latest applied: 20260103123000_ensure_guests_booking_timeline_columns.sql
INFO: Applied migrations: 15
INFO: Pending migrations: 0
✓ All migrations applied - database schema is up to date
```

#### 2. Preview Pending Migrations (Dry-Run)

```bash
# Show what would be applied without executing
bash backend/scripts/ops/apply_supabase_migrations.sh --dry-run
```

**Expected Output:**
```
INFO: 2 pending migration(s):
  - 20260103140000_add_new_feature.sql
  - 20260103150000_update_constraints.sql

INFO: Dry-run mode - no migrations will be applied
```

#### 3. Apply Pending Migrations

```bash
# Set production confirmation flag
export CONFIRM_PROD=1

# Apply migrations
bash backend/scripts/ops/apply_supabase_migrations.sh
```

**Expected Output:**
```
WARNING: Production mode confirmed (CONFIRM_PROD=1)
✓ Database connection OK
✓ Tracking table ready: public.pms_schema_migrations

---
INFO: Applying: 20260103140000_add_new_feature.sql
✓ Applied: 20260103140000_add_new_feature.sql
---
INFO: Applying: 20260103150000_update_constraints.sql
✓ Applied: 20260103150000_update_constraints.sql

✓ All pending migrations applied successfully
```

### Environment Variables

**Option 1: DATABASE_URL (Recommended)**
```bash
export DATABASE_URL="postgresql://postgres:password@host:port/database"
bash backend/scripts/ops/apply_supabase_migrations.sh --status
```

**Option 2: Individual PG* Variables**
```bash
export PGHOST="your-host.supabase.co"
export PGPORT="5432"
export PGUSER="postgres"
export PGPASSWORD="your-password"
export PGDATABASE="postgres"
bash backend/scripts/ops/apply_supabase_migrations.sh --status
```

### Production Safety Guards

**Required Confirmation:**
```bash
# Without confirmation → Error
bash backend/scripts/ops/apply_supabase_migrations.sh
# ERROR: Production safety guard: set CONFIRM_PROD=1 to proceed

# With confirmation → Proceeds
export CONFIRM_PROD=1
bash backend/scripts/ops/apply_supabase_migrations.sh
```

**Dev/Staging Override:**
```bash
# Skip confirmation for non-production environments
export ALLOW_NON_PROD=1
bash backend/scripts/ops/apply_supabase_migrations.sh
```

### Failure Modes & Troubleshooting

#### Migration Fails Mid-Execution

**Symptom:** Script stops with error:
```
ERROR: Failed to apply: 20260103140000_add_feature.sql
ERROR: psql returned non-zero exit code
```

**Cause:** SQL syntax error, constraint violation, or missing dependencies

**Fix:**
1. Check the migration file for errors:
   ```bash
   cat supabase/migrations/20260103140000_add_feature.sql
   ```
2. Review psql error output (run migration manually for details):
   ```bash
   psql "$DATABASE_URL" -f supabase/migrations/20260103140000_add_feature.sql
   ```
3. Fix the SQL file or revert changes
4. Re-run migration script (only pending migrations will be applied)

**Important:** Failed migrations are NOT recorded in tracking table. Fix the issue and re-run.

#### "Database connection failed"

**Symptom:**
```
ERROR: Failed to connect to database
Connection: host:5432/database as user
```

**Fix:**
- Verify DATABASE_URL or PG* environment variables are set correctly
- Check network connectivity to database host
- Verify credentials are correct
- Check firewall rules allow connection from host server

#### "Migrations directory not found"

**Symptom:**
```
ERROR: Migrations directory not found: /data/repos/pms-webapp/supabase/migrations
```

**Fix:**
- Ensure you're running from repo root: `cd /data/repos/pms-webapp`
- Verify migrations directory exists: `ls -la supabase/migrations/`
- Pull latest code if directory is missing: `git pull`

### Manual Migration Verification

To verify migrations were applied correctly:

```bash
# Connect to database
psql "$DATABASE_URL"

# List applied migrations
SELECT filename, applied_at FROM public.pms_schema_migrations ORDER BY applied_at DESC LIMIT 10;

# Check specific table exists
\dt guests

# Check column exists
\d guests
```

### When to Run Migrations

**Always run after:**
- Deploying new code with schema changes
- Pulling latest code from git (check `supabase/migrations/` for new files)
- Seeing 503 errors with "schema not installed" or "schema out of date"

**Recommended workflow:**
1. Pull latest code: `git pull`
2. Check status: `bash backend/scripts/ops/apply_supabase_migrations.sh --status`
3. If pending migrations exist:
   - Preview with `--dry-run`
   - Apply with `CONFIRM_PROD=1`
4. Verify with `--status` again
5. Run smoke tests to confirm

### Migration Tracking Table Schema

```sql
CREATE TABLE public.pms_schema_migrations (
    filename TEXT PRIMARY KEY,                    -- Migration filename
    applied_at TIMESTAMPTZ NOT NULL DEFAULT now(), -- When applied
    sha256 TEXT,                                   -- File hash (integrity check)
    notes TEXT                                     -- Optional notes
);
```

**Example Data:**
```
filename                                          | applied_at                  | sha256
--------------------------------------------------+-----------------------------+--------
20260103120000_ensure_guests_metrics_columns.sql  | 2026-01-03 10:30:00+00     | abc123...
20260103123000_ensure_guests_booking_timeline... | 2026-01-03 10:35:00+00     | def456...
```

---

## Smoke Script Pitfalls

### Symptom

Smoke script (`pms_phase23_smoke.sh`) fails with:
- `"Required environment variable not set"`
- `"Token is empty"`
- `"PID not set and could not auto-derive"`
- Bash errors: `"unbound variable"`

### Root Causes & Fixes

#### 1. Empty TOKEN (Authentication Failure)

**Cause:** Invalid credentials or Supabase unreachable.

**Fix:**
```bash
# Verify credentials (do NOT print PASSWORD or full ANON_KEY)
echo "SB_URL: $SB_URL"
echo "ANON_KEY length: ${#ANON_KEY}"
echo "EMAIL: $EMAIL"

# Test auth manually
curl -X POST "$SB_URL/auth/v1/token?grant_type=password" \
  -H "apikey: $ANON_KEY" \
  -H "Content-Type: application/json" \
  -d '{"email":"'$EMAIL'","password":"'$PASSWORD'"}'

# Expected: JSON with "access_token" field
# If error: check credentials, network, Supabase status
```

#### 2. Empty PID (Auto-Derive Failed)

**Cause:** No properties exist, or properties endpoint returned empty items.

**Fix:**
```bash
# Check if properties exist
curl -X GET "$API/api/v1/properties?limit=1" \
  -H "Authorization: Bearer $TOKEN" \
  -H "apikey: $ANON_KEY"

# Expected: {"items": [{"uuid": "...", ...}], ...}
# If empty: create a test property first

# Override PID explicitly
export PID="your-property-uuid"
bash scripts/pms_phase23_smoke.sh
```

#### 3. Bash `set -u` (Unbound Variable Errors)

**Cause:** Script uses `set -u` (strict mode) - accessing undefined variables causes immediate exit.

**Fix:**
```bash
# Always use ${VAR:-default} syntax
export PID="${PID:-}"  # Empty string if not set
export API="${API:-https://api.fewo.kolibri-visions.de}"

# Or export all required vars before running
export SB_URL="..."
export ANON_KEY="..."
export EMAIL="..."
export PASSWORD="..."
bash scripts/pms_phase23_smoke.sh
```

#### 4. Running on Host vs Container

**Problem:** Script expects `/app/scripts/` paths (container), but you're on host.

**Fix:**

**Run in Container (Recommended):**
```bash
# SSH to host
ssh root@your-host

# Run in container terminal (via Coolify dashboard)
# OR via docker exec:
docker exec -it $(docker ps -q -f name=pms) bash

# Inside container:
export ENV_FILE=/root/pms_env.sh
bash /app/scripts/pms_phase23_smoke.sh
```

**Run on Host (Alternative):**
```bash
# SSH to host
ssh root@your-host

# Use docker exec one-liner
docker exec $(docker ps -q -f name=pms) bash -c '
export ENV_FILE=/root/pms_env.sh
bash /app/scripts/pms_phase23_smoke.sh
'
```

#### 5. Environment File Not Found

**Cause:** ENV_FILE path is wrong (host vs container filesystem).

**Fix:**
```bash
# Check if file exists in container
docker exec $(docker ps -q -f name=pms) ls -la /root/pms_env.sh

# If missing, create it:
cat > /tmp/pms_env.sh <<'ENVEOF'
export SB_URL="https://your-project.supabase.co"
export ANON_KEY="eyJhbGc..."
export EMAIL="admin@example.com"
export PASSWORD="your-password"
export API="https://api.fewo.kolibri-visions.de"
ENVEOF

# Copy to container
docker cp /tmp/pms_env.sh $(docker ps -q -f name=pms):/root/pms_env.sh

# Run script
docker exec $(docker ps -q -f name=pms) bash -c '
export ENV_FILE=/root/pms_env.sh
bash /app/scripts/pms_phase23_smoke.sh
'
```

### Prevention

- Document required environment variables clearly
- Provide example env file template
- Add validation at script start (require_env)
- Test scripts in CI/CD pipeline

---

## Optional: Availability Block Conflict Test

### Overview

The Phase 23 smoke script includes an **opt-in** test for availability block/booking conflict validation. This test:
- Creates an availability block for a future window (today + 30 days, 3 days duration)
- Verifies the block appears in `/api/v1/availability` response
- Attempts to create an overlapping booking (expects 409 conflict)
- Always cleans up the block (via trap, even on failure)

**Use Case:** Validate inventory conflict detection after schema changes or deployment.

**Safety:** Future window (30 days out) avoids interfering with real operations.

### Enable the Test

Set `AVAIL_BLOCK_TEST=true` to enable:

```bash
# SSH to host server
ssh root@your-host

# Run in container with block test enabled
docker exec $(docker ps -q -f name=pms) bash -c '
export ENV_FILE=/root/pms_env.sh
export AVAIL_BLOCK_TEST=true
bash /app/scripts/pms_phase23_smoke.sh
'
```

**Expected Output (when enabled):**
```
ℹ️  Test 8: Availability block conflict test (opt-in via AVAIL_BLOCK_TEST=true)
ℹ️  Creating availability block: 2026-01-25 to 2026-01-28 (PID: abc-123...)
ℹ️  Block created: def-456...
ℹ️  Verifying block appears in /api/v1/availability...
ℹ️  ✓ Block found in availability response
ℹ️  Attempting to create overlapping booking (expect 409 conflict)...
ℹ️  ✓ Booking correctly rejected with 409 (conflict_type: inventory_overlap)
ℹ️  Deleting block def-456...
ℹ️  ✓ Block deleted successfully
ℹ️  ✅ PASS - Availability block conflict test complete

Summary:
  ...
  ✓ Availability block conflict test passed
```

**Expected Output (when disabled, default):**
```
ℹ️  Test 8: Availability block conflict test (opt-in via AVAIL_BLOCK_TEST=true)
⚠️  AVAIL_BLOCK_TEST not set to 'true' - skipping block conflict test
⚠️  To enable: export AVAIL_BLOCK_TEST=true

Summary:
  ...
  ⊘ Availability block conflict test skipped (AVAIL_BLOCK_TEST not enabled)
```

### Requirements

- Requires PID (uses auto-derived or explicit `export PID=...`)
- Requires JWT token (automatic via SB_URL/ANON_KEY/EMAIL/PASSWORD)
- Requires write access to `/api/v1/availability/blocks` (admin/manager role)

### What It Tests

1. **Block Creation** (`POST /api/v1/availability/blocks`)
   - Verifies block can be created for future window
   - Captures block ID for cleanup

2. **Block Visibility** (`GET /api/v1/availability`)
   - Verifies block appears in availability response
   - Checks: `kind=block`, `state=blocked`, `block_id` present
   - **Property Validation:** Returns 404 if property_id does not exist (prevents false positives from demo UUIDs)

3. **Conflict Detection** (`POST /api/v1/bookings`)
   - Verifies overlapping booking is rejected with 409
   - Checks: `conflict_type=inventory_overlap`

4. **Block Deletion** (`DELETE /api/v1/availability/blocks/{id}`)
   - Verifies block can be deleted (cleanup)
   - Trap ensures cleanup even on test failure

### Cleanup Guarantee

The test uses a bash trap to ensure cleanup:
```bash
trap cleanup_block EXIT
```

**If test fails mid-execution:**
- Block will still be deleted automatically
- No orphaned test data left in database

**Manual cleanup (if needed):**
```bash
# List blocks with reason="smoke-test"
curl -X GET "$API/api/v1/availability/blocks?reason=smoke-test" \
  -H "Authorization: Bearer $TOKEN"

# Delete specific block
curl -X DELETE "$API/api/v1/availability/blocks/<block-id>" \
  -H "Authorization: Bearer $TOKEN"
```

### When to Use

**Recommended:**
- After schema migrations affecting `availability_blocks` or `inventory_ranges`
- After deployment of conflict detection logic changes
- When validating EXCLUSION constraint behavior
- Pre-production smoke test before go-live

**Not Recommended:**
- In CI/CD pipelines (adds ~5-10s, requires write access)
- Production health checks (read-only tests preferred)
- Frequent monitoring (creates/deletes data)

### Important Notes

**Demo UUIDs vs Real Properties:**
- Some documentation examples use demo UUIDs like `550e8400-e29b-41d4-a716-446655440000`
- These may exist in `channel_connections` mocks but are NOT real properties in the database
- `GET /api/v1/availability` now returns **404 Property not found** for non-existent property_id
- This prevents false positives from smoke tests using invalid property IDs

**Smoke Script PID Validation:**
- The smoke script (`pms_phase23_smoke.sh`) now validates PID before running availability tests
- If PID is invalid (property not found), script auto-selects a valid PID from `/api/v1/properties`
- Warning displayed: `⚠️ PID invalid (Property not found). Using fallback PID=<id> from /properties`
- This ensures tests run against real properties, not demo UUIDs

**To override PID:**
```bash
export PID="<valid-property-uuid>"
bash scripts/pms_phase23_smoke.sh
```

---

## Phase 21: Inventory/Availability Production Hardening

**Date:** 2026-01-03

**Summary:** Production readiness validation for inventory/availability APIs with common gotchas documentation and operational guidance.

### What Phase 20 Proved

Phase 20 smoke tests validated core inventory mechanics:
- ✅ **Manual blocks prevent bookings**: Availability blocks correctly reject overlapping bookings with HTTP 409 `inventory_overlap`
- ✅ **Deleting blocks unblocks inventory**: Block deletion immediately frees inventory for booking
- ✅ **Cancel frees inventory**: Booking cancellation releases inventory instantly
- ✅ **Idempotent cancellation**: Canceling already-cancelled bookings returns 200 (safe retry)
- ✅ **Cancelled bookings don't prevent rebooking**: Same dates can be rebooked after cancellation
- ✅ **Race-safe concurrency**: Under concurrent requests, exactly 1 booking succeeds (201), rest rejected (409)

### Common Gotchas Checklist

#### 1. Missing Query Parameters (422 Validation Error)

**Symptom:** `GET /api/v1/availability` returns HTTP 422 with validation errors

**Cause:** `from_date` and `to_date` query parameters are required but missing

**Example Error (FastAPI/Pydantic format):**
```json
{
  "detail": [
    {"type": "missing", "loc": ["query", "from_date"], "msg": "Field required"},
    {"type": "missing", "loc": ["query", "to_date"], "msg": "Field required"}
  ]
}
```

**Example Error (PMS custom envelope):**
```json
{
  "error": "validation_error",
  "message": "Request validation failed",
  "errors": [
    {"field": "query.from_date", "message": "Field required", "type": "missing"},
    {"field": "query.to_date", "message": "Field required", "type": "missing"}
  ],
  "path": "/api/v1/availability"
}
```

**Note:** The API may return either error format. Both indicate missing required query parameters. When using curl or smoke scripts, you may see extra artifacts (HTTP headers, status lines, trailing text) mixed with the JSON response. The Phase 21 smoke script handles this by extracting JSON boundaries and falling back to raw string checks.

**Fix:** Always include both query parameters:
```bash
# Correct usage
curl -H "Authorization: Bearer $TOKEN" \
  "$API/api/v1/availability?property_id=$PID&from_date=2026-01-10&to_date=2026-01-20"

# Incorrect (422 error)
curl -H "Authorization: Bearer $TOKEN" \
  "$API/api/v1/availability?property_id=$PID"
```

#### 2. Availability Block Overlap Returns 500 Instead of 409

**Date Added:** 2026-01-08 (Phase 21C Bugfix)

**Symptom:** `POST /api/v1/availability/blocks` with overlapping dates returns HTTP 500 Internal Server Error with `{"detail":"Failed to create availability block"}` instead of 409 Conflict (observed in Phase 21 smoke test TEST 3)

**Root Cause:** PostgreSQL EXCLUSION constraint violation (SQLSTATE 23P01 on `inventory_ranges_no_overlap`) was being raised as generic `asyncpg.PostgresError` instead of specific `asyncpg.exceptions.ExclusionViolationError`. Original exception handler only caught the specific type, causing overlap errors to fall through to generic 500 handler.

**Fix Applied:** Two-layer robust overlap detection to prevent 500 fallthrough:
- **Service layer** (`backend/app/services/availability_service.py:357-387`): Catches `asyncpg.PostgresError` and detects overlap by: `sqlstate == '23P01'` OR `constraint_name == 'inventory_ranges_no_overlap'` OR message contains `'inventory_ranges_no_overlap'` OR message contains `'exclusion constraint'`. Raises `ConflictException` (409) when detected.
- **Route layer** (`backend/app/api/routes/availability.py:321-354`): Safety net with same detection logic. Added explicit `except HTTPException: raise` to prevent generic `except Exception` from swallowing `ConflictException`. Logs sqlstate/constraint on overlap detection and exception type on unexpected errors.

**Logs for debugging (if regression occurs):**
```bash
# Capture sqlstate/constraint from backend logs (HOST-SERVER-TERMINAL)
docker logs pms-backend 2>&1 | grep -A2 "Overlap detected\|Inventory range conflict" | tail -20
# Look for: "sqlstate=23P01" or "constraint=inventory_ranges_no_overlap"
```

**Verification (HOST-SERVER-TERMINAL):**
```bash
# 1. Verify deployed commit includes fix (check /api/v1/ops/version)
curl -k -sS https://api.fewo.kolibri-visions.de/api/v1/ops/version | jq -r '.commit'
# → Should show commit hash containing this fix (post-cc42fe7)

# 2. Run Phase 21 smoke test (all 6 tests should pass, especially TEST 3 overlap → 409)
JWT_TOKEN="<admin-or-manager-token>" PID="<property-id>" ./backend/scripts/pms_availability_phase21_smoke.sh ; echo "Exit code: $?"
# → Exit code: 0 (all tests passed)
# → TEST 3: Create overlapping block → ✓ PASS: HTTP 409 - overlap correctly rejected

# 3. Manual verification (optional)
curl -k -X POST "https://api.fewo.kolibri-visions.de/api/v1/availability/blocks" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"property_id":"'$PID'","start_date":"2026-02-01","end_date":"2026-02-08","reason":"Test"}' ; echo ""
# → HTTP 201 Created

curl -k -X POST "https://api.fewo.kolibri-visions.de/api/v1/availability/blocks" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"property_id":"'$PID'","start_date":"2026-02-05","end_date":"2026-02-10","reason":"Overlap"}' ; echo ""
# → HTTP 409 Conflict (NOT 500) with {"error":"conflict","message":"Availability block overlaps..."}
```

**Related Endpoints:**
- `GET /api/v1/availability/blocks/{block_id}` - Retrieve single block (added in Phase 21C)
- `DELETE /api/v1/availability/blocks/{block_id}` - Delete block (existing)

**Smoke Test:** `backend/scripts/pms_availability_phase21_smoke.sh` validates all block operations including overlap detection

#### 3. Schema Drift Symptoms

**Symptom:** API returns `503 Service Unavailable` with error message `"Schema not installed"` or `"Schema out of date"`

**Common Missing Columns:**
- `guests.total_bookings`, `guests.total_spent`, `guests.last_booking_at` → Apply migration `20260103120000_ensure_guests_metrics_columns.sql`
- `guests.first_booking_at`, `guests.average_rating`, `guests.updated_at`, `guests.deleted_at` → Apply migration `20260103123000_ensure_guests_booking_timeline_columns.sql`

**What to Do:**
1. Check which column is missing from error logs
2. Apply corresponding migration via Supabase CLI or SQL Editor
3. Verify with `\d guests` in psql
4. See [Schema Drift](#schema-drift) section for full troubleshooting steps

### Minimum Production Checklist for Inventory

Before deploying inventory/availability features to production:

- [ ] **Migrations Applied**
  - Run `supabase migration list` or check migration table
  - Ensure all inventory-related migrations present (guests metrics, timeline columns, exclusion constraints)

- [ ] **Exclusion Constraint Present**
  - Verify `bookings.no_double_bookings` constraint exists
  - Check with: `\d bookings` in psql and look for `EXCLUDE USING gist` constraint
  - Migration: `20251229200517_enforce_overlap_prevention_via_exclusion.sql`

- [ ] **Smoke Scripts Pass**
  - Run `pms_phase20_final_smoke.sh` → Should complete without 503/422 errors
  - Run `pms_booking_concurrency_test.sh` → Should show exactly 1 success, rest 409
  - Run `pms_phase21_inventory_hardening_smoke.sh` → Should validate availability API contract

- [ ] **Environment Variables Configured**
  - `DATABASE_URL` set and reachable
  - `JWT_SECRET` configured for token validation
  - `ALLOWED_ORIGINS` includes admin console domain (CORS)

### What We Do Next

Phase 21 focuses on edge cases and operational robustness:

- **Back-to-Back Bookings**: Validate check-out day can be another booking's check-in (end-exclusive semantics)
- **Timezone Boundaries**: Test bookings crossing DST transitions and UTC midnight boundaries
- **Min Stay Constraints**: Enforce minimum night requirements per property/season
- **Booking Window Rules**: Validate advance booking limits (e.g., max 365 days future)
- **Availability Read Contract**: Negative tests for missing query params, malformed dates
- **Concurrency Edge Cases**: Multi-property parallel bookings, rapid cancel-rebook cycles

---

## Phase 30 — Inventory Final Validation

**Date:** 2025-12-27

**Summary:** Comprehensive validation of inventory/availability conflict detection and date semantics.

### What Was Validated

#### Test 8: Availability Block Conflict (AVAIL_BLOCK_TEST=true)
- ✅ Availability block creation (future window: 2026-01-25 to 2026-01-28)
- ✅ Block visibility in `/api/v1/availability` response
- ✅ Overlapping booking rejection with HTTP 409 `conflict_type=inventory_overlap`
- ✅ Block deletion cleanup

**Result:** PASS — Availability blocks correctly prevent overlapping bookings.

#### Test 9: Back-to-Back Booking Boundary (B2B_TEST=true)
- ✅ Free gap detection (found 2026-02-26 to 2026-03-02)
- ✅ Booking A creation (2026-02-26 to 2026-02-28, 2 nights)
- ✅ Booking B creation (2026-02-28 to 2026-03-02, check-in = A's check-out)
- ✅ Both bookings returned HTTP 201 (no boundary conflict)
- ✅ Booking cancellation cleanup via PATCH

**Result:** PASS — Confirms end-exclusive date semantics (check-out date is NOT occupied).

### How to Run Validation

**Location:** HOST-SERVER-TERMINAL

```bash
# SSH to host server
ssh root@your-host

# Load environment (contains SB_URL, ANON_KEY, EMAIL, PASSWORD, API)
source /root/pms_env.sh

# Enable opt-in tests (optional - choose one or both)
export AVAIL_BLOCK_TEST=true  # Availability block conflict test
export B2B_TEST=true          # Back-to-back booking boundary test

# Run smoke script
bash backend/scripts/pms_phase23_smoke.sh
```

**Notes:**
- Tests are **opt-in** (disabled by default)
- Tests use **future dates** (30-60+ days out) to avoid production conflicts
- Tests **clean up after themselves**:
  - Availability blocks: deleted via DELETE `/api/v1/availability/blocks/{id}`
  - Bookings: cancelled via PATCH `/api/v1/bookings/{id}` with `status=cancelled`
- Cleanup runs via trap (executes even on test failure)
- No data left behind on success or failure

### Production Impact

**Zero** — Tests create and delete temporary data in far-future date ranges.

---

## Module System Kill-Switch

**Purpose:** Emergency fallback to bypass the module mounting system if issues are detected.

### Overview

The PMS backend uses a modular monolith architecture (Phase 33B) where routers are registered and mounted via a module system. If module system issues are detected in production, the `MODULES_ENABLED` environment variable provides a kill-switch to bypass it.

**Default:** `MODULES_ENABLED=true` (module system active)

**Fallback behavior when disabled:**
- Mounts health router (core_pms)
- Mounts `/api/v1` routers: properties, bookings, availability
- Same API paths and behavior as module system
- No module validation or dependency checks

### When to Use the Kill-Switch

**Symptoms that may require kill-switch:**
- Routes appear missing (404 errors on expected endpoints)
- Module import errors in startup logs
- Circular dependency errors on startup
- Module registration failures
- Routers not mounting correctly

**DO NOT use unless:**
- Module system is confirmed broken
- Rollback to previous deployment is not possible
- Business impact requires immediate resolution

### How to Disable Module System

**Location:** Coolify Dashboard (Environment Variables)

**Steps:**
1. Open Coolify dashboard
2. Navigate to: **Applications > PMS-Webapp > Environment Variables**
3. Add or update variable:
   - Name: `MODULES_ENABLED`
   - Value: `false`
4. Click **Save**
5. **Restart** the application

**Expected Behavior:**
- Application startup logs: `"MODULES_ENABLED=false → Mounting routers via fallback"`
- Fallback mounts health + `/api/v1/properties`, `/api/v1/bookings`, `/api/v1/availability`
- Same API paths and behavior as module system
- No module validation or dependency checks

### How to Re-enable Module System

**Steps:**
1. Open Coolify dashboard
2. Navigate to: **Applications > PMS-Webapp > Environment Variables**
3. Update variable:
   - Name: `MODULES_ENABLED`
   - Value: `true` (or remove the variable - defaults to `true`)
4. Click **Save**
5. **Restart** the application

**Expected Behavior:**
- Application startup logs: `"MODULES_ENABLED=true → Mounting modules via module system"`
- Module validation runs (detects circular dependencies)
- Routers mounted via registry in dependency order

### Verification

**After changing MODULES_ENABLED:**

```bash
# SSH to host server
ssh root@your-host

# Check application logs
docker logs pms-backend --tail 50 | grep "MODULES_ENABLED"

# Expected: "MODULES_ENABLED=true →" or "MODULES_ENABLED=false →"

# Verify health endpoint
curl http://localhost:8000/health
# Expected: {"status": "healthy"}

# Verify API endpoints
curl http://localhost:8000/api/v1/properties
# Expected: 200 OK or 401 Unauthorized (depends on auth)
```

### Important Notes

- **No API changes**: Both modes mount the same routers with the same prefixes and tags
- **No data loss**: Kill-switch only affects router mounting, not database or data
- **Backwards compatible**: `/docs` and `/openapi.json` show identical routes in both modes
- **Temporary measure**: After using kill-switch, investigate root cause and restore module system
- **Module system preferred**: Default mode includes dependency validation and better error detection

### Rollback Plan

If disabling the module system causes issues:
1. Set `MODULES_ENABLED=true` in Coolify
2. Restart application
3. If still broken, rollback to previous deployment

---

## Backend Restart Loop (Module Import Failures)

**Symptom:** Traefik returns 503 "no available server", pms-backend container in restart loop.

**Logs Show:**
```
ImportError: cannot import name 'require_agency_roles' from 'app.api.deps'
ValueError: Module 'channel_manager' depends on unknown module: 'core_pms'
```

**Root Cause:**
1. Missing dependency export in `backend/app/api/deps.py` (e.g., `require_agency_roles` not re-exported)
2. Module registry validation fails hard, crashing entire app when optional module has missing dependencies

**How to Debug:**
```bash
# [HOST-SERVER-TERMINAL] Check container logs
docker logs pms-backend --tail 100

# Look for ImportError or ValueError during module registration
# Common patterns:
# - "cannot import name '<func>' from 'app.api.deps'"
# - "Module '<name>' depends on unknown module: '<dep>'"
```

**Solution (Production Fix):**

Fixed in commit post-3624cab:
1. **Import Fix**: Added `require_agency_roles` to `backend/app/api/deps.py` exports
2. **Registry Fail-Soft**: Module registry now skips modules with missing deps (degraded mode) instead of crashing

**Verification Commands:**

```bash
# [HOST-SERVER-TERMINAL] Check backend health after fix deployed
docker logs pms-backend --tail 50 | grep -E "(Started|Skipping module|ImportError)"

# Expected: "Application startup complete" or similar, no ImportError

# Test health endpoint
curl -sS https://api.fewo.kolibri-visions.de/health
# Expected: HTTP 200 {"status":"healthy"}

# Test version endpoint
curl -sS https://api.fewo.kolibri-visions.de/api/v1/ops/version | jq '.source_commit'
# Expected: Shows current commit hash (not 3624cab)

# Check for degraded mode warnings (optional modules skipped)
docker logs pms-backend 2>&1 | grep "Skipping module"
# If present: Module with missing deps was skipped (degraded operation)
# If absent: All modules loaded successfully
```

**Prevention:**
- Backend now runs in fail-soft mode by default (registry.register(spec, fail_soft=True))
- Missing optional module dependencies logged as errors but don't crash app
- Core functionality remains available even if optional modules fail to load
- Regression test: `backend/tests/unit/test_module_registry_failsoft.py`

**Related:**
- Module System Kill-Switch (MODULES_ENABLED=false) for emergency disabling
- Module registry fail-soft behavior prevents single module from killing entire app

---

## Module Feature Flags

**Purpose:** Control which optional modules are loaded and exposed via API.

### Channel Manager Module

**Environment Variable:** `CHANNEL_MANAGER_ENABLED`

**Default:** `false` (disabled)

**Purpose:**
- Controls whether Channel Manager API endpoints are exposed
- Channel Manager handles OAuth credentials and platform integrations
- Disabled by default for security

**Endpoints (when enabled):**
- `/api/v1/channel-connections/*` - Channel connection management (CRUD, sync, health checks)

**⚠️ SECURITY WARNING:**

**NEVER enable CHANNEL_MANAGER_ENABLED in production unless authentication is verified.**

All Channel Manager endpoints require Bearer JWT authentication. Before enabling in production:

1. Verify authentication is enforced (without token → 401):
   ```bash
   curl -k -i https://api.fewo.kolibri-visions.de/api/v1/channel-connections/ | head
   # Expected: HTTP/1.1 401 Unauthorized (or 403 Forbidden)
   ```

2. Verify authenticated access works (with token → 200):
   ```bash
   TOKEN="<valid-jwt-token>"
   curl -k -i https://api.fewo.kolibri-visions.de/api/v1/channel-connections/ \
     -H "Authorization: Bearer $TOKEN" | head
   # Expected: HTTP/1.1 200 OK
   ```

If authentication check fails (returns 200 without token), **DO NOT enable** and escalate immediately.

**How to Enable:**

1. Open Coolify dashboard
2. Navigate to: **Applications > PMS-Webapp > Environment Variables**
3. Add variable:
   - Name: `CHANNEL_MANAGER_ENABLED`
   - Value: `true`
4. Click **Save**
5. **Restart** the application

**Verification:**

```bash
# Check application logs
docker logs pms-backend --tail 50 | grep "Channel Manager"

# Expected when enabled:
# "Channel Manager module enabled via CHANNEL_MANAGER_ENABLED=true"

# Expected when disabled:
# "Channel Manager module disabled (CHANNEL_MANAGER_ENABLED=false, default)"

# Verify endpoints are exposed (when enabled)
curl http://localhost:8000/docs
# Check for /api/v1/channel-connections endpoints in Swagger UI
```

**Important Notes:**
- Requires `MODULES_ENABLED=true` (module system must be active)
- If `MODULES_ENABLED=false`, the Channel Manager module is bypassed regardless of this flag
- OpenAPI documentation (`/docs`) only shows Channel Manager endpoints when enabled
- Ensure proper authentication and RBAC policies are configured before enabling

---

### Verify Sync Batch Details (PROD)

**Purpose:** Verify channel manager sync batch API endpoints are working correctly in production.

**Prerequisites:**
- Channel Manager enabled (`CHANNEL_MANAGER_ENABLED=true`)
- Valid Bearer TOKEN (JWT)
- At least one channel connection exists (CID = connection UUID)
- At least one sync batch has been created (trigger sync via Admin UI or API)

**EXECUTION LOCATION:** HOST-SERVER-TERMINAL

**Manual Verification Commands:**

```bash
# 1. List sync batches for a connection (paginated)
export API="https://api.fewo.kolibri-visions.de"
export TOKEN="your-jwt-token"
export CID="your-connection-uuid"

curl -L -H "Authorization: Bearer $TOKEN" \
  "$API/api/v1/channel-connections/$CID/sync-batches?limit=10&offset=0" | python3 -m json.tool

# Expected: HTTP 200 with JSON response containing "items" array
# Each item: batch_id, batch_status, status_counts, created_at_min, operations array

# 2. Get details for a specific batch
export BATCH_ID="batch-uuid-from-above"

curl -L -H "Authorization: Bearer $TOKEN" \
  "$API/api/v1/channel-connections/$CID/sync-batches/$BATCH_ID" | python3 -m json.tool

# Expected: HTTP 200 with JSON response containing:
# - batch_id
# - connection_id
# - batch_status (failed/running/success/unknown)
# - status_counts (triggered, running, success, failed)
# - created_at_min, updated_at_max
# - operations: array of BatchOperation (operation_type, status, direction, task_id, error, duration_ms, log_id)
```

**Automated Smoke Test:**

Use the smoke test script for quick verification:

```bash
# Run smoke test (auto-picks first batch if BATCH_ID not set)
export API="https://api.fewo.kolibri-visions.de"
export TOKEN="your-jwt-token"
export CID="your-connection-uuid"

bash backend/scripts/pms_sync_batch_details_smoke.sh

# Expected output:
# ✓ List batches returned HTTP 200
# ✓ Get batch details returned HTTP 200
# Summary: All endpoints verified successfully
```

**Common Issues:**

- **No batches found:** Trigger a sync operation first via Admin UI (`/channel-sync` page) or API (`POST /api/v1/channel-connections/{id}/sync`)
- **HTTP 404 on batch details:** Batch may have been deleted or batch_id is incorrect
- **HTTP 401:** TOKEN expired or invalid (re-fetch from Supabase auth)
- **HTTP 503:** Database schema not installed or out of date (run migration: `20251227000000_create_channel_sync_logs.sql`)
- **HTTP 307 redirects:** Use `-L` flag with curl to follow redirects
- **HTTP 405 on HEAD requests:** Batch endpoints reject HEAD method; use GET for sanity checks (never use `curl -I`)
- **List endpoint JSON shape:** `/api/v1/channel-connections` may return top-level array OR `{items: [...]}` object; scripts handle both shapes robustly

**Batch Status Logic:**

- **failed**: Any operation has status='failed'
- **running**: Any operation has status='triggered' or 'running' (and none failed)
- **success**: All operations have status='success'
- **unknown**: No operations found or other states

**Admin UI Integration:**

The "Batch Details Modal" in Admin UI (`/channel-sync` page) uses these endpoints to display:
- Batch ID and overall status
- Operation breakdown with statuses, durations, errors
- Task IDs (Celery task UUIDs)
- Direction indicators (→ outbound, ← inbound)

Verify modal displays data correctly by clicking "View Details" on any sync batch row.

**UI E2E Verification:** Click the "Batch" badge in Sync Logs table to open Batch Details Modal. Confirm operations list renders with correct batch_id, connection_id, operation types, statuses, and durations. Modal should display direction indicators (→/←) and handle loading/error states gracefully.

**Manual Sync Trigger Form:**
- Auto-detect connection now auto-derives Platform and Property fields from the selected connection
- Platform and Property fields are locked (disabled) when derived from connection with "from connection" badge indicator
- Use Clear button to reset derived state and unlock fields for manual selection
- Manually changing Platform or Property while connection is derived will auto-clear the connection and unlock fields

---

### Channel Sync Console UX Verification Checklist

**Purpose:** Verify Channel Sync Console (`/channel-sync` page) handles errors, empty states, and destructive actions correctly.

**EXECUTION LOCATION:** WEB-BROWSER (Admin UI)

**Prerequisites:**
- Admin user logged in
- At least one channel connection configured

**Error State Verification:**

1. **401 Unauthorized (Session Expired)**
   - Test: Clear session token or wait for expiration, refresh page
   - Expected: "Session expired. Redirecting to login..." message, automatic redirect to /auth/logout
   - Applies to: Sync Logs list, Batch Details modal

2. **403 Forbidden (Access Denied)**
   - Test: Try accessing as non-admin user (if RBAC enforced at API level)
   - Expected: "Access denied. You don't have permission to view sync logs." (or batch details)
   - Applies to: Sync Logs list, Batch Details modal

3. **404 Not Found**
   - Test: Delete connection or batch, then try to fetch
   - Expected: "Connection not found. It may have been deleted." (logs) or "Batch not found. It may have been deleted or purged." (batch details)
   - Applies to: Sync Logs list, Batch Details modal

4. **503 Service Unavailable**
   - Test: Stop backend service temporarily
   - Expected: "Service temporarily unavailable. Please try again shortly."
   - Applies to: Sync Logs list, Batch Details modal

**Empty State Verification:**

1. **No Sync Logs Yet**
   - Test: Select connection with no sync history
   - Expected: "No sync logs yet" with hint "Trigger a manual sync or wait for automatic sync to create logs"
   - Location: Main Sync Logs table

2. **No Matching Search Results**
   - Test: Enter search query that matches no logs
   - Expected: "No logs match your search."
   - Location: Main Sync Logs table

3. **No Failed Logs**
   - Test: Filter by status=failed when no failures exist
   - Expected: "No failed logs yet. (Note: invalid requests (422) do not create logs.)"
   - Location: Main Sync Logs table

**Destructive Actions Verification:**

1. **Purge Logs Confirmation**
   - Test: Click "Purge Logs" button (admin only)
   - Expected:
     - Modal opens with purge preview (shows count to be deleted)
     - Requires typing "PURGE" exactly (case-sensitive)
     - "Purge" button disabled until phrase entered correctly
     - Button disabled while purge in-flight (shows loading state)
     - Error displayed if confirm phrase incorrect
   - Location: Purge modal (triggered from admin controls)

**Copy Helpers Verification:**

1. **curl Commands Use Safe Placeholders**
   - Test: Click "📋 Copy 'List Logs' curl" or "📋 Copy 'Trigger Sync' curl"
   - Expected:
     - Copied command includes placeholders: `$CID`, `$TOKEN`, `$PROPERTY_UUID`
     - NO actual tokens embedded (prevents accidental secret exposure)
     - Command is syntactically valid bash with placeholders
   - Location: API Helpers section

**Loading States Verification:**

1. **Spinners and Disabled Buttons**
   - Test: Trigger sync, open batch details modal, purge logs
   - Expected:
     - Loading spinners visible during fetch
     - Buttons disabled during in-flight requests (no double-click triggers)
     - Errors clear properly on retry
   - Location: All fetch operations

2. **Search Field Text Visibility**
   - Test: Click into Sync Logs search field and type text
   - Expected:
     - Typed text is clearly visible (not white on white or invisible)
     - Typed text has high contrast in light mode (dark text on white background, not low-contrast gray)
     - Works in both light mode and dark mode
     - Placeholder text remains readable
   - Location: Sync Logs search input

**RBAC Alignment:**

- Purge logs action requires **admin** role (aligned with sync trigger permissions)
- Non-admin users should NOT see "Purge Logs" button or link
- Admin UI gracefully degrades for non-admin users (403 errors handled)

---

## Branding & Theming Verification (PROD)

**Purpose:** Verify tenant branding system is working correctly in production (logo, colors, theme tokens).

**EXECUTION LOCATION:** HOST-SERVER-TERMINAL (migration) + WEB-BROWSER (UI verification)

### Apply Migration (HOST-SERVER-TERMINAL)

```bash
# Check migration status
cd /path/to/repo
bash backend/scripts/ops/apply_supabase_migrations.sh --status

# Apply branding migration
bash backend/scripts/ops/apply_supabase_migrations.sh --apply

# Verify table exists
psql $DATABASE_URL -c "\d tenant_branding"
# Expected: Table with tenant_id, logo_url, primary_color, accent_color, etc.
```

### Verify API Endpoints (HOST-SERVER-TERMINAL or WEB-BROWSER)

**Get Branding (defaults if no custom config):**
```bash
export API="https://api.fewo.kolibri-visions.de"
export TOKEN="your-jwt-token"

curl -L -H "Authorization: Bearer $TOKEN" \
  "$API/api/v1/branding" | python3 -m json.tool

# Expected: HTTP 200 with tokens object (primary, accent, background, surface, text, etc.)
```

**Update Branding (admin/manager only):**
```bash
curl -L -X PUT "$API/api/v1/branding" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "primary_color": "#4F46E5",
    "accent_color": "#10B981",
    "logo_url": "https://example.com/logo.png"
  }' | python3 -m json.tool

# Expected: HTTP 200 with updated tokens
```

### Verify UI (WEB-BROWSER)

**1. Default Branding:**
- Open Admin UI (fresh tenant with no branding set)
- Expected: Default indigo primary, emerald accent, no logo

**2. Set Custom Branding:**
- Log in as admin
- Navigate to Branding Settings (if UI implemented) OR use API via curl
- Set logo_url, primary_color, accent_color
- Save changes

**3. Verify Theme Application:**
- Refresh admin UI page
- Expected:
  - Logo appears in sidebar/header (if UI implemented)
  - Primary color applied to buttons, links
  - Accent color applied to success badges, highlights
  - Theme persists across page navigations

**4. Light/Dark Mode Toggle:**
- Toggle OS/browser dark mode
- Expected:
  - Background/surface colors invert
  - Text colors invert (dark text in light mode, light text in dark mode)
  - Primary/accent colors remain consistent
  - Search inputs NOT inverted (proper contrast in both modes)

### Troubleshooting

**Issue:** GET /api/v1/branding returns 503

**Solution:**
- Migration not applied: Run migration script
- RLS policy blocking: Check user's agency_id matches tenant_id

**Issue:** PUT /api/v1/branding returns 403

**Solution:**
- User is not admin or manager
- Check role via: `SELECT role FROM users WHERE id = auth.uid()`

**Issue:** Theme not applying in UI

**Solution:**
- Frontend not fetching branding on load (check browser DevTools network tab)
- CSS variables not injected (check :root styles in DevTools elements tab)
- Cache issue: Hard refresh (Cmd+Shift+R / Ctrl+Shift+R)

**Issue:** Migration pending but table already exists (ERROR: idx_tenant_branding_tenant already exists)

**Symptom:**
- Table `tenant_branding` exists
- Index `idx_tenant_branding_tenant` exists
- Policies missing (Policies: (none))
- Migration `20260103150000_create_tenant_branding.sql` not tracked in `pms_schema_migrations`

**Solution:**
- Partial apply state detected (table + index created but policies failed)
- Migration has been patched for full idempotency (v2026-01-03)
- Re-run migration via: `bash backend/scripts/ops/apply_supabase_migrations.sh --apply`
- DO NOT manually INSERT into `pms_schema_migrations` (runner tracks automatically on success)
- Verify completion: `bash backend/scripts/ops/apply_supabase_migrations.sh --status` (should show 0 pending)
- Verify policies: `\d+ tenant_branding` in psql (should show 3 policies: select, insert, update)

**Issue:** Migration fails with "relation public.users does not exist"

**Symptom:**
- Migration apply fails with ERROR: relation "public.users" does not exist
- Context: CREATE POLICY tenant_branding_select ... SELECT agency_id FROM public.users WHERE id = auth.uid()
- Policies not created, migration not tracked

**Solution:**
- Policy SQL referenced non-existent table (early version used public.users instead of JWT claims)
- Migration patched (2026-01-03) to use JWT-based pattern: `auth.jwt() ->> 'agency_id'` and `auth.jwt() ->> 'role'`
- Pull latest migration: `git pull origin main`
- Re-run migration: `bash backend/scripts/ops/apply_supabase_migrations.sh --apply`
- Verify policies installed:
  ```bash
  psql $DATABASE_URL -c "\d+ tenant_branding"
  # Should show: Policies: tenant_branding_select, tenant_branding_insert, tenant_branding_update
  ```
- Verify migration tracked:
  ```bash
  psql $DATABASE_URL -c "SELECT filename, applied_at FROM public.pms_schema_migrations WHERE filename LIKE '%tenant_branding%' ORDER BY applied_at DESC LIMIT 1;"
  # Should show: 20260103150000_create_tenant_branding.sql with recent timestamp
  ```

**Issue:** GET /api/v1/branding returns 404 while MODULES_ENABLED=true

**Symptom:**
- Migration applied successfully, policies exist, but API endpoint unreachable
- openapi.json does not contain branding paths
- Logs show: "Mounting N module(s): ['core_pms', 'inventory', 'properties', 'bookings', 'channel_manager']" (branding missing)

**Cause:**
- Branding router not part of module system, only mounted in fallback (MODULES_ENABLED=false)
- Module system skips non-registered modules

**Solution:**
- Branding module now registered in module system (backend/app/modules/branding.py)
- Auto-imported in bootstrap.py for self-registration

**Issue:** GET /api/v1/guests returns 404 while MODULES_ENABLED=true

**Symptom:**
- Guests table exists, migrations applied, but API endpoints unreachable
- openapi.json does not contain /api/v1/guests* paths
- Logs show mounted modules list without 'guests' entry

**Cause:**
- Guests router not part of module system (module registration missing)
- Module system skips non-registered modules when MODULES_ENABLED=true

**Solution:**
- Guests module now registered in module system (backend/app/modules/guests.py)
- Auto-imported in bootstrap.py for self-registration
- Enabled by default (no env var required)

**Verification:**
```bash
# Check startup logs for guests module
docker logs pms-backend --tail 100 | grep -i "guests"
# Expected: "Module 'guests' (v1.0.0, 1 router(s))"

# Verify openapi.json contains guests paths
curl http://localhost:8000/openapi.json | jq '.paths | keys | map(select(startswith("/api/v1/guests")))'
# Expected: ["/api/v1/guests", "/api/v1/guests/{guest_id}", "/api/v1/guests/{guest_id}/timeline"]
```

**Detailed Verification (CONTAINER - prove routes exist):**
```bash
# Execute inside container to verify routes are registered
docker exec pms-backend python3 - <<'PY'
from app.main import app
routes=[]
for r in app.routes:
    p=getattr(r,"path",None)
    m=getattr(r,"methods",None)
    if p: routes.append((p,sorted(list(m)) if m else []))
guest=[(p,m) for (p,m) in routes if "guest" in p.lower()]
print("guest_routes_found=",len(guest))
for p,m in guest: print("ROUTE",p,"methods=",m)
spec=app.openapi()
paths=spec.get("paths",{}) or {}
guest_paths=[p for p in paths.keys() if "guest" in p.lower()]
print("guest_paths_found=",len(guest_paths))
for p in sorted(guest_paths): print("OPENAPI",p,"methods=",sorted((paths.get(p) or {}).keys()))
PY

# Expected output:
# guest_routes_found= 6
# ROUTE /api/v1/guests methods= ['GET']
# ROUTE /api/v1/guests/{guest_id} methods= ['GET']
# ROUTE /api/v1/guests/{guest_id} methods= ['PATCH']
# ROUTE /api/v1/guests/{guest_id} methods= ['PUT']
# ROUTE /api/v1/guests methods= ['POST']
# ROUTE /api/v1/guests/{guest_id}/timeline methods= ['GET']
# guest_paths_found= 5
# OPENAPI /api/v1/guests methods= ['get', 'post']
# OPENAPI /api/v1/guests/{guest_id} methods= ['get', 'patch', 'put']
# OPENAPI /api/v1/guests/{guest_id}/timeline methods= ['get']
```

**Detailed Verification (HOST-SERVER-TERMINAL - prove external OpenAPI):**
```bash
# Verify external OpenAPI exposes guests endpoints
API="https://api.fewo.kolibri-visions.de"
curl -k -sS "$API/openapi.json" | grep -oE '"/api/v1/guests[^"]*"' | head

# Expected output:
# "/api/v1/guests"
# "/api/v1/guests/{guest_id}"
# "/api/v1/guests/{guest_id}/timeline"
```



**Issue:** GET /api/v1/guests returns 500 with validation errors or missing fields

**Symptom:**
- Endpoint returns HTTP 500 Internal Server Error
- Logs show: "validation errors for GuestResponse" with missing fields like agency_id, updated_at
- OR logs show: "UndefinedColumn" or "column does not exist"
- OR language/vip_status/blacklisted fields contain NULL causing validation failures

**Cause:**
- Database schema drift: required columns missing or nullable when code expects non-null
- Migration 20260105120000_fix_guests_list_required_fields.sql not applied
- Columns language, vip_status, blacklisted lack defaults and contain NULLs

**Fix:**
1. Apply migration via SQL Editor:
   ```sql
   -- Run migration: supabase/migrations/20260105120000_fix_guests_list_required_fields.sql
   -- Or apply manually:

   -- Ensure updated_at exists
   ALTER TABLE public.guests
   ADD COLUMN IF NOT EXISTS updated_at timestamptz DEFAULT now() NOT NULL;

   -- Set defaults and backfill NULLs
   ALTER TABLE public.guests ALTER COLUMN language SET DEFAULT 'unknown';
   UPDATE public.guests SET language = 'unknown' WHERE language IS NULL;

   ALTER TABLE public.guests ALTER COLUMN vip_status SET DEFAULT false;
   UPDATE public.guests SET vip_status = false WHERE vip_status IS NULL;
   ALTER TABLE public.guests ALTER COLUMN vip_status SET NOT NULL;

   ALTER TABLE public.guests ALTER COLUMN blacklisted SET DEFAULT false;
   UPDATE public.guests SET blacklisted = false WHERE blacklisted IS NULL;
   ALTER TABLE public.guests ALTER COLUMN blacklisted SET NOT NULL;

   -- Backfill updated_at
   UPDATE public.guests SET updated_at = COALESCE(updated_at, created_at, now()) WHERE updated_at IS NULL;
   ```

2. Restart backend to reload schema metadata:
   ```bash
   docker restart pms-backend
   ```

**Verification (DB SQL Editor):**
```sql
-- Verify required columns exist with correct defaults
SELECT column_name, data_type, is_nullable, column_default
FROM information_schema.columns
WHERE table_schema='public' AND table_name='guests'
  AND column_name IN ('agency_id','updated_at','city','country','language','vip_status','blacklisted','source')
ORDER BY column_name;

-- Expected output:
-- agency_id    | uuid         | NO  | (not null)
-- blacklisted  | boolean      | NO  | false
-- city         | text         | YES | NULL
-- country      | text         | YES | NULL
-- language     | text         | YES | 'unknown'::text
-- source       | text         | YES | NULL
-- updated_at   | timestamptz  | NO  | now()
-- vip_status   | boolean      | NO  | false
```

**Verification (HOST-SERVER-TERMINAL):**
```bash
# Test endpoint with valid JWT
API="https://api.fewo.kolibri-visions.de"
curl -k -sS -i -L "$API/api/v1/guests?limit=1&offset=0" -H "Authorization: Bearer $JWT_TOKEN" | sed -n '1,120p'

# Expected: HTTP/1.1 200 OK (not 500)
# Expected response body: {"items":[...],"total":N,"limit":1,"offset":0}
```

**Verification (CONTAINER):**
```bash
# Verify routes and OpenAPI paths
docker exec pms-backend python3 - <<'PY'
from app.main import app
print("guest_routes=", [getattr(r,"path",None) for r in app.routes if getattr(r,"path",None) and "guest" in getattr(r,"path","").lower()])
spec=app.openapi()
paths=spec.get("paths",{}) or {}
print("guest_paths=", sorted([p for p in paths.keys() if "guest" in p.lower()]))
PY

# Expected guest_routes= ['/api/v1/guests', '/api/v1/guests/{guest_id}', '/api/v1/guests/{guest_id}/timeline', ...]
# Expected guest_paths= ['/api/v1/guests', '/api/v1/guests/{guest_id}', '/api/v1/guests/{guest_id}/timeline']
```

**Issue:** GET /api/v1/guests/{guest_id}/timeline returns 500 with UndefinedColumnError

**Symptom:**
- Endpoint returns HTTP 500 Internal Server Error
- Logs show: "asyncpg.exceptions.UndefinedColumnError: column b.check_in_date does not exist"
- Hint suggests: "Perhaps you meant b.check_in_at"

**Cause:**
- Timeline query expected old column names (check_in_date, check_out_date)
- Database schema uses check_in_at and check_out_at columns
- Code updated to use correct column names in commit that fixes this issue

**Fix:**
- Deploy latest code (includes corrected timeline query using check_in_at/check_out_at)
- Restart backend:
  ```bash
  docker restart pms-backend
  ```

**Verification (HOST-SERVER-TERMINAL):**
```bash
# Test timeline endpoint with valid guest ID and JWT
API="https://api.fewo.kolibri-visions.de"
GID="1e9dd87c-ba39-4ec5-844e-e4c66e1f4dc1"
curl -k -sS -i -L "$API/api/v1/guests/$GID/timeline?limit=5&offset=0" \
  -H "Authorization: Bearer $JWT_TOKEN" | sed -n '1,160p'

# Expected: HTTP/1.1 200 OK (empty bookings list is fine)
# Expected response: {"guest_id":"...","guest_name":"...","bookings":[...],"total":N,"limit":5,"offset":0}
```

**Verification (DB SQL Editor):**
```sql
-- Verify bookings table has correct column names
SELECT column_name, data_type
FROM information_schema.columns
WHERE table_schema='public' AND table_name='bookings'
  AND column_name IN ('check_in_at','check_out_at','check_in_date','check_out_date')
ORDER BY column_name;

-- Expected: check_in_at and check_out_at exist (not check_in_date/check_out_date)
```

**Issue:** GET /api/v1/guests/{guest_id}/timeline returns 500 with response validation error

**Symptom:**
- Endpoint returns HTTP 500 Internal Server Error
- Logs show: "ValidationError" for GuestTimelineResponse
- Missing required fields: check_in_date and check_out_date in bookings array
- Timeline dict contains check_in_at and check_out_at instead

**Cause:**
- Response schema expects check_in_date and check_out_date (DATE fields)
- Timeline query was returning check_in_at and check_out_at (timestamp fields)
- Field name mismatch between service layer and response schema

**Fix:**
- Deploy latest code (timeline query now casts timestamps to date with correct aliases)
- Query uses: `b.check_in_at::date as check_in_date, b.check_out_at::date as check_out_date`
- Restart backend:
  ```bash
  docker restart pms-backend
  ```

**Verification (HOST-SERVER-TERMINAL):**
```bash
# Test timeline endpoint with valid guest ID and JWT
API="https://api.fewo.kolibri-visions.de"
GID="<guest_uuid>"
curl -k -sS -i -L "$API/api/v1/guests/$GID/timeline?limit=5&offset=0" \
  -H "Authorization: Bearer $JWT_TOKEN" | sed -n '1,200p'

# Expected: HTTP/1.1 200 OK
# Expected response: {"guest_id":"...","guest_name":"...","bookings":[{"check_in_date":"2024-01-15","check_out_date":"2024-01-20",...}],...}
```

**Issue:** GET /api/v1/guests/{guest_id}/timeline returns 500 with null date fields

**Symptom:**
- Endpoint returns HTTP 500 Internal Server Error
- Logs show: "ValidationError" or "Timeline response validation failed"
- Error mentions check_in_date or check_out_date fields are null
- Booking records exist but have NULL check_in_at or check_out_at timestamps

**Cause:**
- Response schema previously required non-null date fields
- Legacy or incomplete booking records may have NULL check_in_at or check_out_at
- Query casts NULL timestamps to NULL dates, causing validation error

**Fix:**
- Deploy latest code (response schema now allows nullable date fields)
- check_in_date and check_out_date fields are now Optional (may be null)
- Restart backend:
  ```bash
  docker restart pms-backend
  ```

**Verification (HOST-SERVER-TERMINAL):**
```bash
# Test timeline endpoint with valid guest ID and JWT
API="https://api.fewo.kolibri-visions.de"
GID="<guest_uuid>"
curl -k -sS -i -L "$API/api/v1/guests/$GID/timeline?limit=5&offset=0" \
  -H "Authorization: Bearer $JWT_TOKEN" | sed -n '1,200p'

# Expected: HTTP/1.1 200 OK (even if some bookings have null dates)
# Expected response: {"guest_id":"...","guest_name":"...","bookings":[{"check_in_date":null,"check_out_date":null,...}],...}
```

**Verification (DB SQL Editor - Find bookings with null dates):**
```sql
-- Find bookings with NULL check_in_at or check_out_at for a guest
SELECT id, status, created_at, check_in_at, check_out_at
FROM bookings
WHERE guest_id = '<guest_uuid>' AND agency_id = '<agency_uuid>'
ORDER BY created_at DESC;

-- Expected: Some rows may show NULL in check_in_at or check_out_at columns
```

**Optional Data Cleanup:**
If you need to backfill missing dates, update rows with appropriate values based on business logic. Do not use arbitrary fake dates without verifying business requirements first.

**Issue:** POST /api/v1/guests returns 500 with UndefinedColumnError for auth_user_id

**Symptom:**
- Endpoint returns HTTP 500 Internal Server Error
- Logs show: "asyncpg.exceptions.UndefinedColumnError: column 'auth_user_id' does not exist"

**Cause:**
- guests.auth_user_id column missing from database schema
- Migration 20260105130000_add_guests_auth_user_id.sql not applied

**Fix:**
1. Apply migration via SQL Editor:
   ```sql
   -- Run migration: supabase/migrations/20260105130000_add_guests_auth_user_id.sql
   -- Or apply manually:

   ALTER TABLE public.guests
   ADD COLUMN IF NOT EXISTS auth_user_id uuid;

   COMMENT ON COLUMN public.guests.auth_user_id IS 'Optional link to authenticated user account (for guest portal access)';

   CREATE INDEX IF NOT EXISTS idx_guests_auth_user_id
   ON public.guests(auth_user_id)
   WHERE auth_user_id IS NOT NULL;
   ```

2. Restart backend to reload schema metadata:
   ```bash
   docker restart pms-backend
   ```

**Verification (DB SQL Editor):**
```sql
-- Verify auth_user_id column exists
SELECT column_name, data_type, is_nullable
FROM information_schema.columns
WHERE table_schema='public' AND table_name='guests'
  AND column_name='auth_user_id';

-- Expected output:
-- auth_user_id | uuid | YES
```

**Verification (HOST-SERVER-TERMINAL):**
```bash
# Run full CRUD smoke test including POST
cd /data/repos/pms-webapp
export API_BASE_URL="https://api.fewo.kolibri-visions.de"
export GUESTS_CRUD_TEST=true
./backend/scripts/pms_guests_smoke.sh

# Expected: All tests pass including "POST /api/v1/guests"
```

**Issue:** Guests endpoints return 503 with missing column errors

**Symptom:**
- Endpoints return HTTP 503 Service Unavailable
- Logs show: "asyncpg.exceptions.UndefinedColumnError: column does not exist"
- Missing columns: address_line1, address_line2, marketing_consent, marketing_consent_at, profile_notes, blacklist_reason

**Cause:**
- Database schema drift: optional guest profile columns not created
- Migration 20260105140000_guests_missing_columns.sql not applied
- Fresh database installations or schema restores missing these columns

**Fix:**
1. Apply migration via SQL Editor:
   ```sql
   -- Run migration: supabase/migrations/20260105140000_guests_missing_columns.sql
   -- Or apply manually:

   ALTER TABLE public.guests
   ADD COLUMN IF NOT EXISTS address_line1 text;

   ALTER TABLE public.guests
   ADD COLUMN IF NOT EXISTS address_line2 text;

   ALTER TABLE public.guests
   ADD COLUMN IF NOT EXISTS marketing_consent boolean NOT NULL DEFAULT false;

   ALTER TABLE public.guests
   ADD COLUMN IF NOT EXISTS marketing_consent_at timestamptz;

   ALTER TABLE public.guests
   ADD COLUMN IF NOT EXISTS profile_notes text;

   ALTER TABLE public.guests
   ADD COLUMN IF NOT EXISTS blacklist_reason text;
   ```

2. Restart backend to reload schema metadata:
   ```bash
   docker restart pms-backend
   ```

**Verification (DB SQL Editor):**
```sql
-- Verify all optional profile columns exist
SELECT column_name, data_type, is_nullable, column_default
FROM information_schema.columns
WHERE table_schema='public' AND table_name='guests'
  AND column_name IN ('address_line1', 'address_line2', 'marketing_consent', 'marketing_consent_at', 'profile_notes', 'blacklist_reason')
ORDER BY column_name;

-- Expected: 6 rows returned with all columns present
```

**Verification (HOST-SERVER-TERMINAL):**
```bash
# Run full Guests CRUD smoke test
cd /data/repos/pms-webapp
export API_BASE_URL="https://api.fewo.kolibri-visions.de"
export GUESTS_CRUD_TEST=true
./backend/scripts/pms_guests_smoke.sh

# Expected: All tests pass (list, search, create, details, update, timeline)
```

- Redeploy to apply changes

**Verification:**
```bash
# Verify branding paths in OpenAPI schema
curl -s https://your-domain.com/openapi.json | grep -i branding
# Expected: "/api/v1/branding" appears

# Verify module mounting in logs (after redeploy)
# Expected: "Mounting 6 module(s): ['core_pms', 'inventory', 'properties', 'bookings', 'branding', ...]"

# Test endpoint (should return 401 without token, or 200 with valid token)
curl -i https://your-domain.com/api/v1/branding
# Expected: HTTP 401 Unauthorized (no token) or HTTP 200 OK (with Authorization header)
```

**Issue:** /api/v1/branding returns 404 and openapi.json lacks branding paths

**Symptom:**
- Migration applied successfully, table and policies exist
- openapi.json does not contain /api/v1/branding paths
- Endpoint returns 404
- Container logs show: "Branding module not available: cannot import name 'User' from 'app.core.auth'"

**Cause:**
- ImportError in branding router prevents module from loading
- Invalid import: `from app.core.auth import User` (User does not exist in app.core.auth)
- get_current_user returns dict, not User object

**Solution:**
- Fixed branding.py to remove invalid User import
- Changed type annotations to use dict instead of non-existent User class
- Changed current_user.agency_id to current_user["agency_id"] (dict access)
- Redeploy to apply changes

**Verification:**
```bash
# Check container logs for successful module mounting (after redeploy)
docker logs pms-backend 2>&1 | grep -i "mounting.*branding"
# Expected: "Mounting 6 module(s): ['core_pms', 'inventory', 'properties', 'bookings', 'branding', ...]"

# Verify openapi.json contains branding paths
curl -s https://your-domain.com/openapi.json | grep -o '"/api/v1/branding"'
# Expected: "/api/v1/branding"

# Test endpoint (should return 401 without token)
curl -i https://your-domain.com/api/v1/branding
# Expected: HTTP 401 Unauthorized (NOT 404)
```

**Issue:** Branding endpoints return 404 because module import failed (require_roles import path)

**Symptom:**
- Container logs show: "Branding module not available: cannot import name 'require_roles' from 'app.core.auth'"
- openapi.json does not contain /api/v1/branding paths
- GET /api/v1/branding returns 404

**Cause:**
- branding.py imports require_roles from wrong module (app.core.auth)
- require_roles is actually in app.api.deps (same as other routes)

**Solution:**
- Fixed import in branding.py: from app.api.deps import require_roles
- Redeploy to apply changes

**Verification:**
```bash
# Check mounted modules in container logs
docker logs pms-backend 2>&1 | grep -i "mounting.*module"
# Expected: "Mounting 6 module(s): ['core_pms', 'inventory', 'properties', 'bookings', 'branding', ...]"

# Check INTERNAL openapi has branding paths (from inside container)
docker exec pms-backend curl -s http://localhost:8000/openapi.json | grep -o '"/api/v1/branding"'
# Expected: "/api/v1/branding"

# Test external endpoint (should return 401 without token, 200 with token)
curl -i https://your-domain.com/api/v1/branding
# Expected: HTTP 401 Unauthorized (NOT 404)
```

**Verify Branding Endpoint (PROD)**

**Expected GET Response (200 OK):**
```json
{
  "tenant_id": "uuid-here",
  "logo_url": null,
  "primary_color": "#4F46E5",
  "accent_color": "#10B981",
  "font_family": "system",
  "radius_scale": "md",
  "mode": "system",
  "tokens": {
    "primary": "#4F46E5",
    "accent": "#10B981",
    "background": "#FFFFFF",
    "surface": "#F9FAFB",
    "text": "#111827",
    "text_muted": "#6B7280",
    "border": "#E5E7EB",
    "radius": "0.5rem"
  }
}
```

**Common Failure Modes:**

| Status | Cause | Solution |
|--------|-------|----------|
| 400 Bad Request | JWT missing agency_id claim | Regenerate token with proper claims |
| 401 Unauthorized | Token expired or invalid | Get fresh token |
| 403 Forbidden | Token valid but no auth header | Add Authorization: Bearer header |
| 503 Service Unavailable | DB unavailable or migration not applied | Check DB connectivity, apply migrations |

**Smoke Test (HOST-SERVER-TERMINAL):**
```bash
# Set environment variables
export API_BASE_URL="https://your-domain.com"
export JWT_TOKEN="eyJhbGc..."  # Valid token with agency_id claim

# Run smoke test
bash backend/scripts/pms_branding_smoke.sh

# Expected output:
# ✅ GET /api/v1/branding: SUCCESS
# HTTP Status: 200
```

**Manual Verification:**
```bash
# Test GET endpoint with curl
curl -H "Authorization: Bearer $JWT_TOKEN" https://your-domain.com/api/v1/branding | jq

# Expected: HTTP 200 with defaults (if no custom branding)
# Expected: tenant_id matches user's agency_id from JWT
```

**Branding Tenant Context Resolution**

**Symptom:** GET /api/v1/branding returns 400 "Tenant context not available"

**Cause:**
- JWT token missing agency_id claim
- User belongs to multiple tenants and no tenant specified

**Solution (tenant resolution order):**
1. JWT claim agency_id (if present)
2. x-agency-id header (validated via membership check)
3. Auto-pick (if user belongs to exactly one tenant)

**Fix with x-agency-id header (HOST-SERVER-TERMINAL):**
```bash
# Using curl directly
export JWT_TOKEN="your-jwt-token"
export TENANT_ID="ffd0123a-10b6-40cd-8ad5-66eee9757ab7"

curl -H "Authorization: Bearer $JWT_TOKEN" \
     -H "x-agency-id: $TENANT_ID" \
     https://your-domain.com/api/v1/branding

# Expected: HTTP 200 with branding data
```

**Fix with smoke script:**
```bash
# EXECUTION LOCATION: HOST-SERVER-TERMINAL
export API_BASE_URL="https://your-domain.com"
export JWT_TOKEN="your-jwt-token"
export AGENCY_ID="ffd0123a-10b6-40cd-8ad5-66eee9757ab7"

bash backend/scripts/pms_branding_smoke.sh

# Expected: GET /api/v1/branding: SUCCESS (HTTP 200)
```

**Error Messages:**
- "User belongs to N tenants. Provide x-agency-id header" → Set AGENCY_ID env var
- "Not authorized for this tenant" (403) → User not member of specified tenant
- "No tenant context available" → User not assigned to any tenant (contact admin)

---

### Guest CRM API Smoke Test

**Problem:** After deployment, guests API endpoints (list, create, update, timeline) may fail due to database issues, schema mismatches, or RBAC configuration errors.

**Symptoms:**
- GET /api/v1/guests returns 500 or 503
- Search functionality returns no results
- Guest creation fails with validation errors
- Timeline endpoint returns 404 for valid guest IDs

**Diagnostic Steps:**

1. **Verify API Availability:**
   ```bash
   # EXECUTION LOCATION: HOST-SERVER-TERMINAL
   curl -I https://your-domain.com/api/v1/guests
   # Expected: HTTP 401 (auth required)
   ```

2. **Check Database Connection:**
   ```bash
   # Verify guests table exists
   docker exec pms-backend psql -U postgres -c "\d guests"
   # Expected: Table structure with columns id, agency_id, email, etc.
   ```

3. **Verify RBAC Configuration:**
   ```bash
   # Check if JWT token has correct role
   echo $JWT_TOKEN | cut -d'.' -f2 | base64 -d | jq '.role'
   # Expected: "admin", "manager", or "staff" for CRUD operations
   ```

**Common Errors:**

| Error Code | Symptom | Cause | Solution |
|------------|---------|-------|----------|
| 200 Empty List | GET /api/v1/guests returns `{"items":[],"total":0}` | No guests in database | Expected for new deployment |
| 403 Forbidden | POST /api/v1/guests fails | User has owner/accountant role | Use admin/manager/staff token |
| 404 Not Found | GET /api/v1/guests/{id} fails | Guest belongs to different agency | Verify multi-tenant isolation |
| 422 Validation | POST /api/v1/guests fails | Invalid email or phone format | Check payload validation |
| 503 Service Unavailable | All endpoints fail | Database connection lost | Check DB connectivity, restart container |

**Smoke Test (HOST-SERVER-TERMINAL):**
```bash
# Set environment variables
export API_BASE_URL="https://your-domain.com"
export JWT_TOKEN="eyJhbGc..."  # Valid token with admin/manager/staff role
export AGENCY_ID="your-tenant-uuid"  # Optional: if JWT lacks agency_id claim

# Run smoke test (read-only)
bash backend/scripts/pms_guests_smoke.sh

# Expected output:
# ✅ GET /api/v1/guests: SUCCESS
# ✅ GET /api/v1/guests?q=search: SUCCESS

# Run smoke test (full CRUD)
export GUESTS_CRUD_TEST=true
bash backend/scripts/pms_guests_smoke.sh

# Expected output:
# ✅ POST /api/v1/guests: SUCCESS
# ✅ PATCH /api/v1/guests/{id}: SUCCESS
# ✅ GET /api/v1/guests/{id}/timeline: SUCCESS
```

**Manual Verification:**
```bash
# Test list endpoint
curl -H "Authorization: Bearer $JWT_TOKEN" https://your-domain.com/api/v1/guests | jq

# Test search endpoint
curl -H "Authorization: Bearer $JWT_TOKEN" "https://your-domain.com/api/v1/guests?q=test" | jq

# Test create endpoint (requires admin/manager/staff role)
curl -X POST \
  -H "Authorization: Bearer $JWT_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"first_name":"Test","last_name":"Guest","email":"test@example.com"}' \
  https://your-domain.com/api/v1/guests | jq

# Test timeline endpoint (replace {guest_id} with actual ID)
curl -H "Authorization: Bearer $JWT_TOKEN" https://your-domain.com/api/v1/guests/{guest_id}/timeline | jq
```

**Fix Steps:**

1. **Database Connection Issues:**
   ```bash
   # Check if database is accepting connections
   docker exec pms-backend pg_isready -U postgres
   
   # Restart backend if needed
   docker restart pms-backend
   ```

2. **RBAC Issues:**
   ```bash
   # Verify token role
   echo $JWT_TOKEN | cut -d'.' -f2 | base64 -d | jq '.role'
   
   # If role is "owner" or "accountant", use a different token
   # Admin, manager, or staff role required for CRUD operations
   ```

3. **Multi-Tenant Isolation Issues:**
   ```bash
   # If JWT lacks agency_id claim, provide it explicitly
   export AGENCY_ID="your-tenant-uuid"
   bash backend/scripts/pms_guests_smoke.sh
   ```

**Validation Checklist:**
- [ ] GET /api/v1/guests returns HTTP 200 with guest list
- [ ] GET /api/v1/guests?q=search returns HTTP 200 with search results
- [ ] POST /api/v1/guests creates new guest (HTTP 201)
- [ ] PATCH /api/v1/guests/{id} updates guest (HTTP 200)
- [ ] GET /api/v1/guests/{id}/timeline returns booking history (HTTP 200)
- [ ] Cross-tenant access returns 404 (not 403 to avoid leaking existence)
- [ ] Search works across first_name, last_name, email, phone
- [ ] Pagination works correctly (limit, offset parameters)

---


**Admin Branding UI Verification**

**Purpose:** Verify branding tokens are applied in Admin UI and settings page works correctly.

**EXECUTION LOCATION:** WEB-BROWSER

**Prerequisites:**
- Admin or manager role
- Valid JWT token
- Frontend deployed and accessible
- Backend branding API working (verified via smoke script)

**Verification Steps (WEB-BROWSER):**

1. **Login and Navigate:**
   - Login to Admin UI at `https://your-domain.com/login`
   - Click "Branding" tab in navigation (admin/manager only)
   - Expected: `/settings/branding` page loads

2. **Verify CSS Variables Applied:**
   - Open browser developer tools (F12)
   - Select Elements/Inspector tab
   - Inspect `<html>` or `<body>` element
   - Check Computed Styles for CSS variables:
     ```
     --t-primary: #3b82f6 (or custom value)
     --t-accent: #8b5cf6 (or custom value)
     --t-bg: #ffffff (light) or #111827 (dark)
     --t-surface: #f9fafb (light) or #1f2937 (dark)
     --t-text: #111827 (light) or #f9fafb (dark)
     --t-radius: 0.375rem (or custom)
     ```
   - Expected: All theme variables present with correct values

3. **Verify Theme Mode:**
   - Check `<html data-theme="...">` attribute
   - Expected values: `light`, `dark`, or `system`
   - If system mode: verify auto-switches based on OS preference

4. **Test Branding Settings Form:**
   - Change Primary Color to `#4169E1` (royal blue)
   - Change Accent Color to `#32CD32` (lime green)
   - Change Mode to "Dark"
   - Click "Save Changes"
   - Expected: "Branding updated successfully!" message
   - Verify CSS variables update immediately (inspect developer tools)
   - Verify background switches to dark mode

5. **Test Form Validation:**
   - Enter invalid hex color (e.g., `#ZZZ`)
   - Try to save
   - Expected: Browser validation error or API 400 error message

6. **Test Access Control:**
   - Logout and login as non-admin/non-manager user
   - Try accessing `/settings/branding` directly
   - Expected: "Access Denied" page with diagnostics
   - Expected: "Branding settings are restricted to administrators and managers only."

**Error Scenarios and Expected UI Behavior:**

| Scenario | Expected UI Behavior |
|----------|---------------------|
| JWT lacks `agency_id` claim | Theme loads with defaults + console warning (graceful degradation) |
| User belongs to multiple tenants | Theme loads with defaults if no `x-agency-id` header |
| API returns 400 tenant error | Error toast: "Tenant context not available. Using default theme." |
| API returns 401/403 | Error toast: "Not authorized to view branding. Using default theme." |
| API returns 503 | Error toast: "Branding service temporarily unavailable. Using default theme." |
| PUT fails with 400 | Form error message: "Validation error: ..." |
| PUT fails with 403 | Form error message: "Access denied. Only admins and managers..." |
| Network error | Form error message: "Failed to update branding. Please try again." |

**Alternative Verification (HOST-SERVER-TERMINAL):**

Check branding API returns correct tokens:
```bash
# EXECUTION LOCATION: HOST-SERVER-TERMINAL
export JWT_TOKEN="your-jwt-token"
export TENANT_ID="ffd0123a-10b6-40cd-8ad5-66eee9757ab7"

curl -H "Authorization: Bearer $JWT_TOKEN" \
     -H "x-agency-id: $TENANT_ID" \
     https://your-domain.com/api/v1/branding | jq '.tokens'

# Expected output:
# {
#   "primary": "#3b82f6",
#   "primaryHover": "#2563eb",
#   "accent": "#8b5cf6",
#   ...
# }
```

**Troubleshooting:**

| Issue | Cause | Solution |
|-------|-------|----------|
| CSS variables not applied | ThemeProvider not rendering | Check browser console for errors, verify ThemeProvider in root layout |
| Form doesn't load | Auth check failed | Verify user has admin or manager role in `team_members` table |
| Save button disabled | Form validation error | Check form inputs match validation patterns |
| Theme doesn't change after save | refreshBranding() failed | Check network tab for API errors, verify PUT request succeeded |
| Dark mode not working | CSS not loaded or `data-theme` attr missing | Verify globals.css loaded, check `<html data-theme>` attribute |

**CSS Variable "undefined" String Bug (Fixed):**

**Symptom:** Browser console shows CSS variables set to string "undefined" instead of valid color values.

Example:
```
--t-primary: "#3B82F6"   ✓ correct
--t-bg: "undefined"      ✗ bug (fixed in latest release)
```

**Root Cause:**
- API response token field mismatch (API returns `background`, frontend expected `bg`)
- Missing token sanitization allowed undefined values to be stringified
- No validation before setting CSS properties

**Fix Applied:**
1. Added `normalizeTokenValue()` sanitizer: rejects undefined/null/"undefined"/"null" strings
2. Created API token mapper: `background` → `bg`, `text_muted` → `textMuted`
3. Derived missing tokens: `primaryHover`, `accentHover`, `surfaceHover`, `borderSubtle`
4. Safe CSS property setter: `applyCssVariable()` uses `removeProperty()` for null values instead of setting "undefined"

**Verification (POST-FIX):**

```bash
# EXECUTION LOCATION: WEB-BROWSER (developer tools console)

# Check CSS variables are valid hex colors (not "undefined" strings)
getComputedStyle(document.documentElement).getPropertyValue('--t-bg')
# Expected: "#ffffff" or "#111827" (valid hex)
# NOT: "undefined"

getComputedStyle(document.documentElement).getPropertyValue('--t-primary')
# Expected: "#3b82f6" or custom hex
# NOT: "undefined"
```

**Expected Result:**
- All theme variables (`--t-*`) resolve to valid CSS values
- No "undefined" or "null" strings in computed styles
- Theme applies correctly on page load and after save

**If Bug Persists:**
1. Clear browser cache and hard reload
2. Check browser console for API errors
3. Verify API response format matches expected schema
4. Check network tab: `/api/v1/branding` response should include `tokens.background` field

**Theme Mode Palette Mismatch (Fixed):**

**Symptom:** `data-theme="dark"` but CSS variables still show light palette values (white background, dark text).

Example:
```
<html data-theme="dark" ...>
--t-bg: "#ffffff"    ✗ bug (should be "#111827" for dark mode)
--t-text: "#111827"  ✗ bug (should be "#f9fafb" for dark mode)
```

**Root Cause:**
- Backend returns flat light-mode tokens only (no per-mode token sets)
- Frontend applied those light tokens to :root regardless of mode setting
- data-theme attribute changed but CSS variable values did not

**Fix Applied:**
1. Separate light and dark default palettes defined in frontend
2. `deriveDarkTokens()` function creates dark palette from light tokens (keeps primary/accent, changes bg/surface/text)
3. `getEffectiveMode()` determines active mode (light/dark/system with OS detection)
4. `applyThemeTokens()` applies correct palette based on effective mode
5. Added `data-effective-theme` attribute for debugging (shows resolved mode)

**Verification (POST-FIX):**

```bash
# EXECUTION LOCATION: WEB-BROWSER (developer tools console)

# Check data-theme and effective theme
document.documentElement.getAttribute('data-theme')
# Expected: "dark", "light", or "system"

document.documentElement.getAttribute('data-effective-theme')
# Expected: "dark" or "light" (resolved from system if mode=system)

# Check CSS variables match the effective theme
const mode = document.documentElement.getAttribute('data-effective-theme');
const bg = getComputedStyle(document.documentElement).getPropertyValue('--t-bg').trim();
const text = getComputedStyle(document.documentElement).getPropertyValue('--t-text').trim();

console.log(`Mode: ${mode}, BG: ${bg}, Text: ${text}`);
# Expected for dark mode: Mode: dark, BG: #111827 (dark gray), Text: #f9fafb (light)
# Expected for light mode: Mode: light, BG: #ffffff (white), Text: #111827 (dark)
```

**Expected Result:**
- Dark mode: bg="#111827", surface="#1f2937", text="#f9fafb"
- Light mode: bg="#ffffff", surface="#f9fafb", text="#111827"
- System mode: follows OS preference automatically

**If Bug Persists:**
1. Clear browser cache and hard reload
2. Check mode setting in branding form matches data-theme
3. Verify no CSS overrides in globals.css
4. Check browser console for theme provider errors

**Auth Client Multiple Instances Warning (Fixed):**

**Symptom:** Browser console shows "multiple auth-client instances detected in same browser context"

**Root Cause:**
- Auth client created on each function call instead of singleton
- Module-level caching didn't survive HMR (hot module reload)

**Fix Applied:**
1. Created dedicated singleton module: `auth-client-singleton.ts`
2. Uses `globalThis` to cache instance (survives HMR and page reloads)
3. `getAuthClient()` function returns same instance on all calls
4. All auth client creation refactored to use singleton

**Verification (POST-FIX):**

```bash
# EXECUTION LOCATION: WEB-BROWSER (developer tools console)

# Check for warning message (should NOT appear after fix)
# Open console and reload page
# Expected: No "multiple instances" warning
```

**Expected Result:**
- No console warnings about multiple auth-client instances
- Single client instance used throughout application
- Instance persists across HMR and page reloads

**Branding UI (Phase B) — Automated Verification (Playwright)**

**Purpose:** Automated UI smoke test to verify branding theme tokens (CSS variables) are applied in Admin UI using Playwright.

**EXECUTION LOCATION:** HOST-SERVER-TERMINAL

**Prerequisites:**
- Docker installed and running
- Admin credentials with manager/admin role
- Admin UI deployed and accessible
- Backend branding API working

**Smoke Script:** `backend/scripts/pms_branding_ui_smoke.sh`

**Note:** Script auto-installs `@playwright/test@1.57.0` inside the Docker container before running tests. No host node_modules required. First run may take 30-60 seconds for npm install; subsequent runs use Docker layer caching.

**Usage:**

```bash
# EXECUTION LOCATION: HOST-SERVER-TERMINAL

# Required environment variables
export E2E_ADMIN_EMAIL="admin@example.com"
export E2E_ADMIN_PASSWORD="your-password"

# Optional: Custom URLs (defaults to production)
export ADMIN_BASE_URL="https://admin.fewo.kolibri-visions.de"  # default
export API_BASE_URL="https://api.fewo.kolibri-visions.de"      # default

# Run smoke test
./backend/scripts/pms_branding_ui_smoke.sh

# Expected output:
# ℹ  Starting Branding UI Smoke Test (Phase B)...
# ✅ Admin credentials provided
# ✅ Docker is available and running
# ℹ  Temp directory: /tmp/tmp.XXX
# ✅ Generated Playwright test script
# ✅ Generated Playwright config
# ℹ  Running Playwright test in Docker...
#
# Running 1 test using 1 worker
# ℹ️  Test: Branding CSS variables verification...
# ℹ️  Navigating to: https://admin.fewo.kolibri-visions.de/organisation
# ✅ Already logged in (no redirect detected)
# ✅ Page loaded
# ℹ️  CSS Variables read:
#    --t-primary: #3b82f6
#    --t-accent: #8b5cf6
#    --t-bg: #ffffff
#    --t-surface: #f9fafb
#    --t-text: #111827
#    --t-border: #e5e7eb
#    --t-radius: 0.375rem
# ✅ All CSS theme tokens are non-empty
# ✅ Test passed: Branding CSS tokens verified
# ✅ Playwright test passed!
# ℹ  Screenshots saved:
# -rw-r--r--  1 user  staff  123456 Jan 15 10:00 /tmp/tmp.XXX/screenshots/branding-page-loaded.png
# -rw-r--r--  1 user  staff  123456 Jan 15 10:00 /tmp/tmp.XXX/screenshots/branding-success.png
```

**What It Tests:**
1. Login via UI form (reuses Epic A `ensureLoggedIn` pattern)
2. Navigate to `/organisation` (stable authenticated page)
3. Read CSS variables from `documentElement`:
   - `--t-primary`, `--t-accent`, `--t-bg`, `--t-surface`, `--t-text`, `--t-border`, `--t-radius`
4. Assert all CSS vars are non-empty (not empty strings)
5. Screenshot on success

**Exit Codes:**
- `0` - All tests passed (CSS tokens applied and non-empty)
- `1` - Configuration error or test failure

**Troubleshooting:**

| Issue | Cause | Solution |
|-------|-------|----------|
| "E2E_ADMIN_EMAIL required" | Missing env var | Export `E2E_ADMIN_EMAIL` before running script |
| "E2E_ADMIN_PASSWORD required" | Missing env var | Export `E2E_ADMIN_PASSWORD` before running script |
| "Docker not running" | Docker daemon not started | Start Docker Desktop or `sudo systemctl start docker` |
| Login fails | Invalid credentials | Verify credentials are correct and user has admin/manager role |
| CSS variables empty | Theme provider not rendering | Check browser console for errors, verify API `/api/v1/branding` returns tokens |
| Test timeout | Page slow to load | Increase timeout or check network/server performance |

**Note:** This script is READ-ONLY and PROD-safe. It only reads CSS variables and takes screenshots; no data is created or modified.

---

## Redis + Celery Worker Setup (Channel Manager)

**Purpose:** Configure Redis and Celery worker for Channel Manager background sync operations.

### Background / Symptoms

If Redis or Celery worker are not properly configured, you may encounter these issues:

**Redis Connection Failures:**
- `/health/ready` endpoint shows `redis: down` with error:
  ```
  "Authentication required."
  "invalid username-password pair or user is disabled."
  ```

**Celery Worker Issues:**
- `/health/ready` shows `celery: down` with error:
  ```
  "Celery inspect timeout (broker may be unreachable)"
  "No active Celery workers detected"
  ```

**Channel Manager Sync Failures:**
- Channel sync endpoints return connection refused
- Celery tasks fail with "Broker connection error"
- Background jobs (Airbnb/Booking.com sync) do not execute

---

### Required Coolify Resources

To enable Channel Manager background processing, you need:

#### 1. Redis Service

**Service Name:** `coolify-redis` (or your chosen name)

**Configuration:**
- Type: Redis
- Enable authentication: **YES**
- Set `requirepass` in Redis config or via environment variable
- Network: Must be on same Docker network as backend/worker

**How to Deploy in Coolify:**
1. Go to: **Services > Add New Service > Redis**
2. Set service name: `coolify-redis`
3. Set Redis password in configuration
4. Deploy and note the password for next steps

#### 2. PMS Backend App

**App Name:** `pms-backend` (already exists)

**Purpose:** Main FastAPI application serving HTTP API

**Configuration:**
- Already deployed
- Will connect to Redis for health checks
- Will trigger Celery tasks via broker

#### 3. PMS Worker App (NEW)

**App Name:** `pms-worker`

**Purpose:** Celery worker process for background jobs (sync operations)

**Configuration:**
- Type: Git-based Application
- Repository: Same as pms-backend
- Branch: Same as pms-backend (usually `main`)
- Base Directory: `/backend`
- Build Pack: Nixpacks
- Start Command: See [Worker Start Command](#worker-start-command) section below

**Important:**
- Worker does NOT need public domain/Traefik proxy
- Worker does NOT serve HTTP traffic (background processing only)
- Coolify requires "Ports Exposes" field: set to `8000` (harmless value, not actually used)

---

### Required Environment Variables

The worker app must have **identical configuration** to the backend for task execution consistency.

**Copy ALL of these environment variables from `pms-backend` to `pms-worker`:**

#### Core Application
```
DATABASE_URL
ENCRYPTION_KEY
JWT_SECRET
SUPABASE_JWT_SECRET
JWT_AUDIENCE
```

#### Module Feature Flags
```
CHANNEL_MANAGER_ENABLED=true
MODULES_ENABLED=true
```

#### Redis & Celery
```
REDIS_URL
CELERY_BROKER_URL
CELERY_RESULT_BACKEND
```

#### Health Checks
```
ENABLE_REDIS_HEALTHCHECK=true
ENABLE_CELERY_HEALTHCHECK=true
```

#### Optional (if used in backend)
```
SUPABASE_URL
SUPABASE_SERVICE_ROLE_KEY
SENTRY_DSN
LOG_LEVEL
ENVIRONMENT
```

**Why copy all variables?**
- Worker executes the same application code as backend
- Tasks may need database access, encryption, JWT validation, etc.
- Missing variables cause cryptic task failures

**Important Notes:**
- Worker should NOT have public domains configured
- Worker does NOT need port exposure (but Coolify may require "Ports Exposes" field - use `8000`)
- Worker and backend must share the same Redis/Celery configuration

---

### Redis URL Format + Password Encoding

#### Redis URL Structure

Redis with authentication uses this format:
```
redis://:<PASSWORD>@<HOST>:<PORT>/<DB>
```

**Example:**
```bash
redis://:my_secure_password_123@coolify-redis:6379/0
```

**Components:**
- `<PASSWORD>`: Redis requirepass value
- `<HOST>`: Redis service name (e.g., `coolify-redis`)
- `<PORT>`: Redis port (usually `6379`)
- `<DB>`: Redis database number (usually `0`)

#### Special Characters MUST Be URL-Encoded

**⚠️ CRITICAL: Password Encoding Required**

If your Redis password contains special characters (`+`, `=`, `@`, `:`, `/`, `?`, `#`, `&`, `%`), you **MUST** URL-encode the password in environment variables.

**Why?**
- Special characters have meaning in URLs
- `+` is interpreted as space if not encoded
- `@` and `:` are URL delimiters
- Unencoded passwords cause "Authentication required" or "invalid username-password pair" errors

#### How to URL-Encode Password

**Location:** HOST-SERVER-TERMINAL (your local machine or SSH to host server)

**Method 1: Using Python (recommended)**
```bash
python3 - <<'PY'
import urllib.parse
# Replace PASTE_PASSWORD_HERE with your actual Redis password
password = "PASTE_PASSWORD_HERE"
encoded = urllib.parse.quote(password, safe="")
print(f"Encoded password: {encoded}")
PY
```

**Example:**
```bash
# Original password: my+pass=word@123
# Encoded password: my%2Bpass%3Dword%40123
```

**Method 2: Using Node.js**
```bash
node -e "console.log(encodeURIComponent('PASTE_PASSWORD_HERE'))"
```

**Method 3: Online tool** (not recommended for production secrets)
- Use https://www.urlencoder.org/ (avoid for sensitive passwords)

#### Setting Encoded Password in Environment Variables

**Example with encoded password:**
```bash
# Original password: complex+pass=word
# Encoded password: complex%2Bpass%3Dword

REDIS_URL=redis://:complex%2Bpass%3Dword@coolify-redis:6379/0
CELERY_BROKER_URL=redis://:complex%2Bpass%3Dword@coolify-redis:6379/0
CELERY_RESULT_BACKEND=redis://:complex%2Bpass%3Dword@coolify-redis:6379/0
```

**Where to set:**
1. Coolify Dashboard → `pms-backend` → Environment Variables
2. Coolify Dashboard → `pms-worker` → Environment Variables
3. **Both apps must have identical Redis URLs**

---

### Worker Start Command

**NOTE:** This section applies to **Nixpacks build pack** deployments. If using **Dockerfile.worker** (recommended), the Start Command is **automatic** and cannot be set in Coolify UI. See [Alternative: Build with Dockerfile.worker](#alternative-build-with-dockerfileworker-recommended-for-non-root) for Dockerfile deployments.

**Location:** Coolify Dashboard → `pms-worker` → Settings → Start Command

**Command (Nixpacks only):**
```bash
celery -A app.channel_manager.core.sync_engine:celery_app --broker "$CELERY_BROKER_URL" --result-backend "$CELERY_RESULT_BACKEND" worker -l INFO
```

**Breakdown:**
- `-A app.channel_manager.core.sync_engine:celery_app`: Celery app module path
- `--broker "$CELERY_BROKER_URL"`: Redis broker URL (from environment)
- `--result-backend "$CELERY_RESULT_BACKEND"`: Redis result backend (from environment)
- `worker`: Run as worker process
- `-l INFO`: Log level (INFO for production, DEBUG for troubleshooting)

**Alternative log levels:**
- `-l DEBUG`: Verbose logging (troubleshooting)
- `-l WARNING`: Minimal logging (production)
- `-l ERROR`: Only errors

**Why use Dockerfile.worker instead?**
- Runs as non-root user (no SecurityWarning)
- Includes wait-for-deps preflight (prevents DNS failures)
- Configurable via environment variables
- Start Command handled automatically by Dockerfile CMD

---

### Deployment Steps

#### Step 1: Verify Redis Password

**Location:** HOST-SERVER-TERMINAL

Get Redis password from Coolify Redis service configuration or container:

```bash
# Option A: Check Redis container command/env
docker inspect coolify-redis | grep -i requirepass

# Option B: Check Redis config inside container
docker exec -it coolify-redis cat /etc/redis/redis.conf | grep requirepass

# Note the password - you'll need it for URL encoding
```

#### Step 2: URL-Encode Password

**Location:** HOST-SERVER-TERMINAL

```bash
python3 - <<'PY'
import urllib.parse
# Replace with your actual Redis password
password = "YOUR_REDIS_PASSWORD_HERE"
encoded = urllib.parse.quote(password, safe="")
print(f"Original: {password}")
print(f"Encoded:  {encoded}")
print(f"\nRedis URL: redis://:{encoded}@coolify-redis:6379/0")
PY
```

Copy the encoded password for next steps.

#### Step 3: Test Redis Connection

**Location:** HOST-SERVER-TERMINAL

```bash
# Test with raw password (before encoding)
redis-cli -h coolify-redis -a 'YOUR_RAW_PASSWORD' ping
# Expected output: PONG

# If you get "Authentication required" or "invalid username-password pair":
# - Password is wrong
# - Redis requirepass is not set
# - Network connectivity issue
```

#### Step 4: Configure pms-backend Environment

**Location:** Coolify Dashboard → pms-backend → Environment Variables

Add or update these variables with your encoded password:

```bash
REDIS_URL=redis://:YOUR_ENCODED_PASSWORD@coolify-redis:6379/0
CELERY_BROKER_URL=redis://:YOUR_ENCODED_PASSWORD@coolify-redis:6379/0
CELERY_RESULT_BACKEND=redis://:YOUR_ENCODED_PASSWORD@coolify-redis:6379/0
ENABLE_REDIS_HEALTHCHECK=true
ENABLE_CELERY_HEALTHCHECK=true
CHANNEL_MANAGER_ENABLED=true
```

Click **Save** and **Restart** pms-backend.

#### Step 5: Create pms-worker App

**Location:** Coolify Dashboard

1. Click **Add New Resource > Application**
2. Select **Git Repository**
3. Configure:
   - Repository: Same as pms-backend
   - Branch: Same as pms-backend
   - Base Directory: `/backend`
   - Build Pack: Nixpacks
   - Start Command: (see [Worker Start Command](#worker-start-command))
   - Ports Exposes: `8000` (required by Coolify, not actually used)
4. **Do NOT configure public domain** (worker doesn't serve HTTP)

#### Step 6: Configure pms-worker Environment

**Location:** Coolify Dashboard → pms-worker → Environment Variables

**Copy ALL environment variables from pms-backend**, especially:

```bash
DATABASE_URL=<same as backend>
ENCRYPTION_KEY=<same as backend>
JWT_SECRET=<same as backend>
SUPABASE_JWT_SECRET=<same as backend>
JWT_AUDIENCE=<same as backend>

REDIS_URL=redis://:YOUR_ENCODED_PASSWORD@coolify-redis:6379/0
CELERY_BROKER_URL=redis://:YOUR_ENCODED_PASSWORD@coolify-redis:6379/0
CELERY_RESULT_BACKEND=redis://:YOUR_ENCODED_PASSWORD@coolify-redis:6379/0

ENABLE_REDIS_HEALTHCHECK=true
ENABLE_CELERY_HEALTHCHECK=true
CHANNEL_MANAGER_ENABLED=true
MODULES_ENABLED=true

# Optional (if used)
SUPABASE_URL=<same as backend>
SUPABASE_SERVICE_ROLE_KEY=<same as backend>
SENTRY_DSN=<same as backend>
LOG_LEVEL=INFO
ENVIRONMENT=production
```

Click **Save** and **Deploy**.

#### Step 7: Verify Deployment

Wait for both apps to deploy, then proceed to [Verification Steps](#verification-steps).

---

### Verification Steps

#### 1. Check /health/ready Endpoint

**Location:** Browser or curl from HOST-SERVER-TERMINAL

```bash
curl -s https://api.fewo.kolibri-visions.de/health/ready | jq .
```

**Expected output:**
```json
{
  "status": "ready",
  "checks": {
    "database": "up",
    "redis": "up",
    "celery": "up"
  },
  "celery_workers": [
    "celery@<worker-hostname>"
  ]
}
```

**If redis shows "down":**
- Check Redis URL encoding
- Verify Redis password matches
- Check network connectivity

**If celery shows "down":**
- Worker not running
- Worker cannot connect to Redis
- Wrong start command

#### 2. Check Worker Logs

**Location:** Coolify Dashboard → pms-worker → Logs

**Look for:**
```
[INFO] Connected to redis://coolify-redis:6379/0
[INFO] celery@<hostname> ready.
[INFO] Tasks: [...list of registered tasks...]
```

**Red flags:**
```
Authentication required
Connection refused
invalid username-password pair
Cannot connect to redis
```

#### 3. Test Celery Connection from Backend

**Location:** Coolify Terminal (pms-backend container)

```bash
# Ping Celery workers
celery -A app.channel_manager.core.sync_engine:celery_app \
  --broker "$CELERY_BROKER_URL" \
  inspect ping -t 3

# Expected output:
# -> celery@<worker-hostname>: {'ok': 'pong'}
```

**If timeout:**
- Worker not running
- Worker on different network
- Redis broker unreachable

#### 4. Verify Redis Connection from Backend

**Location:** Coolify Terminal (pms-backend container)

```bash
# Test Redis connection (masked password)
python3 - <<'PY'
import os
import redis
from urllib.parse import urlparse

redis_url = os.environ.get("REDIS_URL", "")
parsed = urlparse(redis_url)

# Mask password for logging
masked_url = redis_url.replace(parsed.password or "", "***") if parsed.password else redis_url
print(f"Testing Redis connection to: {masked_url}")

try:
    r = redis.from_url(redis_url)
    result = r.ping()
    print(f"✓ Redis PING successful: {result}")
    print(f"✓ Password length: {len(parsed.password or '')}")
except Exception as e:
    print(f"✗ Redis connection failed: {e}")
PY
```

**Expected output:**
```
Testing Redis connection to: redis://:***@coolify-redis:6379/0
✓ Redis PING successful: True
✓ Password length: 16
```

#### 5. Test Channel Sync Endpoint

**Location:** Browser or curl

```bash
# Trigger a manual sync (requires valid channel connection ID)
curl -X POST https://api.fewo.kolibri-visions.de/api/v1/channel-connections/{id}/sync \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"sync_type": "full"}'
```

**Expected:**
- HTTP 200 OK
- Returns task IDs
- Check worker logs for task execution

---

### Troubleshooting

#### Redis Authentication Errors

**Symptom:**
```
Authentication required.
invalid username-password pair or user is disabled.
```

**Causes & Solutions:**

1. **Password not URL-encoded**
   - Solution: URL-encode password (see [Password Encoding](#how-to-url-encode-password))
   - Example: `pass+word` → `pass%2Bword`

2. **Password mismatch**
   - Solution: Verify Redis requirepass:
     ```bash
     docker exec -it coolify-redis redis-cli CONFIG GET requirepass
     ```
   - Ensure encoded password matches requirepass

3. **Wrong Redis URL format**
   - Correct: `redis://:password@host:6379/0`
   - Wrong: `redis://password@host:6379/0` (missing colon)
   - Wrong: `redis://user:password@host:6379/0` (Redis usually no username)

#### Celery Worker Not Detected

**Symptom:**
```
No active Celery workers detected
Celery inspect timeout
```

**Causes & Solutions:**

1. **Worker not running**
   - Check Coolify: pms-worker app status
   - View logs: Look for "ready" message
   - Restart worker app

2. **Wrong start command**
   - Verify Start Command in Coolify matches [Worker Start Command](#worker-start-command)
   - Check for typos in module path

3. **Worker cannot reach Redis**
   - Verify CELERY_BROKER_URL is correct
   - Check network: worker and Redis on same Docker network
   - Test Redis connection from worker container

4. **Environment variables missing**
   - Ensure worker has all required env vars
   - Compare with backend environment variables

#### Database Connection Issues

**Symptom:**
```
Connection refused to database
Database name resolution failed
```

**Causes & Solutions:**

1. **DNS flapping after deploy**
   - Wait 30-60 seconds after deployment
   - DNS resolution can be temporarily unstable
   - Check again after services stabilize

2. **Network misconfiguration**
   - Worker must be on Coolify network AND Supabase network
   - Verify network configuration in Coolify
   - Check docker network ls on host

3. **Wrong DATABASE_URL**
   - Verify worker has correct DATABASE_URL
   - Must match backend exactly
   - Test connection from worker container:
     ```bash
     python3 -c "import asyncpg; import asyncio; asyncio.run(asyncpg.connect(os.environ['DATABASE_URL']).close())"
     ```

4. **Worker database operations fail**
   - **Symptom:** Worker logs show `"Database is temporarily unavailable"` or `"Failed to update sync log ... 503"`
   - **Cause:** Celery workers must NOT rely on FastAPI lifespan pool (pool doesn't exist in worker context)
   - **Solution (Current Architecture):** Workers use direct `asyncpg.connect()` connections
     - `_check_database_availability()`: Creates short-lived connection with 5s timeout for health checks
     - `_update_log_status()`: Creates connection per sync log update with JSON/JSONB codec registration
     - All connections are properly closed in finally blocks (fork/event-loop safe)
   - **Verification checklist:**
     a) Worker is connected to supabase network (check Coolify network config)
     b) DATABASE_URL environment variable is set correctly in worker
     c) Worker has latest code with direct connection architecture
   - **Test from worker container:**
     ```bash
     docker exec pms-worker python3 -c "import asyncpg; import asyncio; import os; asyncio.run(asyncpg.connect(os.environ['DATABASE_URL']).close()); print('DB OK')"
     ```

#### Worker Logs Show Task Failures

**Symptom:**
```
Task failed: KeyError, AttributeError, etc.
```

**Causes & Solutions:**

1. **Missing environment variables**
   - Worker needs same env as backend
   - Check for missing: ENCRYPTION_KEY, JWT_SECRET, etc.

2. **Code version mismatch**
   - Worker and backend must be on same git commit
   - Redeploy both to sync versions

3. **Database schema mismatch**
   - Run migrations if needed
   - Ensure worker has latest schema

#### Password Special Characters Reference

**Characters requiring URL encoding:**

| Character | URL Encoded | Example |
|-----------|-------------|---------|
| `+` | `%2B` | `pass+word` → `pass%2Bword` |
| `=` | `%3D` | `pass=word` → `pass%3Dword` |
| `@` | `%40` | `pass@word` → `pass%40word` |
| `:` | `%3A` | `pass:word` → `pass%3Aword` |
| `/` | `%2F` | `pass/word` → `pass%2Fword` |
| `?` | `%3F` | `pass?word` → `pass%3Fword` |
| `#` | `%23` | `pass#word` → `pass%23word` |
| `&` | `%26` | `pass&word` → `pass%26word` |
| `%` | `%25` | `pass%word` → `pass%25word` |
| Space | `%20` | `pass word` → `pass%20word` |

**Tool to check encoding:**
```bash
python3 -c "import urllib.parse; print(urllib.parse.quote('YOUR_PASSWORD', safe=''))"
```

---

### Quick Smoke (5 minutes)

**Purpose:** Rapid health check after Redis/Celery deployment or configuration changes.

**Prerequisites:**

Check in **Coolify UI**:
- ✅ `pms-backend` deployed and running (green status)
- ✅ `pms-worker` deployed and running (green status)
- ✅ `coolify-redis` service running
- ✅ Required environment variables set (see [Required Environment Variables](#required-environment-variables))

**Execution Location:** HOST-SERVER-TERMINAL (SSH to host server)

#### Step 1: Load Environment

```bash
# Load environment file
source /root/pms_env.sh

# Verify required variables are set
echo "SB_URL: ${SB_URL:0:30}..."
echo "EMAIL: $EMAIL"
```

#### Step 2: Get JWT Token

```bash
# Fetch JWT token from Supabase auth
TOKEN="$(curl -sS "$SB_URL/auth/v1/token?grant_type=password" \
  -H "apikey: $ANON_KEY" \
  -H "Content-Type: application/json" \
  -d "{\"email\":\"$EMAIL\",\"password\":\"$PASSWORD\"}" \
  | python3 -c 'import sys,json; print(json.load(sys.stdin).get("access_token",""))')"

# Verify token (without printing secret)
echo "TOKEN len=${#TOKEN} parts=$(awk -F. '{print NF}' <<<"$TOKEN")"
# Expected: len > 100, parts = 3 (JWT format: header.payload.signature)
```

**Expected output:**
```
TOKEN len=857 parts=3
```

**If token fetch fails:**
- Check Supabase URL: `curl -s "$SB_URL/health" | head -c 50`
- Verify credentials in `/root/pms_env.sh`
- Check network connectivity to Supabase

#### Step 3: Check Health Endpoint

```bash
# Check /health/ready endpoint
curl -k -sS https://api.fewo.kolibri-visions.de/health/ready
```

**Expected output:**
```json
{
  "status": "ready",
  "checks": {
    "database": "up",
    "redis": "up",
    "celery": "up"
  },
  "celery_workers": [
    "celery@pms-worker-abc123"
  ]
}
```

**If redis shows "down":**
- Check Redis URL encoding (see [Password Encoding](#redis-url-format--password-encoding))
- Verify Redis service is running: `docker ps | grep redis`

**If celery shows "down":**
- Check worker is running: `docker ps | grep worker`
- View worker logs: Coolify UI → pms-worker → Logs

**If database shows "down":**
- Check DATABASE_URL in backend environment
- Verify Supabase database is running

#### Step 4: Get Channel Connection ID

```bash
# Fetch first channel connection ID
# NOTE: Use -L to follow redirects (trailing slash redirect)
CID="$(curl -k -sS -L "https://api.fewo.kolibri-visions.de/api/v1/channel-connections/" \
  -H "Authorization: Bearer $TOKEN" \
  | python3 -c 'import sys,json; d=json.load(sys.stdin); print(d[0]["id"] if d else "")')"

echo "CID=$CID"
```

**Expected output:**
```
CID=550e8400-e29b-41d4-a716-446655440000
```

**If CID is empty:**
- No channel connections exist
- Create one first via POST `/api/v1/channel-connections`
- Or use Swagger UI (`/docs`) to create a test connection

**If you get 307 redirect:**
- Add trailing slash: `/api/v1/channel-connections/`
- Or use `-L` flag (already included above)

**If you get 401 Unauthorized:**
- Token expired or invalid
- Re-fetch token (Step 2)
- Verify CHANNEL_MANAGER_ENABLED=true in backend

#### Step 5: Trigger Manual Sync

```bash
# Trigger full sync on channel connection
curl -k -sS -i -X POST "https://api.fewo.kolibri-visions.de/api/v1/channel-connections/$CID/sync" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"sync_type":"full"}'
```

**Expected output:**
```
HTTP/2 200
content-type: application/json

{
  "status": "queued",
  "message": "Sync queued successfully",
  "task_ids": ["abc123-def456-..."]
}
```

**If you get 405 Method Not Allowed:**
- Check OpenAPI spec: endpoint requires POST
- Verify URL is correct (no trailing slash after `$CID/sync`)

**If you get 422 Validation Error:**
- CID must be valid UUID
- Request body must include `{"sync_type":"full"}` (or "availability", "pricing", "bookings")

**If you get 404 Not Found:**
- Channel connection doesn't exist
- Verify CID is correct: `echo $CID`

#### Step 6: Check Sync Logs

```bash
# Fetch sync logs (save to file to avoid pipe JSON errors)
OUT="/tmp/sync_logs.json"
HDR="/tmp/sync_logs.headers.txt"
CODE="$(curl -k -sS -L -D "$HDR" -o "$OUT" -w '%{http_code}' \
  "https://api.fewo.kolibri-visions.de/api/v1/channel-connections/$CID/sync-logs?limit=20&offset=0" \
  -H "Authorization: Bearer $TOKEN")"

echo "HTTP=$CODE"
head -c 200 "$OUT"; echo
```

**Parse logs:**
```bash
python3 - <<'PY'
import json
d = json.load(open("/tmp/sync_logs.json"))
logs = d.get("logs", [])
print(f"Total logs: {len(logs)}")
if logs:
    last = logs[0]
    print(f"Last operation: {last.get('operation_type')}")
    print(f"Last status: {last.get('status')}")
    print(f"Last created_at: {last.get('created_at')}")
PY
```

**Expected output:**
```
HTTP=200
Total logs: 5
Last operation: full_sync
Last status: success
Last created_at: 2025-12-27T10:30:45.123Z
```

**If last.status is "failed":**
- Check worker logs for task errors
- Review sync_logs error_message field
- Verify channel connection credentials are valid

**If HTTP=404:**
- No sync logs exist for this connection
- Sync may not have completed yet (check worker logs)

---

### Deep Diagnostics (15–30 minutes)

**Purpose:** Comprehensive troubleshooting for Redis, Celery, and Channel Manager issues.

**When to use:**
- Quick Smoke fails
- Persistent connection errors
- Task execution failures
- After configuration changes

---

#### Common HTTP Responses Reference

Understanding API response codes:

| Code | Meaning | Common Causes | Solution |
|------|---------|---------------|----------|
| 200 | Success | - | Expected for GET/POST/PATCH |
| 201 | Created | - | Expected for POST (create) |
| 307 | Temporary Redirect | Missing trailing slash | Add `/` or use `-L` flag |
| 401 | Unauthorized | Missing/invalid token | Re-fetch JWT token |
| 403 | Forbidden | Insufficient permissions | Check user role/permissions |
| 404 | Not Found | Wrong URL or missing resource | Verify endpoint and resource ID |
| 405 | Method Not Allowed | Wrong HTTP method | Check OpenAPI: `/sync` needs POST |
| 422 | Validation Error | Invalid request body | CID must be UUID; sync needs `sync_type` |
| 500 | Internal Server Error | Backend exception | Check backend logs |
| 503 | Service Unavailable | Database/Redis down | Check /health/ready |

**Examples:**

**307 Redirect (trailing slash):**
```bash
# Without -L flag, you'll see:
curl -k -i "https://api.fewo.kolibri-visions.de/api/v1/channel-connections"
# HTTP/2 307
# location: /api/v1/channel-connections/

# Solution: Add -L or trailing slash
curl -k -L "https://api.fewo.kolibri-visions.de/api/v1/channel-connections/"
```

**Common symptom with 307 redirects:**
```bash
# Without -L, curl returns empty body + redirect header
curl -k "https://api.fewo.kolibri-visions.de/api/v1/channel-connections" > /tmp/output.json
# HTTP_CODE=307, output file is empty

# Parsing fails with JSONDecodeError:
cat /tmp/output.json | jq .
# jq: parse error: Invalid numeric literal at line 1, column 2

# Fix: Always use -L to follow redirects
curl -k -L "https://api.fewo.kolibri-visions.de/api/v1/channel-connections/" > /tmp/output.json
# HTTP_CODE=200, output contains valid JSON array
```

**Note:** When using `curl` in runbook examples for endpoints like:
- `GET /api/v1/channel-connections?...`
- `GET /api/v1/availability/blocks?...`
- Any GET endpoint that may have trailing slash redirects

Always include `-L` flag to automatically follow 307 redirects and avoid empty response bodies.

**422 Validation (sync endpoint):**
```bash
# Missing sync_type causes 422
curl -X POST ".../channel-connections/$CID/sync" -H "..." -d '{}'

# Solution: Include required field
curl -X POST ".../channel-connections/$CID/sync" -H "..." \
  -d '{"sync_type":"full"}'
```

---

#### Redis Connection Diagnostics

**Execution Location:** HOST-SERVER-TERMINAL

##### 1. Get Redis Password (requirepass)

**⚠️ Security:** Do NOT paste raw password in production logs/docs.

```bash
# Extract requirepass from Redis container config
# This shows the password - use with caution
docker inspect coolify-redis --format '{{range .Config.Cmd}}{{println .}}{{end}}' \
  | awk 'p{print; exit} $0=="--requirepass"{p=1}'

# Store in variable (don't echo to logs)
REDIS_PASS="$(docker inspect coolify-redis --format '{{range .Config.Cmd}}{{println .}}{{end}}' \
  | awk 'p{print; exit} $0=="--requirepass"{p=1}')"

# Verify password length (safe to log)
echo "Redis password length: ${#REDIS_PASS}"
```

##### 2. Test Redis Connection

```bash
# Test with raw password
redis-cli -h coolify-redis -a "$REDIS_PASS" ping
# Expected output: PONG

# If you get "Authentication required":
# - Password is wrong
# - Redis requirepass not set
# - Network connectivity issue
```

##### 3. Verify Password Encoding

**Compare Redis password with REDIS_URL:**

```bash
# Get decoded password from REDIS_URL (from pms-backend env)
python3 - <<'PY'
import os
import urllib.parse

# Get REDIS_URL from environment (set this to your actual REDIS_URL)
# In production: docker exec pms-backend env | grep REDIS_URL
redis_url = os.environ.get("REDIS_URL", "redis://:password@host:6379/0")

parsed = urllib.parse.urlparse(redis_url)
decoded_pass = urllib.parse.unquote(parsed.password or "")

print(f"REDIS_URL password length: {len(decoded_pass)}")
print(f"REDIS_URL password SHA256: ", end="")

import hashlib
print(hashlib.sha256(decoded_pass.encode()).hexdigest()[:16] + "...")
PY
```

**Compare with Redis requirepass:**
```bash
# Get SHA256 of Redis requirepass (don't print password)
echo -n "$REDIS_PASS" | sha256sum | cut -c1-16
```

**If hashes don't match:**
- Password mismatch between Redis and REDIS_URL
- Check if password needs URL encoding
- Re-encode password: see [Password Encoding](#how-to-url-encode-password)

##### 4. Test Connection from Backend Container

**Execution Location:** Coolify Terminal (pms-backend container)

```bash
# Test Redis connection from backend
python3 - <<'PY'
import os
import redis
from urllib.parse import urlparse

redis_url = os.environ.get("REDIS_URL", "")
parsed = urlparse(redis_url)

# Mask password for logging
masked_url = redis_url.replace(parsed.password or "", "***") if parsed.password else redis_url
print(f"Testing: {masked_url}")

try:
    r = redis.from_url(redis_url)
    result = r.ping()
    print(f"✓ Redis PING: {result}")
    print(f"✓ Password length: {len(parsed.password or '')}")

    # Test basic operations
    r.set("test_key", "test_value", ex=10)
    val = r.get("test_key")
    print(f"✓ SET/GET test: {val.decode() if val else 'None'}")
except Exception as e:
    print(f"✗ Redis connection failed: {e}")
PY
```

---

#### Celery Worker Diagnostics

**Execution Location:** HOST-SERVER-TERMINAL

##### 1. Verify Worker Container Exists

```bash
# Check if worker container is running
docker ps -a | egrep -i 'pms-worker|celery|worker'

# Expected: pms-worker container with "Up" status
# If "Exited": worker crashed - check logs
```

##### 2. Check Worker Logs for Tasks

```bash
# View recent worker logs (last 60 minutes)
# Replace <worker_container> with actual container name/ID
docker logs <worker_container> --since 60m | egrep -n 'received|succeeded|ERROR|Traceback'

# Look for:
# - "Task ... received" (task queued)
# - "Task ... succeeded" (task completed)
# - "ERROR" or "Traceback" (task failed)
```

**Search for specific task ID:**
```bash
# If you have a task ID from sync trigger response
TASK_ID="abc123-def456-..."
docker logs <worker_container> | egrep -n "$TASK_ID"
```

##### 3. Test Celery Connection from Backend

**Execution Location:** Coolify Terminal (pms-backend container)

```bash
# Ping Celery workers from backend
celery -A app.channel_manager.core.sync_engine:celery_app \
  --broker "$CELERY_BROKER_URL" \
  inspect ping -t 3

# Expected output:
# -> celery@pms-worker-abc123: {'ok': 'pong'}
```

**If timeout:**
```bash
# Check broker URL is correct
echo "CELERY_BROKER_URL (masked):"
python3 -c "import os; url=os.environ.get('CELERY_BROKER_URL',''); print(url[:20] + '***' + url[-10:] if len(url) > 30 else url)"

# Verify worker can reach Redis
docker exec <worker_container> redis-cli -h coolify-redis -a "$REDIS_PASS" ping
```

##### 4. Check Active Workers and Registered Tasks

**Execution Location:** Coolify Terminal (pms-backend container)

```bash
# List active workers
celery -A app.channel_manager.core.sync_engine:celery_app \
  --broker "$CELERY_BROKER_URL" \
  inspect active

# List registered tasks
celery -A app.channel_manager.core.sync_engine:celery_app \
  --broker "$CELERY_BROKER_URL" \
  inspect registered
```

**Expected output includes:**
- `app.channel_manager.sync_tasks.full_sync`
- `app.channel_manager.sync_tasks.availability_sync`
- etc.

---

#### Coolify / Nixpacks Quirks

##### Start Command Quoting Issues

**Problem:** Quoting `$ENV_VAR` in Coolify Start Command can break Nixpacks build.

**Bad (may break):**
```bash
celery -A app.celery_app --broker "$CELERY_BROKER_URL" worker
```

**Good (recommended):**
```bash
# Rely on environment variables (unquoted)
celery -A app.channel_manager.core.sync_engine:celery_app worker -l INFO
```

**Why?** Nixpacks may interpret quotes literally during build phase.

**Workaround:** Use simple start command, ensure `CELERY_BROKER_URL` and `CELERY_RESULT_BACKEND` are in environment variables.

##### COOLIFY_URL Null Issue

**Symptom:** Build fails with "COOLIFY_URL is null" or similar.

**Workaround:**
```bash
# Add to Environment Variables (build + runtime)
COOLIFY_URL=https://coolify.example.com
# Or any non-empty string - value may not matter for worker
```

---

#### Database Connection Diagnostics

##### DNS Flapping After Deploy

**Symptom:** `/health/ready` shows database errors like:
```
"name resolution failed"
"connection refused"
```

**Cause:** Docker DNS can be temporarily unstable after deployment.

**Solution:**
1. Wait 30-60 seconds after deployment
2. Recheck `/health/ready`
3. If persists, check network configuration

##### Network Configuration

**Execution Location:** HOST-SERVER-TERMINAL

```bash
# Check if pms-backend is on both networks
docker inspect pms-backend | grep -A 10 '"Networks"'

# Expected: coolify network AND supabase network
```

**If backend can't resolve `supabase-db`:**
```bash
# Test DNS from backend container
docker exec pms-backend nslookup supabase-db

# If fails, backend may not be on supabase network
# Fix: Add supabase network in Coolify UI
```

##### Database URL Verification

**Execution Location:** Coolify Terminal (pms-backend container)

```bash
# Test database connection (masked URL)
python3 - <<'PY'
import os
import asyncio
import asyncpg
from urllib.parse import urlparse

db_url = os.environ.get("DATABASE_URL", "")
parsed = urlparse(db_url)

# Mask password
masked = db_url.replace(parsed.password or "", "***") if parsed.password else db_url
print(f"Testing: {masked}")

async def test_connection():
    try:
        conn = await asyncpg.connect(db_url)
        version = await conn.fetchval("SELECT version()")
        print(f"✓ Connected: {version[:50]}...")
        await conn.close()
    except Exception as e:
        print(f"✗ Connection failed: {e}")

asyncio.run(test_connection())
PY
```

---

#### Worker Configuration Checklist

**Verify in Coolify UI → pms-worker:**

- ✅ **No public domain configured** (worker doesn't serve HTTP)
- ✅ **No Traefik labels** (no proxy/routing needed)
- ✅ **Ports Exposes:** `8000` (Coolify UI requirement, not actually used)
- ✅ **Start Command:** `celery -A app.channel_manager.core.sync_engine:celery_app worker -l INFO`
- ✅ **Environment variables:** All copied from pms-backend
- ✅ **Network:** On same Docker network as Redis and backend

**Common mistakes:**
- Adding public domain → Worker gets Traefik labels → Port conflicts
- Missing environment variables → Worker can't connect to database/Redis
- Wrong start command → Worker starts but doesn't register tasks

---

#### Special Characters in Passwords

**Characters requiring URL encoding:**

| Character | URL Encoded | Example |
|-----------|-------------|---------|
| `+` | `%2B` | `pass+word` → `pass%2Bword` |
| `=` | `%3D` | `pass=word` → `pass%3Dword` |
| `@` | `%40` | `pass@word` → `pass%40word` |
| `:` | `%3A` | `pass:word` → `pass%3Aword` |
| `/` | `%2F` | `pass/word` → `pass%2Fword` |
| `?` | `%3F` | `pass?word` → `pass%3Fword` |
| `#` | `%23` | `pass#word` → `pass%23word` |
| `&` | `%26` | `pass&word` → `pass%26word` |
| `%` | `%25` | `pass%word` → `pass%25word` |

**URL encoding script:**
```bash
python3 - <<'PY'
import urllib.parse
password = "YOUR_PASSWORD_HERE"
encoded = urllib.parse.quote(password, safe="")
print(f"Original: {password}")
print(f"Encoded:  {encoded}")
PY
```

**Why `+` is problematic:**
- In URLs, `+` is interpreted as space (URL encoding legacy)
- `redis://:pass+word@host` → server sees `redis://:pass word@host`
- Must encode as `redis://:pass%2Bword@host`

---

#### Execution Location Quick Reference

| Task | Location | Access Method |
|------|----------|---------------|
| Check Coolify app status | Coolify UI | Browser → Dashboard |
| Get Redis password | HOST-SERVER-TERMINAL | SSH to host, `docker inspect` |
| Test Redis connection | HOST-SERVER-TERMINAL | `redis-cli -h coolify-redis` |
| Check worker logs | Coolify UI or HOST | Dashboard → Logs or `docker logs` |
| Test Celery ping | Coolify Terminal (backend) | Dashboard → pms-backend → Terminal |
| Verify environment vars | Coolify UI | Dashboard → Environment Variables |
| Run smoke tests | HOST-SERVER-TERMINAL | SSH + `/root/pms_env.sh` |

---

## Channel Manager Error Handling & Retry Logic

**Purpose:** Understand how the Channel Manager handles failures and retries sync operations.

### Overview

The Channel Manager implements comprehensive error handling at two layers:
1. **API Layer** (immediate retry for user-facing endpoints)
2. **Celery Task Layer** (background retry for async operations)

Both layers use exponential backoff to prevent overwhelming failed services.

---

### Exponential Backoff Strategy

**Formula:** `delay = base_delay * (2 ^ retry_count)`

**Default Configuration:**
- Base delay: 1 second
- Max retries: 3
- Total duration: 7 seconds (1s + 2s + 4s)

**Retry Progression:**
```
Attempt 1: Execute immediately
├─ Fails → Wait 1 second (2^0)
Attempt 2: Execute after 1s delay
├─ Fails → Wait 2 seconds (2^1)
Attempt 3: Execute after 2s delay
├─ Fails → Wait 4 seconds (2^2)
Attempt 4: Execute after 4s delay
└─ Fails → Mark as permanently failed
```

**Benefits:**
- ✅ Fast recovery from transient failures (starts with 1s)
- ✅ Reduces load on failing services (exponentially increasing delays)
- ✅ Predictable total duration (7 seconds max vs. previous 180 seconds)

---

### Error Types and Handling

#### 1. Database Unavailable (503)

**Errors Caught:**
```python
asyncpg.PostgresError
asyncpg.exceptions.PostgresConnectionError
asyncpg.exceptions.ConnectionDoesNotExistError
asyncpg.exceptions.TooManyConnectionsError
```

**Response:**
```json
{
  "error": "service_unavailable",
  "message": "Database is temporarily unavailable.",
  "retry_count": 3
}
```

**What Happens:**
1. **Pre-flight Check**: Database availability validated BEFORE sync
2. **Retry**: Up to 3 retries with exponential backoff (1s, 2s, 4s)
3. **Logging**: Each retry logged with countdown duration
4. **Sync Log**: Status updated to "running" with retry details
5. **Final Failure**: After 3 retries, marked as "failed" with error details

**Example Log Output:**
```
WARNING: Database error on attempt 2/4: connection timeout. Retrying in 2 seconds...
ERROR: Database still unavailable after 3 retries. Final error: connection timeout
```

#### 2. General Exceptions (500)

**Response:**
```json
{
  "error": "internal_server_error",
  "message": "Failed to trigger availability sync: [error details]"
}
```

**What Happens:**
1. **Retry**: Up to 3 retries with exponential backoff
2. **Logging**: Full exception stacktrace logged
3. **Sync Log**: Error type and message stored in JSONB details
4. **Best-Effort Cleanup**: Attempts to update sync log even on failure

**Example Log Output:**
```
ERROR: Error updating availability on airbnb (task_id=abc123, retry=1): API timeout
WARNING: Retrying task abc123 after error (countdown=2s, retry=2/3)
```

#### 3. Validation Errors (400)

**No Retry** — These are permanent failures requiring user correction.

**Common Causes:**
- Invalid `sync_type` (must be "availability" or "pricing")
- Invalid `platform` (must be one of: airbnb, booking_com, expedia, etc.)
- Invalid UUID format

**Response:**
```json
{
  "detail": "Invalid sync_type. Must be one of: ['availability', 'pricing']"
}
```

#### 4. Not Found (404)

**No Retry** — Resource doesn't exist.

**Response:**
```json
{
  "detail": "Property not found or does not belong to your agency"
}
```

---

### Database Pre-flight Check

**When:** Before every sync operation
**Why:** Fail-fast to avoid wasted work
**How:** Simple `SELECT 1` query to verify connection pool health

**Implementation:**
```python
async def _check_database_availability():
    """Verify database is reachable before sync"""
    pool = get_pool()
    async with pool.acquire() as conn:
        await conn.fetchval("SELECT 1")
```

**Benefits:**
- ✅ Detects DB issues immediately (before triggering tasks)
- ✅ Clear error message: "Database is temporarily unavailable."
- ✅ Prevents queuing tasks that will fail

---

### Celery Task Retry Logic

**Configured on Task Decorator:**
```python
@celery_app.task(bind=True, max_retries=3, autoretry_for=(Retry,))
def update_channel_availability(self, ...):
```

**Retry Behavior:**
1. **Immediate Execution**: First attempt runs immediately
2. **Exponential Backoff**: Retries at 1s, 2s, 4s intervals
3. **Status Updates**: Sync log updated on each attempt
4. **Max Retries**: After 3 retries, task marked as permanently failed

**Retry Details Stored in Sync Log:**
```json
{
  "retry_count": 2,
  "error_type": "database_unavailable",
  "next_retry_seconds": 4
}
```

---

### Monitoring Retry Attempts

**Check /health/ready:**
```bash
curl https://api.your-domain.com/health/ready | jq .
```

**Check Sync Logs:**
```bash
curl https://api.your-domain.com/api/v1/channel-connections/{id}/sync-logs \
  -H "Authorization: Bearer TOKEN" | jq '.logs[0]'
```

**Sample Retry Log Entry:**
```json
{
  "id": "log-uuid",
  "operation_type": "availability_update",
  "status": "running",
  "details": {
    "retry_count": 2,
    "error_type": "database_unavailable",
    "next_retry_seconds": 4,
    "platform": "airbnb"
  },
  "error": null,
  "task_id": "celery-task-uuid",
  "created_at": "2025-12-28T10:00:00Z",
  "updated_at": "2025-12-28T10:00:03Z"
}
```

**Sample Failed Log Entry:**
```json
{
  "id": "log-uuid",
  "operation_type": "availability_update",
  "status": "failed",
  "details": {
    "retry_count": 3,
    "error_type": "database_unavailable"
  },
  "error": "Database unavailable after 3 retries: connection timeout",
  "task_id": "celery-task-uuid",
  "created_at": "2025-12-28T10:00:00Z",
  "updated_at": "2025-12-28T10:00:07Z"
}
```

---

## Mock Mode for Channel Providers

**Purpose:** Test Channel Manager endpoints without making real API calls to external platforms (Airbnb, Booking.com, etc.).

### Overview

Mock mode allows you to test connection health and sync operations without requiring actual platform credentials or API connectivity. This is useful for:

- **Development:** Testing Channel Manager logic without external dependencies
- **CI/CD:** Running integration tests without platform API keys
- **Staging:** Validating deployment before configuring real connections
- **Debugging:** Isolating issues from external API failures

### Configuration

**Environment Variable:** `CHANNEL_MOCK_MODE`

**Default:** `false` (real API calls)

**Enable Mock Mode:**
```bash
# In .env or Coolify environment variables
CHANNEL_MOCK_MODE=true
```

**Disable Mock Mode (Production):**
```bash
CHANNEL_MOCK_MODE=false
```

### Behavior

#### Test Connection Endpoint

**Endpoint:** `POST /api/v1/channel-connections/{connection_id}/test`

**Mock Mode Enabled (`CHANNEL_MOCK_MODE=true`):**
- Returns simulated health response without calling external platform APIs
- Health status based on connection's current `status` field:
  - `active` → `healthy=true`
  - `paused` / `inactive` → `healthy=false`
- Response includes `mock_mode=true` flag in details

**Mock Mode Disabled (Default):**
- Calls actual platform API for real health check
- Returns genuine connection status from external service

**Example Mock Response (200):**
```json
{
  "connection_id": "abc-123-456-...",
  "platform_type": "airbnb",
  "healthy": true,
  "message": "Mock: Connection is healthy",
  "details": {
    "mock_mode": true,
    "simulated": true,
    "connection_status": "active",
    "note": "This is a simulated response. Set CHANNEL_MOCK_MODE=false for real API calls."
  }
}
```

**Example Mock Response (Inactive Connection):**
```json
{
  "connection_id": "def-456-789-...",
  "platform_type": "booking_com",
  "healthy": false,
  "message": "Mock: Connection status is paused",
  "details": {
    "mock_mode": true,
    "simulated": true,
    "connection_status": "paused",
    "note": "This is a simulated response. Set CHANNEL_MOCK_MODE=false for real API calls."
  }
}
```

### Verification

**Check if Mock Mode is Active:**

WHERE: Coolify Terminal (pms-backend)
```bash
# Check environment variable
echo $CHANNEL_MOCK_MODE

# Test a connection and look for mock_mode flag
curl -X POST "$API/api/v1/channel-connections/$CID/test" \
  -H "Authorization: Bearer $TOKEN" | jq '.details.mock_mode'
# Expected: true (if mock mode enabled)
```

**Enable Mock Mode in Coolify:**
1. Navigate to: Coolify → Applications → pms-backend → Environment Variables
2. Add/Update: `CHANNEL_MOCK_MODE=true`
3. Restart backend: `docker restart pms-backend`

### Safety Considerations

**Production Deployment:**
- **ALWAYS** set `CHANNEL_MOCK_MODE=false` (or omit entirely) in production
- Mock mode is intended for testing/staging environments only
- Real sync operations will fail if mock mode is enabled (no actual API calls made)

**Audit Trail:**
- All mock responses include `mock_mode=true` flag for transparency
- Sync logs will reflect simulated operations (check `details.mock_mode`)

### Error Codes and Messages

When a connection test fails (mock or real mode), the response includes an `error_code` field for programmatic handling:

#### `CREDENTIALS_MISSING`

**Meaning:** Integration is enabled but platform-specific credentials are not configured.

**Example Response:**
```json
{
  "connection_id": "abc-123-456-...",
  "platform_type": "booking_com",
  "healthy": false,
  "message": "Booking.com credentials not configured. Add BOOKING_COM_API_KEY and BOOKING_COM_SECRET to environment.",
  "details": {
    "error_code": "CREDENTIALS_MISSING",
    "required_env_vars": ["BOOKING_COM_API_KEY", "BOOKING_COM_SECRET"],
    "mock_mode": false
  }
}
```

**Action Required:**
1. Add missing environment variables to Coolify (pms-backend)
2. Restart backend container
3. Re-test connection

#### `INTEGRATION_DISABLED`

**Meaning:** Platform integration is not implemented or is disabled in the current deployment.

**Example Response:**
```json
{
  "connection_id": "def-456-789-...",
  "platform_type": "expedia",
  "healthy": false,
  "message": "Expedia integration is disabled. Contact support to enable.",
  "details": {
    "error_code": "INTEGRATION_DISABLED",
    "mock_mode": false
  }
}
```

**Action Required:**
- Contact platform vendor or system administrator
- Enable integration via feature flag or code deployment

#### `CONNECTION_INACTIVE`

**Meaning:** Connection exists but is paused or inactive (not an error, but expected behavior).

**Example Response:**
```json
{
  "connection_id": "ghi-789-012-...",
  "platform_type": "airbnb",
  "healthy": false,
  "message": "Connection is paused",
  "details": {
    "error_code": "CONNECTION_INACTIVE",
    "connection_status": "paused",
    "mock_mode": true
  }
}
```

**Action Required:**
- Update connection status to `active` via Admin UI or API
- Re-test connection

### Mock Mode Platform Responses

When `CHANNEL_MOCK_MODE=true`, test connection returns platform-specific mock data:

#### Airbnb (Mock)
```json
{
  "connection_id": "...",
  "platform_type": "airbnb",
  "healthy": true,
  "message": "Mock: Connection is healthy",
  "details": {
    "mock_mode": true,
    "simulated": true,
    "connection_status": "active",
    "remote_account_id": "mock_airbnb_host_123456",
    "remote_listing_id": "mock_listing_987654",
    "capabilities": ["availability_sync", "pricing_sync", "booking_retrieval"],
    "note": "This is a simulated response. Set CHANNEL_MOCK_MODE=false for real API calls."
  }
}
```

#### Booking.com (Mock)
```json
{
  "connection_id": "...",
  "platform_type": "booking_com",
  "healthy": true,
  "message": "Mock: Connection is healthy",
  "details": {
    "mock_mode": true,
    "simulated": true,
    "connection_status": "active",
    "remote_account_id": "mock_booking_hotel_789012",
    "remote_listing_id": "mock_property_345678",
    "capabilities": ["availability_sync", "pricing_sync", "booking_retrieval", "channel_manager"],
    "note": "This is a simulated response. Set CHANNEL_MOCK_MODE=false for real API calls."
  }
}
```

#### Expedia (Mock)
```json
{
  "connection_id": "...",
  "platform_type": "expedia",
  "healthy": true,
  "message": "Mock: Connection is healthy",
  "details": {
    "mock_mode": true,
    "simulated": true,
    "connection_status": "active",
    "remote_account_id": "mock_expedia_partner_456789",
    "remote_listing_id": "mock_vrbo_listing_234567",
    "capabilities": ["availability_sync", "pricing_sync", "booking_retrieval"],
    "note": "This is a simulated response. Set CHANNEL_MOCK_MODE=false for real API calls."
  }
}
```

#### FeWo-direkt (Mock)
```json
{
  "connection_id": "...",
  "platform_type": "fewo_direkt",
  "healthy": true,
  "message": "Mock: Connection is healthy",
  "details": {
    "mock_mode": true,
    "simulated": true,
    "connection_status": "active",
    "remote_account_id": "mock_fewo_owner_654321",
    "remote_listing_id": "mock_fewo_property_876543",
    "capabilities": ["availability_sync", "pricing_sync"],
    "note": "This is a simulated response. Set CHANNEL_MOCK_MODE=false for real API calls."
  }
}
```

#### Google (Mock)
```json
{
  "connection_id": "...",
  "platform_type": "google",
  "healthy": true,
  "message": "Mock: Connection is healthy",
  "details": {
    "mock_mode": true,
    "simulated": true,
    "connection_status": "active",
    "remote_account_id": "mock_google_hotel_112233",
    "remote_listing_id": "mock_google_listing_998877",
    "capabilities": ["availability_sync", "pricing_sync"],
    "note": "This is a simulated response. Set CHANNEL_MOCK_MODE=false for real API calls."
  }
}
```

### Production Readiness

To go production-ready (disable mock mode and enable real platform integrations), configure the following environment variables:

#### Required for All Platforms
```bash
# Disable mock mode
CHANNEL_MOCK_MODE=false
```

#### Platform-Specific Credentials

**Airbnb:**
```bash
AIRBNB_API_KEY=your_airbnb_api_key
AIRBNB_API_SECRET=your_airbnb_secret
```

**Booking.com:**
```bash
BOOKING_COM_API_KEY=your_booking_api_key
BOOKING_COM_SECRET=your_booking_secret
BOOKING_COM_HOTEL_ID=your_hotel_id
```

**Expedia:**
```bash
EXPEDIA_API_KEY=your_expedia_key
EXPEDIA_API_SECRET=your_expedia_secret
EXPEDIA_PARTNER_ID=your_partner_id
```

**FeWo-direkt:**
```bash
FEWO_DIREKT_API_KEY=your_fewo_key
FEWO_DIREKT_SECRET=your_fewo_secret
```

**Google Vacation Rentals:**
```bash
GOOGLE_HOTELS_API_KEY=your_google_key
GOOGLE_HOTELS_PARTNER_ID=your_partner_id
```

**After Adding Credentials:**
1. Restart backend: `docker restart pms-backend`
2. Test connection via Admin UI or API: `POST /api/v1/channel-connections/{id}/test`
3. Verify `healthy=true` and `mock_mode=false` in response
4. If `healthy=false`, check error_code and message for next steps

### Admin UI Indicator

The Admin UI (Connections page) automatically detects and displays Mock Mode status when testing connections.

**Visual Indicators:**

**Mock Mode Enabled:**
- Blue "Mock Mode (Simulated)" badge displayed above test result
- Explanatory text: "This response is simulated. For production set CHANNEL_MOCK_MODE=false."
- Link to this runbook section for configuration details

**Credentials Missing (Production Mode):**
- Yellow "Not Configured" badge displayed
- Lists required environment variables for the platform
- Link to Production Readiness section above

**How It Works:**
- UI checks `details.mock_mode` or `details.simulated` flags in test response
- UI checks `error_code` field for CREDENTIALS_MISSING status
- No manual configuration needed - indicators appear automatically based on backend response

**Example:**
1. Navigate to Admin UI → Connections
2. Click "Open" on any connection
3. Click "Run Test"
4. If mock mode is enabled, you'll see the blue badge immediately
5. If credentials are missing, you'll see the yellow badge with required env vars

### Limitations

**Current Implementation (Phase C1):**
- Mock mode ONLY affects `/test` endpoint
- Sync operations (`/api/availability/sync`) are NOT yet mocked
- CRUD endpoints (`POST`, `GET`, `PUT`, `DELETE` connections) operate normally regardless of mock mode

**Planned Future Enhancements:**
- Mock sync operations (availability/pricing updates)
- Configurable mock data (success/failure scenarios)
- Per-platform mock behavior customization

---

## Channel Manager - channel_connections Schema Drift

**Purpose:** Diagnose and fix missing table/columns that cause 500/503 errors when calling Channel Manager endpoints.

### Symptoms

**HTTP 500 Internal Server Error:**
```json
{
  "detail": "Failed to simulate connection test: column \"platform_type\" does not exist"
}
```

**HTTP 503 Service Unavailable (after hardening):**
```json
{
  "detail": "Database schema not installed/out of date. Missing column in channel_connections: column \"status\" does not exist. Run Supabase migrations: supabase/migrations/20260101030000_channel_connections_schema_upgrade.sql"
}
```

**HTTP 404 Not Found:**
```
Connection not found
```
This occurs when schema exists but no test row seeded (expected in fresh deployments).

**HTTP 405 Method Not Allowed:**
```
Method Not Allowed
```
The `/test` endpoint is **POST**, not GET. Use `curl -X POST`.

### Root Cause

The `channel_connections` table was created in migration `20250101000002_channels_and_financials.sql` but is missing columns expected by backend code:

**Missing columns:**
- `platform_type` (backend queries this instead of `channel`)
- `status` (used for health check simulation in mock mode)
- `platform_metadata` (JSONB for connection details)
- `deleted_at` (soft delete filtering with `WHERE deleted_at IS NULL`)

**Original schema** only had: `id`, `agency_id`, `channel`, `is_active`, etc.

### Verification

**Check if table exists:**

WHERE: Supabase SQL Editor or HOST-SERVER-TERMINAL (psql)
```sql
SELECT to_regclass('public.channel_connections');
-- Expected: 'channel_connections' (if exists) or NULL (if missing)
```

**Check for missing columns:**

```sql
SELECT column_name, data_type, is_nullable
FROM information_schema.columns
WHERE table_schema = 'public'
  AND table_name = 'channel_connections'
ORDER BY ordinal_position;

-- Expected columns:
-- id, agency_id, channel, platform_type, status, platform_metadata, deleted_at
```

**Check if columns are missing (quick):**
```sql
SELECT
  EXISTS(SELECT 1 FROM information_schema.columns WHERE table_name='channel_connections' AND column_name='platform_type') AS has_platform_type,
  EXISTS(SELECT 1 FROM information_schema.columns WHERE table_name='channel_connections' AND column_name='status') AS has_status,
  EXISTS(SELECT 1 FROM information_schema.columns WHERE table_name='channel_connections' AND column_name='platform_metadata') AS has_platform_metadata,
  EXISTS(SELECT 1 FROM information_schema.columns WHERE table_name='channel_connections' AND column_name='deleted_at') AS has_deleted_at;

-- Expected: all true (t, t, t, t)
-- If any false, schema drift detected
```

### Fix: Apply Migration

**Migration File:**
```
supabase/migrations/20260101030000_channel_connections_schema_upgrade.sql
```

**What it does:**
- Adds `platform_type`, `status`, `platform_metadata`, `deleted_at` columns (idempotent)
- Backfills `platform_type` from existing `channel` column
- Backfills `status` from existing `is_active` (true → 'active', false → 'inactive')
- Adds check constraints for valid enum values
- Adds indexes for performance

**Apply via Supabase CLI:**

WHERE: Local development machine (with Supabase CLI installed)
```bash
cd /path/to/PMS-Webapp
supabase db push

# Or apply single migration:
supabase migration up --file supabase/migrations/20260101030000_channel_connections_schema_upgrade.sql
```

**Apply via Supabase Dashboard:**

1. Navigate to: Supabase Dashboard → SQL Editor
2. Copy contents of `20260101030000_channel_connections_schema_upgrade.sql`
3. Paste and execute (safe: idempotent, will skip if columns exist)

**Apply via psql (HOST-SERVER-TERMINAL):**

```bash
# SSH to host server
cd /app/supabase/migrations
psql "$DATABASE_URL" < 20260101030000_channel_connections_schema_upgrade.sql
```

### Seed Test Row (Optional)

**For testing /test endpoint, seed a row:**

WHERE: Supabase SQL Editor or psql
```sql
INSERT INTO channel_connections (
  agency_id,
  channel,
  platform_type,
  status,
  platform_metadata
) VALUES (
  (SELECT id FROM agencies LIMIT 1),  -- Use existing agency
  'airbnb',
  'airbnb',
  'active',
  '{"listing_id": "12345678", "host_id": "host_abc"}'::jsonb
)
ON CONFLICT (agency_id, channel) DO NOTHING;

-- Get the connection ID for testing:
SELECT id, agency_id, platform_type, status
FROM channel_connections
WHERE deleted_at IS NULL;
```

**Verify seed:**
```bash
# In backend logs or via API
curl -X POST "$API/api/v1/channel-connections/$CID/test" \
  -H "Authorization: Bearer $TOKEN"

# Expected (mock mode): {"healthy": true, "details": {"mock_mode": true, ...}}
```

### Mock Mode Behavior

**When `CHANNEL_MOCK_MODE=true`:**

**Schema exists + row exists:**
- Returns simulated health check based on `status` column
- `healthy=true` if `status='active'`, `healthy=false` otherwise
- Response includes `"mock_mode": true` flag

**Schema missing (before hardening):**
- HTTP 500 with cryptic error message
- No actionable guidance

**Schema missing (after hardening - Phase C1):**
- HTTP 503 with clear migration guidance
- Error message points to specific migration file
- Prevents confusing 500 errors

**Row missing (404):**
- Expected in fresh deployments (no seed data)
- Fix: Insert a test row (see seed snippet above)
- OR: Create connection via POST `/api/v1/channel-connections/` endpoint

### Common Errors

| Error | Cause | Fix |
|-------|-------|-----|
| 500 "column platform_type does not exist" | Schema drift (old migration) | Apply 20260101030000 migration |
| 503 "Missing column in channel_connections" | Schema drift (after hardening) | Apply 20260101030000 migration |
| 404 "Connection not found" | No rows in table | Seed test row or create via API |
| 405 "Method Not Allowed" | Using GET instead of POST | Use `curl -X POST` |
| 401 "Unauthorized" | Missing/invalid JWT token | Fetch token via auth endpoint |

### Expected Response (Mock Mode)

**Success (200):**
```json
{
  "connection_id": "abc-123-456-...",
  "platform_type": "airbnb",
  "healthy": true,
  "message": "Mock: Connection is healthy",
  "details": {
    "mock_mode": true,
    "simulated": true,
    "connection_status": "active",
    "note": "This is a simulated response. Set CHANNEL_MOCK_MODE=false for real API calls."
  }
}
```

**Note:** `/test` endpoint is **POST**, not GET. Using GET returns HTTP 405.

### Related Sections

- [Mock Mode for Channel Providers](#mock-mode-for-channel-providers) - Mock mode configuration
- [Channel Manager API Endpoints](#channel-manager-api-endpoints) - Complete API reference
- [Seeding Test Connections](#seeding-test-connections) - Automated seeding via script

---

## Seeding Test Connections

**Purpose:** Quickly seed or reset channel connections for testing without manual SQL Editor operations.

**Script:** `backend/scripts/pms_channel_seed_connection.sh`

**Key Feature: NO psql required on host** - uses `docker exec` in Supabase DB container automatically.

**Why Use This Script:**
- **No psql required:** Runs via `docker exec` in supabase-db-* container (auto-detects)
- **Idempotent:** Safe to run multiple times (uses `ON CONFLICT DO UPDATE`)
- **Auto-pick agency:** No manual UUID lookup required
- **Validation:** UUID and JSON validation before INSERT
- **Clean state:** Optional purge of sync logs with confirmation
- **Automation-ready:** `--print-cid` flag for scripting/CI/CD
- **No secrets in output:** Passwords redacted in logs

**Requirements:**
- `docker` command on host (for docker exec into DB container)
- `python3` (for JSON validation)
- **NO psql required** on host (runs inside container)

### Quick Start

**Basic usage (auto-pick agency, default airbnb):**

WHERE: HOST-SERVER-TERMINAL
```bash
# NO DATABASE_URL needed - script auto-detects DB container and reads credentials

# Seed connection (auto-detects supabase-db-* container)
bash backend/scripts/pms_channel_seed_connection.sh

# Expected output:
# ℹ️  Auto-detecting Supabase DB container...
# ℹ️  ✓ Auto-detected DB container: supabase-db-bccg4gs4o4kgsowocw08wkw4
# ℹ️  Reading DB credentials from container...
# ℹ️  ✓ DB credentials loaded (user: postgres, db: postgres)
# ℹ️  No --agency-id provided, auto-picking first agency from database...
# ℹ️  ✓ Auto-picked agency: abc-123-def-456...
# ✅ CHANNEL CONNECTION SEEDED
# Connection ID:   xyz-789-ghi-012...
# Agency ID:       abc-123-def-456...
# Channel:         airbnb
# Platform Type:   airbnb
# Status:          active
```

**Scripting mode (automation):**
```bash
# Export connection ID for use in other scripts (no output except CID)
export CID=$(bash backend/scripts/pms_channel_seed_connection.sh --print-cid)

# Use with sync poll script
bash backend/scripts/pms_channel_sync_poll.sh --cid $CID --sync-type availability
```

**Explicit DB container (if multiple containers exist):**
```bash
# Via flag
bash backend/scripts/pms_channel_seed_connection.sh --db-container supabase-db-xyz123

# Via ENV
export SUPABASE_DB_CONTAINER=supabase-db-xyz123
bash backend/scripts/pms_channel_seed_connection.sh
```

### Common Use Cases

**1. Seed booking.com connection:**
```bash
bash backend/scripts/pms_channel_seed_connection.sh \
  --channel booking_com \
  --platform-type booking_com \
  --platform-listing-id booking_12345678
```

**2. Seed with explicit agency and property:**
```bash
# Option A: Let script auto-pick agency (recommended)
bash backend/scripts/pms_channel_seed_connection.sh \
  --channel airbnb \
  --platform-type airbnb

# Option B: Get agency ID manually first (if you need a specific agency)
# Note: Script uses docker exec internally, no local psql needed
export AGENCY_ID=$(bash backend/scripts/pms_channel_seed_connection.sh --print-cid --channel temp 2>&1 | grep "Auto-picked agency" | awk '{print $NF}')

# Then seed with explicit IDs
bash backend/scripts/pms_channel_seed_connection.sh \
  --agency-id $AGENCY_ID \
  --property-id <your-property-uuid> \
  --channel airbnb \
  --platform-type airbnb
```

**3. Reset connection and clear sync logs:**
```bash
# Reset (clear logs) with confirmation prompt
bash backend/scripts/pms_channel_seed_connection.sh --reset

# Reset without confirmation (automation)
bash backend/scripts/pms_channel_seed_connection.sh --reset --yes

# Legacy: --purge-logs still works (alias for --reset)
bash backend/scripts/pms_channel_seed_connection.sh --purge-logs --yes
```

**What does --reset do?**
- Deletes all `channel_sync_logs` entries for the seeded connection
- Useful for testing full sync operations in UI without old log clutter
- Connection itself remains (only logs are cleared)
- Requires confirmation unless `--yes` flag is used

**4. Seed inactive/error connection for testing:**
```bash
bash backend/scripts/pms_channel_seed_connection.sh \
  --status error \
  --metadata '{"listing_id": "test_123", "error_reason": "auth_failed"}'
```

### Script Options

| Option | Description | Default |
|--------|-------------|---------|
| `--agency-id <uuid>` | Agency ID (auto-picks first if not provided) | Auto-pick |
| `--channel <string>` | Channel name | `airbnb` |
| `--platform-type <string>` | Platform type | `airbnb` |
| `--status <status>` | Status: active/inactive/paused/disabled/error | `active` |
| `--tenant-id <uuid>` | Tenant ID (optional, if column exists) | - |
| `--property-id <uuid>` | Property ID (optional, if column exists) | - |
| `--platform-listing-id <str>` | Platform listing ID | `airbnb_listing_789` |
| `--metadata <json>` | Platform metadata JSON | `{}` |
| `--db-container <name>` | **NEW:** Explicit DB container name (overrides auto-detect) | Auto-detect |
| `--print-cid` | Print seeded connection ID only (for scripting) | - |
| `--reset` | Clear channel_sync_logs for this connection (alias: `--purge-logs`) | - |
| `--seed` | No-op flag (script always seeds, for CLI consistency) | - |
| `--yes` | Skip confirmation prompts (use with `--reset`) | - |

### Idempotent Behavior

**Conflict Key:** `(agency_id, channel)` — One connection per agency per channel

**First Run (INSERT):**
```sql
-- Creates new connection
INSERT INTO channel_connections (...) VALUES (...);
```

**Subsequent Runs (UPDATE):**
```sql
-- Updates existing connection
ON CONFLICT (agency_id, channel)
DO UPDATE SET
  platform_type = EXCLUDED.platform_type,
  status = EXCLUDED.status,
  platform_metadata = EXCLUDED.platform_metadata,
  deleted_at = NULL,  -- Revives soft-deleted connections
  updated_at = NOW();
```

**Safe to run repeatedly:**
- No duplicate connections created
- Existing connections updated to match new parameters
- Soft-deleted connections (`deleted_at IS NOT NULL`) are revived

### Environment Variables

**Required:**
- `DATABASE_URL` or `SUPABASE_DB_URL` — PostgreSQL connection string

**Optional:**
- `ENV_FILE` — Path to environment file (default: `/root/pms_env.sh`)

**Example env file (`/root/pms_env.sh`):**
```bash
export DATABASE_URL="postgresql://postgres:your-password@supabase-db:5432/postgres"
export SUPABASE_DB_URL="$DATABASE_URL"  # Alias
```

### Validation

The script validates inputs before execution:

**UUID validation:**
```bash
# Regex pattern: [0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}
# Validates: --agency-id, --tenant-id, --property-id
```

**JSON validation:**
```bash
# Uses python3 to validate --metadata JSON
# Example valid: '{"listing_id":"123","host_id":"abc"}'
# Example invalid: '{listing_id:123}'  # Missing quotes
```

**Status validation:**
```bash
# Allowed values: active, inactive, paused, disabled, error
# Example valid: --status active
# Example invalid: --status ACTIVE  # Case-sensitive
```

### Log Purge Safety

**Preview before deletion:**
```bash
# Script shows count of logs before confirmation
⚠️  Found 42 sync logs for connection abc-123-456...
⚠️  Delete all sync logs for this connection? [y/N]
```

**Auto-confirm for automation:**
```bash
# Use --yes flag to skip confirmation
bash backend/scripts/pms_channel_seed_connection.sh --reset --yes
# ✓ Purged 42 sync logs (--yes flag)
```

**Scope:** Only purges logs for the seeded connection (not global)

### Common Errors

| Error | Cause | Fix |
|-------|-------|-----|
| `Required command not found: psql` | Old version of script (pre docker exec) | Update script to latest version |
| `docker command not found` | Docker not installed on host | Install docker: `apt-get install docker.io` |
| `No supabase-db-* container found` | Supabase DB not running | Start Supabase: `docker ps \| grep supabase` |
| `Multiple supabase-db-* containers found` | Multiple DB containers running | Use `--db-container <name>` or `export SUPABASE_DB_CONTAINER=<name>` |
| `Container 'xyz' not found or not running` | Wrong container name | Check: `docker ps \| grep supabase-db` |
| `Failed to read POSTGRES_* env vars` | Container missing DB env vars | Verify container has POSTGRES_USER, POSTGRES_DB, POSTGRES_PASSWORD |
| `Incomplete DB credentials from container` | Partial env vars in container | Check: `docker exec <container> env \| grep POSTGRES` |
| `Invalid UUID format` | Malformed UUID | Check UUID format (lowercase, hyphens) |
| `Invalid JSON for --metadata` | Malformed JSON | Use double quotes: `'{"key":"value"}'` |
| `Invalid status` | Wrong status value | Use: active/inactive/paused/disabled/error |
| `No agencies found` | Empty agencies table | Create agency first or use explicit `--agency-id` |
| `Failed to seed connection` | SQL error | Check database logs, verify schema exists |

### Integration with Other Scripts

**Use with sync poll script:**
```bash
# 1. Seed connection and capture ID
export CID=$(bash backend/scripts/pms_channel_seed_connection.sh --print-cid)

# 2. Trigger availability sync and poll for completion
bash backend/scripts/pms_channel_sync_poll.sh --cid $CID --sync-type availability

# Expected flow:
# ✅ CHANNEL CONNECTION SEEDED (Connection ID: abc-123...)
# ℹ️  Fetching JWT token...
# ✅ SYNC COMPLETED SUCCESSFULLY (Status: success)
```

**CI/CD pipeline example:**
```bash
#!/bin/bash
set -euo pipefail

# Setup
export DATABASE_URL="postgresql://..."
export SB_URL="https://..."
export ANON_KEY="..."
export EMAIL="test@example.com"
export PASSWORD="..."

# Seed test connection
CID=$(bash backend/scripts/pms_channel_seed_connection.sh \
  --channel airbnb \
  --status active \
  --print-cid \
  --yes)

echo "Seeded connection: $CID"

# Run smoke test
bash backend/scripts/pms_channel_sync_poll.sh \
  --cid $CID \
  --sync-type availability \
  --poll-limit 30

# Cleanup (optional)
bash backend/scripts/pms_channel_seed_connection.sh \
  --channel airbnb \
  --status inactive \
  --reset \
  --yes
```

### Next Steps After Seeding

**Test connection health:**
```bash
# Via UI
https://admin.fewo.kolibri-visions.de/connections

# Via API
curl -X POST "$API/api/v1/channel-connections/$CID/test" \
  -H "Authorization: Bearer $TOKEN"
```

**Trigger sync:**
```bash
# Via script (recommended)
bash backend/scripts/pms_channel_sync_poll.sh --cid $CID --sync-type availability

# Via API
curl -X POST "$API/api/v1/channel-connections/$CID/sync" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"sync_type":"availability"}'
```

**Monitor logs:**
```bash
# Via script
bash backend/scripts/pms_channel_sync_poll.sh --cid $CID

# Via API
curl "$API/api/v1/channel-connections/$CID/sync-logs?limit=10" \
  -H "Authorization: Bearer $TOKEN"
```

### When to Use

✅ **Use this script when:**
- Setting up test connections for development
- Resetting connection state after schema changes
- Automating connection setup in CI/CD pipelines
- Testing Channel Manager sync operations
- Creating connections without UI access

❌ **Don't use this script when:**
- Setting up production connections (use Admin UI instead)
- Updating OAuth credentials (use Admin UI or encrypted SQL)
- Bulk operations (use migration scripts instead)
- Managing multiple connections at once (use batch SQL instead)

### Related Sections

- [Channel Manager - Schema Drift](#channel-manager---channel_connections-schema-drift) - Fix missing columns
- [pms_channel_sync_poll.sh](../scripts/README.md#pms_channel_sync_pollsh) - Trigger and poll syncs
- [Channel Connections Management](../scripts/README.md#channel-connections-management-curl-examples) - curl API examples
- [Admin UI – Channel Manager Operations](#admin-ui--channel-manager-operations) - Web UI guide

---

## Admin UI – Channel Manager Operations

**Purpose:** Guide for using the admin web UI to manage channel connections, test health, trigger syncs, and monitor logs.

**Access:** Admin-only (requires admin role). Navigate to: `https://admin.fewo.kolibri-visions.de/connections`

### UI Notifications

**Important:** The Admin UI uses in-app notifications exclusively. No browser alerts (alert/confirm/prompt) are used.

**Notification System:**
- All feedback is shown via non-blocking in-app banners/toasts
- Success messages: Green banner with auto-dismiss (5 seconds)
- Error messages: Red banner with auto-dismiss (5 seconds)
- Manual dismiss: Click X button to close immediately
- Notifications appear at top of page (Connections page) or below page header (System Status page)

**Examples:**
- "Connection test passed: Mock: Connection is healthy (Mock Mode - see runbook for production setup)"
- "Connection status updated to paused"
- "Diagnostics copied to clipboard"
- "Failed to update status: Network error"

**Affected Pages:**
- `/connections` - Connection management and testing
- `/ops/status` - System health diagnostics

**Test Connection Feedback:**
When clicking "Test" button (inline in connections table):
1. Button shows "Testing..." while API call is in progress
2. On success/failure, notification banner appears at top of connections page
3. Banner auto-dismisses after 10 seconds or click X to close manually
4. Success example: "Connection test passed: Mock: Connection is healthy (Mock Mode - see runbook for production setup)"
5. Failure example: "Test failed: Connection timeout" or "Connection test failed: Invalid credentials"
6. If `error_code` is present (e.g., CREDENTIALS_MISSING), it's shown in the detailed test result modal, not the inline toast

**Where to Look for Feedback:**
- **Inline test (table row "Test" button):** Notification banner appears at top of main page above search box
- **Modal test ("Run Test" button in connection details):** Test result displays below button in modal with full details including badges
- Both locations use same notification system (green for success, red for error)

**Common Error Messages:**

The Admin UI displays user-friendly error messages instead of technical errors. Here's what they mean and how to resolve them:

**Network Errors:**
- **"Network error (API not reachable). Check your connection, VPN, or firewall."**
  - **Meaning:** Browser cannot reach the API endpoint (typically "Failed to fetch" from browser)
  - **Common Causes:**
    - Internet connection lost
    - VPN blocking API domain
    - Corporate firewall/proxy blocking requests
    - CORS policy blocking cross-origin requests
    - Ad blocker or browser extension interfering
    - API server is down (check `/ops/status` page)
  - **Action:**
    - Check internet connection
    - Disable VPN temporarily and retry
    - Try different network (e.g., mobile hotspot)
    - Check browser console for CORS/network errors
    - Verify API is reachable: `curl https://api.fewo.kolibri-visions.de/health`
    - Contact IT if corporate firewall is blocking

- **"You appear to be offline. Check your internet connection."**
  - **Meaning:** Browser detects no network connectivity (navigator.onLine = false)
  - **Action:** Reconnect to Wi-Fi/Ethernet and retry

**Authentication & Authorization Errors:**
- **"Session expired. Please reload and log in again."** (HTTP 401)
  - **Meaning:** JWT token expired or invalid
  - **Action:** Refresh page (F5) and log in again with valid credentials

- **"Not authorized to perform this action."** (HTTP 403)
  - **Meaning:** User lacks permission for the operation
  - **Action:** Contact admin to grant required role/permissions

**Service Availability Errors:**
- **"Service temporarily unavailable. Try again shortly."** (HTTP 503)
  - **Meaning:** Backend API is temporarily down (database error, maintenance, etc.)
  - **Action:**
    - Wait 30-60 seconds and retry
    - Check `/ops/status` page for component health
    - Check Coolify logs if persistent: `docker logs pms-backend`
    - See [Quick Smoke Test](#quick-smoke-5-minutes) for diagnostics

**Other Errors:**
- **"Unexpected error occurred"**
  - **Meaning:** Unknown error type (not network, not HTTP)
  - **Action:** Check browser console for details, report to developers

**Why No Browser Alerts:**
Browser popups (window.alert/confirm/prompt) are blocking and interrupt workflow. In-app notifications provide better UX:
- Non-blocking: User can continue working
- Consistent styling: Matches app design
- Context-aware: Stays within the application UI
- Auto-dismiss: Reduces manual clicks (10 seconds)

**Regression Guard:**

To prevent accidental reintroduction of browser popups, the codebase includes automated checks:

**ESLint Rule:**
The frontend has ESLint configured with `"no-alert": "error"` (`.eslintrc.json`). This prevents:
- `alert()` / `window.alert()`
- `confirm()` / `window.confirm()`
- `prompt()` / `window.prompt()`

If ESLint detects a browser popup during development:
1. Remove the popup call
2. Replace with in-app notification banner (see examples above)
3. Use the existing notification system (`setNotification()` pattern)

**Manual Check Script:**
Run the regression guard script to verify no browser popups exist:

```bash
bash frontend/scripts/check_no_browser_popups.sh
```

**Expected Output (Clean):**
```
Checking frontend/ for browser popups (alert/confirm/prompt)...
Repo root: /Users/.../PMS-Webapp

✅ OK: No browser popups (alert/confirm/prompt) found in frontend/
```

**If Popups Found:**
```
❌ FAILED: Browser popups detected!

frontend/app/connections/page.tsx:142:        alert("Test failed!");

Action required:
  - Replace alert() with in-app notification banners/toasts
  - Replace confirm() with in-app confirmation dialogs
  - Replace prompt() with in-app input forms
  - See backend/docs/ops/runbook.md for guidance
```

**Monthly Maintenance:**
Run `check_no_browser_popups.sh` as part of monthly code quality checks (see [Daily Ops Checklist](#daily-ops-checklist) for automation).

### Overview

The Connections page provides a centralized interface for Channel Manager operations:
- **List Connections:** View all channel connections with search/filter
- **Test Health:** Quick inline tests or detailed test results
- **Connection Details:** Modal view with full management capabilities
- **Trigger Syncs:** Manual sync operations (full/availability/pricing/bookings)
- **Monitor Logs:** Real-time sync log tracking with auto-refresh and filters

### Connections List

**Features:**
- **Search box:** Filter by connection ID, platform type, or status (client-side)
- **Table columns:** ID (copyable), Platform, Status, Last Sync, Updated
- **Inline actions:** Test (quick toast result), Open (detailed modal)

**Common Actions:**

1. **Create new connection:**
   - Click "New Connection" button (indigo, next to Refresh)
   - Opens modal form with fields:
     - **Property:** Select from dropdown (loads via API)
     - **Platform:** airbnb, booking_com, expedia, fewo_direkt, google
     - **Platform Listing ID:** Text input (default: `mock_<platform>_<timestamp>`)
     - **Access Token:** Text input (default: `mock_access_token`)
     - **Refresh Token:** Optional (default: `mock_refresh_token`)
     - **Platform Metadata:** JSON textarea (default: `{"mock_mode": true}`)
   - **Mock Mode Support:** In mock mode, mock tokens are accepted (no real OAuth required)
   - Click "Create Connection" to submit POST `/api/v1/channel-connections/`
   - On success: modal closes, success toast appears, list refreshes
   - On error: shows friendly error message (e.g., "Not allowed: need admin or manager role")
   - **Requirements:** Admin or Manager role

2. **Search connections:**
   - Type in search box to filter by ID/platform/status
   - Results update instantly (client-side filter)

3. **Copy connection ID:**
   - Click on truncated ID (e.g., "abc123...") to copy full UUID
   - Useful for API calls or debugging

4. **Quick test:**
   - Click "Test" button for inline health check
   - Shows in-app notification: "Connection test passed: Mock: Connection is healthy (Mock Mode - see runbook for production setup)"
   - Non-blocking, auto-dismisses after 10 seconds

5. **Open details:**
   - Click "Open" to view full connection details modal
   - See sync operations, logs, and advanced controls

### Connection Details Modal

**Sections:**

#### 1. Summary
Shows key connection fields:
- Platform type (airbnb, booking_com, etc.)
- Status (active, paused, inactive, error)
- Last sync timestamp
- Last updated timestamp

#### 2. Test Connection
- **Button:** "Run Test" - performs POST `/api/v1/channel-connections/{id}/test`
- **Result display:**
  - Green banner: Connection healthy (status=active in mock mode)
  - Red banner: Connection unhealthy (status!=active or test failed)
  - Details section: Shows `mock_mode`, `simulated`, connection status
- **Mock mode:** If `CHANNEL_MOCK_MODE=true` (current production), response includes `mock_mode: true` flag

#### 3. Trigger Sync
- **Dropdown:** Select sync type (Full, Availability, Pricing, Bookings)
- **Button:** "Trigger Sync" - performs POST `/api/v1/channel-connections/{id}/sync` with `{"sync_type": "..."}`
- **Result:** In-app notification (non-blocking inline banner, auto-clears after 5s):
  - **Success:** Green banner showing "Sync gestartet: {sync_type}" with clickable batch_id (if Full Sync)
  - **Error:** Red banner showing "Fehler beim Starten (HTTP {status}): {detail}" (or generic error message)
  - **Dismiss:** Click X button to close notification manually
- **Auto-refresh:** Logs table refreshes 1s after sync trigger to show new log entry

#### 4. Sync Logs
- **Auto-refresh toggle:** Default ON, refreshes every 10 seconds while modal open
- **Filters:**
  - Status: All / Triggered / Running / Success / Failed
  - Sync Type: All / Full / Availability / Pricing / Bookings
- **Table columns:**
  - Time (created_at timestamp)
  - Type (operation_type or sync_type)
  - Status (color-coded badges: green=success, red=failed, blue=running, gray=triggered)
  - Error (first 50 chars, truncated)
  - Actions: "Details" button opens full log JSON modal
- **Details modal:** Shows complete log object in formatted JSON (error details, retry count, task ID, metadata)

### UX Features

#### New Connection Modal

**Platform Selection:**
- Platform dropdown starts **unselected** (shows placeholder: "Select a platform...")
- User **must explicitly choose** a platform before submitting
- Property dropdown also starts unselected (default behavior)
- Submit button is disabled until both property and platform are selected

**Validation:**
- Backend requires both `property_id` and `platform_type`
- Frontend validates before submission to prevent errors

**Design Rationale:**
- Prevents accidental default selections
- Ensures deliberate platform choice
- Reduces user errors from auto-selected platforms

#### Batch Details Navigation

**Back Arrow Behavior:**
- Batch Details modal always shows icon-only back arrow (left arrow, no text)
- **If opened from Log Details:** Back arrow returns to Log Details modal (same log entry)
- **If opened directly from Batch History table:** Back arrow closes Batch Details and returns to Connection Details
- Tooltip dynamically shows "Back to log details" or "Back to connection" based on context

**Implementation:**
- Modal stack approach: Log Details (z-index 60) and Batch Details (z-index 70) can coexist
- When navigating Log Details → Batch Details, Log Details stays open in background
- Back arrow simply closes Batch Details, revealing whatever was underneath
- Source context is tracked via `sourceLogId` state for tooltip accuracy

**User Flow:**
```
Connection Details
  → Open Log Details
    → Click "Open Batch Details →" button
      → Batch Details opens on top (Log Details still in state)
      → Click back arrow ←
        → Returns to Log Details (same log entry)
        → Click X to close Log Details
          → Returns to Connection Details
```

### API Endpoints Used

**List connections:**
```
GET /api/v1/channel-connections/
Response: JSON array of connection objects (not wrapped in {items:...})
```

**Test connection:**
```
POST /api/v1/channel-connections/{id}/test
Request: {}
Response: {healthy: bool, message: str, details: {...}}
```

**Trigger sync:**
```
POST /api/v1/channel-connections/{id}/sync
Request: {sync_type: "full"|"availability"|"pricing"|"bookings"}
Response: {
  status: "triggered",
  message: "...",
  task_ids: ["celery-task-id-1", ...],
  batch_id: "uuid" (only for full sync, groups 3 operations)
}
```

**IMPORTANT:** Response ALWAYS includes `task_ids` array (not `task_id`). Full sync also returns `batch_id` to group the 3 operations (availability_update, pricing_update, bookings_sync). Backend triggers Celery tasks and creates sync log entries.

### Sync Type → Operation Type Mapping

When you trigger a sync via UI or API, the backend creates sync log entries with specific `operation_type` values:

| sync_type (Request) | operation_type (Logs) | Direction | Description |
|---------------------|----------------------|-----------|-------------|
| `availability` | `availability_update` | outbound | Push availability to platform |
| `pricing` | `pricing_update` | outbound | Push pricing to platform |
| `bookings` | `bookings_sync` | inbound | Import bookings from platform |
| `full` | `availability_update` + `pricing_update` + `bookings_sync` | both | Triggers all three syncs (3 log entries) |

**Key Points:**
- `full` creates **3 separate log entries** (one per operation type)
- Each log entry has a unique `task_id` from Celery
- Filter by "Type" in UI to see specific operation types
- Poll script validates `operation_type` matches requested `sync_type` (prevents false positives)

**Troubleshooting:**

| Issue | Cause | Fix |
|-------|-------|-----|
| Bookings sync shows "success" but wrong operation_type | Backend not emitting bookings logs (old version) | Update to commit `d4434cf` or later |
| Full sync shows only `availability_update` | Backend not triggering all tasks | Update to commit `d4434cf` or later |
| Poll script reports "success" for wrong type | Old poll script (no type validation) | Update to latest `pms_channel_sync_poll.sh` |
| No `task_ids` in sync response | Backend error or old version | Check backend logs, verify migration applied |

**Fetch sync logs:**
```
GET /api/v1/channel-connections/{id}/sync-logs?limit=50&offset=0
Response: Array or {logs: [...]} depending on backend version
UI handles both formats defensively
```

### Common Errors

| Error | Symptom | UI Behavior | Fix |
|-------|---------|-------------|-----|
| 401 Unauthorized | Token expired | Error banner: "Unauthorized - please log in again" | Re-login via `/login` |
| 403 Forbidden | Non-admin user | Error banner: "Forbidden - admin access required" | Use admin account or check RLS policies |
| 404 Connection not found | Connection ID invalid or deleted | Test/sync fails with "Connection not found" | Verify connection exists with `deleted_at IS NULL` |
| 405 Method Not Allowed | Using GET on /test | Backend returns 405 | UI uses POST (correct), only applies to curl errors |
| 503 Service Unavailable | DB schema missing columns | Error banner: "Service unavailable - database may need migrations" | Apply migration 20260101030000 (see [Schema Drift](#channel-manager---channel_connections-schema-drift)) |
| Empty logs array | No sync operations run yet | Logs table shows "No logs found" | Trigger a sync or wait for scheduled sync |

### Troubleshooting

**Connections list empty:**
1. Check if backend returns 200: Open browser DevTools → Network tab → Refresh page
2. If 200 but empty array: No connections seeded in database (expected in fresh deployments)
3. Solution: Create connection via API (see [Channel Connections Management](../scripts/README.md#channel-connections-management-curl-examples))

**Test always shows "unhealthy":**
1. Check if mock mode enabled: Backend env `CHANNEL_MOCK_MODE=true`
2. Check connection status: Only `status='active'` returns healthy in mock mode
3. If status='paused', test returns: `healthy: false, message: "Mock: Connection status is paused"`

**Sync trigger does nothing:**
1. Check browser console for errors (F12 → Console tab)
2. Verify in-app notification appears (green success banner or red error banner below the Trigger Sync button)
3. If no notification: POST /sync may have failed silently (check Network tab for API response)
4. Check sync logs table for new entry (should appear within 1-2s)
5. If no log entry: Backend may not support /sync endpoint or Celery worker offline

**Auto-refresh not working:**
1. Verify toggle is ON (checkbox should be checked)
2. Open browser console to see if fetch errors occur every 10s
3. If errors: Backend /sync-logs endpoint may be failing (check HTTP status)

**Logs show cryptic field names:**
1. UI expects fields: `id`, `created_at`, `status`, `operation_type`/`sync_type`, `error`
2. If backend uses different field names, logs may display "N/A"
3. Click "Details" to see raw log JSON and identify actual field names

**Input text not visible (white-on-white):**
1. Symptom: Search input, dropdown selections, or modal summary text appears invisible
2. Cause: Browser cache serving old CSS or stale build
3. Fix: Hard refresh (Ctrl+F5 or Cmd+Shift+R), clear browser cache, verify latest deploy
4. Verify: Check SOURCE_COMMIT in backend logs matches latest git commit
5. Note: UI now enforces explicit text colors (text-gray-900) to prevent this issue

**Log Details JSON text invisible/too light:**
1. Symptom: Clicking "Details" in Sync Logs shows JSON modal, but text appears white/faint (only readable when highlighted)
2. Cause: Browser cache serving old CSS without explicit code block styles
3. Fix: Hard refresh (Ctrl+F5 or Cmd+Shift+R), clear browser cache, ensure latest deploy
4. Verify: JSON should appear in dark text (slate-900) on light background (slate-50) with border
5. Note: UI now enforces readable code block styles (bg-slate-50, text-slate-900, border)

**Text/JSON white-on-white across admin UI (Inputs/Modals/JSON blocks):**
1. Symptom: Multiple UI elements show invisible white text (search inputs, modal content, JSON viewers in /connections and /ops/status)
2. Root Cause: System Dark Mode preference triggers `globals.css` media query that sets body text to white (--foreground-rgb: 255,255,255)
3. Components affected: Search inputs, Connection Details modal summary, Log Details JSON, /ops/status raw JSON blocks
4. Fix: Deploy frontend with JsonViewer component (components/ui/json-viewer.tsx) for all JSON displays, then hard refresh
5. Verify: All mentioned components should display dark text clearly in both Light and Dark system modes
6. Technical Note: JsonViewer component uses explicit bg-slate-50/text-slate-900 (not inherited color) to override globals.css dark mode defaults
7. Pages using JsonViewer: /ops/status (health + ready raw JSON), /connections (Log Details modal + test result details)

---

## Full Sync Operations (Channel Manager)

**Purpose:** Understand and validate full sync behavior, which triggers ALL Channel Manager operations concurrently.

### What is Full Sync?

Full sync (`sync_type=full`) triggers **three concurrent Celery tasks**:
1. **Availability Sync** (`availability_update`) - Syncs property availability/calendar
2. **Pricing Sync** (`pricing_update`) - Syncs rates and pricing rules
3. **Bookings Sync** (`bookings_sync` or `bookings_import`) - Imports platform bookings

In **mock mode** (development/testing), all three operations return instant mock success. In **production mode**, tasks make real API calls to external platforms (Airbnb, Booking.com, etc.).

### Validation (Full Sync Success Criteria)

Full sync is **only successful** when:
- ✅ ALL three operation types are present in `channel_sync_logs`
- ✅ ALL three have `status='success'`
- ✅ All logs created AFTER trigger timestamp (no old logs)

**Partial success is treated as failure:**
- ❌ If only `availability_update` + `pricing_update` appear → **NOT complete**
- ❌ If any operation has `status='failed'` → **Entire sync failed**
- ❌ If bookings task is `triggered` but stuck → **Incomplete** (wait or timeout)

### How to Validate Full Sync

**1. Via Script (Recommended):**
```bash
# Trigger full sync and wait for all operations
bash backend/scripts/pms_channel_sync_poll.sh \
  --sync-type full \
  --poll-limit 30 \
  --poll-interval 2

# Expected output:
# ℹ️  Found operations: [availability_update, bookings_sync, pricing_update] (need: [availability_update, pricing_update, bookings_sync])
# ✅ SYNC COMPLETED SUCCESSFULLY
# Operation Summary:
#   Operation Type            Status          Task ID
#   ------------------------- --------------- ----------------------------------------
#   availability_update       success         abc-123...
#   bookings_sync             success         def-456...
#   pricing_update            success         ghi-789...
```

**2. Via UI:**
```
1. Navigate to: https://admin.fewo.kolibri-visions.de/connections
2. Click "Sync" button → Select "Full Sync"
3. Wait for sync to complete (alert shows "Sync triggered successfully")
4. Check "Sync Logs" table → Should show 3 recent entries:
   - availability_update (success)
   - pricing_update (success)
   - bookings_sync (success)
```

**3. Via Database:**
```sql
-- Check last 5 sync logs for connection
SELECT
  operation_type,
  status,
  created_at,
  task_id,
  error
FROM channel_sync_logs
WHERE connection_id = '<your-connection-uuid>'
ORDER BY created_at DESC
LIMIT 5;

-- Expected (after full sync):
-- availability_update | success | 2026-01-01 12:00:01 | abc-123 | NULL
-- pricing_update      | success | 2026-01-01 12:00:01 | def-456 | NULL
-- bookings_sync       | success | 2026-01-01 12:00:02 | ghi-789 | NULL
```

### Troubleshooting Full Sync

**Issue: Poll script exits early (no "Found operations:" output)**

**Symptoms:**
- Script prints "Poll attempt 1/20..." and "GET /sync-logs => HTTP 200, XXXX bytes"
- Then terminates immediately (exit=0) WITHOUT printing operation summary

**Root Cause (Fixed in Commit TBD):**
- Fragile string injection in Python parser caused silent failures
- Empty POLL_RESULT led to false "all_success" condition

**Fix:**
```bash
# Ensure you have the latest poll script
git pull origin main
bash backend/scripts/pms_channel_sync_poll.sh --sync-type full --poll-limit 30
```

**Verification:**
- Script should now print: `ℹ️  Found operations: [...]` on EVERY poll
- If missing: Check `/tmp/pms_poll_*.{json,txt}` for write errors

---

**Issue: Only 1 or 2 operations appear (missing bookings_sync)**

**Symptoms:**
- Script finds `availability_update` + `pricing_update` only
- Missing `bookings_sync` or `bookings_import`

**Diagnosis:**
```sql
-- Check if bookings task was created
SELECT operation_type, status, created_at, error
FROM channel_sync_logs
WHERE connection_id = '<cid>'
  AND operation_type IN ('bookings_sync', 'bookings_import')
ORDER BY created_at DESC
LIMIT 1;
```

**Possible Causes:**
1. **Backend not triggering bookings task:** Update to commit `d4434cf` or later
2. **Celery worker offline:** Check `docker ps | grep celery` and verify worker is running
3. **Bookings task failed:** Check worker logs: `docker logs <celery-container> | grep bookings`
4. **DB constraint blocks bookings_sync:** Check for error: `channel_sync_logs_operation_type_check` violation
   - Fix: Apply migration `20260101140000_fix_channel_sync_logs_operation_type_check.sql`

---

**Issue: Full sync succeeds in UI but script says "not complete"**

**Symptom:**
- UI shows all 3 logs with status=success
- Poll script continues waiting or times out

**Diagnosis:**
```bash
# Check trigger timestamp vs log timestamps
TRIGGER_TS=$(date +%s)
echo "Trigger timestamp: $TRIGGER_TS"

# In DB, check created_at (must be AFTER trigger)
SELECT operation_type,
       EXTRACT(EPOCH FROM created_at) as log_ts
FROM channel_sync_logs
WHERE connection_id = '<cid>'
ORDER BY created_at DESC
LIMIT 3;
```

**Possible Cause:**
- Script is filtering out OLD logs (created before trigger timestamp)
- Full sync might have triggered tasks, but logs existed from previous sync

**Fix:**
- Clear old logs before triggering full sync:
```bash
bash backend/scripts/pms_channel_seed_connection.sh --reset --yes
bash backend/scripts/pms_channel_sync_poll.sh --sync-type full
```

---

### See Also

- [pms_channel_sync_poll.sh Documentation](../scripts/README.md#pms_channel_sync_pollsh) - Full script reference
- [Channel Manager Sync Logs Migration](#channel-manager-sync-logs-migration) - DB schema for logs
- [Celery Worker Troubleshooting](#celery-worker-pms-worker-v2-start-verify-troubleshoot) - Worker issues

---

### Navigation Path

**From login:**
1. Log in as admin user at `/login`
2. Admin layout auto-loads with navigation tabs: Status | Runbook | Connections | Sync
3. Click "Connections" tab
4. See connections list and search box

**Direct URL:**
- `https://admin.fewo.kolibri-visions.de/connections`
- Requires admin authentication (layout enforces server-side check)
- Non-admins redirected to `/channel-sync`

### Robust Sync Trigger + Polling (HOST-SERVER-TERMINAL)

**Purpose:** Production-grade script for triggering channel syncs and waiting for completion with robust error handling.

**Script:** `backend/scripts/pms_channel_sync_poll.sh`

**Why Use This Instead of curl:**
- **Auto-recovery:** Never crashes on empty responses (SSH disconnects, network drops)
- **Invalid JSON handling:** Shows first 200 bytes for debugging, continues polling
- **Redirect support:** Follows 307/302 redirects automatically with `curl -L`
- **Exit codes:** Clean 0/1/2 exit codes for automation
- **Polling:** Waits for completion (success/failed) instead of fire-and-forget

**Common Failure Modes This Handles:**
1. **Empty response due to disconnect:**
   - Symptom: `JSONDecodeError` when parsing empty body from `curl`
   - Old behavior: Script crashes with Python traceback
   - New behavior: Prints `⚠️ Empty response (bytes=0), retrying...` and continues

2. **JSONDecodeError when parsing empty body:**
   - Symptom: `jq` or `python -m json.tool` fails on partial/corrupt JSON
   - Old behavior: Script exits with error
   - New behavior: Shows first 200 bytes preview, continues polling

3. **Redirect chain without -L:**
   - Symptom: `/api/v1/channel-connections/` returns HTTP 307, no data
   - Old behavior: Script tries to parse redirect HTML as JSON
   - New behavior: Uses `curl -L` to follow redirects automatically

4. **Transient 502 during deploy/proxy restart:**
   - Symptom: Auto-pick list call returns HTTP 502 (11 bytes) during deploy or proxy hiccup
   - Old behavior: Script exits immediately with "Failed to list connections (HTTP 502)"
   - New behavior: Retries up to 5 times with exponential backoff (1s, 2s, 3s, 5s, 8s), prints `⚠️ Transient 502 error (deploy/proxy hiccup), retrying...`
   - Alternative: Pass `--cid` explicitly to bypass auto-pick entirely

**Quick Start:**
```bash
# On HOST-SERVER-TERMINAL
source /root/pms_env.sh
bash /app/scripts/pms_channel_sync_poll.sh
```

**Example Usage:**
```bash
# Auto-pick first connection, trigger availability sync
bash scripts/pms_channel_sync_poll.sh

# Explicit connection ID, pricing sync
bash scripts/pms_channel_sync_poll.sh --cid abc-123-456 --sync-type pricing

# Longer polling for slow syncs (30 attempts, 3s interval)
bash scripts/pms_channel_sync_poll.sh --poll-limit 30 --poll-interval 3
```

**Exit Codes:**
- `0` - Sync completed successfully
- `1` - Sync failed (check error message in output)
- `2` - Sync task not found after polling limit (worker offline or queue stuck)

**Troubleshooting:**

**Exit code 2 (task not found):**
1. Check Celery worker is running: `docker ps | grep pms-worker`
2. Check Redis is accessible: `docker logs pms-worker | grep -i redis`
3. Increase `--poll-limit` if sync takes longer than 40s (20 attempts × 2s default)

**See Also:**
- [Script Documentation](../scripts/README.md#pms_channel_sync_pollsh) - Full script reference with all options
- [Channel Manager Sync (curl examples)](../scripts/README.md#channel-manager-sync-curl-examples) - Manual curl commands for debugging

### Related Sections

- [Channel Manager - Schema Drift](#channel-manager---channel_connections-schema-drift) - Fix 503 schema errors
- [Mock Mode for Channel Providers](#mock-mode-for-channel-providers) - Mock mode configuration
- [Channel Manager API Endpoints](#channel-manager-api-endpoints) - Complete API reference
- [Channel Connections Management (curl examples)](../scripts/README.md#channel-connections-management-curl-examples) - Server-side testing

---

## Channel Manager API Endpoints

**Purpose:** Complete reference for Channel Manager API endpoints with request/response examples.

### Authentication

**All endpoints require Bearer JWT authentication:**
```bash
curl -H "Authorization: Bearer YOUR_JWT_TOKEN" ...
```

**Without token → 401 Unauthorized**

---

### POST /api/availability/sync

**Purpose:** Trigger availability or pricing sync to external booking platform

**RBAC:** admin, manager only (NOT owner, staff, accountant)

**Request Body:**
```json
{
  "sync_type": "availability",           // Required: "availability" or "pricing"
  "platform": "airbnb",                   // Required: "airbnb", "booking_com", "expedia", "fewo_direkt", "google"
  "property_id": "uuid-here",             // Required: Property UUID
  "connection_id": "uuid-optional",       // Optional: Specific connection UUID
  "manual_trigger": true,                 // Optional: Default true
  "start_date": "2025-12-28",            // Optional: Default today
  "end_date": "2026-03-28"               // Optional: Default today + 90 days
}
```

**Success Response (200):**
```json
{
  "status": "triggered",
  "message": "Availability sync task triggered successfully",
  "task_id": "abc123-def456-ghi789",
  "sync_log_id": "xyz789-uvw012-rst345",
  "platform": "airbnb",
  "retry_count": 0
}
```

**Error Responses:**

**400 Bad Request** (Invalid sync_type):
```json
{
  "detail": "Invalid sync_type. Must be one of: ['availability', 'pricing']"
}
```

**400 Bad Request** (Invalid platform):
```json
{
  "detail": "Invalid platform. Must be one of: ['airbnb', 'booking_com', 'expedia', 'fewo_direkt', 'google']"
}
```

**404 Not Found** (Property not found):
```json
{
  "detail": "Property not found or does not belong to your agency"
}
```

**503 Service Unavailable** (Database down after retries):
```json
{
  "error": "service_unavailable",
  "message": "Database is temporarily unavailable.",
  "retry_count": 3
}
```

**500 Internal Server Error** (Other failures):
```json
{
  "detail": "Failed to trigger availability sync: [error details]"
}
```

**Example Usage:**
```bash
# Trigger availability sync to Airbnb
curl -X POST https://api.your-domain.com/api/availability/sync \
  -H "Authorization: Bearer YOUR_JWT" \
  -H "Content-Type: application/json" \
  -d '{
    "sync_type": "availability",
    "platform": "airbnb",
    "property_id": "550e8400-e29b-41d4-a716-446655440000",
    "manual_trigger": true,
    "start_date": "2025-12-28",
    "end_date": "2026-03-28"
  }'
```

---

### POST /api/v1/channel-connections/{id}/sync

**Purpose:** Trigger manual sync for a channel connection

**RBAC:** Requires valid JWT (all authenticated users)

**Request Body:**
```json
{
  "sync_type": "full"  // Required: "full", "availability", "pricing", "bookings"
}
```

**Success Response (200):**
```json
{
  "status": "triggered",
  "message": "Manual full sync triggered successfully",
  "task_ids": ["task-uuid-1", "task-uuid-2"]
}
```

**Error Responses:**

**400 Bad Request** (Invalid sync_type):
```json
{
  "detail": "Invalid sync_type. Must be one of: ['full', 'availability', 'pricing', 'bookings']"
}
```

**404 Not Found** (Connection not found):
```json
{
  "status": "error",
  "message": "Connection not found",
  "task_ids": []
}
```

---

### GET /api/v1/channel-connections/{id}/sync-logs

**Purpose:** Retrieve sync operation logs for a connection

**RBAC:** Requires valid JWT

**Query Parameters:**
- `limit` (optional): Number of logs to return (max 100, default 50)
- `offset` (optional): Pagination offset (default 0)

**Success Response (200):**
```json
{
  "connection_id": "connection-uuid",
  "logs": [
    {
      "id": "log-uuid",
      "connection_id": "connection-uuid",
      "operation_type": "availability_update",
      "direction": "outbound",
      "status": "success",
      "details": {
        "platform": "airbnb",
        "property_id": "property-uuid",
        "manual_trigger": true,
        "start_date": "2025-12-28",
        "end_date": "2026-03-28",
        "check_in": "2025-12-28",
        "check_out": "2025-12-30",
        "available": true
      },
      "error": null,
      "task_id": "celery-task-uuid",
      "created_at": "2025-12-28T10:00:00Z",
      "updated_at": "2025-12-28T10:00:05Z"
    }
  ],
  "limit": 50,
  "offset": 0
}
```

**Error Response (503):**
```json
{
  "error": "service_unavailable",
  "message": "Channel sync logs schema not installed (missing table: channel_sync_logs). Run DB migration: supabase/migrations/20251227000000_create_channel_sync_logs.sql"
}
```

---

### GET /api/v1/channel-connections

**Purpose:** List all channel connections for current agency

**RBAC:** Requires valid JWT

**Success Response (200):**
```json
[
  {
    "id": "connection-uuid",
    "tenant_id": "agency-uuid",
    "property_id": "property-uuid",
    "platform_type": "airbnb",
    "platform_listing_id": "airbnb-listing-123",
    "status": "active",
    "platform_metadata": {"listing_id": "123"},
    "last_sync_at": "2025-12-28T10:00:00Z",
    "created_at": "2025-12-01T00:00:00Z",
    "updated_at": "2025-12-28T10:00:00Z"
  }
]
```

---

### POST /api/v1/channel-connections

**Purpose:** Create new channel connection (OAuth integration)

**RBAC:** admin, manager only

**Request Body:**
```json
{
  "property_id": "property-uuid",
  "platform_type": "airbnb",
  "platform_listing_id": "airbnb-listing-123",
  "access_token": "oauth-access-token",
  "refresh_token": "oauth-refresh-token",
  "platform_metadata": {
    "listing_id": "123",
    "host_id": "456"
  }
}
```

**Success Response (201):**
```json
{
  "id": "new-connection-uuid",
  "tenant_id": "agency-uuid",
  "property_id": "property-uuid",
  "platform_type": "airbnb",
  "platform_listing_id": "airbnb-listing-123",
  "status": "active",
  "platform_metadata": {"listing_id": "123", "host_id": "456"},
  "last_sync_at": null,
  "created_at": "2025-12-28T10:00:00Z",
  "updated_at": "2025-12-28T10:00:00Z"
}
```

**Query Parameters:**

- `skip_connection_test` (boolean, default: `false`) - Skip external platform connection test (dev/mock mode only)

**Mock Mode / Development:**

When `skip_connection_test=true`:
- ✅ Skips OAuth validation and platform API health checks
- ✅ Allows creating connections for unsupported platforms (e.g., `booking_com`)
- ✅ Skips initial sync trigger
- ⚠️ Platform tokens are still encrypted and stored (but not validated)
- ⚠️ Backend logs warning: "Creating connection in MOCK MODE"

**Example - Create Connection in Mock Mode:**
```bash
curl -X POST "https://api.fewo.kolibri-visions.de/api/v1/channel-connections/?skip_connection_test=true" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "property_id": "property-uuid",
    "platform_type": "booking_com",
    "platform_listing_id": "mock_booking_123",
    "access_token": "mock_access_token",
    "refresh_token": "mock_refresh_token",
    "platform_metadata": {"mock_mode": true}
  }'
```

**Example - Production (with validation):**
```bash
# Default behavior - validates tokens and tests connection
curl -X POST "https://api.fewo.kolibri-visions.de/api/v1/channel-connections/" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "property_id": "property-uuid",
    "platform_type": "airbnb",
    "platform_listing_id": "airbnb-listing-123",
    "access_token": "real-oauth-access-token",
    "refresh_token": "real-oauth-refresh-token",
    "platform_metadata": {"listing_id": "123"}
  }'
```

**Common Errors (when skip_connection_test=false):**

| Error | Cause | Solution |
|-------|-------|----------|
| `400 Platform booking_com not yet supported` | Platform adapter not implemented | Use `skip_connection_test=true` for development |
| `400 Connection test failed: verify your OAuth tokens` | Invalid tokens or OAuth flow incomplete | Complete OAuth flow first, or use skip flag for testing |
| `400 Connection test failed: [platform error]` | Platform API rejected request | Check token validity, permissions, and platform status |

**Admin UI:**

The Admin UI's "New Connection" modal includes a checkbox:
- ✅ **"Mock mode (skip connection test)"** - Default: ON
- When checked: Passes `skip_connection_test=true` to the API
- Allows creating connections without valid OAuth tokens for development/testing

**Database Persistence:**

Channel connections are persisted in PostgreSQL:
- **Table:** `channel_connections`
- **Migration:** `supabase/migrations/20260102000000_add_property_fields_to_channel_connections.sql`
- **Key columns:** `id`, `agency_id`, `property_id`, `platform_type`, `platform_listing_id`, `status`, `platform_metadata`, `access_token_encrypted`, `refresh_token_encrypted`
- **Soft delete:** Rows marked with `deleted_at` timestamp instead of hard deletion

**List Endpoint:**

GET `/api/v1/channel-connections/` returns JSON array of all connections:
```json
[
  {
    "id": "connection-uuid",
    "tenant_id": "agency-uuid",
    "property_id": "property-uuid",
    "platform_type": "booking_com",
    "platform_listing_id": "mock_booking_123",
    "status": "active",
    "platform_metadata": {"mock_mode": true},
    "last_sync_at": null,
    "created_at": "2026-01-02T10:00:00Z",
    "updated_at": "2026-01-02T10:00:00Z"
  }
]
```

**Troubleshooting - List Returns Only Stub:**

If GET `/api/v1/channel-connections/` returns only hardcoded stub data (single airbnb entry with fixed UUIDs):
1. **WHERE:** HOST-SERVER-TERMINAL - Check deployed commit
   ```bash
   docker exec pms-backend env | grep SOURCE_COMMIT
   ```
2. **WHERE:** HOST-SERVER-TERMINAL - Verify API schema has skip_connection_test param
   ```bash
   curl -s https://api.fewo.kolibri-visions.de/openapi.json | jq '.paths."/api/v1/channel-connections/".post.parameters'
   # Should show skip_connection_test query parameter
   ```
3. **If old build:** Redeploy from main branch (commit c097acd or later)
4. **WHERE:** SUPABASE-SQL-EDITOR - Verify schema migration applied
   ```sql
   SELECT column_name, data_type
   FROM information_schema.columns
   WHERE table_name = 'channel_connections'
   AND column_name IN ('property_id', 'platform_listing_id');
   -- Should return both columns
   ```
5. **If schema missing:** Apply migration (see below)

**Applying Schema Migration:**

**WHERE:** SUPABASE-SQL-EDITOR (Supabase Dashboard → SQL Editor)
```sql
-- Run migration: 20260102000000_add_property_fields_to_channel_connections.sql
-- This adds property_id and platform_listing_id columns
-- Copy-paste migration file contents here or use Supabase CLI
```

**WHERE:** HOST-SERVER-TERMINAL (Alternative: Supabase CLI)
```bash
supabase migration up --file supabase/migrations/20260102000000_add_property_fields_to_channel_connections.sql
```

**Expected Result After Fix:**
- Created connections appear in list immediately
- No hardcoded stub data
- Empty array `[]` when no connections exist

**Troubleshooting - List Returns HTTP 500:**

If GET `/api/v1/channel-connections/` returns HTTP 500 and breaks Admin UI:
1. **WHERE:** HOST-SERVER-TERMINAL - Check backend logs for ResponseValidationError
   ```bash
   docker logs pms-backend --tail 100 | grep -i "ResponseValidationError\|property_id"
   # Look for: loc=('response', 0, 'property_id') msg='UUID input should be a string/bytes/UUID' input=None
   ```
2. **Root Cause:** Legacy connection rows have `property_id = null` but response model expected non-null UUID
3. **Fix:** Deploy includes:
   - Response model accepts `Optional[UUID]` for property_id
   - Per-row validation skips invalid rows instead of crashing entire endpoint
4. **WHERE:** HOST-SERVER-TERMINAL - Verify fix deployed
   ```bash
   docker exec pms-backend env | grep SOURCE_COMMIT
   # Should be commit 4c28afd or later
   ```
5. **If old build:** Redeploy from main branch (commit 4c28afd or later)
6. **Expected Result:**
   - GET returns HTTP 200 with array (may exclude legacy rows with validation errors)
   - Backend logs warnings for skipped rows but endpoint stays up
   - Admin UI Connections page loads successfully

**Admin UI - Properties Fetch 422:**

If Admin UI shows 422 error when fetching properties:
- **Cause:** Frontend requested `limit=200` but backend validation rejects high limits
- **Fix:** Deployed frontend uses safe limit=100
- **Verify:** Check browser DevTools → Network → `/api/v1/properties` request shows `?limit=100`

**Admin UI - Property Dropdown Empty:**

If New Connection modal shows empty Property dropdown despite 200 response:
1. **WHERE:** BROWSER - Check DevTools → Network → `/api/v1/properties` response format
   - **Expected:** `{items: [...], total: 16, limit: 100, offset: 0, has_more: false}`
   - **Issue:** Properties endpoint returns paginated object, not array
2. **Root Cause:** Frontend expected array but API returns `{items: [...]}`
3. **Fix:** Deployed frontend parses response correctly:
   ```typescript
   const items = Array.isArray(data) ? data : (data.items || data.properties || []);
   ```
4. **Expected Result:**
   - Dropdown shows all properties with readable labels
   - Loading state: "Loading properties..." (disabled)
   - Error state: "No properties available" (red background)
   - Success state: "Property Name (internal_name) - 12345678"

**Multi-Tenant Filtering (Channel Connections):**

Channel connections list is filtered by agency_id for tenant isolation:
- **Backend:** GET `/api/v1/channel-connections/` uses `get_current_agency_id` dependency
- **Filter:** `WHERE agency_id = $1 AND deleted_at IS NULL`
- **Result:** Each tenant only sees their own connections
- **Security:** Prevents cross-tenant data leaks

If booking_com connection created via API doesn't appear in UI:
1. **WHERE:** HOST-SERVER-TERMINAL - Verify connection was created for correct agency
   ```bash
   # Check connection's agency_id matches user's agency
   curl -s "https://api.fewo.kolibri-visions.de/api/v1/channel-connections/" \
     -H "Authorization: Bearer $TOKEN" | jq '.[] | {id, agency_id, platform_type}'
   ```
2. **Verify:** Created connection's `agency_id` matches property's `agency_id`
3. **Expected:** Backend derives `agency_id` from property, not from hardcoded values

**Mock Mode Behavior:**

When creating connections with `skip_connection_test=true`:
- ✅ Skips OAuth validation and platform API health checks
- ✅ Allows unsupported platforms (e.g., booking_com)
- ✅ Persists to database immediately
- ✅ Skips initial sync trigger
- ⚠️ Tokens still encrypted before storage (but not validated)

**Backoffice Console → New Connection:**

When opening the New Connection modal in Admin UI:
- **Property:** Empty by default - user must select from dropdown
- **Platform:** Empty by default - user must explicitly select (no preselection)
- **Intentional Design:** Forces user to make conscious choice rather than defaulting to Airbnb
- **Mock Mode:** After platform selection, mock values are auto-suggested:
  - `platform_listing_id`: `mock_{platform}_{timestamp}`
  - `access_token`: `mock_access_token`
  - `refresh_token`: `mock_refresh_token`
- **Validation:** "Create Connection" button disabled until both Property and Platform selected

**Expected Behavior:**
1. Open modal → both Property and Platform show placeholder text
2. Select Property → dropdown populated from `/api/v1/properties?limit=100`
3. Select Platform → `platform_listing_id` auto-populated with `mock_{platform}_{timestamp}`
4. Fill remaining fields → click "Create Connection"

---

### GET /api/v1/channel-connections/{connection_id}

**Purpose:** Get details for a specific channel connection

**RBAC:** Requires valid JWT

**Success Response (200):**
```json
{
  "id": "connection-uuid",
  "tenant_id": "agency-uuid",
  "property_id": "property-uuid",
  "platform_type": "airbnb",
  "platform_listing_id": "airbnb-listing-123",
  "status": "active",
  "platform_metadata": {"listing_id": "123"},
  "last_sync_at": "2025-12-28T10:00:00Z",
  "created_at": "2025-12-01T00:00:00Z",
  "updated_at": "2025-12-28T10:00:00Z"
}
```

**Error Response (404):**
```json
{
  "detail": "Connection not found"
}
```

**Example:**
```bash
curl -X GET "https://api.fewo.kolibri-visions.de/api/v1/channel-connections/$CID" \
  -H "Authorization: Bearer $TOKEN"
```

---

### POST /api/v1/channel-connections/{connection_id}/test

**Purpose:** Test if a channel connection is healthy (validates OAuth tokens and platform API connectivity)

**RBAC:** Requires valid JWT

**Request Body:**
```json
{}
```

**Success Response (200):**
```json
{
  "connection_id": "connection-uuid",
  "platform_type": "airbnb",
  "healthy": true,
  "message": "Connection is healthy",
  "details": {
    "platform_listing_id": "airbnb-listing-123",
    "last_sync_at": "2025-12-28T10:00:00Z"
  }
}
```

**Failed Health Check (200 with healthy=false):**
```json
{
  "connection_id": "connection-uuid",
  "platform_type": "airbnb",
  "healthy": false,
  "message": "Connection test failed",
  "details": {}
}
```

**Error Response (404):**
```json
{
  "connection_id": "connection-uuid",
  "platform_type": "unknown",
  "healthy": false,
  "message": "Connection not found",
  "details": {}
}
```

**Example:**
```bash
curl -X POST "https://api.fewo.kolibri-visions.de/api/v1/channel-connections/$CID/test" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{}'
```

---

### GET /api/v1/channel-connections/{connection_id}/sync-batches

**Purpose:** List recent sync batches for a connection (aggregated status for batch operations)

**RBAC:** Requires valid JWT

**Query Parameters:**
- `limit` (optional): Number of batches to return (max 200, default 50)
- `offset` (optional): Pagination offset (default 0)
- `status` (optional): Filter by batch status: `any`, `running`, `success`, `failed` (default `any`)

**Success Response (200):**
```json
[
  {
    "batch_id": "batch-uuid",
    "connection_id": "connection-uuid",
    "batch_status": "success",
    "status_counts": {
      "triggered": 0,
      "running": 0,
      "success": 3,
      "failed": 0,
      "other": 0
    },
    "created_at_min": "2025-12-28T10:00:00Z",
    "updated_at_max": "2025-12-28T10:00:15Z",
    "operations": [
      {
        "operation_type": "availability_update",
        "status": "success",
        "updated_at": "2025-12-28T10:00:05Z"
      },
      {
        "operation_type": "pricing_update",
        "status": "success",
        "updated_at": "2025-12-28T10:00:10Z"
      },
      {
        "operation_type": "bookings_sync",
        "status": "success",
        "updated_at": "2025-12-28T10:00:15Z"
      }
    ]
  }
]
```

**Example:**
```bash
# List recent batches
curl -s "https://api.fewo.kolibri-visions.de/api/v1/channel-connections/$CID/sync-batches?limit=10&status=any" \
  -H "Authorization: Bearer $TOKEN"
```

**Batch Status Values:**
- `running`: At least one operation still in progress (triggered/queued/running)
- `success`: All operations completed successfully
- `failed`: At least one operation failed
- `unknown`: No operations or unexpected state

---

### GET /api/v1/channel-connections/{connection_id}/sync-batches/{batch_id}

**Purpose:** Get detailed status for a specific sync batch (for polling until completion)

**RBAC:** Requires valid JWT

**Success Response (200):**
```json
{
  "batch_id": "batch-uuid",
  "connection_id": "connection-uuid",
  "batch_status": "success",
  "status_counts": {
    "triggered": 0,
    "running": 0,
    "success": 3,
    "failed": 0,
    "other": 0
  },
  "created_at_min": "2025-12-28T10:00:00Z",
  "updated_at_max": "2025-12-28T10:00:15Z",
  "operations": [
    {
      "operation_type": "availability_update",
      "status": "success",
      "updated_at": "2025-12-28T10:00:05Z"
    }
  ]
}
```

**Error Response (404):**
```json
{
  "error": "batch_not_found",
  "message": "Batch not found or does not belong to this connection"
}
```

**Example - Polling Pattern:**
```bash
# 1. Trigger sync and capture batch_id
RESPONSE=$(curl -s -X POST "https://api.fewo.kolibri-visions.de/api/v1/channel-connections/$CID/sync" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"sync_type":"full"}')

BATCH_ID=$(echo "$RESPONSE" | jq -r '.batch_id')
echo "Triggered batch: $BATCH_ID"

# 2. Poll batch status until completion
while true; do
  BATCH_STATUS=$(curl -s "https://api.fewo.kolibri-visions.de/api/v1/channel-connections/$CID/sync-batches/$BATCH_ID" \
    -H "Authorization: Bearer $TOKEN" | jq -r '.batch_status')

  echo "Batch status: $BATCH_STATUS"

  if [[ "$BATCH_STATUS" != "running" ]]; then
    echo "Batch finished: $BATCH_STATUS"
    break
  fi

  sleep 1
done
```

**Note on Trailing Slash:**

⚠️ **IMPORTANT:** The list endpoint (`GET /api/v1/channel-connections/`) requires a trailing slash when using query parameters to avoid HTTP 307 redirects:

```bash
# ✅ CORRECT - Trailing slash before query params
curl "https://api.fewo.kolibri-visions.de/api/v1/channel-connections/?limit=5&offset=0"

# ❌ WRONG - No trailing slash causes 307 redirect
curl "https://api.fewo.kolibri-visions.de/api/v1/channel-connections?limit=5&offset=0"
```

**Why:** FastAPI treats `/channel-connections` and `/channel-connections/` as different routes. The list endpoint is registered with trailing slash.

**Fix:** Frontend uses `buildApiUrl()` helper to ensure consistent URL construction.

**Detail/Sync/Test Endpoints:** Do NOT use trailing slash (no query params needed):
```bash
# ✅ CORRECT - No trailing slash for detail/action endpoints
curl "https://api.fewo.kolibri-visions.de/api/v1/channel-connections/$CID"
curl -X POST "https://api.fewo.kolibri-visions.de/api/v1/channel-connections/$CID/test"
curl -X POST "https://api.fewo.kolibri-visions.de/api/v1/channel-connections/$CID/sync"
```

**Troubleshooting curl Verification:**

If you encounter issues when verifying channel-connections endpoints via curl:

**Issue: 307 Redirect with Empty Body**
```bash
# Symptom: curl returns 307 and empty response
curl "https://api.fewo.kolibri-visions.de/api/v1/channel-connections?limit=5"

# Fix 1: Add trailing slash before query params
curl "https://api.fewo.kolibri-visions.de/api/v1/channel-connections/?limit=5"

# Fix 2: Use -L flag to follow redirects automatically
curl -L "https://api.fewo.kolibri-visions.de/api/v1/channel-connections?limit=5"
```

**Issue: 401 "Token has expired" or 403 "Not authenticated"**
```bash
# Symptom: curl returns 401 Unauthorized or 403 Forbidden

# Fix 1: Ensure environment variables are loaded
source /root/pms_env.sh

# Fix 2: Verify SB_URL is set (required for token refresh)
echo $SB_URL
# Expected: https://supabase-kong-url (not empty)

# Fix 3: Refresh JWT token
curl -X POST "$SB_URL/auth/v1/token?grant_type=password" \
  -H "Content-Type: application/json" \
  -H "apikey: $SB_ANON_KEY" \
  -d "{\"email\":\"admin@example.com\",\"password\":\"your-password\"}" \
  | jq -r '.access_token'

# Fix 4: Use refreshed token
TOKEN=$(curl -X POST "$SB_URL/auth/v1/token?grant_type=password" \
  -H "Content-Type: application/json" \
  -H "apikey: $SB_ANON_KEY" \
  -d "{\"email\":\"admin@example.com\",\"password\":\"your-password\"}" \
  | jq -r '.access_token')

curl -H "Authorization: Bearer $TOKEN" \
  "https://api.fewo.kolibri-visions.de/api/v1/channel-connections/?limit=5"
```

**Common Pitfall:** In new SSH sessions, environment variables (SB_URL, SB_ANON_KEY) are not loaded automatically. Always run `source /root/pms_env.sh` first.

---

## Backoffice Console — Connections (E2E Check)

**Purpose:** End-to-end verification of Channel Connections management in the Admin UI (Backoffice Console).

**URL:** `https://admin.fewo.kolibri-visions.de/connections`

**RBAC:** Admin and Manager roles only

### A) Connections Quick Actions

Each connection row in the Connections table provides inline quick actions for rapid E2E testing and operational workflows:

**Quick Action Buttons:**

1. **Test** - Test connection health
   - Endpoint: `POST /api/v1/channel-connections/{id}/test`
   - Response: Health status, platform API connectivity check
   - Display: Shows pass/fail notification banner
   - Mock Mode: Displays "Mock Mode (Simulated)" badge when `CHANNEL_MOCK_MODE=true`

2. **View Logs** - Navigate to Channel Sync page with logs preloaded
   - Action: Sets `localStorage.setItem("channelSync:lastConnectionId", connection_id)`
   - Navigation: Redirects to `/channel-sync` page
   - Result: Logs load immediately for the selected connection (no manual Auto-detect needed)
   - UX: Does NOT auto-open sync log details modal (modal opens only on explicit user row click)

3. **Avail** (previously **A**) = Availability sync (quick trigger)
   - Endpoint: `POST /api/v1/channel-connections/{id}/sync` with `{"sync_type": "availability"}`
   - Response: Returns `task_ids` array (single task for availability-only sync)
   - Display: Shows success toast "Availability sync triggered (Batch: ...)" with auto-dismiss after 10 seconds

4. **Price** (previously **P**) = Pricing sync (quick trigger)
   - Endpoint: `POST /api/v1/channel-connections/{id}/sync` with `{"sync_type": "pricing"}`
   - Response: Returns `task_ids` array (single task for pricing-only sync)
   - Display: Shows success toast "Pricing sync triggered (Batch: ...)" with auto-dismiss after 10 seconds

5. **Book** (previously **B**) = Bookings sync (quick trigger)
   - Endpoint: `POST /api/v1/channel-connections/{id}/sync` with `{"sync_type": "bookings"}`
   - Response: Returns `task_ids` array (single task for bookings-only sync)
   - Display: Shows success toast "Bookings sync triggered (Batch: ...)" with auto-dismiss after 10 seconds

6. **Full** (previously **F**) = Full sync (quick trigger)
   - Endpoint: `POST /api/v1/channel-connections/{id}/sync` with `{"sync_type": "full"}`
   - Response: Returns `batch_id` + `task_ids` array (3 tasks: availability, pricing, bookings)
   - Display: Shows success toast "Full sync triggered (Batch: ...)" with batch ID and task count, auto-dismiss after 10 seconds

**Expected UX Behavior:**

- **No Dangerous Defaults:** No sync type or platform is preselected in form fields
- **Trigger Disabled Until Ready:** Sync trigger buttons disabled until all required selections are made
- **Auto-Refresh After Trigger:** After triggering sync, connections list refetches to update `last_sync_at` column
- **Logs Auto-Load After Trigger:** After successful sync trigger, logs automatically refresh to show new sync operation
- **Clear Resets All State:** Clicking "Clear" on Connection ID field:
  - Clears logs list
  - Closes sync log details modal if open
  - Hides stale success banners
  - Resets all filters and search state
- **Inline Status Feedback:** Each quick action button shows loading state during operation (e.g., "Testing..." or "...")
- **Last Sync Age Display:** `last_sync_at` column shows relative time ("3m ago", "2h ago", "never") instead of full ISO timestamp

**Fastest UI-Based E2E Check:**

The quick actions provide the fastest path for operators to verify end-to-end Channel Manager functionality:

1. Click **Test** → Verify connection health (< 1 second)
2. Click **A** (Availability) → Trigger sync (< 1 second to queue)
3. Click **View Logs** → Navigate to logs page and verify sync completed (logs preloaded)
4. Verify status badge shows "success" (green) or "failed" (red)

This workflow validates: API authentication, connection health, sync task queueing, Celery workers, database writes, and UI state management.

---

### UI Flow (Step-by-Step)

**1. Create New Connection**

1. Navigate to Connections page
2. Click "New Connection" button
3. **Verify:** Platform dropdown is **NOT preselected** (shows "Select a platform..." placeholder)
   - This is intentional UX - user must explicitly choose platform
   - Submit button disabled until platform selected
4. Select Property from dropdown
5. Select Platform (e.g., `booking_com`)
6. **For dev/staging:** Check "Skip connection test (Mock mode)" checkbox
   - This bypasses real API calls during connection creation
   - Backend accepts `?skip_connection_test=true` query parameter
7. Fill Platform Listing ID (e.g., `test_booking_123`)
8. Click "Create Connection"
9. **Expected:** New connection row appears in table

**2. Test Connection Health**

1. Find the newly created connection in the table
2. Click "Test" button (inline action)
3. **Expected (Mock Mode):**
   - Green notification: "Connection test passed: Mock: Connection is healthy (Mock Mode - see runbook for production setup)"
   - Badge shows "Mock Mode (Simulated)"
4. **Expected (Real Mode):**
   - Green notification if healthy, red if failed
   - No mock mode badge

**3. Trigger Sync & Monitor Logs**

1. Click "Open" on the connection row
2. In Connection Details modal, select "Availability" from sync type dropdown
3. Click "Trigger Sync"
4. **Expected:**
   - Green notification: "Sync gestartet: availability"
   - Sync Logs section shows new log entry with status "triggered" or "running"
5. Wait ~5-10 seconds (auto-refresh enabled by default)
6. **Expected:** Log status changes to "success" (green badge)

**4. View Batch History**

1. Scroll to "Batch History" section in Connection Details modal
2. **Expected:** Recent batch appears with:
   - Batch ID (truncated, click to copy full UUID)
   - Status badge (green for success)
   - Operation counts (e.g., "1/1 success")
   - Timestamp

**5. Navigate Batch Details**

1. Click on a batch row in Batch History table
2. **Expected:** Batch Details modal opens (z-index 70, on top of Connection Details)
3. **Verify:** Back arrow (←) visible in header (icon-only, no text)
4. Click back arrow
5. **Expected:** Returns to Connection Details modal (Batch Details closes)

**6. Log Details → Batch Details Navigation**

1. In Sync Logs section, click "Details" on any batched log entry
2. **Expected:** Log Details modal opens
3. If log has a `batch_id`, "Open Batch Details →" button appears
4. Click "Open Batch Details →"
5. **Expected:** Batch Details modal opens on top (Log Details stays in background)
6. Click back arrow ←
7. **Expected:** Returns to Log Details modal (same log entry, Batch Details closes)
8. Close Log Details modal (X button)
9. **Expected:** Returns to Connection Details

### API Flow (HOST-SERVER-TERMINAL)

**Prerequisites:**
```bash
# Export credentials
export API="https://api.fewo.kolibri-visions.de"
export SB_URL="https://sb-pms.kolibri-visions.de"
export ANON_KEY="your-anon-key"
export EMAIL="admin@example.com"
export PASSWORD="your-password"

# Fetch JWT token
export TOKEN=$(curl -sX POST "$SB_URL/auth/v1/token?grant_type=password" \
  -H "apikey: $ANON_KEY" \
  -H "Content-Type: application/json" \
  -d "{\"email\":\"$EMAIL\",\"password\":\"$PASSWORD\"}" | \
  python3 -c 'import sys,json; print(json.load(sys.stdin)["access_token"])')
```

**1. Create Connection (Skip Connection Test)**

```bash
# Get first property ID
export PID=$(curl -sX GET "$API/api/v1/properties?limit=1" \
  -H "Authorization: Bearer $TOKEN" | \
  python3 -c 'import sys,json; data=json.load(sys.stdin); print(data["items"][0]["id"] if "items" in data and len(data["items"]) > 0 else "")')

# Create connection with skip_connection_test=true (for dev/mock mode)
curl -X POST "$API/api/v1/channel-connections?skip_connection_test=true" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d "{
    \"property_id\": \"$PID\",
    \"platform_type\": \"booking_com\",
    \"platform_listing_id\": \"test_booking_e2e_$(date +%s)\",
    \"status\": \"active\"
  }"

# Expected response (201):
# {
#   "id": "new-connection-uuid",
#   "property_id": "...",
#   "platform_type": "booking_com",
#   "status": "active",
#   "created_at": "2026-01-02T..."
# }
```

**2. List Connections**

```bash
# List all connections
curl -sX GET "$API/api/v1/channel-connections" \
  -H "Authorization: Bearer $TOKEN" | python3 -m json.tool

# IMPORTANT: Response is a JSON ARRAY (not wrapped in {items:...})
# Example:
# [
#   {
#     "id": "abc-123",
#     "property_id": "prop-456",  // May be null for legacy rows
#     "platform_type": "booking_com",
#     "status": "active",
#     ...
#   },
#   {
#     "id": "legacy-airbnb",
#     "property_id": null,  // Legacy row without property_id
#     "platform_type": "airbnb",
#     ...
#   }
# ]
```

**3. Test Connection**

```bash
# Save connection ID
export CID="new-connection-uuid"

# Test connection health
curl -X POST "$API/api/v1/channel-connections/$CID/test" \
  -H "Authorization: Bearer $TOKEN"

# Expected (Mock Mode):
# {
#   "healthy": true,
#   "message": "Mock: Connection is healthy",
#   "details": {"mock_mode": true, "simulated": true, ...}
# }
```

**4. Trigger Sync**

```bash
# Trigger availability sync
curl -X POST "$API/api/v1/channel-connections/$CID/sync" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"sync_type": "availability"}'

# Expected response (200):
# {
#   "status": "triggered",
#   "message": "Availability sync triggered",
#   "task_ids": ["celery-task-uuid"],
#   "batch_id": "batch-uuid"  // For full sync only
# }
```

**5. Fetch Sync Logs**

```bash
# Get sync logs for connection
curl -sX GET "$API/api/v1/channel-connections/$CID/sync-logs?limit=10&offset=0" \
  -H "Authorization: Bearer $TOKEN" | python3 -m json.tool

# Response may be array or {logs: [...]} (admin UI handles both)
```

### Troubleshooting

**Platform dropdown empty / "Select a platform..." not showing**
- **Cause:** Platform select has `disabled` attribute on placeholder option (browser compatibility issue)
- **Fix:** Latest deploy removes `disabled` attribute (commit `f446745` or later)
- **Workaround:** Select any platform, save, then edit to change platform

**Failed to fetch connections (HTTP 404) in Admin UI**
- **Symptom:** Connections page shows error, browser console shows `GET .../channel-connections/ -> 404 Not Found`
- **Cause:** Frontend using wrong URL with trailing slash (e.g., `/api/v1/channel-connections/` instead of `/api/v1/channel-connections`)
- **Fix:** Latest deploy removes trailing slashes from collection endpoints (commit after `33e1357`)
- **Verify endpoint works:** `curl -i "$API/api/v1/channel-connections" -H "Authorization: Bearer $TOKEN"`
  - Expected: HTTP 200 with JSON array of connections
  - Wrong: `curl "$API/api/v1/channel-connections/"` → HTTP 404
- **Root cause:** FastAPI routes `/channel-connections` and `/channel-connections/` as different endpoints
- **Prevention:** Use `buildApiUrl()` helper in frontend for consistent URL construction

**Properties dropdown empty in New Connection modal**
- **Cause:** API returns `{items: [...]}` but frontend expects plain array, OR filtering by agency_id returns empty
- **Fix:** Frontend parses `data.items || data.properties || data` (handles all response formats)
- **Verify API:** `curl "$API/api/v1/properties?limit=10" -H "Authorization: Bearer $TOKEN"`
  - Should return `{items: [...], total: N}`
- **Check role:** Ensure user has access to properties (admin/manager role, correct agency_id)

**curl: command not found / sed: command not found (smoke/monitoring scripts)**
- **Symptom:** Scripts fail before making requests: `curl: command not found` or `sed: command not found`
- **Cause:** PATH missing `/usr/bin` and `/bin` in minimal shells (cron, Coolify exec, non-interactive)
- **Commands exist at:** `/usr/bin/curl`, `/usr/bin/sed` but aren't found due to broken PATH
- **Fix (immediate):**
  ```bash
  # Export PATH before running script
  export PATH="/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"
  bash backend/scripts/pms_phase23_smoke.sh
  ```
- **Fix (permanent):** Latest deploy includes automatic PATH bootstrap in:
  - `backend/scripts/pms_smoke_common.sh` (sourced by all smoke scripts)
  - `backend/scripts/pms_channel_seed_connection.sh` (standalone script)
  - Docker exec calls include `-e PATH="..."` for container execution
- **Verification (endpoints work once PATH is fixed):**
  ```bash
  # External endpoints
  curl -I https://api.fewo.kolibri-visions.de/health  # Expect: 200

  # Internal endpoints (from container)
  curl -I /health/ready    # Expect: 200
  curl -I /openapi.json    # Expect: 200
  curl -I /docs            # Expect: 200
  ```
- **Debug:**
  ```bash
  # Check if commands exist
  command -v curl  # Should show: /usr/bin/curl
  command -v sed   # Should show: /usr/bin/sed

  # Check current PATH
  echo $PATH       # Should include: /usr/bin:/bin
  ```
- **Related:** See [backend/scripts/README.md](../../scripts/README.md#troubleshooting) for PATH bootstrap details

**Channel Manager module not mounted - /api/v1/channel-connections returns 404**
- **Symptom:** Requests to `/api/v1/channel-connections` return 404 even though `CHANNEL_MANAGER_ENABLED=true`
- **Cause:** Channel Manager module failed to import due to `ImportError: cannot import name 'Json' from 'asyncpg'`
- **Logs show:**
  ```
  Channel Manager module enabled via CHANNEL_MANAGER_ENABLED=true
  Channel Manager module not available: cannot import name 'Json' from 'asyncpg' (/opt/venv/lib/python3.12/site-packages/asyncpg/__init__.py)
  ```
- **Root cause:** asyncpg version incompatibility - some versions don't export `Json` at top level
- **Fix:** Latest deploy includes compatibility shim (`backend/app/core/pg_json.py`) that:
  - Tries `from asyncpg.types import Json` (most common)
  - Falls back to `from asyncpg import Json` (older versions)
  - Falls back to native dict (asyncpg auto-converts dicts to jsonb)
- **Verification (endpoints work once module mounts):**
  ```bash
  # Check OpenAPI includes channel-connections routes
  curl -s https://api.fewo.kolibri-visions.de/openapi.json | grep -o '/api/v1/channel-connections' | head -1
  # Expected: /api/v1/channel-connections

  # Check endpoint works
  curl -i https://api.fewo.kolibri-visions.de/api/v1/channel-connections -H "Authorization: Bearer $TOKEN"
  # Expected: HTTP 200 with JSON array (not 404)
  ```
- **Check module is mounted:**
  ```bash
  # Inside pms-backend container
  docker exec pms-backend bash -c 'curl -s localhost:8000/openapi.json | python3 -c "import sys, json; routes = [p for p in json.load(sys.stdin)[\"paths\"].keys()]; print(f\"TOTAL_ROUTES={len(routes)}\"); print(f\"CHANNEL_ROUTES={len([r for r in routes if \"channel\" in r])}\");"'
  # Expected: CHANNEL_ROUTES > 0 (should show routes like /api/v1/channel-connections, /api/v1/channel-connections/{connection_id}, etc.)
  ```
- **Related:** See `backend/app/core/pg_json.py` for compatibility implementation

**GET /channel-connections returns HTTP 500 (ResponseValidationError)**
- **Cause:** Backend response validation fails when `property_id` is `null` (legacy rows)
- **Symptom:** `"detail": "Response validation error", "errors": [{"loc": ["property_id"], "msg": "none is not an allowed value"}]`
- **Fix:** Latest deploy includes tolerant response validation (per-row validation, skips invalid rows)
- **Verify migration:** Ensure DB has `property_id uuid NULL` (nullable column)
  ```sql
  -- In Supabase SQL Editor
  SELECT column_name, is_nullable, data_type
  FROM information_schema.columns
  WHERE table_name = 'channel_connections'
    AND column_name IN ('property_id', 'platform_listing_id');

  -- Expected: Both columns nullable (is_nullable = 'YES')
  ```
- **DB Hotfix (if needed):**
  ```sql
  -- Add columns if missing (idempotent)
  ALTER TABLE public.channel_connections
  ADD COLUMN IF NOT EXISTS property_id uuid NULL REFERENCES properties(id) ON DELETE CASCADE;

  ALTER TABLE public.channel_connections
  ADD COLUMN IF NOT EXISTS platform_listing_id text NULL;
  ```

**Token expired (401 Unauthorized)**
- **Cause:** JWT tokens expire after ~1 hour (Supabase default)
- **Fix:** Re-fetch token using Prerequisites command above
- **Automation:** Use `pms_phase23_smoke.sh` or `pms_channel_sync_poll.sh` (auto-fetches token)

**Back arrow missing in Batch Details**
- **Cause:** Old deploy (before commit `f446745`)
- **Fix:** Back arrow now always visible (context-aware tooltip)
- **Verify:** Check `frontend/app/connections/page.tsx:1667-1677` for unconditional back button

**Sync logs empty / "No logs found"**
- **Cause:** Auto-refresh disabled, OR sync not triggered yet, OR agency_id filtering issue
- **Fix:**
  1. Enable "Auto-refresh (10s)" toggle in Connection Details modal
  2. Wait ~10 seconds after triggering sync
  3. Click "Trigger Sync" again to create a new log entry
- **Verify API:** `curl "$API/api/v1/channel-connections/$CID/sync-logs?limit=10" -H "Authorization: Bearer $TOKEN"`

**Batch Details modal doesn't close when clicking back arrow**
- **Cause:** JavaScript error in console (check browser DevTools)
- **Fix:** Hard refresh page (Cmd+Shift+R / Ctrl+Shift+F5)
- **Check:** Modal stack z-index conflict (Log Details z-[60], Batch Details z-[70])

**Connection test shows "Connection test failed: Invalid credentials"**
- **Cause:** Real mode enabled (`CHANNEL_MOCK_MODE=false`) but no credentials configured
- **Fix (Dev):** Enable mock mode: `export CHANNEL_MOCK_MODE=true` in backend container
- **Fix (Prod):** Add platform-specific credentials (see [Production Readiness](runbook.md#production-readiness))

### Related Documentation

- [Admin UI – Channel Manager Operations](#admin-ui--channel-manager-operations) - API endpoints and UI features
- [UX Features](#ux-features) - New Connection modal and Batch Details navigation details
- [Channel Manager Connection Testing](../scripts/README.md#channel-manager-connection-testing) - curl examples
- [Mock Mode for Channel Providers](#mock-mode-for-channel-providers) - Dev/staging mock mode setup

---

## Connections → Property Mapping

**Purpose:** Map channel connections to PMS properties and fix legacy unmapped connections.

**Context:**
- Each channel connection should be mapped to a specific PMS property via `property_id`
- Legacy connections may have `property_id = null` (created before multi-property support)
- Unmapped connections have limited functionality - property mapping is required for full features

### UI Features

#### Connections List

**Property Column:**
- Shows mapped property name (from `/api/v1/properties`)
- Falls back to truncated `property_id` if name not resolvable
- Displays **"Unmapped"** badge (yellow) if `property_id` is null

#### Connection Details Modal

**Property Mapping Section:**

**When Mapped (`property_id` exists):**
- Displays property name
- Shows full property ID with copy-to-clipboard icon
- Shows platform listing ID with copy-to-clipboard icon (if available)
- "Change Property" link to reassign

**When Unmapped (`property_id` is null):**
- Prominent yellow warning banner:
  - "No Property Assigned"
  - "This is a legacy connection without a mapped property. Assign a property to enable full functionality."
- **"Assign Property"** button (yellow, primary action)

#### Assign Property Modal

**Workflow:**
1. Click "Assign Property" button (for unmapped) or "Change Property" link (for mapped)
2. Modal opens with property dropdown
3. Select property from list (fetched from `/api/v1/properties`)
4. Click "Assign Property" to save
5. Modal closes, connection details refresh
6. Connections list updates to show property name

**Validation:**
- "Assign Property" button disabled until property selected
- Shows "Assigning..." during save operation
- Success toast: "Property assigned successfully"
- Error toast: "Failed to assign property: [error message]"

### API Endpoint

**Update Connection (Partial Update):**

```bash
# PATCH or PUT /api/v1/channel-connections/{connection_id}
# Endpoint supports partial updates - only provide fields to update

# Example: Assign property to legacy connection
export CID="connection-uuid"
export PID="property-uuid"

curl -X PUT "$API/api/v1/channel-connections/$CID" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d "{\"property_id\": \"$PID\"}"

# Expected response (200):
# {
#   "id": "connection-uuid",
#   "tenant_id": "agency-uuid",
#   "property_id": "property-uuid",  // Now mapped
#   "platform_type": "booking_com",
#   "platform_listing_id": "test_booking_123",
#   "status": "active",
#   "platform_metadata": {...},
#   "last_sync_at": null,
#   "created_at": "2026-01-02T...",
#   "updated_at": "2026-01-02T..."  // Updated timestamp
# }
```

**Supported Update Fields:**
- `property_id` (UUID or null) - Map/unmap connection to property
- `platform_listing_id` (string or null) - Platform-specific listing identifier
- `platform_metadata` (object or null) - Platform-specific configuration
- `status` (string) - Connection status (active, paused, error)

**RBAC:** Admin and Manager roles only

**Tenant Scoping:**
- Backend enforces agency_id filtering via `get_current_agency_id` dependency
- Users can only update connections belonging to their agency

### Troubleshooting

**Property dropdown empty in Assign Property modal**
- **Cause:** No properties exist for the agency, or properties fetch failed
- **Fix:**
  1. Verify `/api/v1/properties?limit=100` returns data
  2. Check user's agency has properties created
  3. Check browser console for fetch errors
- **Workaround:** Create property first via Properties page

**"Failed to assign property: 404"**
- **Cause:** Property ID does not exist or user doesn't have access to it
- **Fix:** Ensure property UUID is correct and belongs to same agency
- **Verify:** `curl "$API/api/v1/properties" -H "Authorization: Bearer $TOKEN"`

**"Failed to assign property: 403"**
- **Cause:** User lacks admin/manager role
- **Fix:** Verify role in token: `jwt_decode($TOKEN) | jq .role`
- **Expected:** Role should be "admin" or "manager", not "staff"

**Connection still shows "Unmapped" after assignment**
- **Cause:** Frontend cache not refreshed
- **Fix:**
  1. Close and reopen Connection Details modal
  2. Hard refresh page (Cmd+Shift+R / Ctrl+Shift+F5)
  3. Check backend actually updated: `curl "$API/api/v1/channel-connections/$CID" -H "Authorization: Bearer $TOKEN"`

**PUT request returns 500 (database error)**
- **Cause:** Database connection pool not available in service layer, or UPDATE query failed
- **Fix:** Check backend logs for asyncpg errors
- **Verify service:** Ensure `ChannelConnectionService.update_connection()` has DB connection
- **Check migration:** Ensure `property_id` column exists and is nullable

**Legacy connections with property_id=null cause validation errors**
- **Symptom:** GET `/channel-connections` returns 500 for some connections
- **Fix:** Already handled via per-row validation (skips invalid rows)
- **Long-term:** Assign properties to all legacy connections via UI or bulk script

### Related Documentation

- [Backoffice Console — Connections (E2E Check)](#backoffice-console--connections-e2e-check) - Full E2E workflow
- [Admin UI – Channel Manager Operations](#admin-ui--channel-manager-operations) - API endpoints
- Database Migration: `supabase/migrations/20260102000000_add_property_fields_to_channel_connections.sql`

---

## Admin UI - Channel Sync

**Purpose:** Web-based admin interface for triggering and monitoring channel sync operations.

**URL:** `https://admin.fewo.kolibri-visions.de/channel-sync`

**RBAC:** Admin and Manager roles only (same as API `/api/availability/sync`)

---

### Sync Types

The Channel Sync Admin UI supports three sync types, each using different API endpoints:

**1. Availability Sync**
- **Endpoint:** `POST /api/v1/availability/sync`
- **Purpose:** Sync availability calendar only
- **Required Fields:** platform, property_id
- **Optional Fields:** connection_id (auto-detected), start_date, end_date

**2. Pricing Sync**
- **Endpoint:** `POST /api/v1/availability/sync`
- **Purpose:** Sync pricing information only
- **Required Fields:** platform, property_id
- **Optional Fields:** connection_id (auto-detected), start_date, end_date

**3. Full Sync**
- **Endpoint:** `POST /api/v1/channel-connections/{connection_id}/sync`
- **Purpose:** Complete sync (availability + pricing + bookings)
- **Required Fields:** connection_id (must be valid UUID)
- **Optional Fields:** start_date, end_date
- **Response:** Returns `batch_id` and `task_ids` array for tracking multiple concurrent operations

**Key Differences:**
- **Availability/Pricing:** Use property-scoped endpoint, connection_id is optional
- **Full:** Uses connection-scoped endpoint, connection_id is REQUIRED
- **Full sync** triggers orchestrated backend flow handling all sync types simultaneously
- **Connection ID vs Property ID:**
  - `connection_id` → channel_connections table (platform linkage UUID)
  - `property_id` → properties table (actual property UUID)
  - Full sync operates at connection level, not property level

---

### Features

The Channel Sync Admin UI provides:

1. **Trigger Sync Operations:**
   - Select sync type (availability, pricing, or full)
   - Select platform (airbnb, booking_com, expedia, fewo_direkt, google)
   - Select property from dropdown
   - Optional: Choose specific channel connection (REQUIRED for full sync)
   - Optional: Set custom date range (default: today → +90 days)
   - Full sync automatically resolves connection_id if platform + property selected

2. **View Sync Logs:**
   - Real-time table of recent sync operations
   - Columns: Status, Platform, Sync Type, Property, Error (if failed), Duration, Started At, Finished At
   - Status badges: triggered (blue), running (yellow), success (green), failed (red)
   - Click any row to view full log details in slide-in drawer
   - **Auto-load on Login:** Logs load automatically after login **if** a last-used connection ID exists in localStorage
     - Fast prefill: uses `localStorage.getItem("channelSync:lastConnectionId")` if present and valid
     - Badge indicator shows **"auto-detected"** (blue) when connection ID is prefilled from localStorage
     - If no last-used connection ID exists, logs panel shows: **"Enter Connection ID or use Auto-detect button above"**
   - **Explicit Auto-detect Button:** Appears when Connection ID field is empty
     - On click, fetches all connections from `/api/v1/channel-connections/?limit=100&offset=0`
     - **Smart Matching:** Filters connections by `platform_type` + `property_id` (if both Platform and Property are selected)
     - **If exactly 1 match:** Sets connection.id automatically and persists to localStorage
     - **If 0 matches:** Shows error message with platform and property info
     - **If multiple matches:** Shows modal selector with platform_type, property_id, platform_listing_id, status for user to choose
     - Shows loading state ("Detecting...") and error messages with HTTP status on failure
     - **Important:** Auto-detection sets the actual `connection.id` from the matched connection object, NOT the property_id
   - **Manual Entry:** Connection ID input field allows manual entry
     - Badge indicator shows **"manual"** (gray) when user manually edits the connection ID
     - Valid UUIDs are automatically persisted to localStorage for future sessions
   - **"Clear" button:** Removes connection ID, clears localStorage, and shows Auto-detect button again
     - Displays toast: "Connection ID cleared. Use Auto-detect or enter manually."
   - **Note:** Logs are fetched via `GET /api/v1/channel-connections/{connection_id}/sync-logs`
     - Connection ID must be a valid UUID format
     - Invalid connection ID shows helpful message instead of attempting fetch
     - After successful trigger, UI automatically retries fetch (0s, 1s, 2s, 3s) until new log appears

3. **Detail Drawer:**
   - Full log JSON with syntax highlighting
   - Copy individual fields to clipboard (task_id, sync_log_id, etc.)
   - View error messages and retry counts
   - View complete details JSONB payload
   - **Connection & Property IDs:** Shows both Connection ID (from log) and Property ID (from connection details)
   - **Null Timestamp Handling:** Started At / Finished At show "—" when null (instead of "01.01.1970")
   - **JSON Pretty-Printing:** Details drawer automatically detects and parses JSON-encoded strings (including double-escaped JSON) and renders them as pretty-printed JSON for readability
   - **Copy JSON** button copies the parsed, pretty-printed payload (not the raw escaped string)
   - **Duration Calculation:** Shows sync duration using `started_at`/`finished_at` timestamps (preferred) or falls back to `created_at`/`updated_at` if start/finish times are missing

4. **Filters & Search:**
   - **Status Filter:** All / Triggered / Running / Success / Failed / Active (triggered+running)
   - **Operation Type Filter:** All / availability_update / pricing_update / bookings_sync
   - **Direction Filter:** All / outbound / inbound
   - **Search Input:** Case-insensitive free-text search across all log fields:
     - IDs: batch_id, task_id, log_id, connection_id, property_id
     - Fields: operation_type, status, direction, error
     - Timestamps: created_at, updated_at
     - Placeholder: "Search logs…"
     - Clear button (✕) appears when search is active
     - Empty state: "No logs match your search." when no results
   - **Copy Buttons:** Click 📋 icons in table and drawer to copy IDs (Log ID, Connection ID, Property ID, Task ID, Batch ID) to clipboard
   - **Sorting:** Logs always sorted by created_at (newest first)
   - Filter buttons and search work together (both applied simultaneously)
   - Smart auto-refresh (only polls when active triggered/running logs exist)

5. **In-App Notifications:**
   - **Sync Trigger:** Inline green/red banner (auto-clears after 5s)
     - Success: "Sync gestartet: {sync_type}" with clickable batch_id
     - Error: "Fehler beim Starten (HTTP {status}): {detail}"
   - **Clipboard:** Browser-native alert for copy confirmations (unchanged)
   - **Success Message Fields:**
     - **Full Sync:** Displays "Batch ID: {uuid} (N tasks)" with copy buttons for batch_id and individual task_ids (first 3 shown, rest collapsed)
     - **Single Sync (Availability/Pricing):** Displays "Task ID: {uuid}" or "N tasks queued" depending on response format
     - No empty/undefined fields shown (dynamic display based on response shape)

6. **Troubleshooting Link:**
   - "Troubleshooting (Runbook)" link below Sync Logs title navigates directly to Ops Runbook tab (`/ops/runbook`)
   - Provides quick access to full operational documentation and runbook

7. **Duration Column:**
   - Shows elapsed time for each sync operation
   - Calculated from `started_at` → `finished_at` (preferred) or `created_at` → `updated_at` (fallback)
   - Format: `Xs` (seconds), `Ym Zs` (minutes/seconds), or `-` if timestamps unavailable
   - Active syncs (triggered/running) may show `-` until completion

8. **Safety Defaults (Form Validation):**
   - **Intentionally NO preselected values:** Sync Type, Platform, and Property fields start empty
   - **Trigger button disabled** until all required fields are selected
   - **Helper text displayed:** "ℹ️ Please select Sync Type, Platform, and Property to avoid triggering the wrong sync"
   - **Why:** Prevents accidental syncs against wrong platform/property due to overlooked defaults
   - **User must explicitly choose:**
     - Sync Type (Availability / Pricing / Full)
     - Platform (Airbnb / Booking.com / Expedia / FeWo-direkt / Google)
     - Property (from dropdown loaded from database)
   - **Connection ID:** Optional, can be auto-detected or manually entered after selecting platform + property
   - **Form remains disabled** until user makes conscious selection of each required field

9. **UX Behavior:**
   - **Trigger Sync Auto-loads Logs:** After clicking "Trigger Sync", logs automatically appear immediately
     - If Connection ID was not set, it gets set from the triggered sync's connection
     - Logs refresh automatically to show the new sync operation
     - Success panel shows batch_id/task_ids with copy buttons
   - **Auto-detect Only Sets Connection:** Clicking "Auto-detect" button:
     - Populates Connection ID field
     - Fetches and displays logs for that connection
     - Does NOT open the Sync Log Details modal
     - User must explicitly click a log row to view details
   - **Clearing Connection Resets UI:** Clicking "Clear" button:
     - Clears Connection ID field
     - Clears logs list (table becomes empty)
     - Closes Sync Log Details modal if open
     - Hides success panel (green banner)
     - Clears any error messages
     - Resets all filters and search state
   - **Success Panel Auto-dismiss:** Green success banner after triggering sync:
     - Includes dismiss button (×) in top-right corner
     - Auto-dismisses after 15 seconds
     - User can manually dismiss anytime by clicking ×
   - **Note:** Jobs can complete very quickly (< 1 second), so status may jump directly from "triggered" to "success" without showing "running" state

---

### Channel Sync safety defaults

**Intentionally NO preselected values:** The Channel Sync form starts with all fields empty (Sync Type, Platform, Property) to prevent accidental syncs.

**Trigger button disabled** until all required fields are selected, with helper text: "ℹ️ Please select Sync Type, Platform, and Property to avoid triggering the wrong sync"

**Why:** Prevents accidental syncs against wrong platform/property due to overlooked defaults. User must explicitly choose each field.

See: **Features → Safety Defaults (Form Validation)** for full details.

---

### Channel Sync Logs: lifecycle

**Status Progression:**
1. `triggered` (blue) - Sync queued in Celery
2. `running` (yellow) - Worker executing sync
3. `success` (green) - Sync completed successfully
4. `failed` (red) - Sync failed with error

**Important Notes:**
- Jobs can complete very quickly (< 1 second), so status may jump directly from "triggered" to "success" without showing "running" state
- Logs automatically refresh when active (triggered/running) logs exist
- Each log entry includes: status, platform, sync type, property, error (if failed), duration, timestamps
- Connection's `last_sync_at` field updates on trigger (immediate) and on success (completion)

See: **Features → View Sync Logs** and **Channel Connections: Last Sync Semantics** for full details.

---

### UX behavior

**Trigger Sync Auto-loads Logs:**
- After clicking "Trigger Sync", logs automatically appear immediately
- If Connection ID was not set, it gets set from the triggered sync's connection
- Success panel shows batch_id/task_ids with copy buttons

**Auto-detect Only Sets Connection:**
- Clicking "Auto-detect" button populates Connection ID and fetches logs
- Does NOT open the Sync Log Details modal
- User must explicitly click a log row to view details

**Clearing Connection Resets UI:**
- Clears Connection ID field and logs list
- Closes Sync Log Details modal if open
- Hides success panel and error messages
- **Clears all active toasts immediately** (no lingering success/error messages)
- Resets all filters and search state

**Toast Auto-dismiss Behavior:**
- **Channel Sync page:** Toasts auto-dismiss after 6 seconds
- **Connections page:** Notifications auto-dismiss after 10 seconds
- Green success banner (syncResult panel) auto-dismisses after 15 seconds
- All toasts include manual dismiss button (×)
- **Navigation cleanup:** Toasts and banners are cleared automatically when navigating away from page

See: **Features → UX Behavior** for full details.

---

### Sync Trigger Payload Architecture

**Important:** The sync page sends two distinct IDs to the API:

1. **property_id (REQUIRED):**
   - The actual property UUID from the "Property" dropdown
   - Always sent in POST `/api/v1/availability/sync` request body
   - Example: `"property_id": "6da0f8d2-677f-4182-a06c-db155f43704a"`

2. **connection_id (OPTIONAL):**
   - The channel connection UUID (auto-detected or user-entered)
   - Only sent if a connection is selected
   - Example: `"connection_id": "abc-123-def-456"`
   - Omitted from payload if empty

**Example Request Body:**
```json
{
  "sync_type": "availability",
  "platform": "booking_com",
  "property_id": "6da0f8d2-677f-4182-a06c-db155f43704a",
  "connection_id": "abc-123-def-456",
  "manual_trigger": true
}
```

**State Management:**
- UI maintains separate state variables: `propertyId` and `connectionId`
- After sync trigger, `connectionId` is NOT overwritten from API response
- This prevents confusion where API might return `property_id` in the response

**Property ID Display (No 404 Calls):**
- Sync page does NOT call `GET /api/v1/channel-connections/{id}` to fetch connection details
- Instead, uses cached connections list from auto-detect (`GET /api/v1/channel-connections/?limit=100&offset=0`)
- Property ID extracted from:
  1. Log details/metadata (`log.details.property_id`)
  2. Cached connections list (lookup by `connection_id`)
- This avoids 404 errors when connection_id is invalid or missing

**Trailing Slash Requirement:**
- List endpoint MUST use trailing slash before query params: `/api/v1/channel-connections/?limit=100`
- Without trailing slash: `/api/v1/channel-connections?limit=100` → 307 redirect → fails

**API Response Shape:**
- `GET /api/v1/channel-connections/` can return two formats:
  1. Direct array: `[{id, property_id, platform_type, ...}, ...]`
  2. Paginated object: `{items: [...], total: N, ...}`
- Frontend normalizes both: `const connections = Array.isArray(res) ? res : res.items ?? []`
- Prioritize `items` key if present, fallback to array check

**DevTools Verification:**
To verify sync trigger includes `connection_id`:
1. Open DevTools → Network tab
2. Select Platform + Property in Admin UI
3. Click "Trigger Sync"
4. Find POST request to `/api/v1/availability/sync`
5. Check Payload tab - should include:
   ```json
   {
     "sync_type": "availability",
     "platform": "booking_com",
     "property_id": "6da0f8d2-677f-4182-a06c-db155f43704a",
     "connection_id": "c1df8491-197a-4881-aec6-18e4297f5f79",
     "manual_trigger": true
   }
   ```
6. If `connection_id` is missing: cache not populated or no matching connection

**Auto-Resolution Logic:**
- On page load, Admin UI fetches `/api/v1/channel-connections/?limit=100` and caches results
- At sync trigger, if `connection_id` field is empty:
  - Searches cache for match: `platform_type === platform && property_id === propertyId`
  - If exactly 1 match: includes `connection.id` in payload automatically
  - If 0 or multiple matches: omits `connection_id` from payload

---

### API Response Shapes (Normalized)

**POST `/api/v1/channel-connections/{connection_id}/sync` Response:**

The sync trigger endpoint returns a consistent response shape with all required fields:

```json
{
  "status": "triggered",
  "message": "Manual full sync triggered successfully",
  "connection_id": "c1df8491-197a-4881-aec6-18e4297f5f79",
  "sync_type": "full",
  "task_ids": [
    "abc-123-task-1",
    "abc-123-task-2",
    "abc-123-task-3"
  ],
  "batch_id": "550e8400-e29b-41d4-a716-446655440000",
  "created_log_ids": [
    "log-uuid-1",
    "log-uuid-2",
    "log-uuid-3"
  ]
}
```

**Field Guarantees:**
- `status`: Always present (string: "triggered" or "error")
- `message`: Always present (human-readable description)
- `connection_id`: **Always present** (UUID, echoed from request)
- `sync_type`: **Always present** (string: "availability"|"pricing"|"bookings"|"full")
- `task_ids`: **Always array** (never single string, may be empty on error)
- `batch_id`: Nullable UUID (present for full sync, null for single-type syncs)
- `created_log_ids`: Nullable array of UUIDs (sync log entry IDs, null if creation failed)

**GET `/api/v1/channel-connections/{connection_id}/sync-logs` Response:**

The sync logs endpoint returns logs with normalized `details` field:

```json
{
  "connection_id": "c1df8491-197a-4881-aec6-18e4297f5f79",
  "logs": [
    {
      "id": "log-uuid-1",
      "connection_id": "c1df8491-197a-4881-aec6-18e4297f5f79",
      "operation_type": "availability_update",
      "direction": "outbound",
      "status": "success",
      "details": [
        "{\"sync_type\": \"full\", \"manual_trigger\": true}",
        "{\"property_id\": \"6da0f8d2-677f-4182-a06c-db155f43704a\"}"
      ],
      "error": null,
      "task_id": "abc-123-task-1",
      "batch_id": "550e8400-e29b-41d4-a716-446655440000",
      "created_at": "2026-01-02T12:45:30.123456Z",
      "updated_at": "2026-01-02T12:45:35.654321Z"
    }
  ],
  "limit": 50,
  "offset": 0
}
```

**Details Field Normalization:**

The `details` field is **always** an array of JSON-encoded strings:
- Database stores JSONB (dict/list/str/null)
- API normalizes on read to array[str] for consistent UI rendering
- Normalization rules:
  - `null` → `[]` (empty array)
  - `string` → `[string]` (wrap in array)
  - `dict` → `[json.dumps(dict)]` (serialize and wrap)
  - `list` → normalize each element to string (already in array format)

**Error Response Format:**

Endpoints return structured error responses with `error_code` and `hint`:

```json
{
  "detail": {
    "error": "not_found",
    "message": "Connection not found",
    "error_code": "NOT_FOUND",
    "hint": "Verify connection_id via GET /api/v1/channel-connections/?limit=100"
  }
}
```

**Common Error Codes:**
- `NOT_FOUND`: Resource not found (404)
- `SERVICE_UNAVAILABLE`: Database or external service unavailable (503)
- `INVALID_STATUS`: Invalid status parameter (400)
- `CONNECTION_INACTIVE`: Connection not active (health check failed)

---

### Channel Connections: Last Sync Semantics

**last_sync_at Persistence:**

The `last_sync_at` field in channel_connections is updated in TWO places:

1. **On Sync Trigger** (Immediate):
   - When manual sync is triggered via API (`POST /api/v1/availability/sync` or `POST /api/v1/channel-connections/{id}/sync`)
   - Sets `last_sync_at = NOW()` immediately when sync is queued
   - Updates `updated_at = NOW()` as well
   - Best-effort operation (logs warning if fails, doesn't block sync)

2. **On Sync Success** (Completion):
   - When Celery worker marks sync log as "success"
   - Sets `last_sync_at = NOW()` (finish time)
   - Updates `updated_at = NOW()` as well
   - Best-effort operation (logs warning if fails, doesn't affect task result)

**Why Two Updates?**
- First update: Shows sync was attempted (user sees "Last Sync: moments ago" in UI)
- Second update: Confirms sync completed successfully (reflects actual sync finish time)
- If worker crashes, first timestamp remains (shows attempted, not succeeded)

**Debugging Last Sync:**

Check connection's `last_sync_at` via API:
```bash
# List all connections with last_sync_at
curl -X GET "$API/api/v1/channel-connections/?limit=100" \
  -H "Authorization: Bearer $TOKEN" | jq '.[] | {id, platform_type, property_id, last_sync_at, updated_at}'

# Expected output:
# {
#   "id": "c1df8491-197a-4881-aec6-18e4297f5f79",
#   "platform_type": "booking_com",
#   "property_id": "6da0f8d2-677f-4182-a06c-db155f43704a",
#   "last_sync_at": "2026-01-02T12:45:30.123456Z",  # <-- Should be non-null after sync
#   "updated_at": "2026-01-02T12:45:30.123456Z"
# }
```

Check directly in database (optional):
```sql
-- Connect to Supabase/Postgres
SELECT id, platform_type, property_id, last_sync_at, updated_at
FROM channel_connections
WHERE deleted_at IS NULL
ORDER BY updated_at DESC LIMIT 10;
```

**Troubleshooting:**

If `last_sync_at` remains `null` after running sync:
1. Check worker logs for warnings: `grep "Failed to touch connection" worker.log`
2. Verify database permissions (UPDATE on channel_connections table)
3. Verify connection_id is valid UUID (not property_id mistakenly)
4. Check if connection was soft-deleted (`deleted_at IS NOT NULL`)

If last_sync_at shows trigger time but not finish time:
- Worker may have crashed before updating (check Celery worker status)
- Check sync logs for task status: `GET /api/v1/channel-connections/{id}/sync-logs`

---

### Log Retention & Purge Policy

**Default Retention Policy:** 30 days (recommended)

**Admin-Only Purge Feature:**
- **Admin UI**: "Purge logs" button in `/channel-sync` page
- **API Endpoint**: `POST /api/v1/channel-connections/{connection_id}/sync-logs/purge`
- **Access**: Admin role only (403 for non-admins)
- **Safety**: Requires typing "PURGE" to confirm (irreversible deletion)

**Retention Options:**
- 7 days: Short-term cleanup for testing/debugging
- 30 days: Recommended default for production
- 90 days: Extended retention for audit/compliance

**How to Purge Logs (Admin UI):**

1. **Navigate to Channel Sync:**
   - Go to: `https://admin.fewo.kolibri-visions.de/channel-sync`
   - Select connection ID (auto-detect or manual entry)

2. **Click "Purge logs" button** (upper right, next to Refresh)
   - Only visible when valid connection ID is set
   - Disabled for non-admin users

3. **Configure Purge:**
   - Select retention period: 7 / 30 / 90 days
   - Type `PURGE` in confirmation field (case-sensitive)
   - Click "Purge Logs" button

4. **Verify Result:**
   - Success toast shows number of deleted logs
   - Logs table refreshes automatically
   - Purge is immediate and irreversible

**How to Purge Logs (curl):**

See [scripts/README.md - Purge Sync Logs](#) for curl examples.

```bash
# Example: Purge logs older than 30 days
curl -X POST "$API/api/v1/channel-connections/$CID/sync-logs/purge" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "retention_days": 30,
    "confirm_phrase": "PURGE"
  }'

# Expected response (200):
# {
#   "connection_id": "abc-123...",
#   "retention_days": 30,
#   "cutoff": "2025-01-01T12:00:00Z",
#   "deleted_count": 42
# }
```

**Error Responses:**
- **400**: Invalid confirm_phrase (must be exactly "PURGE")
- **401**: Not authenticated (missing/invalid token)
- **403**: Forbidden (not an admin)
- **422**: Invalid retention_days (must be 1-3650)

**Purge Preview (Safety Feature):**
- **Before Deletion**: Modal shows preview count ("Will delete: X logs")
- **Scope Display**: Clearly shows "This connection only" with connection ID
- **Real-time Update**: Preview refreshes when retention period changes (7/30/90 days)
- **Safe Decision**: User sees exactly how many logs will be deleted before confirming
- **Loading State**: Shows "Loading..." while fetching preview count
- **Error Handling**: Displays error if preview fails (doesn't block purge if needed)

**Safety Notes:**
- **Irreversible**: Deleted logs cannot be recovered
- **Scoped**: Only deletes logs for specified connection_id (not global)
- **Preview First**: Always shows count before deletion (requires connection ID)
- **Audit Trail**: Lost after purge — export logs before purging if needed for compliance
- **No Automatic Purge**: Manual trigger only (no cron/scheduled purge)

---

### How to Access

1. **Login to Admin UI:**
   - Navigate to: `https://admin.fewo.kolibri-visions.de`
   - Login with admin or manager credentials

2. **Navigate to Channel Sync:**
   - Click "Channel Sync" in sidebar navigation
   - Or directly: `https://admin.fewo.kolibri-visions.de/channel-sync`

---

### How to Trigger a Sync

1. **Fill out the Trigger Form:**
   - **Sync Type:** Select "availability" or "pricing"
   - **Platform:** Select target booking platform
   - **Property:** Select property from dropdown (auto-fetched from `/api/v1/properties`)
   - **Connection (Optional):** Select specific connection if needed
   - **Date Range (Optional):** Defaults to today → +90 days

2. **Click "Trigger Sync"**
   - Request sent to `POST /api/availability/sync`
   - Success toast shows task_id and sync_log_id
   - UI automatically fetches logs with retry (up to 4 attempts: 0s, 1s, 2s, 3s)
   - New log appears at top of sync logs table and detail drawer opens automatically
   - Status starts as "triggered", transitions to "running" → "success" or "failed"
   - If log doesn't appear after retries, click Refresh button manually

3. **Monitor Progress:**
   - Watch status badge change in real-time
   - Duration updates when log completes
   - Click row to view full details

---

### Interpreting Sync Statuses

| Status | Badge Color | Meaning | Next Action |
|--------|-------------|---------|-------------|
| **triggered** | Blue | Sync task queued in Celery | Wait for worker to pick up task |
| **running** | Yellow | Worker actively processing sync | Wait for completion (usually < 30s) |
| **success** | Green | Sync completed successfully | No action needed |
| **failed** | Red | Sync failed with error | Click row → view error → troubleshoot |

**Duration Calculation:**
- Shows time from `started_at` to `finished_at`
- Only visible after log completes (status = success/failed)
- Format: `XXXms` (if < 1s) or `X.Xs` (if ≥ 1s)

---

### Troubleshooting Common Issues

#### 1. Validation Errors (422) — No Sync Log Created

**Symptom:**
- Trigger button disabled or form shows red field errors
- Yellow banner: "Validation failed — fix highlighted fields"
- No sync log appears in table

**Cause:**
- **Client-side validation failed:**
  - Connection ID is not a valid UUID format
  - End date is before start date
- **Server-side validation failed (422):**
  - Invalid field values sent to API
  - Missing required fields

**Why No Log Appears:**
- **422 validation errors mean the request was rejected before queuing**
- The sync task was never created, so no log entry exists in `channel_sync_logs` table
- Only successfully triggered syncs (status 200) create log entries

**Fix:**
1. **Check field-level error messages** under each input (red text)
2. **Fix highlighted fields:**
   - Connection ID must be valid UUID format: `xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx`
   - End date must be on or after start date
3. **Trigger button will re-enable** once validation passes
4. **After fixing:** Click "Trigger Sync" again

**Note:** Client-side validation now prevents most 422 errors before API call. If you still see 422 from the server, check the field error messages for details.

---

#### 2. Trigger Button Disabled or "No properties available"

**Symptom:**
- Trigger form shows "No properties available"
- Property dropdown is empty

**Cause:**
- GET `/api/v1/properties` returned empty array or failed
- User's agency has no properties created yet
- RLS policies preventing property access

**Fix:**
```bash
# Check if properties exist for this user
curl -X GET https://api.fewo.kolibri-visions.de/api/v1/properties \
  -H "Authorization: Bearer YOUR_JWT" | jq '.properties'

# Expected: Array with at least one property
# If empty: Create a property first or check RLS policies
```

---

#### 3. Sync Triggered but Logs Stay "triggered" Forever

**Symptom:**
- POST `/api/availability/sync` returns 200 with task_id
- Sync log appears in table with status "triggered"
- Status never transitions to "running" or "success/failed"

**Cause:**
- Celery worker not running or not connected to Redis broker
- Worker crashed or stuck in infinite loop
- Worker old version (code drift)

**Fix:**
```bash
# SSH to host server
ssh root@your-host

# Check if pms-worker-v2 is running
docker ps | grep pms-worker

# Check worker logs for errors
docker logs --tail 100 pms-worker-v2

# Ping Celery workers from backend
docker exec pms-backend \
  celery -A app.channel_manager.core.sync_engine:celery_app \
  --broker "$CELERY_BROKER_URL" inspect ping -t 3

# Expected: -> celery@pms-worker-v2-...: {'ok': 'pong'}
# If timeout: Worker not connected to Redis

# Restart worker
docker restart pms-worker-v2
```

**See Also:** [Celery Worker Troubleshooting](#celery-worker-pms-worker-v2-start-verify-troubleshoot)

---

#### 4. 401 Unauthorized (Token Expired)

**Symptom:**
- UI shows error toast: "Unauthorized" or "Token expired"
- Trigger sync fails with 401
- Sync logs fail to load

**Cause:**
- JWT token expired (tokens expire after 1 hour by default)
- User session invalidated

**Fix:**
1. **Logout and Login Again:**
   - Click logout in Admin UI
   - Login with credentials again
   - Token will be refreshed

2. **Manual Token Refresh (for testing):**
   ```bash
   # Fetch new token via Supabase auth
   curl -X POST "https://your-project.supabase.co/auth/v1/token?grant_type=password" \
     -H "apikey: YOUR_ANON_KEY" \
     -H "Content-Type: application/json" \
     -d '{"email":"admin@example.com","password":"your-password"}' \
     | jq -r '.access_token'
   ```

---

#### 5. 503 Service Unavailable (Database Temporarily Unavailable)

**Symptom:**
- UI shows error toast: "Service temporarily unavailable"
- API returns 503 with message "Database is temporarily unavailable"

**Cause:**
- Backend container lost connection to Supabase database
- Supabase network attachment dropped during redeploy
- Database DNS resolution failed

**Fix:**

See: [DB DNS / Degraded Mode](#db-dns--degraded-mode)

**Quick check:**
```bash
# SSH to host
ssh root@your-host

# Check if backend can resolve supabase-db
docker exec pms-backend getent hosts supabase-db

# Expected: IP address (e.g., 172.20.0.2)
# If empty: DNS resolution failed → reattach network

# Reattach Supabase network
docker network connect bccg4gs4o4kgsowocw08wkw4 pms-backend
docker restart pms-backend
```

---

#### 6. Sync Logs Table Empty or Stale

**Symptom:**
- Sync logs table shows "No sync logs found"
- OR logs are stale (not updating after triggering sync)

**Cause A:** No syncs triggered yet
- **Fix:** Trigger a sync first

**Cause B:** GET `/api/v1/channel-connections/{id}/sync-logs` failed silently
- **Fix:** Open browser console (F12) → check for API errors

**Cause C:** Database migration not applied (`channel_sync_logs` table missing)
- **Symptom:** API returns 503 with message "Channel sync logs schema not installed"
- **Fix:** Apply migration: `supabase/migrations/20251227000000_create_channel_sync_logs.sql`

```bash
# Check if table exists
docker exec $(docker ps -q -f name=supabase) psql -U postgres -d postgres \
  -c "\dt channel_sync_logs"

# Expected: Table listing
# If not found: Apply migration (see "Channel Manager Sync Logs Migration" section)
```

---

#### 7. Admin UI Shows "Failed to fetch connections" (Mixed Content)

**Symptom:**
- Browser console shows: `Mixed Content: The page at 'https://admin...' was loaded over HTTPS, but requested an insecure resource 'http://api...'.`
- Connections page shows "Failed to fetch connections" error
- Auto-detect fails to load connections
- Error message: "Failed to auto-detect connection ID (TypeError: Failed to fetch)"

**Cause:**
- Frontend environment variable `NEXT_PUBLIC_API_BASE` set to HTTP instead of HTTPS
- API endpoint without trailing slash triggers FastAPI 307 redirect
- Redirect downgrades from HTTPS to HTTP (proxy/load balancer issue)
- Browser blocks the HTTP request as Mixed Content

**Fix 1: Set Frontend API Base to HTTPS**

Ensure `NEXT_PUBLIC_API_BASE` uses HTTPS in deployment environment:

```bash
# In Coolify/deployment environment variables
NEXT_PUBLIC_API_BASE=https://api.fewo.kolibri-visions.de
```

**Fix 2: Automatic HTTPS Upgrade (Built-in Protection)**

The frontend now automatically upgrades HTTP to HTTPS when:
- Frontend loaded via HTTPS (`window.location.protocol === 'https:'`)
- API base URL starts with `http://`
- Console warning appears: `[api-client] Upgrading HTTP API base to HTTPS to avoid mixed content`

This prevents mixed content errors even if `NEXT_PUBLIC_API_BASE` is misconfigured.

**Fix 3: Redeploy Frontend**

If the issue persists after setting env var:

1. **Redeploy frontend** to rebuild with new environment variable
2. **Clear browser cache** and hard reload (Ctrl+Shift+R / Cmd+Shift+R)
3. **Verify API base URL** in browser console: should be `https://api...` (not `http://`)
4. **Check proxy/load balancer config** if the issue persists (ensure HTTPS forwarding is correct)

**Technical Note:**
- ✅ Correct: `GET /api/v1/channel-connections/?limit=50` (with trailing slash, HTTPS)
- ❌ Causes redirect: `GET /api/v1/channel-connections?limit=50` (no trailing slash)
- ❌ Blocked by browser: `GET http://api...` (HTTP from HTTPS page)

---

#### 8. Redirect Location is http:// (Mixed Content from Backend)

**Symptom:**
- Browser console shows: `Mixed Content: ... requested an insecure resource 'http://api...'.`
- API redirect (307) uses `http://` instead of `https://` in Location header
- Request to `https://api.../channel-connections?limit=1` redirects to `http://api.../channel-connections/?limit=1`

**Cause:**
- FastAPI/Starlette generates redirect without trusting X-Forwarded-Proto header
- Reverse proxy (Traefik/Coolify) sets `X-Forwarded-Proto: https` but backend ignores it
- Backend sees scheme=http from internal connection, builds http:// redirect Location
- Browser blocks the HTTP redirect as mixed content

**Verification:**
```bash
# Test redirect Location header
curl -k -sS -D - -o /dev/null "https://api.fewo.kolibri-visions.de/api/v1/channel-connections?limit=1&offset=0" | sed -n '1,30p'

# Expected BEFORE fix: location: http://api.fewo.kolibri-visions.de/...
# Expected AFTER fix:  location: https://api.fewo.kolibri-visions.de/...
```

**Fix: Enable Proxy Header Trust**

The backend now trusts X-Forwarded-Proto by default via `ForwardedProtoMiddleware`.

**Environment Variable (Backend):**
```bash
# Default: true (recommended for production behind reverse proxy)
TRUST_PROXY_HEADERS=true
```

**How It Works:**
1. Middleware reads `X-Forwarded-Proto` header from Traefik/Nginx
2. Sets `scope["scheme"]` to "https" (instead of "http")
3. FastAPI redirect Location header uses https://
4. Browser accepts redirect (no mixed content error)

**Startup Log Verification:**
```bash
# Check backend logs for middleware initialization
docker logs pms-backend --tail 50 | grep TRUST_PROXY_HEADERS

# Expected output:
# TRUST_PROXY_HEADERS=true → trusting X-Forwarded-Proto for scheme
```

**Disable (Not Recommended):**
```bash
# Only disable if NOT behind reverse proxy (direct HTTPS termination)
TRUST_PROXY_HEADERS=false
```

**Related:**
- Frontend already upgrades HTTP to HTTPS (automatic protection)
- Backend middleware ensures redirects stay HTTPS
- Both protections work together for complete mixed-content prevention

---

#### 9. Connections Last Sync shows "Never" (NULL last_sync_at)

**Symptom:**
- Admin UI Connections page shows "Last Sync: Never" despite successful sync logs existing
- `channel_connections.last_sync_at` column remains NULL in database
- Sync logs exist with `status='success'` but connection timestamp not updated

**Causes:**

**A. Sync log updates not triggering connection touch**
- Worker successfully completes sync task but `update_log_by_task_id` not called
- Log status updated manually/directly without using service layer
- Worker crashed before calling update method

**B. Auto-update logic not running (code version mismatch)**
- Deployed code doesn't include auto-update logic in `update_log_by_task_id`
- Service layer bypassed (direct SQL UPDATE on channel_sync_logs)

**C. Database permissions issue**
- Worker has SELECT on `channel_sync_logs` but not UPDATE on `channel_connections`
- RLS policy blocks UPDATE on `channel_connections` for service role

**D. Connection deleted or soft-deleted**
- `channel_connections.deleted_at IS NOT NULL` blocks update
- Connection ID from log doesn't match any active connection

**Verification:**

```bash
# 1. Check if sync logs exist with status=success
docker exec $(docker ps -q -f name=supabase) psql -U postgres -d postgres -c "
  SELECT id, connection_id, operation_type, status, created_at, updated_at
  FROM channel_sync_logs
  WHERE status = 'success'
  ORDER BY updated_at DESC
  LIMIT 5;
"

# 2. Check if corresponding connections have NULL last_sync_at
docker exec $(docker ps -q -f name=supabase) psql -U postgres -d postgres -c "
  SELECT c.id, c.platform_type, c.last_sync_at, c.updated_at, c.deleted_at
  FROM channel_connections c
  WHERE c.id IN (
    SELECT DISTINCT connection_id
    FROM channel_sync_logs
    WHERE status = 'success'
  )
  ORDER BY c.updated_at DESC
  LIMIT 5;
"

# Expected: last_sync_at should NOT be NULL if success logs exist

# 3. Check tenant_id in sync logs (should not be NULL)
docker exec $(docker ps -q -f name=supabase) psql -U postgres -d postgres -c "
  SELECT connection_id, tenant_id, COUNT(*) as log_count
  FROM channel_sync_logs
  GROUP BY connection_id, tenant_id
  ORDER BY log_count DESC
  LIMIT 10;
"

# Expected: tenant_id should match connection's agency_id (not NULL)
```

**Fix 1: Manual Backfill (Immediate Fix)**

If production connections have NULL `last_sync_at` but success logs exist, backfill from logs:

```sql
-- Backfill last_sync_at from most recent successful sync log
UPDATE channel_connections c
SET
  last_sync_at = (
    SELECT MAX(updated_at)
    FROM channel_sync_logs
    WHERE connection_id = c.id
      AND status = 'success'
  ),
  updated_at = NOW()
WHERE c.deleted_at IS NULL
  AND c.last_sync_at IS NULL
  AND EXISTS (
    SELECT 1
    FROM channel_sync_logs
    WHERE connection_id = c.id
      AND status = 'success'
  );

-- Verify backfill
SELECT id, platform_type, last_sync_at, updated_at
FROM channel_connections
WHERE deleted_at IS NULL
  AND last_sync_at IS NOT NULL
ORDER BY last_sync_at DESC
LIMIT 10;
```

**Fix 2: Verify Code Deployment**

Ensure latest code with auto-update logic is deployed:

```bash
# Check deployed commit hash
docker exec pms-backend env | grep SOURCE_COMMIT

# Expected: Latest commit with auto-update logic
# Commit should include:
# - channel_sync_log_service.py: update_log_by_task_id updates connection.last_sync_at on success
# - channel_connection_service.py: list_connections has COALESCE fallback for last_sync_at

# Check backend logs for auto-update messages
docker logs pms-backend --tail 100 | grep "Updated connection.*last_sync_at"

# Expected on successful sync:
# "Updated connection <uuid> last_sync_at due to log <log_id> transitioning to success"
```

**Fix 3: Redeploy Backend + Worker**

If code is out of date:

```bash
# 1. Pull latest main
git checkout main
git pull origin main

# 2. Verify changes
git log --oneline --grep="last_sync_at" -5

# 3. Redeploy via Coolify
# Navigate to Coolify Dashboard → pms-backend → Redeploy
# Navigate to Coolify Dashboard → pms-worker-v2 → Redeploy

# 4. Wait for containers to restart (30-60s)

# 5. Verify deployment
docker ps --format 'table {{.Names}}\t{{.Status}}\t{{.Image}}' | grep -E 'pms-backend|pms-worker'

# 6. Trigger test sync and verify last_sync_at updates
```

**Fix 4: Check Database Permissions**

Verify service role can UPDATE channel_connections:

```sql
-- Test UPDATE as service role (anon JWT)
BEGIN;

UPDATE channel_connections
SET last_sync_at = NOW(), updated_at = NOW()
WHERE id = '<test-connection-uuid>'
  AND deleted_at IS NULL;

-- If ERROR: Check RLS policies
SELECT tablename, policyname, permissive, roles, cmd, qual
FROM pg_policies
WHERE tablename = 'channel_connections'
  AND cmd = 'UPDATE';

ROLLBACK;
```

**Expected Behavior (After Fix):**

1. When sync log transitions to `status='success'`:
   - Service calls `update_log_by_task_id(task_id, status='success')`
   - Service updates log row in `channel_sync_logs`
   - Service automatically updates `channel_connections.last_sync_at = NOW()`
   - Backend logs: "Updated connection <uuid> last_sync_at due to log <log_id> transitioning to success"

2. When `list_connections` is called:
   - If `channel_connections.last_sync_at IS NULL`:
     - Query uses COALESCE to fallback to `MAX(updated_at) FROM channel_sync_logs WHERE status='success'`
     - API returns computed timestamp (not NULL)
   - If `channel_connections.last_sync_at IS NOT NULL`:
     - Query returns actual column value

3. Admin UI Connections page shows accurate "Last Sync" timestamp

**Related Sections:**
- [Admin UI - Channel Sync](#admin-ui---channel-sync)
- [Sync Logs Persistence](#sync-logs-persistence)
- [Channel Manager Error Handling & Retry Logic](#channel-manager-error-handling--retry-logic)

---

## Verify Connection last_sync_at (E2E)

**Purpose:** End-to-end verification that `last_sync_at` updates correctly after sync operations complete successfully. This guide helps new team members verify the feature without stumbling over redirects, token expiration, or SQL column name errors.

**When to Use:**
- After deploying last_sync_at auto-update logic
- When troubleshooting "Last Sync: Never" issues
- During onboarding to verify sync persistence works correctly

**Quick Smoke Test:**
For automated API-level verification, use the smoke test script:
```bash
source /root/pms_env.sh
export TOKEN=$(curl -X POST "$SB_URL/auth/v1/token?grant_type=password" \
  -H "Content-Type: application/json" \
  -H "apikey: $SB_ANON_KEY" \
  -d '{"email":"admin@example.com","password":"your-password"}' \
  | jq -r '.access_token')
bash backend/scripts/pms_channel_last_sync_smoke.sh
```
See [pms_channel_last_sync_smoke.sh](../../scripts/README.md#pms_channel_last_sync_smokesh) for details.

---

### 1. Admin UI Verification (Browser)

**Navigate to Connections page:**
```
https://admin.fewo.kolibri-visions.de/connections
```

**Verification Steps:**

1. **Check Connections Table:**
   - Locate your test connections (booking_com, airbnb, etc.)
   - Verify "Last Sync" column shows actual timestamps (not "Never")
   - If "Never" is shown, trigger a sync and verify it updates after completion

2. **Trigger Sync via Sync Page:**
   - Navigate to Channel Sync page: `https://admin.fewo.kolibri-visions.de/channel-sync`
   - Select platform and property
   - Use "Auto-detect" button if Connection ID is empty (matches platform + property)
   - Trigger any sync type (Availability, Pricing, or Full)
   - Wait for success message with batch_id or task_id

3. **Verify Connection Summary:**
   - On Sync page, scroll to "Connection Summary" section
   - Verify "Last Sync" shows updated timestamp (within last few minutes)
   - Compare with Connections table to ensure consistency

**Expected Result:**
- Last Sync timestamp updates after successful sync completion
- Timestamp reflects when the sync log transitioned to `status='success'`
- Both Connections page and Sync page show consistent timestamps

---

### 2. API Verification (curl)

**Prerequisites:**
```bash
# Load environment variables
source /root/pms_env.sh

# Verify variables are set
echo $SB_URL  # Should not be empty
echo $API     # Should be https://api.fewo.kolibri-visions.de

# Get fresh JWT token
TOKEN=$(curl -X POST "$SB_URL/auth/v1/token?grant_type=password" \
  -H "Content-Type: application/json" \
  -H "apikey: $SB_ANON_KEY" \
  -d '{"email":"admin@example.com","password":"your-password"}' \
  | jq -r '.access_token')

# Verify token is set
echo $TOKEN
```

**Verification Query:**

⚠️ **IMPORTANT:** The list endpoint requires trailing slash to avoid 307/308 redirects.

```bash
# Method 1: Use trailing slash (RECOMMENDED)
curl -s "https://api.fewo.kolibri-visions.de/api/v1/channel-connections/?limit=100&offset=0" \
  -H "Authorization: Bearer $TOKEN" \
  -w "\nHTTP Status: %{http_code}\n" \
  -o /tmp/cc_body.json

# Method 2: Use -L to follow redirects (alternative)
curl -sL "https://api.fewo.kolibri-visions.de/api/v1/channel-connections?limit=100&offset=0" \
  -H "Authorization: Bearer $TOKEN" \
  -w "\nHTTP Status: %{http_code}\n" \
  -o /tmp/cc_body.json

# Check HTTP status (should be 200)
cat /tmp/cc_body.json | head -c 100  # Verify body is not empty

# Parse last_sync_at for booking_com connections
jq '.[] | select(.platform_type == "booking_com") | {id, platform_type, property_id, last_sync_at, updated_at}' /tmp/cc_body.json

# Parse last_sync_at for airbnb connections
jq '.[] | select(.platform_type == "airbnb") | {id, platform_type, property_id, last_sync_at, updated_at}' /tmp/cc_body.json

# Filter connections with NULL last_sync_at (should be empty after syncs)
jq '.[] | select(.last_sync_at == null) | {id, platform_type, property_id, status}' /tmp/cc_body.json
```

**Expected Output:**
```json
{
  "id": "abc-123-def-456",
  "platform_type": "booking_com",
  "property_id": "property-uuid",
  "last_sync_at": "2026-01-03T14:30:00.123456Z",
  "updated_at": "2026-01-03T14:30:00.123456Z"
}
```

**Troubleshooting:**
- **Empty body with 307/308 status:** Missing trailing slash → use `/api/v1/channel-connections/` (with slash) or add `-L` flag
- **401 "Token has expired":** Fetch new token using `$SB_URL/auth/v1/token` endpoint (see Prerequisites)
- **Empty `$SB_URL` variable:** Run `source /root/pms_env.sh` first (env vars not loaded in fresh shells)
- **JSONDecodeError:** Usually means empty body (redirect or auth failure) → check HTTP status code first

---

### 3. Database Verification (Supabase SQL Editor)

**Navigate to Supabase SQL Editor:**
```
https://supabase.com/dashboard/project/<project-id>/sql/new
```

**Query 1: Check channel_connections.last_sync_at**

```sql
-- List all connections with last_sync_at timestamps
SELECT
  id,
  platform_type,
  property_id,
  status,
  last_sync_at,
  updated_at,
  created_at
FROM public.channel_connections
WHERE deleted_at IS NULL
ORDER BY updated_at DESC
LIMIT 20;
```

**Expected Result:**
- `last_sync_at` should NOT be NULL for connections that have completed successful syncs
- Timestamp should match recent sync completions (within reasonable time window)

**Query 2: Check channel_sync_logs for specific connection**

⚠️ **IMPORTANT:** Column is `operation_type` (NOT `operation`)

```sql
-- List recent sync logs for a connection
SELECT
  id,
  connection_id,
  operation_type,    -- CORRECT column name (not "operation")
  direction,
  status,
  error,
  task_id,
  batch_id,
  tenant_id,
  created_at,
  updated_at
FROM public.channel_sync_logs
WHERE connection_id = '<CONNECTION-UUID-HERE>'
ORDER BY created_at DESC
LIMIT 20;
```

**Expected Result:**
- Successful syncs show `status = 'success'`
- `operation_type` values: `availability_update`, `pricing_update`, `bookings_sync`
- `tenant_id` should NOT be NULL (matches connection's `agency_id`)

**Query 3: Cross-check last_sync_at vs most recent success log**

```sql
-- Verify last_sync_at matches most recent successful sync
SELECT
  c.id AS connection_id,
  c.platform_type,
  c.last_sync_at AS connection_last_sync,
  MAX(l.updated_at) AS latest_success_log,
  CASE
    WHEN c.last_sync_at IS NULL AND MAX(l.updated_at) IS NOT NULL
      THEN 'MISMATCH: connection NULL but logs exist'
    WHEN c.last_sync_at < MAX(l.updated_at)
      THEN 'STALE: connection older than latest log'
    WHEN c.last_sync_at >= MAX(l.updated_at)
      THEN 'OK'
    ELSE 'UNKNOWN'
  END AS sync_status
FROM public.channel_connections c
LEFT JOIN public.channel_sync_logs l
  ON l.connection_id = c.id AND l.status = 'success'
WHERE c.deleted_at IS NULL
GROUP BY c.id, c.platform_type, c.last_sync_at
ORDER BY c.updated_at DESC
LIMIT 20;
```

**Expected Result:**
- `sync_status` should be `'OK'` for all connections with recent syncs
- No `'MISMATCH'` entries (indicates auto-update logic not running)
- No `'STALE'` entries (indicates connection not updated on log success)

---

### 4. Troubleshooting Quick Reference

**Issue: "Token has expired" (401)**
```bash
# Fix: Refresh JWT token
source /root/pms_env.sh  # Load env vars first
TOKEN=$(curl -X POST "$SB_URL/auth/v1/token?grant_type=password" \
  -H "Content-Type: application/json" \
  -H "apikey: $SB_ANON_KEY" \
  -d '{"email":"admin@example.com","password":"your-password"}' \
  | jq -r '.access_token')
```

**Issue: JSONDecodeError or empty response**
```bash
# Root cause: Usually redirect (307/308) or auth failure
# Fix 1: Check HTTP status code
curl -I "https://api.fewo.kolibri-visions.de/api/v1/channel-connections?limit=5"
# If 307/308: Add trailing slash OR use -L flag

# Fix 2: Check response body size
curl -s "..." -w "\nBody size: %{size_download} bytes\n" | head -c 200
# If 0 bytes: Auth failure or redirect issue
```

**Issue: Wrong SQL column name**
```sql
-- ❌ WRONG (column doesn't exist)
SELECT operation FROM channel_sync_logs;

-- ✅ CORRECT
SELECT operation_type FROM channel_sync_logs;
```

**Issue: Empty `$SB_URL` or `$SB_ANON_KEY`**
```bash
# Root cause: Environment variables not loaded in fresh SSH session
# Fix: Always source env file first
source /root/pms_env.sh

# Verify
echo $SB_URL       # Should show Supabase URL
echo $SB_ANON_KEY  # Should show anon key
```

**Issue: Admin UI shows "Never" despite successful logs**
- See [Connections Last Sync shows "Never"](#9-connections-last-sync-shows-never-null-last_sync_at) for full troubleshooting steps
- Quick fix: Run manual backfill SQL (see section above)
- Permanent fix: Verify auto-update logic deployed (check commit hash)

---

### Environment Variables

**Frontend (`pms-admin`):**
- `NEXT_PUBLIC_API_BASE`: API base URL (default: `https://api.fewo.kolibri-visions.de`)
  - Used for all API calls: `/api/v1/properties`, `/api/availability/sync`, `/api/v1/channel-connections/*/sync-logs`

**Backend (`pms-backend`):**
- `DATABASE_URL`: PostgreSQL connection string (must include `channel_sync_logs` table)
- `JWT_SECRET`: JWT signing key (must match frontend auth)

---

### Monitoring & Logging

**Frontend Logs:**
- Browser console (F12) shows API request/response
- Inline banner notifications show sync trigger success/error (auto-clears after 5s)

**Backend Logs:**
- Coolify Dashboard → pms-backend → Logs
- Shows incoming POST `/api/availability/sync` requests
- Shows Celery task dispatch

**Worker Logs:**
- Coolify Dashboard → pms-worker-v2 → Logs
- Shows task execution (received → started → success/failed)
- Shows retry attempts and exponential backoff

**Database Logs:**
- Query `channel_sync_logs` table directly:
  ```sql
  SELECT id, operation_type, status, error, created_at, updated_at
  FROM channel_sync_logs
  ORDER BY created_at DESC
  LIMIT 50;
  ```

---

### Smart Auto-Refresh Behavior

The Admin UI uses **conditional polling** to reduce unnecessary API calls:

- **Polls every 5 seconds** ONLY when there are active logs (status = "triggered" or "running")
- **Stops polling** when all logs are in terminal state (success/failed)
- **Resumes polling** when user triggers a new sync

This ensures:
- Real-time updates for active syncs
- Minimal backend load when idle
- No browser tab wake-up spam

---

### Known Limitations

1. **No Bulk Sync:**
   - UI only supports triggering one sync at a time
   - For bulk operations, use API directly or create custom script

2. **No Cancel Operation:**
   - Once triggered, sync cannot be cancelled from UI
   - Must wait for completion or manually kill Celery task

3. **Log Pagination:**
   - Currently loads all sync logs (no pagination in UI)
   - For large log history, use API with `limit`/`offset` parameters

4. **No Real-Time WebSocket:**
   - Uses HTTP polling (not WebSocket)
   - 5-second refresh interval may show delayed status updates

---

### Related Sections

- [Channel Manager API Endpoints](#channel-manager-api-endpoints) - API documentation
- [Sync Logs Persistence](#sync-logs-persistence) - Database schema
- [Celery Worker Troubleshooting](#celery-worker-pms-worker-v2-start-verify-troubleshoot) - Worker issues
- [DB DNS / Degraded Mode](#db-dns--degraded-mode) - Database connectivity

---

## Sync Logs Persistence

**Purpose:** Understand how sync operations are tracked in the database.

### Database Table: `channel_sync_logs`

**Schema:**
```sql
CREATE TABLE public.channel_sync_logs (
  id uuid PRIMARY KEY DEFAULT gen_random_uuid(),
  tenant_id uuid NULL,
  connection_id uuid NOT NULL,
  operation_type text NOT NULL CHECK (operation_type IN (
    'full_sync', 'availability_update', 'pricing_update',
    'bookings_import', 'calendar_sync', 'listing_update'
  )),
  direction text NOT NULL DEFAULT 'outbound' CHECK (direction IN ('outbound', 'inbound')),
  status text NOT NULL CHECK (status IN (
    'triggered', 'queued', 'running', 'success', 'failed', 'cancelled'
  )),
  details jsonb NULL,
  error text NULL,
  task_id text NULL,
  created_at timestamptz NOT NULL DEFAULT now(),
  updated_at timestamptz NULL
);
```

**Indexes:**
- `(connection_id, created_at DESC)` - Fast queries by connection
- `(task_id)` - Fast updates by Celery task ID
- `(tenant_id, created_at DESC)` - Multi-tenant queries
- `(status, created_at DESC)` - Filter by status

---

### Logged Information

**Mandatory Fields:**
- `id`: Unique log entry UUID
- `connection_id`: Channel connection or property UUID
- `operation_type`: Type of sync operation
- `direction`: "outbound" (PMS → Channel) or "inbound" (Channel → PMS)
- `status`: Current status (triggered/queued/running/success/failed/cancelled)
- `created_at`: When log entry was created

**Optional Fields:**
- `tenant_id`: Agency UUID (for multi-tenant filtering)
- `task_id`: Celery task UUID (for async tracking)
- `error`: Error message (if status=failed)
- `updated_at`: Last status update timestamp

**JSONB Details Field:**

Flexible metadata storage, varies by operation:

**Availability Sync:**
```json
{
  "platform": "airbnb",
  "property_id": "uuid",
  "manual_trigger": true,
  "start_date": "2025-12-28",
  "end_date": "2026-03-28",
  "check_in": "2025-12-28",
  "check_out": "2025-12-30",
  "available": true,
  "retry_count": 0,
  "next_retry_seconds": null
}
```

**Pricing Sync:**
```json
{
  "platform": "booking_com",
  "property_id": "uuid",
  "manual_trigger": false,
  "check_in": "2025-12-28",
  "check_out": "2025-12-30",
  "nightly_rate": 150.00,
  "currency": "EUR"
}
```

**During Retry:**
```json
{
  "retry_count": 2,
  "error_type": "database_unavailable",
  "next_retry_seconds": 4
}
```

---

### Status Lifecycle

```
triggered → running → success
                   ↘ failed
                   ↘ cancelled
```

**Status Descriptions:**

| Status | Description | Updated By | Transition |
|--------|-------------|------------|------------|
| `triggered` | Sync request received, log created | API endpoint | Immediately on POST /sync |
| `running` | Task execution started | Celery worker | When worker picks up task |
| `success` | Task completed successfully | Celery worker | On successful completion |
| `failed` | Task failed after all retries | Celery worker | On permanent failure |
| `cancelled` | Task manually cancelled | Manual intervention | Rare, manual only |

**Lifecycle Flow:**
1. **API trigger** (POST `/api/v1/channel-connections/{id}/sync`):
   - Creates log entry with `status="triggered"`
   - Includes `task_id` (Celery task UUID) and `batch_id` (groups multiple operations)
   - Returns immediately with task_ids array

2. **Worker picks up task**:
   - Updates log to `status="running"`
   - Executes sync operation (platform API calls)

3. **Task completes**:
   - On success: Updates log to `status="success"`, updates `connection.last_sync_at`
   - On failure: Updates log to `status="failed"`, sets `error` field

**Note on "queued" status:** Prior to 2026-01-03, logs used `status="queued"` when triggered. This was semantically equivalent to "triggered" and both are treated identically in batch status aggregation. Production systems may still show "queued" in historical logs.

---

### Querying Sync Logs

**Via API (Basic):**
```bash
# Get last 50 logs for connection
curl https://api.your-domain.com/api/v1/channel-connections/{id}/sync-logs?limit=50 \
  -H "Authorization: Bearer TOKEN"

# Get logs with pagination
curl https://api.your-domain.com/api/v1/channel-connections/{id}/sync-logs?limit=20&offset=40 \
  -H "Authorization: Bearer TOKEN"
```

**Via API (Filtering & Polling):**
```bash
# Poll logs for a specific task_id (check if task completed)
curl "https://api.your-domain.com/api/v1/channel-connections/{id}/sync-logs?task_id=abc123" \
  -H "Authorization: Bearer TOKEN"

# Poll logs for entire batch (check batch progress)
curl "https://api.your-domain.com/api/v1/channel-connections/{id}/sync-logs?batch_id={batch_uuid}" \
  -H "Authorization: Bearer TOKEN"

# Filter by status (get only failed syncs)
curl "https://api.your-domain.com/api/v1/channel-connections/{id}/sync-logs?status=failed" \
  -H "Authorization: Bearer TOKEN"

# Filter by operation type (get only availability updates)
curl "https://api.your-domain.com/api/v1/channel-connections/{id}/sync-logs?operation_type=availability_update" \
  -H "Authorization: Bearer TOKEN"

# Filter by direction (get only inbound syncs)
curl "https://api.your-domain.com/api/v1/channel-connections/{id}/sync-logs?direction=inbound" \
  -H "Authorization: Bearer TOKEN"

# Combine filters (get failed availability updates)
curl "https://api.your-domain.com/api/v1/channel-connections/{id}/sync-logs?status=failed&operation_type=availability_update" \
  -H "Authorization: Bearer TOKEN"
```

**Polling Pattern (Check Task Completion):**
```bash
# 1. Trigger sync and capture task_id
RESPONSE=$(curl -X POST "https://api.your-domain.com/api/v1/channel-connections/{id}/sync" \
  -H "Authorization: Bearer TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"sync_type": "availability", "properties": []}')

TASK_ID=$(echo "$RESPONSE" | jq -r '.task_ids[0]')
echo "Task ID: $TASK_ID"

# 2. Poll until task completes (simple loop)
while true; do
  STATUS=$(curl -s "https://api.your-domain.com/api/v1/channel-connections/{id}/sync-logs?task_id=$TASK_ID" \
    -H "Authorization: Bearer TOKEN" | jq -r '.logs[0].status')

  echo "Status: $STATUS"

  if [[ "$STATUS" == "success" ]] || [[ "$STATUS" == "failed" ]]; then
    echo "Task completed with status: $STATUS"
    break
  fi

  sleep 2  # Wait 2 seconds before next poll
done
```

**Via Database:**
```sql
-- Get recent logs for connection
SELECT id, operation_type, status, created_at, updated_at
FROM channel_sync_logs
WHERE connection_id = 'uuid-here'
ORDER BY created_at DESC
LIMIT 50;

-- Get failed logs
SELECT id, operation_type, error, created_at
FROM channel_sync_logs
WHERE status = 'failed'
ORDER BY created_at DESC;

-- Get logs with retry details
SELECT id, operation_type, status,
       details->>'retry_count' as retries,
       created_at
FROM channel_sync_logs
WHERE details->>'retry_count' IS NOT NULL
ORDER BY created_at DESC;
```

---

### Channel Manager — Sync Log Retention & Cleanup

**Purpose:** Prevent indefinite database growth by implementing periodic cleanup of old sync logs. This section provides professional retention guidelines and manual cleanup procedures.

---

#### Why Retention Matters

**Database Growth:**
- Each sync operation creates 1-3 log entries (full sync creates 3: availability, pricing, bookings)
- High-volume properties with hourly syncs generate ~72 logs/day = ~2,160 logs/month
- Without cleanup, logs accumulate indefinitely, consuming disk space and slowing queries
- Example: 10 properties × 2,160 logs/month × 12 months = 259,200 log entries/year

**UI Performance:**
- Admin UI `/channel-sync` page loads all logs for selected connection (no server-side pagination)
- Large log tables (>10,000 entries per connection) cause slow page loads and browser memory issues
- Sync History section fetches paginated batches, but still slows with excessive data

**Query Performance:**
- Indexed queries remain fast up to ~100,000 total rows
- Beyond 1M rows, even indexed queries may slow down
- Regular cleanup maintains optimal performance

---

#### Recommended Retention Periods

**Test/Staging Environments:**
- **90 days** (3 months)
- Sufficient for debugging recent issues
- Keeps DB size manageable for development workflows

**Production Environments:**
- **180 days** (6 months): Standard retention for operational visibility
- **365 days** (1 year): Extended retention for audit/compliance requirements
- **Custom**: Adjust based on legal/regulatory requirements (e.g., GDPR, financial audits)

**Factors to Consider:**
- **Compliance Requirements:** Some industries require longer retention (e.g., 7 years for financial data)
- **Disk Space:** Monitor Supabase storage usage and adjust retention accordingly
- **Query Performance:** If Admin UI becomes slow, reduce retention period
- **Debugging Needs:** Keep at least 90 days to troubleshoot recurring sync issues

---

#### Manual Cleanup (Supabase SQL Editor)

**Access:** Supabase Dashboard → SQL Editor → New Query

**IMPORTANT:** These queries delete data permanently. Always verify cutoff date before execution.

**1. Delete Logs Older Than N Days**

```sql
-- Test/Staging: Delete logs older than 90 days
DELETE FROM public.channel_sync_logs
WHERE created_at < NOW() - INTERVAL '90 days';

-- Production: Delete logs older than 180 days
DELETE FROM public.channel_sync_logs
WHERE created_at < NOW() - INTERVAL '180 days';

-- Production (Extended): Delete logs older than 365 days
DELETE FROM public.channel_sync_logs
WHERE created_at < NOW() - INTERVAL '365 days';
```

**2. Preview Deletion Count (Safe Dry-Run)**

Before executing DELETE, preview how many rows will be deleted:

```sql
-- Preview: Count logs older than 90 days
SELECT COUNT(*) AS logs_to_delete,
       MIN(created_at) AS oldest_log,
       MAX(created_at) AS newest_affected_log
FROM public.channel_sync_logs
WHERE created_at < NOW() - INTERVAL '90 days';
```

**3. Delete by Specific Cutoff Date**

```sql
-- Delete logs before specific date (e.g., before 2025-01-01)
DELETE FROM public.channel_sync_logs
WHERE created_at < '2025-01-01 00:00:00'::timestamptz;
```

**4. Connection-Specific Cleanup**

```sql
-- Delete old logs for specific connection only
DELETE FROM public.channel_sync_logs
WHERE connection_id = 'your-connection-uuid-here'
  AND created_at < NOW() - INTERVAL '90 days';
```

**5. Advanced: Keep Last N Batches Per Connection**

For connections with batch_id (full syncs), keep only the most recent N batches:

```sql
-- Keep only last 50 batches per connection, delete older batches
WITH batches_to_keep AS (
  SELECT DISTINCT batch_id
  FROM public.channel_sync_logs
  WHERE batch_id IS NOT NULL
    AND connection_id = 'your-connection-uuid-here'
  ORDER BY created_at DESC
  LIMIT 50
)
DELETE FROM public.channel_sync_logs
WHERE connection_id = 'your-connection-uuid-here'
  AND batch_id IS NOT NULL
  AND batch_id NOT IN (SELECT batch_id FROM batches_to_keep);
```

---

#### Safety Notes

**Critical Warnings:**

1. **History Only — No Impact on Bookings/Inventory:**
   - Deleting sync logs removes audit history only
   - Does NOT affect current bookings, availability, or pricing data
   - Safe to delete old logs without business impact

2. **Irreversible Deletion:**
   - Deleted logs cannot be recovered (no soft delete)
   - Always preview with COUNT(*) before executing DELETE
   - Consider exporting logs to CSV if long-term archival is needed

3. **Run During Low-Traffic Windows:**
   - Large deletes (>100,000 rows) can cause brief table locks
   - Recommended: Run during off-peak hours (e.g., 2-4 AM local time)
   - Monitor Supabase dashboard for connection pool usage during execution

4. **Database Locks:**
   - DELETE operations acquire row-level locks
   - Concurrent sync operations may briefly slow down during cleanup
   - Use smaller batches if full table scan is too slow (e.g., delete 1 month at a time)

**Best Practices:**

- **Schedule Regular Cleanup:** Run monthly or quarterly (set calendar reminder)
- **Document Execution:** Keep log of cleanup dates and row counts in operations log
- **Monitor Disk Usage:** Check Supabase storage metrics before/after cleanup
- **Test First:** Run on staging environment before executing on production

---

#### Verification Queries

**1. Check Table Size**

```sql
-- Get total row count and date range
SELECT COUNT(*) AS total_logs,
       MIN(created_at) AS oldest_log,
       MAX(created_at) AS newest_log,
       COUNT(*) FILTER (WHERE created_at < NOW() - INTERVAL '90 days') AS logs_older_than_90d,
       COUNT(*) FILTER (WHERE created_at < NOW() - INTERVAL '180 days') AS logs_older_than_180d
FROM public.channel_sync_logs;
```

**2. Logs Per Connection**

```sql
-- Count logs per connection (identify high-volume connections)
SELECT connection_id,
       COUNT(*) AS log_count,
       MIN(created_at) AS oldest,
       MAX(created_at) AS newest
FROM public.channel_sync_logs
GROUP BY connection_id
ORDER BY log_count DESC
LIMIT 20;
```

**3. Storage Size Estimate**

```sql
-- Estimate table size in MB (PostgreSQL)
SELECT pg_size_pretty(pg_total_relation_size('public.channel_sync_logs')) AS total_size,
       pg_size_pretty(pg_relation_size('public.channel_sync_logs')) AS table_size,
       pg_size_pretty(pg_indexes_size('public.channel_sync_logs')) AS indexes_size;
```

**4. Verify Cleanup Success**

Run immediately after DELETE to confirm expected row count:

```sql
-- Should show 0 logs older than retention period
SELECT COUNT(*) AS remaining_old_logs
FROM public.channel_sync_logs
WHERE created_at < NOW() - INTERVAL '90 days';
-- Expected: 0
```

---

#### Automated Cleanup (Future Enhancement)

**Current State:** Manual cleanup required (no automated retention policy implemented)

**Future Options:**

1. **Supabase Cron Job:**
   - Use `pg_cron` extension to schedule monthly cleanup
   - Example: Auto-delete logs older than 180 days on 1st of each month

2. **Application-Level Cleanup:**
   - Add FastAPI scheduled task to run cleanup during off-peak hours
   - Implement configurable retention period via environment variable

3. **Partitioned Tables:**
   - Partition `channel_sync_logs` by month (e.g., `channel_sync_logs_2025_01`)
   - Drop entire partitions for faster cleanup (avoids DELETE scan)

**Tracking Issue:** Consider creating GitHub issue to track automated cleanup implementation.

---

**Related:**

- See [Log Retention & Purge Policy](#log-retention--purge-policy) for Admin UI purge feature (connection-scoped)
- See [Stale Sync Logs (Automatic Cleanup)](#stale-sync-logs-automatic-cleanup) for automatic status-based cleanup
- See [Sync Logs Persistence](#sync-logs-persistence) for table schema and indexes

---

## Celery Worker (pms-worker / pms-worker-v2)

**Purpose:** Comprehensive guide for setting up and troubleshooting the Celery worker application in Coolify.

---

### What is the Celery Worker?

The Celery worker (`pms-worker` or `pms-worker-v2`) is a **headless background task processor** that executes Channel Manager operations:
- Availability sync to external platforms (Airbnb, Booking.com, etc.)
- Pricing sync to external platforms
- Booking imports from external platforms
- Retry logic with exponential backoff for failed operations

**Key Characteristics:**
- No public HTTP endpoints (no domains required)
- Connects to Redis broker for task queue
- Connects to PostgreSQL database for sync log persistence
- Runs independently of pms-backend (different container)
- Must be on same git commit as pms-backend for task compatibility

---

### Coolify Setup (Step-by-Step)

#### Step 1: Create New Application

**Location:** Coolify Dashboard → Add New Resource → Application

1. **Application Name:** `pms-worker-v2` (or `pms-worker`)
2. **Source:** Select your Git repository
3. **Branch:** `main`
4. **Base Directory:** `/backend`
5. **Build Pack:** Nixpacks (auto-detected)

#### Step 2: Configure Start Command

**Location:** Application Settings → Build & Deploy → Start Command

```bash
celery -A app.channel_manager.core.sync_engine:celery_app worker -l INFO
```

**Breakdown:**
- `-A app.channel_manager.core.sync_engine:celery_app`: Celery app module path
- `worker`: Run as worker process (not beat/flower)
- `-l INFO`: Log level (use `DEBUG` for troubleshooting)

#### Step 3: Configure Ports (Coolify UI Requirement)

**Location:** Application Settings → Network → Ports Exposes

**Set:** `8000`

**Important:** This is a dummy value required by Coolify UI. The worker does NOT actually serve HTTP traffic on this port.

#### Step 4: Configure Domains (Leave Empty)

**Location:** Application Settings → Domains

**Action:** Do NOT add any domains. Worker is headless and should not be exposed via Traefik proxy.

#### Step 5: Coolify Beta Workarounds

**Problem:** Deployment may fail with `"Deployment failed: Undefined variable $labels"`

**Solution:**

1. **Enable Read-only Labels:**
   - Application Settings → Advanced → Enable "Read-only labels"

2. **Add Label to Disable Traefik:**
   - Application Settings → Labels
   - Add: `traefik.enable=false`

3. **Disable Proxy Features:**
   - Force HTTPS: **OFF**
   - Gzip Compression: **OFF**
   - Strip Prefixes: **OFF**

4. **Clean Rebuild (if code seems stale):**
   - Deployment Settings → Check "Disable build cache"
   - Pin to specific commit SHA instead of branch reference

---

### Alternative: Build with Dockerfile.worker (Recommended for Non-Root)

**Why use Dockerfile.worker instead of Nixpacks:**

Nixpacks auto-detection installs Python packages under `/root/.nix-profile`, which prevents running as non-root user. Using a dedicated Dockerfile provides:
- **Reliable non-root execution** (appuser UID 10001)
- **No Celery SecurityWarning** (avoids "running as root" warning)
- **Explicit dependency management** (no Nixpacks magic)
- **Better security posture** (principle of least privilege)
- **Configurable worker pool** (threads, prefork, gevent via env vars)

#### Step-by-Step: Switch to Dockerfile.worker

**Step 1: Update Build Pack**

**Location:** Application Settings → Build & Deploy → Build Pack

Change from `Nixpacks` to `Dockerfile`

**Step 2: Set Dockerfile Path**

**Location:** Application Settings → Build & Deploy → Dockerfile

Set: `Dockerfile.worker`

**Base Directory** should still be: `/backend`

**Step 3: Start Command (Automatic via Dockerfile)**

**IMPORTANT:** When using Dockerfile build pack, Coolify does **NOT** allow setting a Start Command in the UI. The worker image automatically boots via the CMD defined in `backend/Dockerfile.worker`:

```bash
CMD ["bash", "/app/scripts/ops/start_worker.sh"]
```

**Note:** With Coolify Build Pack set to "Dockerfile", the Start Command UI field may not be available. The Dockerfile CMD/ENTRYPOINT defines the start command. All customization happens via environment variables (see Step 4 below), not by overriding the start command.

**What this means:**
- Leave "Start Command" field **empty** in Coolify UI (field is disabled/ignored for Dockerfile build pack)
- Worker boots via `scripts/ops/start_worker.sh` by default
- **Wait-for-deps preflight is active automatically** (waits for DB/Redis DNS+TCP before starting Celery)
- Worker pool and concurrency are controlled via environment variables (see Step 4 below)

**Why use the start script:**
- Prevents transient DNS/network failures after Coolify deploys
- Ensures DB and Redis are reachable before Celery starts
- If dependencies unavailable after timeout (default 60s), container exits to trigger restart

**Step 4: Set Worker Pool Environment Variables (Optional)**

**Location:** Application Settings → Environment Variables

Add these optional variables to customize worker behavior:

```bash
# Worker pool type (default: threads)
# Options: threads, prefork, gevent, eventlet, solo
CELERY_POOL=threads

# Number of worker threads/processes (default: 4)
CELERY_CONCURRENCY=4

# Log level (default: INFO)
CELERY_LOGLEVEL=INFO

# Optional: Max tasks before worker restart (prevents memory leaks)
CELERY_MAX_TASKS_PER_CHILD=1000

# Optional: Hard time limit for tasks in seconds
CELERY_TIME_LIMIT=300
```

**Wait-for-deps configuration (prevents transient DNS failures):**

The worker includes a preflight check that waits for database and Redis to be reachable before starting Celery. This prevents transient failures caused by Docker DNS/network timing issues after deployments.

```bash
# Enable dependency wait (default: true)
# Set to false to skip wait (not recommended)
WORKER_WAIT_FOR_DEPS=true

# Max wait time in seconds (default: 60)
# Worker exits if dependencies not ready within this time
WORKER_WAIT_TIMEOUT=60

# Check interval in seconds (default: 2)
# How often to retry DNS/TCP checks
WORKER_WAIT_INTERVAL=2

# Database hostname to wait for (default: supabase-db)
WORKER_WAIT_DB_HOST=supabase-db

# Database port (default: 5432)
WORKER_WAIT_DB_PORT=5432

# Redis hostname to wait for (default: coolify-redis)
WORKER_WAIT_REDIS_HOST=coolify-redis

# Redis port (default: 6379)
WORKER_WAIT_REDIS_PORT=6379
```

**Why wait-for-deps:**
- **Problem:** Docker DNS resolution and network attachment can take 5-30 seconds after container start
- **Symptom:** Worker logs show `socket.gaierror: [Errno -3] Temporary failure in name resolution`
- **Solution:** Wait for DNS + TCP connectivity before starting Celery
- **Behavior:** If timeout exceeded, worker exits (container restarts automatically)
- **Reliability:** Prefer delayed start over starting "half-ready" and failing tasks

**Recommended settings:**
- **Development:** `CELERY_POOL=threads`, `CELERY_CONCURRENCY=4`, `CELERY_LOGLEVEL=DEBUG`
- **Production:** `CELERY_POOL=threads`, `CELERY_CONCURRENCY=8`, `CELERY_LOGLEVEL=INFO`
- **Heavy I/O:** Use `threads` pool (better for async DB/HTTP operations)
- **CPU-bound:** Use `prefork` pool (isolates tasks in separate processes)

**Step 5: Deploy**

Click **Deploy** and monitor logs for:
```
[INFO/MainProcess] celery@pms-worker-v2 ready.
[INFO/MainProcess] Tasks:
  - app.channel_manager.core.sync_engine.update_channel_availability
  - app.channel_manager.core.sync_engine.update_channel_pricing
```

**Verification:**

```bash
# Check process user (should be appuser, not root)
docker exec pms-worker-v2 ps aux | head -3

# Expected output:
# USER       PID  %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
# appuser      1   0.0  0.1 123456 12345 ?        Ss   12:00   0:00 celery -A app.channel_manager...

# Check for SecurityWarning (should be empty)
docker logs pms-worker-v2 2>&1 | grep -i "securitywarning\|running as root"

# Expected: No output (warning is gone)
```

**What changed:**
- Build uses `backend/Dockerfile.worker` instead of Nixpacks auto-detection
- Worker runs as `appuser` (UID 10001) instead of root
- All files owned by `appuser:appuser`
- Celery SecurityWarning no longer appears
- Worker pool and concurrency configurable via env vars

**Rollback to Nixpacks (if needed):**
1. Set Build Pack back to `Nixpacks`
2. Clear "Dockerfile" field
3. Set Start Command to: `celery -A app.channel_manager.core.sync_engine:celery_app worker -l INFO`
4. Redeploy

---

### Required Environment Variables

**Location:** Application Settings → Environment Variables

**Copy ALL environment variables from `pms-backend`, especially:**

#### Core Application
```
ENVIRONMENT=production
DEBUG=false
LOG_LEVEL=INFO
```

#### Redis & Celery (CRITICAL)
```
REDIS_URL=redis://:<password>@coolify-redis:6379/0
CELERY_BROKER_URL=redis://:<password>@coolify-redis:6379/0
CELERY_RESULT_BACKEND=redis://:<password>@coolify-redis:6379/0
```

**Important:** Encode special characters in Redis password (e.g., `+` → `%2B`, `=` → `%3D`)

#### Database (CRITICAL)
```
DATABASE_URL=postgresql+asyncpg://postgres:<password>@supabase-db:5432/postgres
```

**Note:** Use `supabase-db` hostname (requires Supabase network attachment, see below)

#### Health Checks
```
ENABLE_REDIS_HEALTHCHECK=true
ENABLE_CELERY_HEALTHCHECK=true
```

#### Authentication (CRITICAL)
```
JWT_SECRET=<same-as-backend>
SUPABASE_JWT_SECRET=<same-as-goTrue>
JWT_AUDIENCE=authenticated
ENCRYPTION_KEY=<same-as-backend>
```

**Important:** JWT secrets must EXACTLY match pms-backend and Supabase GoTrue for token validation.

#### Optional
```
SENTRY_DSN=<if-using-sentry>
FEATURE_CHANNEL_MANAGER_ENABLED=true
```

---

### Networks (CRITICAL)

**Problem:** Worker needs access to BOTH Coolify infrastructure AND Supabase database.

**Required Networks:**
1. `coolify` (default, for Redis access)
2. `bccg4gs4o4kgsowocw08wkw4` (Supabase network, for database access)

#### Attach to Supabase Network

**Symptom if missing:** Database DNS resolution fails, "Database temporarily unavailable" errors

**Solution (Host Server Terminal):**

```bash
# SSH to host server
ssh root@your-server.com

# Connect worker to Supabase network
docker network connect bccg4gs4o4kgsowocw08wkw4 pms-worker-v2

# Restart worker to pick up network changes
docker restart pms-worker-v2

# Verify networks
docker inspect pms-worker-v2 | grep -A 10 '"Networks"'

# Expected: both "coolify" and "bccg4gs4o4kgsowocw08wkw4" networks
```

**Test DNS Resolution:**

```bash
# Test database DNS from inside worker container
docker exec pms-worker-v2 nslookup supabase-db

# Expected: IP address (e.g., 172.20.0.2)
# If empty/error: Network attachment failed
```

---

### Verification Checklist

#### 1. Container is Running

```bash
# Host server terminal
docker ps | grep pms-worker

# Expected: pms-worker-v2 container with "Up" status
# If "Exited": Check logs for crash reason
```

#### 2. Celery Worker is Ready

```bash
# Host server terminal
docker logs pms-worker-v2 --tail 50 | grep "ready"

# Expected output:
# [INFO/MainProcess] celery@pms-worker-v2 ready.
# [INFO/MainProcess] Tasks:
#   - app.channel_manager.core.sync_engine.update_channel_availability
#   - app.channel_manager.core.sync_engine.update_channel_pricing
```

#### 3. Tasks are Being Received

**Trigger a test sync:**

```bash
# From backend API
curl -X POST https://api.fewo.kolibri-visions.de/api/v1/availability/sync \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "sync_type": "availability",
    "platform": "airbnb",
    "property_id": "<property-uuid>",
    "manual_trigger": true
  }'

# Check worker logs for task receipt
docker logs pms-worker-v2 --tail 20 | grep "received"

# Expected:
# [INFO/MainProcess] Task update_channel_availability[abc123] received
# [INFO/ForkPoolWorker-1] Task update_channel_availability[abc123] succeeded
```

#### 4. Sync Logs are Updated

```bash
# From backend API
curl https://api.fewo.kolibri-visions.de/api/v1/channel-connections/<connection-id>/sync-logs?limit=5 \
  -H "Authorization: Bearer $TOKEN"

# Expected: Logs show status progression
# triggered → running → success (or failed)
# updated_at timestamps should be set
```

#### 5. Verify Code Version (Optional)

**Use Case:** Ensure worker is running latest code after deployment

```bash
# Get SHA256 hash of sync_engine.py in container
docker exec pms-worker-v2 sha256sum /app/app/channel_manager/core/sync_engine.py

# Compare with local repo hash
sha256sum backend/app/channel_manager/core/sync_engine.py

# Hashes should match
```

**Alternative: Check Git Commit**

```bash
# Backend commit
docker exec pms-backend git rev-parse HEAD

# Worker commit
docker exec pms-worker-v2 git rev-parse HEAD

# Must match exactly
```

---

### Common Issues & Troubleshooting

#### Worker Crashes on Startup

**Symptom:** Container exits immediately after deployment

**Diagnosis:**

```bash
docker logs pms-worker-v2 --tail 100
```

**Common Causes:**
1. **Redis connection failed:** Check `REDIS_URL` and password encoding
2. **Module import error:** Check if code compiles (`python3 -m py_compile app/channel_manager/core/sync_engine.py`)
3. **Missing environment variable:** Check logs for "KeyError" or "FieldValidationError"

#### Tasks Not Being Received

**Symptom:** Worker is "ready" but no tasks appear in logs

**Diagnosis:**

```bash
# From backend container, ping Celery workers
docker exec pms-backend celery -A app.channel_manager.core.sync_engine:celery_app \
  --broker "$CELERY_BROKER_URL" inspect ping -t 3

# Expected: -> celery@pms-worker-v2: {'ok': 'pong'}
# If timeout: Worker cannot connect to Redis broker
```

**Solutions:**
1. Verify `CELERY_BROKER_URL` matches between backend and worker
2. Check Redis is running: `docker ps | grep redis`
3. Check worker is on `coolify` network

#### Database Temporarily Unavailable

**Symptom:** Worker logs show `"Database is temporarily unavailable"` or `"Failed to update sync log ... 503"`

**Cause:** Worker not attached to Supabase network

**Solution:** See [Networks (CRITICAL)](#networks-critical) section above

**Test:**

```bash
# Test direct DB connection from worker
docker exec pms-worker-v2 python3 -c "import asyncpg; import asyncio; import os; asyncio.run(asyncpg.connect(os.environ['DATABASE_URL']).close()); print('DB OK')"

# Expected: "DB OK"
# If error: DATABASE_URL wrong or network missing
```

#### Worker Starts with Wrong CMD (start_worker.sh Not Used)

**Symptom:**
- `docker inspect` shows wrong command even though Dockerfile.worker has correct CMD:
  ```bash
  docker inspect pms-worker-v2 --format 'Cmd={{json .Config.Cmd}}'
  # Shows: ["/bin/sh","-c","celery -A app.channel_manager.core.sync_engine:celery_app worker ..."]
  # Expected: ["bash","/app/scripts/ops/start_worker.sh"]
  ```
- Worker logs do NOT show:
  ```
  [worker] start_worker.sh active
  ====== Wait-for-Deps Preflight Check ======
  ```

**Root Causes:**

A) **Coolify Start Command Override:** Legacy "Start Command" field in Coolify UI overrides Dockerfile CMD
B) **Coolify Git Commit Pinned:** Build log shows checkout of old commit SHA before Dockerfile.worker was updated
C) **Dockerfile Path Mismatch:** Coolify prepends "/" automatically; incorrect Base Directory or Dockerfile Location

**Diagnosis (HOST-SERVER-TERMINAL):**

```bash
# 1. Compare container CMD vs image CMD
docker inspect pms-worker-v2 --format 'Image={{.Config.Image}} Cmd={{json .Config.Cmd}}'
# Container shows: Cmd=["/bin/sh","-c","celery ..."]

# Get image ID from above output
IMAGE_ID="<image-id-or-tag-from-above>"
docker inspect $IMAGE_ID --format 'Cmd={{json .Config.Cmd}}'
# Image shows: ["bash","/app/scripts/ops/start_worker.sh"]
# If these differ: Coolify Start Command override is active

# 2. Confirm repo has correct CMD
cd /path/to/PMS-Webapp
git log -n 3 --oneline
# Should show recent commits including Dockerfile.worker updates

grep -nE '^(CMD|ENTRYPOINT)\b' backend/Dockerfile.worker
# Should show: CMD ["bash", "/app/scripts/ops/start_worker.sh"]
```

**Fix Steps (Coolify UI):**

1. **Clear Start Command Override:**
   - Location: Coolify Dashboard → pms-worker-v2 → General → Start Command
   - Action: **Delete any text in the field** (leave it empty)
   - Why: Dockerfile CMD should be used, not UI override

2. **Unpin Git Commit (Build from HEAD):**
   - Location: Coolify Dashboard → pms-worker-v2 → General → Git
   - Field: "Commit SHA" or "Git Commit to deploy"
   - Action: **Clear the field** (empty = build latest commit from main)
   - Why: Ensures latest Dockerfile.worker is used

3. **Verify Build Settings:**
   - Location: Coolify Dashboard → pms-worker-v2 → Build & Deploy
   - Base Directory: `/backend` (with leading slash)
   - Dockerfile Location: `/Dockerfile.worker` (with leading slash)
   - Note: Coolify may prepend "/" automatically; do not use `backend/Dockerfile.worker`

4. **Redeploy (NOT just restart):**
   - Location: Coolify Dashboard → pms-worker-v2 → Deployments
   - Action: Click "Redeploy" (triggers new build)
   - Why: Restart uses existing image; redeploy builds fresh image

**Verification:**

```bash
# After redeploy completes, verify container CMD
docker inspect pms-worker-v2 --format 'Cmd={{json .Config.Cmd}}'
# Expected: ["bash","/app/scripts/ops/start_worker.sh"]

# Check worker logs for startup sequence
docker logs pms-worker-v2 --tail 50 | head -20
# Expected output:
#   [worker] start_worker.sh active
#   [2025-12-29 ...] ====== Wait-for-Deps Preflight Check ======
#   [2025-12-29 ...] Waiting for Database (supabase-db:5432)...
#   [2025-12-29 ...]   ✓ Database is ready (supabase-db:5432)
#   [2025-12-29 ...] Waiting for Redis (coolify-redis:6379)...
#   [2025-12-29 ...]   ✓ Redis is ready (coolify-redis:6379)
#   [2025-12-29 ...] ✓ All dependencies ready
#   [2025-12-29 ...] ====== Celery Worker Starting ======
#   [2025-12-29 ...] Configuration:
#   [2025-12-29 ...]   Pool: threads
#   [2025-12-29 ...]   Concurrency: 4
```

**Related Issues:**
- If worker crashes after fixing CMD: Check [Worker Crashes on Startup](#worker-crashes-on-startup)
- If wait-for-deps times out: Check [Networks (CRITICAL)](#networks-critical) and DNS resolution

#### Stale Code After Deployment

**Symptom:** New code changes don't appear in worker behavior

**Diagnosis:**

```bash
# Check if code hash matches local repo
docker exec pms-worker-v2 sha256sum /app/app/channel_manager/core/sync_engine.py

# Compare with local
sha256sum backend/app/channel_manager/core/sync_engine.py
```

**Solutions:**
1. **Pin to commit SHA** in Coolify deployment settings (instead of branch reference)
2. **Enable "Disable build cache"** in deployment settings
3. **Force rebuild:** Settings → Deployments → Redeploy with cache disabled

#### Sync Logs Stuck at "running"

**Symptom:** Sync logs never transition to `success` or `failed`

**Diagnosis:**

```bash
# Check worker logs for exceptions during task execution
docker logs pms-worker-v2 --tail 100 | grep -i error

# Check if MaxRetriesExceededError is being caught
docker logs pms-worker-v2 | grep "Max retries exceeded"
```

**Common Causes:**
1. Task is hanging indefinitely (no timeout configured)
2. Exception handler not marking log as failed
3. Worker crashed mid-task

**Solution:** Check sync_engine.py retry logic and ensure MaxRetriesExceededError sets status to "failed"

---

### Architecture Notes

**Direct Database Connections (Celery-Safe):**

Celery workers do NOT use the FastAPI connection pool (pool only exists in backend lifespan). Instead, workers use direct connections:

- `_check_database_availability()`: Creates short-lived connection with 5s timeout for health checks
- `_update_log_status()`: Creates connection per sync log update with JSON/JSONB codec registration
- All connections are properly closed in `finally` blocks (fork/event-loop safe)

**Fork-Safety:**

Celery uses prefork pool model. Workers register a `worker_process_init` signal to reset pool state in forked children, ensuring each worker process creates its own connections.

**Celery 6 Broker Connection Retry:**

The worker explicitly sets `broker_connection_retry_on_startup = True` to silence Celery 6 deprecation warnings. This makes the broker connection retry behavior explicit on worker startup.

**Why this setting:**
- **Celery 6 deprecation:** Celery 6 warns if this setting is not explicitly configured
- **Default behavior:** When `True`, the worker retries connecting to Redis broker on startup if the initial connection fails
- **Recommended for production:** Prevents worker crash if Redis is temporarily unavailable during startup
- **Location:** Configured in `app/channel_manager/core/sync_engine.py` (celery_app.conf)

**Expected behavior:**
- Worker startup logs no longer show deprecation warning about `broker_connection_retry_on_startup`
- If Redis is down during worker startup, worker retries connection instead of crashing
- Consistent with existing retry behavior for broker connection during runtime

**Worker Runs as Non-Root User:**

Both pms-backend and pms-worker-v2 containers run as a non-root user (`app` with UID 1000) for improved security and to silence Celery's SecurityWarning.

**Why non-root:**
- **Security best practice:** Reduces attack surface if container is compromised
- **Celery warning:** Celery emits a SecurityWarning when running as root
- **Production hygiene:** Follows principle of least privilege
- **Container standards:** Aligns with Docker/Kubernetes security best practices

**Implementation:**
- Dockerfile creates user `app` (UID 1000) during build
- All application files owned by `app:app`
- Container switches to `USER app` before running commands
- Both backend (uvicorn) and worker (celery) run as this user

**Verification:**

Check which user is running the worker process:

```bash
# Inside worker container
docker exec pms-worker-v2 ps aux

# Expected output:
# USER       PID  %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
# app          1   0.0  0.1 123456 12345 ?        Ss   12:00   0:00 celery -A app.channel_manager...
# app         XX   0.0  0.1 123456 12345 ?        S    12:00   0:00 [celery worker process]
```

Check worker startup logs for absence of SecurityWarning:

```bash
# Check logs for security warning (should be empty)
docker logs pms-worker-v2 2>&1 | grep -i "securitywarning\|running as root"

# Expected: No output (warning is gone)
```

**Files:**
- `backend/Dockerfile`: Defines non-root user and sets USER directive
- Image build: Coolify auto-detects Dockerfile and uses it instead of Nixpacks

**Note:** If you see permission errors after deployment, check that `/app` and `/app/logs` are owned by `app:app` (UID 1000).

---

## Celery Worker (pms-worker-v2): Start, Verify, Troubleshoot

**Purpose:** Reference guide for verifying worker startup, CMD/entrypoint configuration, and diagnosing build/cache drift issues (SOURCE_COMMIT mismatch).

---

### 1. Current Stable Setup (pms-worker-v2)

**Production Container Topology:**

```
Containers:
- pms-backend        (FastAPI application)
- pms-worker-v2      (Celery worker - headless background processor)
- coolify-redis      (Redis broker for Celery task queue)
- supabase-db-<id>   (PostgreSQL database)
```

**Network Configuration:**

```
pms-backend:
  - coolify                          (default Coolify network)
  - bccg4gs4o4kgsowocw08wkw4          (Supabase network - for supabase-db DNS)

pms-worker-v2:
  - coolify                          (default Coolify network)
  - bccg4gs4o4kgsowocw08wkw4          (Supabase network - for supabase-db DNS)

coolify-redis:
  - coolify                          (default Coolify network)
```

**Why both containers need Supabase network:**
- Resolves `supabase-db` hostname for DATABASE_URL
- Without it: `socket.gaierror: [Errno -3] Temporary failure in name resolution`

**Health Evidence:**

Endpoint: `GET /health/ready`

Expected response when worker is operational:
```json
{
  "status": "ready",
  "checks": {
    "database": "up",
    "redis": "up",
    "celery": "up"
  },
  "celery_workers": [
    "celery@pms-worker-v2-<hostname>"
  ]
}
```

**Key indicators:**
- `celery: "up"` → Redis connection OK
- `celery_workers` array not empty → At least one worker is registered and responding to ping

---

### 2. Worker Startup CMD / Entrypoint (from CMD Issue Diagnose)

**Container Boot Path:**

The `pms-worker-v2` container is started via the repository script (not direct Celery command):

**Path in container:** `/app/scripts/ops/start_worker.sh`

**Purpose:**
- Stable Celery startup with configurable pool/concurrency/loglevel
- Preflight dependency checks (wait-for-deps: database + Redis DNS/TCP reachable)
- Environment-driven configuration (no hardcoded values)

**Effective Celery Command:**

The script executes:
```bash
celery -A app.channel_manager.core.sync_engine:celery_app worker \
  --pool=${CELERY_POOL:-threads} \
  --concurrency=${CELERY_CONCURRENCY:-4} \
  --loglevel=${CELERY_LOGLEVEL:-INFO}
```

**Environment Variable Defaults:**
```bash
CELERY_POOL=threads           # Worker pool type (threads, prefork, gevent)
CELERY_CONCURRENCY=4          # Number of worker threads/processes
CELERY_LOGLEVEL=INFO          # Log verbosity (DEBUG, INFO, WARNING, ERROR)
```

**Optional environment overrides:**
```bash
CELERY_MAX_TASKS_PER_CHILD=1000   # Max tasks before worker restart (memory leak prevention)
CELERY_TIME_LIMIT=300             # Hard time limit for tasks (seconds)
```

**Coolify Build Pack Quirk:**

When using **Dockerfile** build pack (recommended for pms-worker-v2):
- Coolify UI does **NOT** allow setting "Start Command" (field disabled/ignored)
- Worker boots via `CMD` defined in `backend/Dockerfile.worker`:
  ```dockerfile
  CMD ["bash", "/app/scripts/ops/start_worker.sh"]
  ```
- **Workaround:** All customization happens via environment variables (not Start Command override)

**Coolify "Ports Exposes" Requirement:**

Even though pms-worker-v2 does NOT serve HTTP traffic, Coolify UI may require a port value:
- **Location:** Application Settings → Network → Ports Exposes
- **Set:** `8000` (placeholder - not actually used)
- **Why:** Coolify beta UI validation bug (worker is headless, no real port needed)

---

### 3. Verification Checklist (Copy/Paste Commands)

**HOST-SERVER-TERMINAL:**

```bash
# Check all PMS containers and their networks
docker ps --format 'table {{.Names}}\t{{.Status}}\t{{.Networks}}' | egrep -i 'pms|redis|supabase' || true

# Expected output:
# NAMES              STATUS          NETWORKS
# pms-backend        Up X hours      coolify, bccg4gs4o4kgsowocw08wkw4
# pms-worker-v2      Up X hours      coolify, bccg4gs4o4kgsowocw08wkw4
# coolify-redis      Up X hours      coolify
# supabase-db-<id>   Up X hours      bccg4gs4o4kgsowocw08wkw4
```

**Coolify TERMINAL (Container: pms-backend):**

```bash
# Check API health and worker registration
curl -k -sS https://api.fewo.kolibri-visions.de/health/ready | sed -n '1,200p'

# Expected JSON:
# {
#   "status": "ready",
#   "checks": { "database": "up", "redis": "up", "celery": "up" },
#   "celery_workers": ["celery@pms-worker-v2-..."]
# }
```

**Coolify TERMINAL (Container: pms-worker-v2) - Optional Verification:**

```bash
# Verify start script exists
ls -la /app/scripts/ops/start_worker.sh

# Expected:
# -rwxr-xr-x 1 app app <size> <date> /app/scripts/ops/start_worker.sh
# (executable bit set, owned by app:app)

# Inspect start script content
head -n 40 /app/scripts/ops/start_worker.sh

# Expected to see:
# - #!/usr/bin/env bash
# - wait-for-deps preflight checks (DNS + TCP for database/Redis)
# - celery -A app.channel_manager.core.sync_engine:celery_app worker ...
```

**Quick Smoke Test:**

```bash
# Test Celery worker responds to ping
docker exec pms-worker-v2 celery -A app.channel_manager.core.sync_engine:celery_app inspect ping

# Expected:
# -> celery@pms-worker-v2-<hostname>: {'ok': 'pong'}
```

---

### 4. Critical Failure Mode: SOURCE_COMMIT Correct but Worker Runs Old Code (Build/Cache Drift)

**Symptoms:**

1. **Worker cannot update channel_sync_logs:**
   - Sync logs stuck in "triggered" status (never transition to "running" → "success/failed")
   - Worker logs show errors about missing columns/tables that SHOULD exist in new schema

2. **Worker errors look "wrong" vs expected commit:**
   - Code references functions/modules that were refactored/removed in latest commit
   - Import errors for recently added modules

3. **SOURCE_COMMIT env says new SHA but code files differ:**
   - Environment variable shows latest commit hash
   - Actual Python files in `/app/` directory contain old code

**Root Cause:**

Coolify built the worker image from an **outdated checkout or stale build cache**:
- Git source directory under `/data/coolify/source` not updated to origin/main
- Build cache from previous deployment used despite new SOURCE_COMMIT
- Image layering cached old dependencies/code even though new commit was deployed

**Diagnosis (Copy/Paste Commands):**

**Coolify TERMINAL (Container: pms-worker-v2):**

```bash
# Check SOURCE_COMMIT environment variable
echo "SOURCE_COMMIT=$SOURCE_COMMIT"

# Expected: Latest commit SHA from origin/main (e.g., 733038a...)
```

```bash
# Verify actual code file hash (example: sync_engine.py)
python -c 'import pathlib,hashlib; p=pathlib.Path("/app/app/channel_manager/core/sync_engine.py"); print(p, p.exists(), hashlib.sha256(p.read_bytes()).hexdigest() if p.exists() else "missing")'

# Expected: File exists + SHA256 hash
# Compare this hash with the file at SOURCE_COMMIT in GitHub repo
# If hashes differ → build/cache drift confirmed
```

```bash
# Inspect start script content
head -n 40 /app/scripts/ops/start_worker.sh

# Expected: Script content matches repo at SOURCE_COMMIT
# If script is missing/outdated → build/cache drift confirmed
```

**Additional Verification:**

```bash
# Check git status inside container (if .git exists)
docker exec pms-worker-v2 git rev-parse HEAD 2>/dev/null || echo ".git not in image"

# If .git exists: compare with SOURCE_COMMIT env
# If they differ OR .git missing → verify via file hash instead
```

**Fix Guidance (High-Level):**

1. **Force Clean Rebuild in Coolify:**
   - Coolify Dashboard → pms-worker-v2 → Deployment Settings
   - Enable: **"Disable build cache"** (forces fresh build)
   - Trigger new deployment

2. **Pin to Specific Commit SHA (instead of branch reference):**
   - Coolify Dashboard → pms-worker-v2 → General → Git
   - Change branch from `main` to specific commit SHA (e.g., `733038a`)
   - This prevents Coolify from using stale branch pointer

3. **Ensure Source Directory is Updated:**
   - **If using host checkout:** Coolify may clone repo to `/data/coolify/source/<app-id>`
   - SSH to host server and verify:
     ```bash
     cd /data/coolify/source/<app-id>
     git fetch origin
     git log --oneline -5  # Should show latest commits
     ```
   - If outdated: `git pull origin main` and redeploy

4. **Verify Base Directory is Correct:**
   - Coolify Dashboard → pms-worker-v2 → General → Base Directory
   - Must be: `/backend` (NOT `/` or `/backend/app`)
   - Dockerfile.worker path: `Dockerfile.worker` (relative to base directory)

**Prevention:**

- Always deploy pms-backend FIRST, then pms-worker-v2 (sequential, not parallel)
- Use commit SHA instead of branch name for critical deployments
- Enable "Disable build cache" after major refactors
- Verify `/health/ready` shows worker registration after each deploy

---

### 5. Decommission Old Worker (if it ever exists again)

**"Done" Criteria:**

Only `pms-worker-v2` exists in production:
```bash
docker ps | egrep -i 'pms-worker'

# Expected output:
# pms-worker-v2   Up X hours   ...

# NOT expected:
# pms-worker      Up X hours   ...  (old worker should NOT exist)
# pms-worker-v2   Up X hours   ...
```

**Why multiple workers are dangerous:**

- **Task distribution is non-deterministic:** Redis/Celery load-balance tasks across ALL registered workers
- **Version skew:** If old worker runs outdated code, some tasks execute with wrong logic
- **Silent failures:** HTTP 201 from POST /availability/sync but task runs on wrong worker → logs stuck "triggered"
- **Diagnosis confusion:** Half of tasks succeed (new worker), half fail (old worker) → intermittent issues

**Decommission Steps:**

1. **Identify old worker container:**
   ```bash
   docker ps -a | grep pms-worker | grep -v v2
   ```

2. **Stop and remove old worker:**
   ```bash
   docker stop pms-worker
   docker rm pms-worker
   ```

3. **Disable old worker service in Coolify:**
   - Coolify Dashboard → Applications
   - Find `pms-worker` (without v2 suffix)
   - Settings → Danger Zone → Delete Application

4. **Verify only pms-worker-v2 remains:**
   ```bash
   curl -k -sS https://api.fewo.kolibri-visions.de/health/ready | jq '.celery_workers'

   # Expected:
   # ["celery@pms-worker-v2-<hostname>"]
   # (only ONE worker, name includes "v2")
   ```

**Migration from pms-worker → pms-worker-v2:**

If you need to migrate (upgrade old worker to new version):
1. Create new `pms-worker-v2` app in Coolify (following setup steps in this runbook)
2. Deploy pms-worker-v2 and verify `/health/ready` shows it registered
3. Stop old `pms-worker` container (verify tasks still processing via v2)
4. Monitor for 24-48 hours (ensure no issues)
5. Delete old `pms-worker` app from Coolify permanently

---

## Deployment Process

**Purpose:** Safe sequential deployment of pms-backend and pms-worker to avoid race conditions.

### Critical Requirement: Sequential Deployment

**⚠️ NEVER deploy pms-backend and pms-worker in parallel!**

**Why Sequential?**
1. Backend and worker must be on the **same git commit**
2. Code changes may affect both HTTP endpoints and Celery tasks
3. Deploying in parallel → version mismatch → task failures

**Correct Order:**
```
1. Deploy pms-backend (wait for completion)
2. Deploy pms-worker (after backend is stable)
```

---

### Deployment Workflow

#### Step 1: Pre-Deployment Checks

**Location:** Coolify Dashboard

**Verify:**
- ✅ All tests pass in CI/CD (if configured)
- ✅ Database migrations ready (if schema changes)
- ✅ No active incidents or alerts
- ✅ Current deployments are stable (check `/health/ready`)

**Health Check:**
```bash
curl https://api.your-domain.com/health/ready | jq .

# Expected:
# {
#   "status": "ready",
#   "checks": {
#     "database": "up",
#     "redis": "up",
#     "celery": "up"
#   }
# }
```

#### Step 2: Deploy pms-backend

**Location:** Coolify Dashboard → pms-backend

1. Click **Deploy** button
2. Monitor deployment logs for errors
3. Wait for "Deployment successful" message
4. **DO NOT proceed to worker deployment yet**

**Verify Backend Deployment:**
```bash
# Check health endpoint
curl https://api.your-domain.com/health | jq .

# Check API version (if versioned)
curl https://api.your-domain.com/docs | grep version

# Verify git commit
docker exec pms-backend git rev-parse HEAD
```

#### Step 3: Verify Backend Stability

**Wait Time:** 2-3 minutes minimum

**Health Checks:**
```bash
# Check /health/ready multiple times
for i in {1..5}; do
  echo "Check $i:"
  curl -s https://api.your-domain.com/health/ready | jq '.status'
  sleep 10
done

# Expected: All checks return "ready"
```

**Check Logs:**
```bash
# Look for errors in backend logs
docker logs pms-backend --tail 100 | grep -i error

# No critical errors should appear
```

#### Step 4: Run Database Migrations (If Needed)

**If schema changes in this deployment:**

```bash
# SSH to host
ssh root@your-host

# Run migrations (adjust path as needed)
docker exec -it pms-backend python -m alembic upgrade head

# OR for Supabase migrations:
cd supabase/migrations
docker exec $(docker ps -q -f name=supabase) supabase migration up
```

**Verify Migration:**
```bash
# Check if channel_sync_logs table exists
docker exec $(docker ps -q -f name=supabase) psql -U postgres -d postgres \
  -c "\dt channel_sync_logs"
```

#### Step 5: Deploy pms-worker

**⚠️ Only proceed if backend is stable!**

**Location:** Coolify Dashboard → pms-worker

1. Click **Deploy** button
2. Monitor deployment logs
3. Wait for "Deployment successful"

**Verify Worker Deployment:**
```bash
# Check worker logs for startup
docker logs pms-worker --tail 50 | grep "ready"

# Expected:
# [INFO] celery@pms-worker ready.
# [INFO] Tasks: [list of registered tasks]
```

#### Step 6: Post-Deployment Verification

**Check Health Endpoint:**
```bash
curl https://api.your-domain.com/health/ready | jq .
```

**Expected Output:**
```json
{
  "status": "ready",
  "checks": {
    "database": "up",
    "redis": "up",
    "celery": "up"
  },
  "celery_workers": [
    "celery@pms-worker-abc123"
  ]
}
```

**Test Celery Connection:**
```bash
# From backend container
docker exec pms-backend \
  celery -A app.channel_manager.core.sync_engine:celery_app \
  --broker "$CELERY_BROKER_URL" inspect ping -t 3

# Expected: -> celery@pms-worker-abc123: {'ok': 'pong'}
```

**Verify Git Commits Match:**
```bash
# Get backend commit
BACKEND_COMMIT=$(docker exec pms-backend git rev-parse HEAD)
echo "Backend: $BACKEND_COMMIT"

# Get worker commit
WORKER_COMMIT=$(docker exec pms-worker git rev-parse HEAD)
echo "Worker:  $WORKER_COMMIT"

# They should match!
if [ "$BACKEND_COMMIT" = "$WORKER_COMMIT" ]; then
  echo "✓ Commits match - deployment consistent"
else
  echo "✗ COMMIT MISMATCH - redeploy worker!"
fi
```

#### Step 7: Smoke Test

**Run Quick Smoke Test:**
```bash
# Load environment
source /root/pms_env.sh

# Get JWT token
TOKEN="$(curl -sS "$SB_URL/auth/v1/token?grant_type=password" \
  -H "apikey: $ANON_KEY" \
  -H "Content-Type: application/json" \
  -d "{\"email\":\"$EMAIL\",\"password\":\"$PASSWORD\"}" \
  | python3 -c 'import sys,json; print(json.load(sys.stdin).get("access_token",""))')"

# Trigger test sync
CID="$(curl -k -sS -L "https://api.your-domain.com/api/v1/channel-connections/" \
  -H "Authorization: Bearer $TOKEN" \
  | python3 -c 'import sys,json; d=json.load(sys.stdin); print(d[0]["id"] if d else "")')"

curl -X POST "https://api.your-domain.com/api/v1/channel-connections/$CID/sync" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"sync_type":"availability"}'

# Check sync logs
sleep 5
curl "https://api.your-domain.com/api/v1/channel-connections/$CID/sync-logs?limit=5" \
  -H "Authorization: Bearer $TOKEN" | jq '.logs[0]'
```

---

### Coolify Auto-Deploy Configuration

**⚠️ DO NOT enable auto-deploy for both apps simultaneously!**

**Safe Configuration:**

**Option 1: Manual Deployment (Recommended)**
- pms-backend: Auto-deploy **OFF**
- pms-worker: Auto-deploy **OFF**
- Deploy manually via dashboard in correct sequence

**Option 2: Backend Auto-Deploy Only**
- pms-backend: Auto-deploy **ON** (trigger: push to `main`)
- pms-worker: Auto-deploy **OFF** (manual only)
- After backend auto-deploys, manually deploy worker

**Option 3: Deployment Script (Advanced)**

Create deployment script that enforces sequence:

```bash
#!/bin/bash
# deploy-pms.sh

set -e  # Exit on error

echo "Step 1: Deploying pms-backend..."
# Trigger backend deployment via Coolify API
curl -X POST https://coolify.example.com/api/deploy/pms-backend \
  -H "Authorization: Bearer $COOLIFY_API_TOKEN"

echo "Waiting for backend to stabilize (2 minutes)..."
sleep 120

echo "Step 2: Verifying backend health..."
STATUS=$(curl -s https://api.your-domain.com/health/ready | jq -r '.status')
if [ "$STATUS" != "ready" ]; then
  echo "✗ Backend health check failed - aborting worker deployment"
  exit 1
fi

echo "Step 3: Deploying pms-worker..."
curl -X POST https://coolify.example.com/api/deploy/pms-worker \
  -H "Authorization: Bearer $COOLIFY_API_TOKEN"

echo "Waiting for worker to start..."
sleep 60

echo "Step 4: Verifying worker health..."
CELERY_STATUS=$(curl -s https://api.your-domain.com/health/ready | jq -r '.checks.celery')
if [ "$CELERY_STATUS" = "up" ]; then
  echo "✓ Deployment complete and healthy"
else
  echo "✗ Worker health check failed"
  exit 1
fi
```

**Trigger via GitHub Actions:**
```yaml
name: Deploy PMS
on:
  push:
    branches: [main]

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
      - name: Run sequential deployment
        run: |
          ssh root@your-host 'bash /root/deploy-pms.sh'
```

---

### Rollback Procedure

**If deployment fails:**

**Step 1: Identify Failed Component**
```bash
# Check which service is unhealthy
curl https://api.your-domain.com/health/ready | jq '.checks'
```

**Step 2: Rollback via Coolify**

**Location:** Coolify Dashboard → pms-backend (or pms-worker) → Deployments

1. Find previous successful deployment
2. Click **Redeploy** on that version
3. Wait for completion
4. Verify health

**Step 3: Verify Rollback Success**
```bash
# Check health
curl https://api.your-domain.com/health/ready

# Verify git commit matches expected version
docker exec pms-backend git rev-parse HEAD
```

---

### Common Deployment Issues

**Issue:** Worker shows old code after backend deployment

**Cause:** Worker not redeployed
**Solution:** Deploy worker manually

**Issue:** Celery tasks fail with "Task not registered"

**Cause:** Backend/worker version mismatch
**Solution:** Redeploy worker to match backend commit

**Issue:** Database migration fails

**Cause:** Schema conflict or missing dependency
**Solution:** Rollback deployment, fix migration, redeploy

**Issue:** pms-admin (frontend) build fails in Coolify/Nixpacks

**Symptom:**
- Coolify deployment shows `npm run build` failing with webpack/TypeScript compile error
- Build log shows: "the name `X` is defined multiple times" (e.g., `copyToClipboard`)
- Error import trace points to Next.js page file (e.g., `./app/channel-sync/page.tsx`)
- Build exits with code 1

**Common Causes:**
1. **Duplicate function declarations** in a single file (TypeScript/webpack cannot resolve)
   - Example: `const copyToClipboard = ...` defined twice in same component
   - Often happens when merging features or copy-pasting helper functions
2. **Import conflicts** or circular dependencies
3. **Type errors** that fail strict TypeScript compilation

**Solution:**
1. **Check build logs** in Coolify → pms-admin → Deployments → Build Logs
2. **Identify duplicate declaration:**
   ```bash
   # Search for duplicate in the file mentioned in error trace
   grep -n "const copyToClipboard" frontend/app/channel-sync/page.tsx
   # Expected: Should show only ONE definition
   ```
3. **Fix locally:**
   - Open the problematic file
   - Find all instances of the duplicate function/const
   - Keep ONE definition (preferably the most robust one with fallbacks)
   - Update all call sites to use consistent signature
   - Remove the duplicate(s)
4. **Commit and push:**
   ```bash
   git add frontend/app/channel-sync/page.tsx
   git commit -m "admin-ui: fix duplicate function declaration"
   git push origin main
   ```
5. **Redeploy** via Coolify → pms-admin → Redeploy
6. **Verify build succeeds** (watch build logs until "Build successful")

**Note:** `$NIXPACKS_PATH` UndefinedVar warnings in build logs are usually **non-fatal**. Build failures are typically due to TypeScript/webpack compile errors (duplicate declarations, type mismatches, import errors).

**Prevention:**
- Before adding helper functions, search the file to check if they already exist
- Use consistent naming conventions to avoid collisions
- Run `npm run build` locally before pushing (if possible)

**Issue:** Frontend build fails with JSX syntax error (unclosed tag)

**Symptom:**
- Coolify deployment shows `npm run build` failing with error: "Unexpected token `div`. Expected jsx identifier"
- Error trace points to a line with valid-looking JSX (e.g., `<div className="...">`)
- TypeScript compiler (`npx tsc --noEmit`) shows "JSX element 'div' has no corresponding closing tag"
- Actual root cause is often LATER in the file (cascading error from missing closing tag)

**Common Causes:**
1. **Missing closing tag** in JSX block (e.g., `<div>...</div>` → missing `</div>`)
2. **Unclosed ternary expression** inside JSX (e.g., `{condition ? <div>...</div> : <div>...</div>}` missing closing tag)
3. **Brace mismatch** in nested JSX blocks

**Solution:**
1. **Run TypeScript compiler locally** to get full error list:
   ```bash
   cd frontend
   npx tsc --noEmit --jsx preserve app/connections/page.tsx
   ```
2. **Identify root cause:** Look for FIRST error (e.g., "JSX element 'div' has no corresponding closing tag")
3. **Check surrounding JSX structure:** Trace opening tags to their closing tags
4. **Common fix:** Add missing `</div>` after ternary expressions or conditional blocks
5. **Re-run build** to verify: `npm run build`
6. **Commit and push:**
   ```bash
   git add frontend/app/connections/page.tsx
   git commit -m "fix: missing closing div tag in batch operations error display"
   git push origin main
   ```

**Example Fix (Real Case):**
```tsx
{/* BEFORE (BROKEN) */}
<div className="text-xs mt-1">
  {op.error ? <div>...</div> : <div>...</div>}
</div>  {/* This closes PARENT div, not "text-xs mt-1" div! */}

{/* AFTER (FIXED) */}
<div className="text-xs mt-1">
  {op.error ? <div>...</div> : <div>...</div>}
</div>  {/* Closes "text-xs mt-1" div */}
</div>  {/* Closes parent div */}
```

**Prevention:**
- Use editor with JSX tag matching/highlighting (VS Code, WebStorm)
- Run `npm run build` locally before pushing
- Watch for cascading errors in TypeScript output (first error is usually root cause)

---

### TLS: Admin Subdomain Shows TRAEFIK DEFAULT CERT (Let's Encrypt Missing)

**Purpose:** Diagnose and fix admin.fewo.kolibri-visions.de (or other subdomains) serving Traefik's default certificate instead of a valid Let's Encrypt certificate.

#### Symptoms

**Browser:**
- Certificate warning: "Your connection is not private" or "Invalid certificate"
- Certificate issuer shown as "TRAEFIK DEFAULT CERT" instead of "Let's Encrypt"

**CLI Verification (HOST-SERVER-TERMINAL):**
```bash
# Check certificate for admin subdomain
echo | openssl s_client -connect admin.fewo.kolibri-visions.de:443 \
  -servername admin.fewo.kolibri-visions.de 2>/dev/null \
  | openssl x509 -noout -subject -issuer -dates

# Output with TRAEFIK DEFAULT CERT:
# subject=CN = TRAEFIK DEFAULT CERT
# issuer=CN = TRAEFIK DEFAULT CERT
# notBefore=...
# notAfter=...
```

**Compare to Working Host (api.fewo.kolibri-visions.de):**
```bash
echo | openssl s_client -connect api.fewo.kolibri-visions.de:443 \
  -servername api.fewo.kolibri-visions.de 2>/dev/null \
  | openssl x509 -noout -subject -issuer -dates

# Expected output with Let's Encrypt:
# subject=CN = api.fewo.kolibri-visions.de
# issuer=C = US, O = Let's Encrypt, CN = R13
# notBefore=...
# notAfter=...
```

#### Root Causes

**A) Router Rule Parse Error → Router Not Loaded → Default Cert**

Traefik fails to parse the router rule, so the router is never registered, and Traefik falls back to the default certificate.

Common Traefik log errors:
```
level=error msg="Router rule parse error: expected operand, found '/'"
level=error msg="PathPrefix: path does not start with a '/'"
level=error msg="invalid rule syntax"
```

**Typical Mistakes:**
- Using hostname in PathPrefix: `PathPrefix(admin.fewo.kolibri-visions.de)` ❌
- Missing backticks: `PathPrefix(/)` ❌
- Correct syntax: `PathPrefix(\`/\`)` ✅ (with backticks)
- Best practice: Use `Host(\`admin.fewo...\`)` alone, no PathPrefix needed for single-domain apps

**B) Missing Certificate Resolver for Router**

Router has `tls=true` but no `tls.certresolver` label, so Traefik does not request/attach a Let's Encrypt certificate for this hostname and falls back to the default certificate.

#### Diagnosis Steps

**1. Check Current Certificate (HOST-SERVER-TERMINAL):**
```bash
echo | openssl s_client -connect admin.fewo.kolibri-visions.de:443 \
  -servername admin.fewo.kolibri-visions.de 2>/dev/null \
  | openssl x509 -noout -subject -issuer

# If subject/issuer = "TRAEFIK DEFAULT CERT": Problem confirmed
```

**2. Confirm Traefik Proxy Container and Cert Resolver Name (HOST-SERVER-TERMINAL):**
```bash
# List Traefik/proxy containers
docker ps --format 'table {{.Names}}\t{{.Image}}' | egrep -i 'proxy|traefik'
# Expected: coolify-proxy (or similar)

# Inspect Traefik args to find cert resolver name
docker inspect coolify-proxy --format '{{range .Args}}{{println .}}{{end}}' \
  | egrep -i 'certificatesresolvers|acme' | head -n 20

# Look for lines like:
# --certificatesresolvers.letsencrypt.acme.email=...
# --certificatesresolvers.letsencrypt.acme.storage=...
# Resolver name is "letsencrypt" in this example
```

**3. Inspect pms-admin Container Labels and Router Rules (HOST-SERVER-TERMINAL):**
```bash
# View Traefik labels on pms-admin
docker inspect pms-admin --format 'Labels={{json .Config.Labels}}' \
  | python3 -m json.tool | grep -A 2 -B 2 traefik | head -n 100

# Check for:
# - traefik.http.routers.pmsadmin-https.rule: Should be Host(`admin.fewo...`)
# - traefik.http.routers.pmsadmin-https.tls: Should be "true"
# - traefik.http.routers.pmsadmin-https.tls.certresolver: Should be "letsencrypt" (or resolver name from step 2)

# Common issues:
# - Rule syntax error: PathPrefix(/) instead of PathPrefix(`/`)
# - Missing tls.certresolver label
```

**4. Check Traefik Logs for Errors (HOST-SERVER-TERMINAL):**
```bash
# Grep Traefik logs for admin subdomain and errors
docker logs --since 48h coolify-proxy 2>&1 \
  | egrep -i 'admin\.fewo\.kolibri-visions\.de|acme|letsencrypt|certificate|tls|router|rule|error' \
  | tail -n 100

# Look for:
# - Rule parsing errors: "expected operand", "PathPrefix: path does not start with a '/'"
# - ACME errors: "unable to obtain certificate", "DNS challenge failed"
# - Router registration: "Creating router pmsadmin-https" (should appear if rule is valid)
```

#### Fix Steps (Coolify UI)

**1. Fix Invalid Router Rules:**

Location: Coolify Dashboard → pms-admin → Domains

- **If using Host-only rule (recommended for single-domain apps):**
  - Rule: `Host(\`admin.fewo.kolibri-visions.de\`)`
  - No PathPrefix needed

- **If PathPrefix is needed (e.g., for path-based routing):**
  - Correct: `Host(\`admin.fewo...\`) && PathPrefix(\`/\`)`
  - Incorrect: `PathPrefix(/)` (missing backticks)
  - Incorrect: `PathPrefix(admin.fewo...)` (hostname in PathPrefix)

**2. Explicitly Set Certificate Resolver on HTTPS Router:**

Location: Coolify Dashboard → pms-admin → Environment Variables or Labels

Add container label (or verify it exists):
```
traefik.http.routers.pmsadmin-https.tls.certresolver=letsencrypt
```

**Important:** Resolver name (`letsencrypt`) must match the name from step 2 diagnosis. Common names: `letsencrypt`, `le`, `default`.

**3. Redeploy pms-admin (NOT Just Restart):**

Location: Coolify Dashboard → pms-admin → Deployments

- Action: Click "Redeploy"
- Why: Container labels are set during build/deploy; restart does not update labels
- Note: Domain changes also require redeploy, not just restart

**4. Wait for Certificate Issuance (30-60 seconds):**

- Traefik will automatically request Let's Encrypt certificate for the hostname
- Check Traefik logs for ACME progress:
  ```bash
  docker logs --since 5m coolify-proxy 2>&1 | grep -i acme
  ```
- Look for: "Certificate obtained for domain admin.fewo..."

#### Verification

**1. Check Certificate via OpenSSL (HOST-SERVER-TERMINAL):**
```bash
echo | openssl s_client -connect admin.fewo.kolibri-visions.de:443 \
  -servername admin.fewo.kolibri-visions.de 2>/dev/null \
  | openssl x509 -noout -subject -issuer -dates

# Expected output with Let's Encrypt:
# subject=CN = admin.fewo.kolibri-visions.de
# issuer=C = US, O = Let's Encrypt, CN = R13
# notBefore=Dec 29 12:00:00 2025 GMT
# notAfter=Mar 29 12:00:00 2026 GMT  (90 days validity)
```

**2. Check Browser:**
- Open https://admin.fewo.kolibri-visions.de
- Click lock icon → Certificate → Verify issuer is "Let's Encrypt"
- No certificate warnings

**3. Optional: Verify Traefik Logs Show No Rule Errors (HOST-SERVER-TERMINAL):**
```bash
docker logs --since 10m coolify-proxy 2>&1 \
  | grep -i admin.fewo | grep -i error

# Expected: No output (no errors for admin.fewo router)
```

#### Related Issues

- If certificate still shows TRAEFIK DEFAULT CERT after fix:
  - Check DNS: `nslookup admin.fewo.kolibri-visions.de` (must resolve to server IP)
  - Check firewall: Port 443 must be open
  - Check Traefik ACME logs for challenge failures
- If router rule still invalid:
  - Review Traefik documentation: https://doc.traefik.io/traefik/routing/routers/
  - Test rule syntax in isolation before deploying

### Public Website SSL Shows TRAEFIK DEFAULT CERT (certresolver Label Typo)

**Purpose:** Diagnose and fix fewo.kolibri-visions.de (public website) serving Traefik's default self-signed certificate instead of a valid Let's Encrypt certificate, specifically when caused by malformed Docker label keys (e.g., trailing whitespace).

#### Symptoms

**Browser:**
- Certificate warning: "Your connection is not private" / "NET::ERR_CERT_AUTHORITY_INVALID"
- Certificate shown as self-signed with subject/issuer: `CN=TRAEFIK DEFAULT CERT`
- Site content loads (200 OK) but SSL verification fails

**CLI Verification (HOST-SERVER-TERMINAL):**
```bash
# Check certificate for public website
openssl s_client -servername fewo.kolibri-visions.de \
  -connect fewo.kolibri-visions.de:443 </dev/null 2>/dev/null \
  | openssl x509 -noout -subject -issuer -dates

# Output with TRAEFIK DEFAULT CERT (PROBLEM):
# subject=CN = TRAEFIK DEFAULT CERT
# issuer=CN = TRAEFIK DEFAULT CERT
# notBefore=Jan  1 00:00:00 2024 GMT
# notAfter=Jan  1 00:00:00 2025 GMT

# After fix (Let's Encrypt):
# subject=CN = fewo.kolibri-visions.de
# issuer=C = US, O = Let's Encrypt, CN = R13
# notBefore=Jan 12 21:00:00 2026 GMT
# notAfter=Apr 12 21:00:00 2026 GMT

# Quick test: curl without -k should work after fix
curl -I https://fewo.kolibri-visions.de
# Expected: HTTP/2 200 (no certificate errors)
```

#### Root Cause: Malformed certresolver Label Key

**Most Common:** Docker label key has trailing whitespace or typo, preventing Traefik from parsing the `certresolver` directive.

**Example of BAD label (trailing space before `=`):**
```
traefik.http.routers.public-website-https.tls.certresolver =letsencrypt
                                                            ↑
                                                     (space here)
```

**Correct label (no whitespace):**
```
traefik.http.routers.public-website-https.tls.certresolver=letsencrypt
```

**Traefik Behavior:**
- Traefik v3 logs: `"field not found, node: certresolver"` when it cannot parse the label
- Router is created with `tls=true` but no cert resolver attached
- Traefik serves its built-in default self-signed certificate

#### Diagnosis Steps

**1. Check Current Certificate (HOST-SERVER-TERMINAL):**
```bash
openssl s_client -servername fewo.kolibri-visions.de \
  -connect fewo.kolibri-visions.de:443 </dev/null 2>/dev/null \
  | openssl x509 -noout -subject -issuer

# If output shows "TRAEFIK DEFAULT CERT": Problem confirmed
```

**2. Check Traefik Logs for certresolver Parsing Errors (HOST-SERVER-TERMINAL):**
```bash
# Check recent Traefik logs for certresolver/ACME errors
docker logs --since 15m coolify-proxy 2>&1 \
  | grep -E "certresolver|acme|letsencrypt|error|field not found"

# Look for:
# - "field not found, node: certresolver" → Label key typo/whitespace
# - "unable to obtain ACME certificate" → DNS or challenge failure
# - "Creating router public-website-https" → Router loaded successfully
```

**3. Inspect Docker Labels for Trailing Whitespace (HOST-SERVER-TERMINAL):**
```bash
# List container for public website (adjust name if needed)
docker ps --format 'table {{.Names}}\t{{.Image}}' | grep -i public

# Inspect labels and show non-printable characters
docker inspect pms-public-website --format '{{json .Config.Labels}}' \
  | python3 -m json.tool | grep -E 'certresolver|tls' | cat -A

# Look for:
# - Trailing whitespace shown as spaces before $ (line end marker)
# - Correct: "...certresolver": "letsencrypt"$
# - Wrong:   "...certresolver ": "letsencrypt"$ (space before closing quote)
# - Wrong:   "... certresolver": "letsencrypt"$ (space before colon)

# Alternative: directly inspect labels array
docker inspect pms-public-website \
  | jq -r '.[0].Config.Labels | to_entries[] | select(.key | contains("certresolver")) | "\(.key) = \(.value)"'

# Check for whitespace in key name
```

**4. Verify Traefik Certificate Resolver Name (HOST-SERVER-TERMINAL):**
```bash
# Confirm cert resolver name configured in Traefik
docker inspect coolify-proxy --format '{{range .Args}}{{println .}}{{end}}' \
  | grep certificatesresolvers | head -5

# Look for:
# --certificatesresolvers.letsencrypt.acme.email=...
# --certificatesresolvers.letsencrypt.acme.storage=...
# Resolver name is "letsencrypt" in this example
```

#### Fix Steps (Coolify UI or Docker Compose)

**Option A: Fix via Coolify UI (Recommended)**

1. Navigate to: Coolify Dashboard → pms-public-website → Configuration → Labels
2. Locate the certresolver label:
   - Key: `traefik.http.routers.public-website-https.tls.certresolver`
   - Value: `letsencrypt`
3. **Check for trailing spaces in key:**
   - Delete the label entry
   - Re-add with exact key (copy-paste from here to avoid typos):
     ```
     traefik.http.routers.public-website-https.tls.certresolver
     ```
   - Set value: `letsencrypt` (or your resolver name from step 4 diagnosis)
4. **Important:** Click "Save" then "Redeploy" (NOT just restart)
   - Container labels only update on redeploy, not restart

**Option B: Fix via Docker Compose (if not using Coolify UI)**

Edit `docker-compose.yml` or service labels section:
```yaml
labels:
  # Ensure NO trailing whitespace in key names
  traefik.enable: "true"
  traefik.http.routers.public-website-https.rule: "Host(`fewo.kolibri-visions.de`)"
  traefik.http.routers.public-website-https.tls: "true"
  traefik.http.routers.public-website-https.tls.certresolver: "letsencrypt"  # ← NO space before colon or =
  traefik.http.routers.public-website-https.entrypoints: "websecure"
```

Redeploy:
```bash
docker compose up -d --force-recreate pms-public-website
```

**5. Wait for Let's Encrypt Certificate Issuance (30-90 seconds):**
```bash
# Monitor ACME logs
docker logs -f --since 2m coolify-proxy 2>&1 | grep -E "fewo\.kolibri-visions\.de|acme|certificate"

# Look for:
# - "Serving default certificate for request..."  → Still using default (wait)
# - "Certificate obtained for domain [fewo.kolibri-visions.de]" → Success!
```

#### Verification After Fix

**1. Check Certificate via OpenSSL (HOST-SERVER-TERMINAL):**
```bash
openssl s_client -servername fewo.kolibri-visions.de \
  -connect fewo.kolibri-visions.de:443 </dev/null 2>/dev/null \
  | openssl x509 -noout -subject -issuer -dates

# Expected output with Let's Encrypt:
# subject=CN = fewo.kolibri-visions.de
# issuer=C = US, O = Let's Encrypt, CN = R13  (or R10, R11, etc.)
# notBefore=Jan 12 21:00:00 2026 GMT
# notAfter=Apr 12 21:00:00 2026 GMT  (90 days validity)
```

**2. Test with curl (HOST-SERVER-TERMINAL):**
```bash
# curl WITHOUT -k flag should work
curl -I https://fewo.kolibri-visions.de

# Expected: HTTP/2 200 (no SSL errors)
# If you still need -k, certificate is not yet valid
```

**3. Browser Test:**
- Open https://fewo.kolibri-visions.de
- Click lock icon → View Certificate
- Verify:
  - Issued to: fewo.kolibri-visions.de
  - Issued by: Let's Encrypt (R13 or similar)
  - Valid dates: Current date within range
  - No security warnings

**4. Optional: Check Traefik Logs (HOST-SERVER-TERMINAL):**
```bash
# Verify no "field not found" errors for certresolver
docker logs --since 10m coolify-proxy 2>&1 \
  | grep -i "fewo.kolibri-visions.de" | grep -i error

# Expected: No output (no errors)
```

#### Common Issues

**Issue: www subdomain also shows default cert**

**Cause:** DNS record for www.fewo.kolibri-visions.de missing or no Traefik router for www.

**Solutions:**
1. **Add www DNS record** (if you want to serve www):
   - Create DNS A or CNAME record: `www.fewo.kolibri-visions.de` → same IP as apex domain
   - Update Traefik router rule to include www:
     ```
     traefik.http.routers.public-website-https.rule: "Host(`fewo.kolibri-visions.de`) || Host(`www.fewo.kolibri-visions.de`)"
     ```
   - Redeploy service
2. **Remove www from router rule** (if you don't want to serve www):
   - Keep rule as: `Host(\`fewo.kolibri-visions.de\`)`
   - www will return DNS error (expected)

**Issue: Certificate still shows TRAEFIK DEFAULT CERT after 5+ minutes**

**Checks:**
- DNS resolves correctly: `nslookup fewo.kolibri-visions.de` (must return server IP)
- Port 80 open for HTTP-01 challenge: `curl http://fewo.kolibri-visions.de/.well-known/acme-challenge/test`
- Traefik ACME storage writable: Check Traefik args for acme.storage path and permissions
- Check ACME logs for specific failure: `docker logs coolify-proxy 2>&1 | grep -A 5 "unable to obtain"`

**Issue: Typo reappears after redeploy from Coolify**

**Cause:** Coolify may store labels with whitespace if originally entered that way.

**Fix:**
- Delete label entirely in Coolify UI
- Re-add from scratch with clean copy-paste
- Or edit Coolify database/config directly (advanced)

#### Related Documentation

- Traefik v3 routers: https://doc.traefik.io/traefik/routing/routers/
- Let's Encrypt with Traefik: https://doc.traefik.io/traefik/https/acme/
- Docker label format: https://docs.docker.com/compose/compose-file/compose-file-v3/#labels

---

## Monitoring & Troubleshooting

**Purpose:** Monitor system health and troubleshoot Channel Manager issues.

### Health Endpoints

#### GET /health

**Purpose:** Basic application health check
**Auth:** None required
**Response:**
```json
{
  "status": "healthy"
}
```

#### GET /health/ready

**Purpose:** Comprehensive readiness check (database, Redis, Celery)
**Auth:** None required
**Response:**
```json
{
  "status": "ready",
  "checks": {
    "database": "up",
    "redis": "up",
    "celery": "up"
  },
  "celery_workers": [
    "celery@pms-worker-abc123"
  ]
}
```

**Possible Status Values:**
- `ready`: All checks passed
- `degraded`: Some checks failed

**Individual Check Status:**
- `up`: Component healthy
- `down`: Component unavailable

**Single-Worker Enforcement (Celery):**

When `ENABLE_CELERY_HEALTHCHECK=true`, the readiness check enforces the single-worker rule for Channel Manager:

- **Expected:** Exactly 1 active Celery worker (pms-worker-v2)
- **If Multiple Workers Detected:** `celery` component returns `status: "down"` with error message
- **Error Message Includes:**
  - Worker count (expected: 1, found: N)
  - Worker names (e.g., "celery@pms-worker-v2-abc123, celery@pms-worker-old-xyz789")
  - Fix instructions (stop extra workers)

**Why Single-Worker Rule:**
- Multiple workers cause duplicate sync tasks (same operation executed 2-3x)
- Race conditions when updating sync log status
- Duplicate log entries in `channel_sync_logs` table
- Inconsistent batch status (some ops "success", others "running" for same batch)

**Fix Multiple Workers:**

```bash
# Check running workers (HOST-SERVER-TERMINAL)
docker ps | grep pms-worker

# Expected output: Only pms-worker-v2 should be listed
# 1a2b3c4d5e6f   ghcr.io/your-org/pms-worker-v2:main   ...   pms-worker-v2

# If multiple workers found:
docker stop <old_worker_container_id>

# Verify only one worker remains
docker ps | grep pms-worker

# Verify health check passes
curl -s https://api.fewo.kolibri-visions.de/health/ready | jq '.components.celery'
# Expected: {"status": "up", "details": {"workers": ["celery@pms-worker-v2-..."]}}
```

**Related:**
- See [Single Worker Rule](#single-worker-rule-critical) for deployment guidelines
- See [Duplicate Sync Tasks](#duplicate-sync-tasks) for troubleshooting duplicate tasks

#### HEAD Support for Health Endpoints

Both `/health` and `/health/ready` support **HEAD** requests for lightweight monitoring.

**Purpose:** External monitors (uptime checkers, load balancers) can use HEAD to check status without fetching response body.

**Behavior:**
- Returns same HTTP status code as GET (200 when healthy, 503 when unhealthy)
- Empty response body (no JSON payload)
- Same logic as GET endpoints (checks DB/Redis/Celery based on feature flags)

**Examples:**

```bash
# HEAD request to /health (always returns 200)
curl -k -I https://api.fewo.kolibri-visions.de/health

# HEAD request to /health/ready (returns 200 when up, 503 when down)
curl -k -I https://api.fewo.kolibri-visions.de/health/ready

# Compare with GET (includes response body)
curl -k https://api.fewo.kolibri-visions.de/health/ready
```

**Expected Responses:**

```bash
# HEAD /health (liveness - always 200)
HTTP/2 200
content-length: 0

# HEAD /health/ready (readiness - 200 when healthy)
HTTP/2 200
content-length: 0

# HEAD /health/ready (readiness - 503 when DB down)
HTTP/2 503
content-length: 0
```

**Troubleshooting:**

If HEAD returns **405 Method Not Allowed** with `Allow: GET` header:
- You are running an older backend version without HEAD support
- Update to commit `62d7c80` or later for HEAD support
- Workaround: Use GET instead (less efficient for monitors)

---

### Monitoring Strategy

**Automated Monitoring (Recommended):**

```bash
# Set up cron job to check health every minute
* * * * * curl -f https://api.your-domain.com/health/ready || echo "Health check failed" | mail -s "PMS Alert" ops@example.com
```

**Manual Monitoring:**

```bash
# Quick health check
watch -n 10 'curl -s https://api.your-domain.com/health/ready | jq .'

# Monitor sync log failures
watch -n 30 'curl -s https://api.your-domain.com/api/v1/channel-connections/{id}/sync-logs?limit=5 -H "Authorization: Bearer TOKEN" | jq ".logs[] | select(.status==\"failed\")"'
```

---

### HEAD vs GET for Health Monitoring

**Purpose:** Understand when to use HEAD (status-only) vs GET (JSON body) for health checks.

**Use Cases:**

- **HEAD (Status-Only):**
  - Uptime monitors, load balancers, Kubernetes probes
  - Lightweight checks (no response body parsing)
  - Returns HTTP status code + headers only
  - Same logic as GET, but no JSON payload

- **GET (JSON Body):**
  - Debugging, troubleshooting, manual checks
  - Full component status details (DB, Redis, Celery)
  - Useful for identifying which component failed

**Production Examples:**

```bash
# HEAD /health (liveness - always 200, no body)
curl -k -sS -I https://api.fewo.kolibri-visions.de/health

# Expected output:
HTTP/2 200
content-type: application/json
content-length: 0

# HEAD /health/ready (readiness - 200 when up, 503 when down, no body)
curl -k -sS -I https://api.fewo.kolibri-visions.de/health/ready

# Expected output when healthy:
HTTP/2 200
content-type: application/json
content-length: 0

# Expected output when degraded (DB down):
HTTP/2 503
content-type: application/json
content-length: 0

# GET /health/ready (readiness - returns JSON body for debugging)
curl -k -sS https://api.fewo.kolibri-visions.de/health/ready

# Expected output when healthy:
{
  "status": "up",
  "components": {
    "db": {"status": "up", "checked_at": "2026-01-01T12:00:00Z"},
    "redis": {"status": "up", "details": {"skipped": true}},
    "celery": {"status": "up", "details": {"skipped": true}}
  },
  "checked_at": "2026-01-01T12:00:00Z"
}

# Expected output when degraded (DB down):
{
  "status": "down",
  "components": {
    "db": {"status": "down", "error": "connection refused", "checked_at": "..."},
    "redis": {"status": "up", "details": {"skipped": true}},
    "celery": {"status": "up", "details": {"skipped": true}}
  },
  "checked_at": "2026-01-01T12:00:00Z"
}
```

**Key Points:**

- HEAD and GET return the **same HTTP status code** (200 or 503)
- HEAD has **no response body** (content-length: 0)
- GET includes **full JSON payload** with component details
- `/health` (liveness) always returns 200 (application is running)
- `/health/ready` (readiness) returns 503 if DB is down (mandatory dependency)
- Redis and Celery are optional (controlled by `ENABLE_REDIS_HEALTHCHECK` and `ENABLE_CELERY_HEALTHCHECK`)

**When to Use:**

| Scenario | Method | Endpoint | Why |
|----------|--------|----------|-----|
| Uptime monitoring | HEAD | `/health/ready` | Lightweight, status-only |
| Load balancer probe | HEAD | `/health/ready` | Fast, no body parsing |
| K8s liveness probe | HEAD | `/health` | Minimal overhead |
| K8s readiness probe | HEAD | `/health/ready` | Status-only check |
| Debugging failed health | GET | `/health/ready` | See which component failed |
| Manual verification | GET | `/health/ready` | Human-readable JSON |

---

### Celery Worker Singleton (Important)

**Rule:** Only **ONE** Celery worker service must be active at any time.

**Current Worker:** `pms-worker-v2` (Coolify app)

**Why This Matters:**

Running multiple Celery workers simultaneously causes:
- **Duplicate sync tasks:** Same sync operation executed multiple times
- **Duplicate log entries:** Multiple workers writing to `channel_sync_logs`
- **Inconsistent batch status:** Race conditions when updating sync log status
- **Resource waste:** Unnecessary DB/Redis connections

**Symptoms of Violation:**

- Sync logs show duplicate entries with different `task_id` for the same operation
- Batch status shows inconsistent results (some ops "success", others "running" for same batch)
- Channel sync operations trigger 2-3x expected tasks
- Worker logs show multiple workers picking up same task

**Verification (HOST-SERVER-TERMINAL):**

```bash
# Check running workers
docker ps --format 'table {{.Names}}\t{{.Status}}\t{{.Image}}' | egrep -i 'pms-worker' || true

# Expected output (ONLY ONE worker running):
pms-worker-v2    Up 2 hours    ghcr.io/...

# Check all workers (including stopped)
docker ps -a --format 'table {{.Names}}\t{{.Status}}' | egrep -i 'pms-worker' || true

# Expected output:
pms-worker-v2    Up 2 hours
pms-worker       Exited (0) 3 days ago     <- OLD, should stay stopped

# If multiple workers are running (BAD):
pms-worker-v2    Up 2 hours
pms-worker       Up 5 minutes              <- PROBLEM! Stop this immediately
```

**Remediation:**

If multiple workers are running:

```bash
# 1. Stop the old worker immediately
docker stop pms-worker

# 2. Verify only pms-worker-v2 is running
docker ps | grep pms-worker

# 3. In Coolify:
#    - Navigate to old worker app (pms-worker)
#    - Click "Stop" or "Disable"
#    - Ensure "Auto Deploy" is OFF
#    - This prevents it from restarting on deploy
```

**Coolify Configuration:**

1. **Active Worker:** `pms-worker-v2`
   - Status: Running
   - Auto Deploy: ON
   - Health Check: Enabled

2. **Old Workers:** `pms-worker` (if exists)
   - Status: Stopped/Disabled
   - Auto Deploy: **OFF** (critical - prevents accidental restart)
   - Action: Archive or delete if no longer needed

**Deployment Best Practice:**

When deploying a new worker version:
1. Stop old worker first
2. Deploy new worker
3. Verify new worker is running (docker ps)
4. Disable auto-deploy on old worker in Coolify
5. Test sync operations (check for duplicates)

---

### Stale Sync Logs (Automatic Cleanup)

**Purpose:** Understand automatic cleanup of stale sync logs to maintain UI cleanliness.

**What Are Stale Logs?**

Sync logs stuck in `triggered` or `running` status for longer than the threshold (default: 60 minutes).

**Common Causes:**

- **Worker restart:** Celery worker restarted mid-task (deployment, crash, OOM kill)
- **DB disconnect:** Worker lost database connection during task execution
- **Redis disconnect:** Worker lost Redis connection (broker/result backend)
- **Task timeout:** Task exceeded Celery soft/hard time limits
- **Manual intervention:** Task manually revoked/terminated in Celery

**Automatic Cleanup Behavior:**

On every sync trigger (POST `/api/v1/channel-connections/{id}/sync`), the system:
1. Identifies logs for that connection older than `SYNC_LOG_STALE_MINUTES`
2. Marks them as `status='failed'`
3. Sets `error` field to: "Marked stale (no update for X minutes). Likely worker restart or lost DB connection."
4. Updates `updated_at` timestamp

**Configuration:**

```bash
# Environment variable (optional, defaults to 60)
SYNC_LOG_STALE_MINUTES=60

# Default: 60 minutes
# Increase if tasks legitimately take longer (e.g., large full syncs)
# Decrease for faster UI cleanup (not recommended < 30 minutes)
```

**Verification (Supabase SQL Editor):**

```sql
-- Check for currently stale logs (before cleanup)
SELECT
  id,
  connection_id,
  operation_type,
  status,
  created_at,
  updated_at,
  NOW() - COALESCE(updated_at, created_at) AS age,
  error
FROM channel_sync_logs
WHERE
  status IN ('triggered', 'running')
  AND COALESCE(updated_at, created_at) < NOW() - INTERVAL '60 minutes'
ORDER BY created_at DESC
LIMIT 20;

-- Check recently auto-cleaned logs
SELECT
  id,
  connection_id,
  operation_type,
  status,
  error,
  updated_at
FROM channel_sync_logs
WHERE
  status = 'failed'
  AND error LIKE 'Marked stale%'
ORDER BY updated_at DESC
LIMIT 20;
```

**Troubleshooting:**

If stale logs occur frequently:

1. **Check worker logs** for crashes or OOM kills:
   ```bash
   # Coolify: pms-worker-v2 → Logs
   # Look for: "Worker shutdown", "MemoryError", "Killed"
   docker logs pms-worker-v2 | grep -i "error\|killed\|shutdown" | tail -50
   ```

2. **Check worker health** (should be running):
   ```bash
   docker ps | grep pms-worker-v2
   ```

3. **Verify Redis connection** (worker needs Redis for task queue):
   ```bash
   # From worker container
   docker exec pms-worker-v2 redis-cli -h redis-host PING
   # Expected: PONG
   ```

4. **Check Celery worker concurrency** (may be overloaded):
   ```bash
   # Coolify env: CELERY_WORKER_CONCURRENCY
   # Default: 4 (increase if tasks timeout frequently)
   ```

5. **Review task timeouts** (may be too aggressive):
   ```python
   # In celery.py or task definition
   soft_time_limit = 300  # 5 minutes
   time_limit = 600       # 10 minutes hard limit
   ```

**When to Investigate:**

- **Occasional stale logs (< 5% of syncs):** Normal after deployments/restarts
- **Frequent stale logs (> 10% of syncs):** Worker stability issue - investigate logs
- **All syncs stale:** Worker offline or not processing tasks - check deployment

**Related:**

- See [Celery Worker Singleton](#celery-worker-singleton-important) for worker management
- See [Worker Logs](#worker-logs) for debugging task failures

---

### Troubleshooting Sync Log Issues

#### Issue: Sync Logs Not Persisting

**Symptom:**
- POST `/availability/sync` returns 200
- GET `/sync-logs` shows no entries or 503 error

**Diagnosis:**
```bash
# Check if channel_sync_logs table exists
docker exec $(docker ps -q -f name=supabase) psql -U postgres -d postgres \
  -c "\dt channel_sync_logs"

# Expected: Table listing
# If not found: Migration not applied
```

**Solution:**
```bash
# Apply migration
cd supabase/migrations
docker exec $(docker ps -q -f name=supabase) supabase migration up

# Verify
docker exec $(docker ps -q -f name=supabase) psql -U postgres -d postgres \
  -c "SELECT COUNT(*) FROM channel_sync_logs;"
```

---

#### Issue: Sync Logs Show "failed" Status

**Symptom:**
- All syncs marked as "failed" in logs
- Worker logs show errors

**Diagnosis:**
```bash
# Get failed log details
curl -s https://api.your-domain.com/api/v1/channel-connections/{id}/sync-logs?limit=10 \
  -H "Authorization: Bearer TOKEN" | jq '.logs[] | select(.status=="failed")'

# Check worker logs for exceptions
docker logs pms-worker --tail 100 | grep -i error
```

**Common Causes & Solutions:**

**1. Database Connection Error:**
```
Solution: Check DATABASE_URL in worker environment
```

**2. Missing Environment Variables:**
```bash
# Compare worker env with backend
docker exec pms-backend env | grep -E "DATABASE_URL|JWT_SECRET|ENCRYPTION_KEY" | sort > /tmp/backend-env.txt
docker exec pms-worker env | grep -E "DATABASE_URL|JWT_SECRET|ENCRYPTION_KEY" | sort > /tmp/worker-env.txt
diff /tmp/backend-env.txt /tmp/worker-env.txt

# Any differences? Add missing vars to worker
```

**3. Code Version Mismatch:**
```bash
# Check git commits match
docker exec pms-backend git rev-parse HEAD
docker exec pms-worker git rev-parse HEAD

# If different: Redeploy worker
```

---

#### Issue: Retry Count Not Updating

**Symptom:**
- Sync fails but retry_count stays at 0
- No retry attempts logged

**Diagnosis:**
```bash
# Check if Celery task has retry configured
docker exec pms-worker celery -A app.channel_manager.core.sync_engine:celery_app inspect registered | grep update_channel

# Should show: max_retries=3
```

**Solution:**
- Verify task decorator has `@celery_app.task(bind=True, max_retries=3)`
- Redeploy worker if code changes needed

---

#### Issue: Bookings Sync Logs Missing (HTTP 503 on Constraint Violation)

**Symptom:**
- POST `/channel-connections/{id}/sync` with `sync_type=bookings` returns HTTP 503
- API returns: "Database schema out of date: channel_sync_logs.operation_type constraint blocks 'bookings'"
- OR: Task executes successfully but no logs appear (silent failure)
- Worker logs: "DB constraint violation creating sync log" or "No sync log found for task_id=..."

**Root Cause:**
The `channel_sync_logs.operation_type` CHECK constraint doesn't allow `bookings_sync` (only allows `bookings_import`), but backend code uses `operation_type='bookings_sync'` when triggering bookings imports.

**Diagnosis:**
```bash
# Check current constraint definition
docker exec $(docker ps -q -f name=supabase-db) psql -U postgres -d postgres -c \
  "SELECT pg_get_constraintdef(oid) FROM pg_constraint
   WHERE conrelid = 'public.channel_sync_logs'::regclass
   AND conname = 'channel_sync_logs_operation_type_check';"

# Expected (before fix): Shows only 'bookings_import' (missing 'bookings_sync')
# Expected (after fix):  Shows both 'bookings_import' and 'bookings_sync' + pattern matching
```

**Solution:**
```bash
# Apply migration to fix constraint
cd /path/to/pms-webapp
supabase migration up --file supabase/migrations/20260101140000_fix_channel_sync_logs_operation_type_check.sql

# Verify constraint updated
docker exec $(docker ps -q -f name=supabase-db) psql -U postgres -d postgres -c \
  "SELECT pg_get_constraintdef(oid) FROM pg_constraint
   WHERE conrelid = 'public.channel_sync_logs'::regclass
   AND conname = 'channel_sync_logs_operation_type_check';"

# Test bookings sync
bash backend/scripts/pms_channel_sync_poll.sh --sync-type bookings --poll-limit 30

# Expected: Sync succeeds, logs show operation_type=bookings_sync
```

**Migration Details:**
- **File:** `supabase/migrations/20260101140000_fix_channel_sync_logs_operation_type_check.sql`
- **Action:** Drops old constraint and recreates with:
  - Explicit support for `bookings_sync` and `bookings_import`
  - Pattern matching for future-proofing: `booking%` and `bookings%`
- **Idempotent:** Safe to run multiple times (uses `IF EXISTS` checks)

**Backend Behavior After Fix:**
- **Before migration:** API returns HTTP 503 with actionable error message (fail-fast)
- **After migration:** API returns HTTP 200, Celery task executes, logs are created

**Related Files:**
- `backend/app/services/channel_sync_log_service.py` (catches CheckViolationError and re-raises)
- `backend/app/api/routers/channel_connections.py` (returns 503 on constraint violations)

---

### Alert Thresholds

**Recommended Alert Rules:**

| Metric | Threshold | Action |
|--------|-----------|--------|
| `/health/ready` status | degraded | Investigate immediately |
| Failed sync logs | >10% in 1 hour | Check worker logs |
| Celery workers | 0 workers | Restart pms-worker |
| Database response time | >500ms | Check Supabase load |
| Redis response time | >100ms | Check Redis container |

---

### Troubleshooting Inventory Conflicts

#### Database-Level Exclusion Constraints (Source of Truth)

**Overview:**

The PMS uses PostgreSQL exclusion constraints to prevent inventory conflicts (double-bookings, overlapping blocks) at the database level. This is the **source of truth** for inventory management and provides race-safe protection under high concurrency.

**Constraints Enforced:**

1. **`bookings.no_double_bookings`**
   - Prevents overlapping bookings for same property within agency
   - Scope: `(agency_id, property_id, daterange)`
   - Excludes: `cancelled`, `declined`, `no_show` statuses
   - Uses: `[)` end-exclusive daterange semantics

2. **`availability_blocks.availability_blocks_no_overlap`**
   - Prevents overlapping blocks for same property
   - Scope: `(property_id, daterange)`
   - Uses: `[)` end-exclusive daterange semantics

3. **`inventory_ranges.inventory_ranges_no_overlap`**
   - Prevents overlapping inventory holds
   - Scope: `(property_id, daterange)` for active ranges only
   - Uses: `[)` end-exclusive daterange semantics

**Date Semantics:**

All constraints use `[)` end-exclusive semantics:
- `check_in` / `start_date`: **Included** (occupied)
- `check_out` / `end_date`: **Excluded** (available)
- **Back-to-back bookings allowed**: Booking A `check_out` = Booking B `check_in`

**Example:**
```
Booking A: 2026-01-10 to 2026-01-12  (occupies: Jan 10, Jan 11)
Booking B: 2026-01-12 to 2026-01-14  (occupies: Jan 12, Jan 13)
Result: ✓ Both allowed (Jan 12 is available for Booking B)
```

**Inquiry Bookings Policy (Non-Blocking):**

As of 2026-01-03, inquiry bookings (`status='inquiry'`) are **non-blocking** for both availability checks and booking creation:

- **Availability API** (`GET /api/v1/availability`): Does NOT show inquiry bookings as blocked ranges
- **Booking Creation** (`POST /api/v1/bookings`): Does NOT treat inquiry bookings as conflicts (409)
- **Rationale**: Inquiry bookings are preliminary/tentative and should not prevent confirmed reservations

**Non-Blocking Statuses:**
- `cancelled` - Cancelled bookings
- `declined` - Declined bookings
- `no_show` - No-show bookings
- `inquiry` - Inquiry/tentative bookings (as of 2026-01-03)

**Blocking Statuses:**
- `confirmed` - Confirmed reservations
- `pending` - Pending confirmation
- `checked_in` - Active stays
- `checked_out` - Completed stays (historical data)

**Database Implementation:**
- `inventory_ranges` table uses `source_id` (UUID) and `kind` (enum) to reference bookings/blocks
- No `booking_id` or `block_id` columns exist in `inventory_ranges`
- API responses may present `booking_id`/`block_id` derived from `source_id` + `kind` for client convenience

**Source of Truth for Availability (2026-01-03 Update):**

Both availability API and booking creation now use `inventory_ranges` as the **single source of truth** for overlap detection:

- **Availability API**: Queries `inventory_ranges` with `state='active'` to return blocked ranges
- **Booking Creation**: `check_availability()` queries `inventory_ranges` (not bookings table) to detect conflicts
- **Consistency**: This ensures both APIs treat inquiry bookings identically (non-blocking)
- **How It Works**:
  - Inquiry bookings: Created in `bookings` table but do NOT create `inventory_ranges` entries
  - Confirmed bookings: Create both `bookings` entry AND `inventory_ranges` entry with `state='active'`
  - Blocks: Create `availability_blocks` entry AND `inventory_ranges` entry with `state='active'`
  - Exclusion constraint on `inventory_ranges` provides race-safe overlap protection

**Troubleshooting: "Free Window" but All 409s:**

If availability API shows a window as free (`ranges=[]`) but booking creation returns 409 for all requests:

1. Check for inquiry bookings overlapping the window:
   ```sql
   SELECT id, status, check_in, check_out
   FROM bookings
   WHERE property_id = 'your-property-uuid'
     AND status = 'inquiry'
     AND daterange(check_in, check_out, '[)') && daterange('2026-01-10', '2026-01-12', '[)');
   ```

2. Verify inquiry bookings do NOT have `inventory_ranges` entries:
   ```sql
   SELECT ir.*
   FROM inventory_ranges ir
   JOIN bookings b ON ir.source_id = b.id
   WHERE b.status = 'inquiry'
     AND ir.state = 'active';
   -- Should return 0 rows (inquiry should not create active ranges)
   ```

3. If inquiry bookings ARE blocking (incorrect behavior):
   - Verify backend code uses `inventory_ranges` query in `check_availability()`
   - Check for stale code that queries `bookings` table directly
   - Ensure migration created `inventory_ranges` properly

#### Issue: HTTP 409 with `conflict_type=inventory_overlap`

**Symptom:**
- POST `/api/v1/bookings` returns 409 Conflict
- Response body: `{"detail": "Property is already booked for these dates", "conflict_type": "inventory_overlap"}`
- Or: `{"detail": "Property is already occupied for dates...", "conflict_type": "inventory_overlap"}`

**Root Cause:**

Database exclusion constraint detected overlapping dates:
- Another booking/block exists for same property and overlapping dates
- Application-level checks passed, but DB constraint fired (race condition)
- Constraint provides **definitive** conflict detection (no false negatives)

**Diagnosis:**

```bash
# Check for overlapping bookings in same property
docker exec $(docker ps -q -f name=supabase) psql -U postgres -d postgres -c "
  SELECT
    id,
    property_id,
    check_in,
    check_out,
    status,
    created_at
  FROM bookings
  WHERE property_id = '<property-uuid>'
    AND status NOT IN ('cancelled', 'declined', 'no_show')
    AND daterange(check_in, check_out, '[)') && daterange('2026-01-10', '2026-01-12', '[)')
  ORDER BY check_in;
"

# Check for overlapping availability blocks
docker exec $(docker ps -q -f name=supabase) psql -U postgres -d postgres -c "
  SELECT
    id,
    property_id,
    start_date,
    end_date,
    created_at
  FROM availability_blocks
  WHERE property_id = '<property-uuid>'
    AND daterange(start_date, end_date, '[)') && daterange('2026-01-10', '2026-01-12', '[)')
  ORDER BY start_date;
"

# Check for overlapping inventory ranges
docker exec $(docker ps -q -f name=supabase) psql -U postgres -d postgres -c "
  SELECT
    id,
    property_id,
    start_date,
    end_date,
    kind,
    state,
    created_at
  FROM inventory_ranges
  WHERE property_id = '<property-uuid>'
    AND state = 'active'
    AND daterange(start_date, end_date, '[)') && daterange('2026-01-10', '2026-01-12', '[)')
  ORDER BY start_date;
"
```

**Expected Behavior:**

- HTTP 409 is **correct** when overlap exists
- Clients should retry with different dates or cancel conflicting booking/block
- This is **not an error** — it's race-safe conflict prevention working as designed

**Conflict Type Semantics:**

The `conflict_type` field distinguishes between different overlap causes:

| conflict_type | Meaning | Example Scenario |
|---------------|---------|------------------|
| `inventory_overlap` | Overlap with **availability block** | Admin blocks Jan 10-12 for maintenance. Guest tries to book Jan 10-12 → HTTP 409 with `conflict_type=inventory_overlap` |
| `double_booking` | Overlap with **existing booking** | Guest A books Jan 10-12. Guest B tries to book Jan 10-12 → HTTP 409 with `conflict_type=double_booking` |

**Detection Logic:**
- When availability check fails, API queries `availability_blocks` table to determine conflict source
- If overlapping block found → `conflict_type=inventory_overlap`
- If no block found → `conflict_type=double_booking` (must be booking overlap)
- This allows clients to provide context-aware error messages (e.g., "Property is blocked for maintenance" vs "Property is already booked")

**Example Response (Availability Block Conflict):**
```json
{
  "detail": "Property is already booked for these dates",
  "conflict_type": "inventory_overlap"
}
```

**Example Response (Booking Overlap):**
```json
{
  "detail": "Property is already booked for these dates",
  "conflict_type": "double_booking"
}
```

**Common Issues:**

*HTTP 500 on booking creation after conflict-type detection changes:*
- **Symptom:** POST `/api/v1/bookings` returns HTTP 500 instead of expected 409 when overlapping availability block
- **Error in logs:** `NameError: name 'conn' is not defined` in BookingService.create_booking()
- **Root cause:** Conflict detection pre-check uses undefined database connection variable (`conn` instead of `self.db`)
- **Location:** `booking_service.py` availability block query in pre-check or update paths
- **Fix:** Replace `await conn.fetchrow(...)` with `await self.db.fetchrow(...)` in all block overlap queries
- **Expected behavior:** Should return HTTP 409 with `conflict_type=inventory_overlap` for block overlaps

**Verify Constraints Exist:**

```bash
# List all exclusion constraints
docker exec $(docker ps -q -f name=supabase) psql -U postgres -d postgres -c "
  SELECT
    conrelid::regclass AS table_name,
    conname AS constraint_name,
    pg_get_constraintdef(oid) AS definition
  FROM pg_constraint
  WHERE contype = 'x'  -- exclusion constraint
    AND connamespace = 'public'::regnamespace
  ORDER BY conrelid::regclass::text;
"
```

**Expected Output:**
```
table_name          | constraint_name                     | definition
--------------------+-------------------------------------+---------------------------
bookings            | no_double_bookings                  | EXCLUDE USING gist (agency_id WITH =, property_id WITH =, daterange(check_in, check_out, '[)'::text) WITH &&) WHERE ((status <> ALL (ARRAY['cancelled'::text, 'declined'::text, 'no_show'::text])))
availability_blocks | availability_blocks_no_overlap      | EXCLUDE USING gist (property_id WITH =, daterange(start_date, end_date, '[)'::text) WITH &&)
inventory_ranges    | inventory_ranges_no_overlap         | EXCLUDE USING gist (property_id WITH =, daterange(start_date, end_date, '[)'::text) WITH &&) WHERE ((state = 'active'::text))
```

**Migration:**

Constraints added via migration: `supabase/migrations/20251229200517_enforce_overlap_prevention_via_exclusion.sql`

**Error Handling:**

Application code catches `asyncpg.exceptions.ExclusionViolationError` (SQLSTATE 23P01) and maps to:
- HTTP 409 Conflict
- `conflict_type=inventory_overlap`
- Human-readable error message

**Locations:**
- `backend/app/services/booking_service.py:678` (booking creation)
- `backend/app/services/availability_service.py:325` (inventory_range creation)
- `backend/app/services/availability_service.py:404` (inventory_range update)

#### Testing Race-Safe Behavior (Concurrency Test)

**Script:** `backend/scripts/pms_booking_concurrency_test.sh`

**Purpose:** Validate that exclusion constraints prevent double-bookings under concurrent requests.

**What it does:**
- Fires N parallel POST /bookings requests (default: 10) to same property/dates
- Expects exactly **1 success** (HTTP 201) and **N-1 conflicts** (HTTP 409)
- Validates DB-level race-safe inventory management

**Usage:**
```bash
# Basic usage (auto-picks property)
export ENV_FILE=/root/pms_env.sh
bash scripts/pms_booking_concurrency_test.sh

# With explicit property and dates
export PID="your-property-uuid"
export FROM="2026-06-01"
export TO="2026-06-03"
bash scripts/pms_booking_concurrency_test.sh
```

**Important:** If you see **HTTP 401 "Token has expired"**, re-fetch your TOKEN:
```bash
source /root/pms_env.sh
bash scripts/pms_phase23_smoke.sh  # Auto-fetches token
# OR manually fetch token (see script output for instructions)
```

**Expected PASS:**
- Exactly 1 booking succeeds (201)
- All other requests rejected with 409 `inventory_overlap`
- No double-booking occurred

**See:** `backend/scripts/README.md` for full documentation.

---

### Booking Concurrency Deadlocks

**Symptom:**
- POST `/api/v1/bookings` returns HTTP 503 (not 500!) with message "Database deadlock detected. Please retry your request."
- Multiple concurrent booking requests for the same property may trigger PostgreSQL deadlock errors

**Root Cause:**

Under high concurrency, multiple booking requests for the same property can trigger PostgreSQL deadlocks due to:
1. Exclusion constraint checks (`bookings.no_double_bookings`)
2. Foreign key constraint checks (property_id, guest_id)
3. Multiple transactions acquiring locks in different orders

**Prevention (Implemented):**

The application prevents deadlocks using a two-layer approach:

1. **Advisory Lock Serialization** (`booking_service.py:470-475`)
   - Each booking transaction acquires a PostgreSQL advisory lock scoped to the property ID
   - Lock is transaction-scoped (automatically released on commit/rollback)
   - Serializes concurrent bookings for the same property (no deadlocks)

2. **Automatic Retry with Exponential Backoff** (`booking_service.py:84-121`)
   - If deadlock still occurs (rare edge case), automatically retries up to 3 times
   - Exponential backoff: 100ms, 200ms between attempts
   - Only deadlocks are retried; other errors propagate immediately

3. **Error Mapping** (`bookings.py:288-298`)
   - If all retries exhausted, maps deadlock to HTTP 503 (not 500!)
   - Client-friendly message: "Database deadlock detected. Please retry your request."
   - Prevents 500 errors from reaching clients

**Expected Behavior:**

- Under normal load: Advisory lock prevents deadlocks entirely
- Under extreme load: Deadlocks auto-retry and succeed on retry
- Worst case (all retries exhausted): HTTP 503 with retry-friendly message

**Verification:**

Test concurrent booking behavior using the concurrency smoke script:
```bash
# Fire 10 parallel booking requests for the same property/dates
export ENV_FILE=/root/pms_env.sh
bash scripts/pms_booking_concurrency_test.sh

# Expected result:
# - Exactly 1 success (HTTP 201)
# - All others: HTTP 409 inventory_overlap (NOT 503 deadlock)
```

**Troubleshooting:**

If you see HTTP 503 deadlock errors in production:
1. Check if concurrency script passes (validates advisory lock works)
2. Check backend logs for "Deadlock detected after 3 attempts" errors
3. If deadlocks persist despite retries, increase retry attempts or backoff in `booking_service.py`
4. Consider property-level rate limiting if single property receives extreme concurrency

**Code Locations:**
- Advisory lock: `backend/app/services/booking_service.py:510-520`
- Retry wrapper: `backend/app/services/booking_service.py:84-121`
- Route error handler: `backend/app/api/routes/bookings.py:288-298`
- Unit tests: `backend/tests/unit/test_booking_deadlock.py`

**Hotfix Note (2026-01-03):**

After initial deployment of advisory lock serialization, a production bug was discovered:

**Symptom:**
- POST `/api/v1/bookings` returned HTTP 500 for all valid requests
- Backend logs showed: `NameError: name 'property_id' is not defined` at line ~516

**Root Cause:**
- Advisory lock code referenced `property_id` variable before it was defined
- The variable needed to be extracted from `booking_data["property_id"]` before use

**Fix Applied:**
- Added `property_id = to_uuid(booking_data["property_id"])` before transaction start (line 511)
- Advisory lock now correctly uses the extracted property_id for lock key
- Lock still executes inside transaction (xact lock, auto-released on commit/rollback)

**Expected Behavior Restored:**
- Single booking for free window → HTTP 201
- Concurrent requests (10 parallel, same property/dates) → 1x201, 9x409 inventory_overlap
- Deadlock (if retries exhausted) → HTTP 503 with retry message (not 500)

**Verification:**
- Concurrency smoke script: `bash scripts/pms_booking_concurrency_test.sh`
- Unit test added: `test_advisory_lock_uses_property_id_from_request()`

---

### Inventory Blocking Behavior (Inquiry vs Confirmed)

**Updated:** 2026-01-03 (Production Fix)

**Contract:**

Not all booking statuses occupy inventory. Only confirmed/hard reservations create `inventory_ranges` entries that block availability:

**BLOCKING Statuses** (create active `inventory_ranges`):
- `pending` - Awaiting payment/confirmation
- `confirmed` - Paid reservation (hard hold)
- `checked_in` - Guest currently on property

**NON-BLOCKING Statuses** (do NOT create `inventory_ranges`):
- `inquiry` - Information request, tentative interest (NOT a reservation)
- `cancelled` - Reservation cancelled (inventory freed)
- `declined` - Inquiry declined by host
- `no_show` - Guest didn't arrive (inventory freed)
- `checked_out` - Guest departed (inventory freed)

**Why This Matters:**

The availability API (`GET /api/v1/availability`) and booking creation (`POST /api/v1/bookings`) both use `inventory_ranges` with `state='active'` as the source of truth. This ensures:

1. **Inquiry bookings never block availability** - They don't create `inventory_ranges`, so overlapping confirmed bookings can be created
2. **API consistency** - If availability shows a window as free, booking creation will succeed (no false 409s)
3. **Concurrency safety** - The exclusion constraint on `inventory_ranges` prevents race conditions

**Common Scenario:**

```
# Scenario: Inquiry exists for 2026-01-10 to 2026-01-12

# Step 1: Check availability
GET /api/v1/availability?property_id=X&from_date=2026-01-10&to_date=2026-01-12
→ Response: { "ranges": [] }  # Free! (inquiry doesn't block)

# Step 2: Create confirmed booking
POST /api/v1/bookings
{ "property_id": "X", "check_in": "2026-01-10", "check_out": "2026-01-12", "status": "confirmed" }
→ Response: HTTP 201 (success! inquiry doesn't block)

# Step 3: Try to create another confirmed booking
POST /api/v1/bookings (same dates)
→ Response: HTTP 409 inventory_overlap (first confirmed booking blocks)
```

**Production Bug Fixed (2026-01-03):**

**Symptom:**
- Concurrency test auto-found "free window" via availability API (ranges=[])
- But ALL 10 concurrent booking requests returned 409 (0 successes)
- DB showed an `inquiry` booking overlapping the window

**Root Cause:**
- `inquiry` was incorrectly included in `OCCUPYING_STATUSES`
- This caused inquiry bookings to create `inventory_ranges` entries
- Violated the contract that inquiry should be non-blocking

**Fix Applied:**
- Removed `inquiry` from `OCCUPYING_STATUSES` (line 147)
- Added `inquiry` to `NON_OCCUPYING_STATUSES` (line 150)
- Now inquiry bookings do NOT create `inventory_ranges` entries
- Confirmed bookings can overlap with inquiry bookings (as intended)

**Code Locations:**
- Status definitions: `backend/app/services/booking_service.py:145-150`
- Inventory range creation: `backend/app/services/booking_service.py:771-788`
- Status transition logic: `backend/app/services/booking_service.py:926-962`
- Unit test: `backend/tests/unit/test_booking_deadlock.py::test_inquiry_non_blocking_full_lifecycle`

**Verification:**
```bash
# Concurrency script should now succeed even if inquiry bookings exist
bash scripts/pms_booking_concurrency_test.sh

# Expected: 1 success (201), 9 conflicts (409)
# Inquiry bookings in the window will NOT cause all-409s
```

**Database Constraint Fix (2026-01-03 Follow-up):**

**Second Production Bug Discovered:**

After the initial OCCUPYING_STATUSES fix, another issue was found:

**Symptom:**
- Availability API shows window as free (0 inventory_ranges)
- Inquiry booking exists overlapping the window
- ALL concurrent booking requests return 409 (0 successes)
- Backend logs show: `ExclusionViolationError: "no_double_bookings"`

**Root Cause:**
- The `bookings` table has its own exclusion constraint `no_double_bookings`
- Original constraint: `WHERE (status NOT IN ('cancelled', 'declined', 'no_show'))`
- This incorrectly included 'inquiry' in the blocking set
- When trying to INSERT a confirmed booking, the constraint blocked it due to overlapping inquiry

**Fix Applied (Migration 20260103140000):**
- Dropped old `no_double_bookings` constraint
- Recreated with positive filter: `WHERE (status IN ('pending', 'confirmed', 'checked_in'))`
- Now inquiry bookings do NOT trigger this database-level constraint
- Aligns with OCCUPYING_STATUSES and inventory_ranges policy

**Diagnostic Logging Added:**
- Enhanced `booking_service.py` ExclusionViolationError handler (line 731-760)
- Now logs which constraint triggered the 409:
  - `bookings.no_double_bookings` → overlapping pending/confirmed/checked_in booking
  - `inventory_ranges.inventory_ranges_no_overlap` → overlapping active inventory_range
  - Includes property_id, dates, status, and error details for admin debugging

**Troubleshooting: Availability Free but All 409s**

If concurrency test shows "free window" (ranges=[]) but ALL requests get 409:

1. **Check for inquiry bookings in window:**
   ```sql
   SELECT id, status, check_in, check_out
   FROM bookings
   WHERE property_id = 'YOUR-PROPERTY-ID'
     AND status = 'inquiry'
     AND daterange(check_in, check_out, '[)') && daterange('2026-01-10', '2026-01-12', '[)');
   ```

2. **Verify no active inventory_ranges exist:**
   ```sql
   SELECT *
   FROM inventory_ranges
   WHERE property_id = 'YOUR-PROPERTY-ID'
     AND state = 'active'
     AND daterange(start_date, end_date, '[)') && daterange('2026-01-10', '2026-01-12', '[)');
   ```
   Expected: 0 rows (inquiry doesn't create ranges)

3. **Check database constraint:**
   ```sql
   SELECT conname, pg_get_constraintdef(oid)
   FROM pg_constraint
   WHERE conname = 'no_double_bookings' AND conrelid = 'bookings'::regclass;
   ```
   Expected: `WHERE (status IN ('pending', 'confirmed', 'checked_in'))`
   If shows `WHERE (status NOT IN (...))` with inquiry not excluded → apply migration 20260103140000

4. **Check backend logs for constraint name:**
   - Look for `ExclusionViolationError` in logs
   - If "no_double_bookings" appears → database constraint issue (apply migration)
   - If "inventory_ranges" appears → inventory_ranges conflict (unexpected if API says free)

**Migration Path:**
```sql
-- Apply fix (run via Supabase migration runner or psql)
\i supabase/migrations/20260103140000_fix_bookings_exclusion_inquiry_non_blocking.sql
```

**Known Issue (Fixed):**
If migration 20260103140000 fails with syntax error near `||` (ERROR 42601), ensure you have the fixed version that uses a single dollar-quoted string literal for `COMMENT ON CONSTRAINT` instead of concatenated strings. This was fixed in commit 82ffc27 (initial) and refined in a follow-up commit to remove string concatenation operators.

**Applying Supabase Migrations in Production (Host Terminal):**

Production migrations must be applied from the host server terminal with proper credentials:

```bash
# Prerequisites:
# - DATABASE_URL must be set to Supabase Postgres connection string
# - User must be supabase_admin (table owner) for ALTER TABLE operations
# - Run from repo root directory

# Golden Check (recommended): verify you are table owner before ALTER TABLE migrations
psql "$DATABASE_URL" -c "select current_user, (select tableowner from pg_tables where schemaname='public' and tablename='bookings') as bookings_owner;"
# Expected: current_user = supabase_admin AND bookings_owner = supabase_admin

# 1. Check migration status (shows applied vs pending)
bash backend/scripts/ops/apply_supabase_migrations.sh --status

# 2. Apply pending migrations (requires confirmation)
export CONFIRM_PROD=1
bash backend/scripts/ops/apply_supabase_migrations.sh

# 3. Verify constraint was updated correctly
psql "$DATABASE_URL" -c "SELECT conname, pg_get_constraintdef(oid) FROM pg_constraint WHERE conname='no_double_bookings' AND conrelid='bookings'::regclass;"
```

**Expected verification output:**
```
WHERE (status = ANY (ARRAY['pending'::text, 'confirmed'::text, 'checked_in'::text]))
```

**Common Pitfalls:**

1. **psql connects to local socket instead of Supabase:**
   - Symptom: Connection succeeds but queries show wrong database
   - Fix: Ensure `DATABASE_URL` is exported in current shell session
   - Verify: `echo $DATABASE_URL` should show full connection string

2. **User is not table owner:**
   - Symptom: `ERROR: must be owner of table bookings`
   - Fix: Use `supabase_admin` role, not `postgres` or `anon`
   - Reliable methods:
     - Include username in URL: `postgresql://supabase_admin:password@host:5432/postgres`
     - OR export `PGUSER=supabase_admin` and `PGPASSWORD=...` before psql
   - Note: Query parameter `?user=supabase_admin` is NOT required (and may not work in all contexts)

3. **Running SQL in Supabase SQL Editor:**
   - Limitation: Supabase SQL Editor does NOT support psql meta-commands (`\i`, `\set`)
   - Limitation: Parameter placeholders like `:pid` only work in psql CLI
   - Fix: Always run migrations from host terminal via psql or migration runner script

---

## Ops Console (Admin UI)

**Purpose:** Optional web-based operations console for system diagnostics and monitoring.

**URL:** `https://admin.fewo.kolibri-visions.de/ops`

**Access Control:**
- **Admin-only:** Only users with `admin` role can access
- **Feature-flagged:** Must be explicitly enabled via environment variable

---

### How to Enable

**Environment Variable (Frontend):**
```bash
NEXT_PUBLIC_ENABLE_OPS_CONSOLE=1
```

**Accepted Values (case-insensitive):**
- `1` (recommended)
- `true`
- `yes`
- `on`

**Default:** Disabled (when not set, or set to `0`, `false`, etc.)

**Where to Set:**
- Coolify Dashboard → pms-admin → Build Pack Variables (Environment Variables)
- Add new variable: `NEXT_PUBLIC_ENABLE_OPS_CONSOLE` = `1`
- Redeploy frontend for changes to take effect

---

### Who Can Access

**Requirements (ALL must be met):**
1. User must be logged in with valid JWT token
2. User role must be `admin` (not manager/staff/owner/accountant)
3. Feature flag `NEXT_PUBLIC_ENABLE_OPS_CONSOLE` must be set to a truthy value

**Access Control Behavior (No Silent Redirects):**
- **If auth loading:** Shows loading skeleton
- **If feature disabled:** Shows "Ops Console is Disabled" error page with:
  - Clear explanation that feature flag must be set
  - List of accepted values (`1`, `true`, `yes`, `on`)
  - Link to return to Channel Sync
- **If non-admin (but feature enabled):** Shows "Access Denied" error page with:
  - Message: "Ops Console is restricted to administrators only"
  - Link to return to Channel Sync
  - Logout button
- **If admin AND feature enabled:** Full access to Ops Console

**Important:** Ops Console **never redirects silently** to `/login` or `/channel-sync`. This makes debugging easier — you always see the exact reason you can't access the console.

---

### Pages

The Ops Console includes three main pages:

#### 1. `/ops/status` - System Health

**What it shows:**
- **Overall system status banner** (Healthy / Checking / Down):
  - **Checking system...** (blue, spinning icon) on initial page load while health checks run
    - This is normal and prevents false "System Down" alarms during loading
    - Appears briefly (usually < 1 second) while fetching `/health` and `/health/ready`
  - **System Healthy** (green) when ALL of the following are true:
    - `/health` endpoint returns `status: "up"`
    - `/health/ready` endpoint returns `status: "up"`
    - All components (db, redis, celery) have `status: "up"`
  - **System Down** (red) when ANY check fails:
    - Lists specific failed checks (e.g., "db component (status: down)")
    - Example: "/health (not 'up')", "redis component (status: down)"
    - **Only shows red after checks complete** (not during initial load)
- GET `/health` response with version/commit if available
- GET `/health/ready` response with component statuses
  - Database (db)
  - Redis (if exposed)
  - Celery workers (if exposed)
  - **Component badges** display `component.status` (e.g., "ok", "healthy", "up", "down")
  - **Error messages** appear below component name if `component.error` is present (truncated to 60 chars)

**Endpoint Metadata:**
Each health endpoint displays operational metrics:
- **Last checked:** Local timestamp of last check (e.g., "3:45:23 PM")
- **Duration:** Request round-trip time in milliseconds (e.g., "152ms")
- **HTTP:** Response status code (e.g., "200" for success, "503" for unavailable)

**Actions:**
- **Refresh:** Manually refresh health checks
- **Copy Diagnostics:** Copies system diagnostics to clipboard:
  - Timestamp
  - Current URL (for context)
  - API base URL
  - User role (admin)
  - **Overall status** ("healthy", "down", or "checking")
  - **Failed checks** (list of specific failures if any)
  - **Endpoint metadata** (duration_ms, http_status, last_checked_at for each endpoint)
  - **Component statuses** (summary of db/redis/celery status)
  - `/health` response
  - `/health/ready` response
  - **NO secrets / NO environment values exposed**

**Use Cases:**
- Quick system health check after deployment
- Verify database/Redis/worker connectivity
- Gather diagnostics for issue reports

**Troubleshooting:**
- **"Checking system..." state:** Normal on page load/refresh (brief, < 1 second). Only red "System Down" indicates confirmed failure.
- If banner shows "System Down", check the listed failed checks
- Expand "Show raw JSON" on `/health` and `/health/ready` sections to see full response
- Verify each component has `status: "up"` (not "ok", "healthy", or other values)
- Check endpoint metadata:
  - High duration (> 5000ms) may indicate network/database slowness
  - HTTP status ≠ 200 indicates endpoint failure
  - Recent "Last checked" confirms data freshness
- Common causes:
  - Database connection lost → db component status ≠ "up"
  - Redis unavailable → redis component status ≠ "up"
  - Celery worker down → celery component status ≠ "up"

#### 2. `/ops/sync` - Channel Sync Operations

**What it shows:**
- Link button to existing `/channel-sync` page
- Info cards explaining sync features:
  - Trigger Sync (manual availability/pricing sync)
  - View Logs (real-time sync logs table)
  - Monitor (auto-refresh, error tracking)
- Related resources links (Runbook, System Status)

**Use Cases:**
- Quick navigation to channel sync console
- Overview of sync capabilities

#### 3. `/ops/runbook` - Troubleshooting Guide

**What it shows:**
- Link to full runbook documentation (GitHub)
- Common issues cards:
  - 503: Database Temporarily Unavailable
  - Celery Worker / Redis Down
  - 401: JWT Token Expired
  - 422: Validation Errors (No Sync Log Created)
- Each card includes:
  - Symptoms
  - Causes
  - Quick fix steps

**Actions:**
- **Copy Troubleshooting Template:** Copies issue report template to clipboard:
  - What I was doing
  - Expected vs actual behavior
  - Paste `/health` + `/health/ready` diagnostics
  - Paste last sync log IDs (if applicable)
  - Browser/timestamp/user role
  - **NO secrets exposed**

**Use Cases:**
- Quick reference for common issues
- Generate structured issue reports
- Link to full runbook documentation

---

### Navigation

**Ops Console Navigation Bar:**
- Status → `/ops/status`
- Sync → `/ops/sync`
- Runbook → `/ops/runbook`
- Channel Sync (external link) → `/channel-sync`

**Default Route:**
- Accessing `/ops` redirects to `/ops/status`

---

### Safety & Security

**Read-Only Operations:**
- All Ops Console pages are **read-only diagnostics**
- No dangerous actions (restart services, delete data, etc.)
- No configuration changes allowed

**No Secrets Exposed:**
- Copy Diagnostics does NOT include:
  - Environment variables
  - Database credentials
  - API keys
  - JWT tokens
  - User passwords
- Only exposes:
  - Public health check responses
  - User's own role (admin)
  - API base URL (already public)
  - Timestamps

**RBAC Enforcement:**
- Backend API endpoints still enforce RBAC
- Ops Console UI only provides convenient access
- Admin-only endpoints (like `/health/ready`) still require admin token

---

### How to Use "Copy Diagnostics"

**When to use:**
- Investigating system issues
- Reporting bugs to ops team
- Post-deployment health checks
- Troubleshooting sync failures

**Steps:**
1. Navigate to `/ops/status`
2. Click "Copy Diagnostics" button
3. Paste into issue tracker, Slack, or email
4. Diagnostics include full JSON from `/health` and `/health/ready`
5. Safe to share (no secrets included)

**Example Output:**
```json
{
  "timestamp": "2025-12-30T12:34:56.789Z",
  "api_base": "https://api.fewo.kolibri-visions.de",
  "user_role": "admin",
  "health": {
    "status": "healthy",
    "version": "1.0.0",
    "commit": "abc123def"
  },
  "health_ready": {
    "status": "degraded",
    "components": {
      "db": "ok",
      "redis": "error"
    }
  }
}
```

---

### Troubleshooting Ops Console Access

#### Issue: "Access Denied" Message Although User is Admin

**Symptom:**
- Admin UI shows "Access Denied" error page when opening `/ops`
- Message: "Ops Console is restricted to administrators only"
- Page shows "Access Check Diagnostics" panel with user details
- User verified as admin in database via PostgREST/psql

**Root Cause:**
The frontend now queries the `team_members` table directly (using `user_id` column) instead of relying on JWT metadata. Common reasons for denial:
1. RLS policy blocks the query (user can't read their own team_members row)
2. `is_active=false` in team_members table
3. User ID mismatch (auth.users.id vs team_members.user_id)
4. Static page cache (old access check cached from before role was granted)

**Diagnostics Panel (New in v2):**
The Access Denied page now shows detailed diagnostics:
- User ID and Email
- Team Members Found count (should be ≥1 for valid users)
- Resolved Role (what role was found in team_members)
- Last Active Agency ID (from profiles table)
- Error message (Supabase error or "No active team_members record found")

**Verify Database State with PostgREST:**
```bash
# Replace with your actual JWT token and Supabase URL
export JWT="eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9..."
export SUPABASE_URL="https://your-project.supabase.co"

# Check team_members (should return at least one row with role='admin')
curl -s "${SUPABASE_URL}/rest/v1/team_members?user_id=eq.USER_ID_HERE&select=user_id,agency_id,role,is_active" \
  -H "apikey: YOUR_ANON_KEY" \
  -H "Authorization: Bearer ${JWT}" | jq

# Expected result:
# [{"user_id":"8036f477-...","agency_id":"ffd0123a-...","role":"admin","is_active":true}]

# Check profiles for last_active_agency_id
curl -s "${SUPABASE_URL}/rest/v1/profiles?id=eq.USER_ID_HERE&select=id,last_active_agency_id" \
  -H "apikey: YOUR_ANON_KEY" \
  -H "Authorization: Bearer ${JWT}" | jq

# Expected result:
# [{"id":"8036f477-...","last_active_agency_id":"ffd0123a-..."}]
```

**Fix Steps:**

1. **Check Diagnostics Panel First:**
   - Look at "Team Members Found" count
   - If 0: User has no active team_members row (see step 2)
   - If ≥1: Check "Resolved Role" (see step 3)
   - If error message shown: See step 4

2. **If Team Members Found = 0:**
   ```sql
   -- Check if row exists but is_active=false
   SELECT user_id, agency_id, role, is_active
   FROM public.team_members
   WHERE user_id = 'USER_ID_HERE';

   -- If exists but is_active=false, activate it
   UPDATE public.team_members
   SET is_active = true
   WHERE user_id = 'USER_ID_HERE';

   -- If no row exists at all, insert one
   INSERT INTO public.team_members (user_id, agency_id, role, is_active)
   VALUES ('USER_ID_HERE', 'AGENCY_ID_HERE', 'admin', true);
   ```

3. **If Resolved Role ≠ 'admin':**
   ```sql
   -- Update role to admin
   UPDATE public.team_members
   SET role = 'admin'
   WHERE user_id = 'USER_ID_HERE'
     AND is_active = true;
   ```

4. **If Error Message Mentions Supabase Error/RLS:**
   - Check RLS policies on `team_members` table
   - User must be able to SELECT their own rows:
     ```sql
     -- Example RLS policy (adjust for your schema)
     CREATE POLICY "Users can view their own team_members"
     ON public.team_members
     FOR SELECT
     USING (auth.uid() = user_id);
     ```
   - Verify Supabase anon key has read access to team_members

5. **If Still Denied After Database Fix:**
   - Hard refresh browser (Ctrl+Shift+R / Cmd+Shift+R)
   - Clear browser cache
   - Logout and login again
   - Check browser console for JavaScript errors
   - Try incognito/private mode to rule out cache

6. **Verify Fix Worked:**
   - Refresh `/ops` page
   - Diagnostics should now show:
     - Team Members Found: ≥1
     - Resolved Role: admin
     - No error message
   - Should redirect to Ops Console status page

#### Issue: "Ops Console is Disabled" Message

**Symptom:**
- Admin UI shows "Ops Console is Disabled" error page
- Message explains that `NEXT_PUBLIC_ENABLE_OPS_CONSOLE` must be set
- Shows accepted values: `1`, `true`, `yes`, `on` (case-insensitive)

**Cause:**
- Feature flag `NEXT_PUBLIC_ENABLE_OPS_CONSOLE` is not set, OR
- Set to a falsy value like `0`, `false`, empty string, etc.

**Fix:**
1. Go to Coolify Dashboard → pms-admin → Build Pack Variables (Environment Variables)
2. Add or update: `NEXT_PUBLIC_ENABLE_OPS_CONSOLE` = `1`
3. Redeploy pms-admin application
4. Wait for deployment to complete (~2-3 minutes)
5. Refresh browser (hard refresh: Ctrl+Shift+R / Cmd+Shift+R)

**Note:** The feature flag now accepts multiple truthy values for flexibility:
- `1` (recommended, works in all contexts)
- `true`
- `yes`
- `on`

All values are case-insensitive, so `TRUE`, `True`, `YES`, etc. all work.

#### Issue: Still Shows Auth Loading Skeleton

**Symptom:**
- Opening `/ops` shows "Loading..." indefinitely
- Never transitions to error page or Ops Console

**Cause:**
- Auth context not initializing properly
- JWT token validation hanging

**Fix:**
1. Check browser console for JavaScript errors
2. Verify JWT_SECRET is set correctly in backend
3. Logout and login again
4. Clear browser cache and cookies
5. Try in incognito/private mode to rule out cache issues

---

### Known Limitations

1. **No Real-Time Monitoring:**
   - Status page requires manual "Refresh" button clicks
   - No WebSocket/SSE auto-updates
   - No alerts or notifications

2. **No Historical Data:**
   - Health checks show current state only
   - No time-series graphs or trends
   - No component uptime statistics

3. **No Celery Worker Control:**
   - Cannot restart workers from UI
   - Cannot view active task queue
   - Cannot cancel running tasks

4. **No Log Streaming:**
   - Ops Console does not stream backend/worker logs
   - Must use Coolify Dashboard for log viewing

5. **Feature Flag is Frontend-Only:**
   - Backend API endpoints are always available (if deployed)
   - Ops Console just gates UI access
   - RBAC on backend still enforces admin-only access

---

### Related Sections

- [Channel Manager API Endpoints](#channel-manager-api-endpoints) - API documentation
- [Celery Worker Troubleshooting](#celery-worker-pms-worker-v2-start-verify-troubleshoot) - Worker issues
- [DB DNS / Degraded Mode](#db-dns--degraded-mode) - Database connectivity

---

## Admin UI Authentication Verification

### Overview

The Admin UI (frontend) uses cookie-based SSR authentication for the Ops Console at `/ops/*`. After login, the session must be stored in HTTP cookies (not just localStorage) so that server-side components can validate access.

### Verify Cookie-Based Auth (curl)

These checks verify that the server login endpoint (`/auth/login`) properly sets session cookies, and that the Ops Console respects those cookies.

**Prerequisites:**
- Admin user credentials (e.g., `test1@example.com` with password `12345678`)
- User must have `role='admin'` and `is_active=true` in `public.team_members` table

#### 1. Unauthenticated Access (Expect 307 Redirect)

```bash
# Check that /ops/status redirects to login when not authenticated
curl -sS -I https://admin.fewo.kolibri-visions.de/ops/status | sed -n '1,30p'

# Expected output:
# HTTP/2 307
# location: /login?next=%2Fops%2Fstatus
# ...
```

**What this verifies:**
- Server-side layout properly checks for session cookies
- Unauthenticated requests are redirected to `/login?next=...` (preserves original path)

#### 2. Login to Get Session Cookies

```bash
# Login via server endpoint to get session cookies
curl -sS -i -c /tmp/admin.cookies \
  -H 'Content-Type: application/json' \
  -d '{"email":"test1@example.com","password":"12345678","next":"/ops/status"}' \
  https://admin.fewo.kolibri-visions.de/auth/login | sed -n '1,60p'

# Expected output:
# HTTP/2 200
# set-cookie: sb-<project>-auth-token=...; Path=/; ...
# set-cookie: sb-<project>-auth-token-code-verifier=...; Path=/; ...
# ...
# {"success":true,"user":{"id":"...","email":"test1@example.com"},"next":"/ops/status"}
```

**What this verifies:**
- Server login endpoint (`/auth/login`) accepts POST JSON
- Sets Supabase session cookies (`sb-*-auth-token`)
- Returns success response with user info and next path
- Cookies are saved to `/tmp/admin.cookies` for subsequent requests

**Troubleshooting:**
- `401 Unauthorized`: Invalid credentials or user not found
- `500 Internal Server Error`: Check backend logs for Supabase connection issues
- No `set-cookie` headers: Check that `@supabase/ssr` is properly configured in route handler

#### 3. Authenticated Access (Expect 200 or 307 if Not Admin)

```bash
# Access /ops/status with session cookies
curl -sS -I -b /tmp/admin.cookies \
  https://admin.fewo.kolibri-visions.de/ops/status | sed -n '1,30p'

# Expected output (if user IS admin):
# HTTP/2 200
# ...
#
# Expected output (if user is NOT admin):
# HTTP/2 307
# location: /ops/status (may show Access Denied page, not redirect)
```

**What this verifies:**
- Server-side layout reads session from cookies
- Admin users get 200 OK (ops page renders)
- Non-admin users see Access Denied page (no redirect loop)

**Troubleshooting:**
- `307 to /login`: Session cookies expired or invalid, middleware didn't refresh
- `200 but user not admin`: Check `team_members` table for `role='admin'` and `is_active=true`

#### 4. Logout and Re-Verify (Expect 307 Redirect)

```bash
# Logout (clears session cookies)
curl -sS -I -b /tmp/admin.cookies \
  https://admin.fewo.kolibri-visions.de/auth/logout | sed -n '1,40p'

# Expected output:
# HTTP/2 307
# location: /login
# set-cookie: sb-<project>-auth-token=; Path=/; Expires=Thu, 01 Jan 1970...
# ...

# Verify cookies are invalid - should redirect to login
curl -sS -I -b /tmp/admin.cookies \
  https://admin.fewo.kolibri-visions.de/ops/status | sed -n '1,30p'

# Expected output:
# HTTP/2 307
# location: /login?next=%2Fops%2Fstatus
```

**What this verifies:**
- Logout route (`/auth/logout`) calls `supabase.auth.signOut()` server-side
- Session cookies are cleared (set to expired)
- Subsequent requests to `/ops/*` redirect to login (session invalidated)

**Troubleshooting:**
- Still get 200 after logout: Session not properly cleared, check `createSupabaseRouteHandlerClient()` implementation
- Cookies not expired: Check that route handler sets cookie expiry to past date

### Common Issues

**Issue**: After login, `/ops/status` still redirects to `/login`

**Cause**: Session stored in localStorage only, not cookies. Server components can't read localStorage.

**Fix**:
1. Verify login page calls `/auth/login` endpoint (not direct `supabase.auth.signInWithPassword`)
2. Check that `/auth/login` route handler uses `createSupabaseRouteHandlerClient()`
3. Verify middleware is active for `/ops/*` routes (refreshes session cookies)

---

**Issue**: Login works in Channel Sync but not Ops Console

**Cause**: Split auth storage - Channel Sync uses client localStorage, Ops uses SSR cookies.

**Fix**: Both should use the same cookie-based auth (`/auth/login` endpoint).

---

**Issue**: curl shows 200 but browser shows "Loading..." forever

**Cause**: Client-side hydration waiting for localStorage session, which doesn't exist.

**Fix**: Remove any client-side auth checks in Ops pages. All auth should be server-side in layout.

---

## Additional Resources
---

## Admin UI: Bookings & Properties Lists

### Overview

The Admin UI provides real list pages for `/bookings` and `/properties` with search, filtering, pagination, and error handling. These pages replace the previous "coming soon" placeholders.

### Bookings List Page

**URL**: `https://admin.fewo.kolibri-visions.de/bookings`

**Requires**: JWT authentication (session cookie or Authorization header)

**Features**:
- **Table view** with columns: Referenz, Check-in, Check-out, Status, Preis, Erstellt
- **Search** (client-side): Filters by booking_reference, property_id, or guest_id
- **Status filter** dropdown: All, Angefragt (requested), In Prüfung (under_review), Anfrage (inquiry), Ausstehend (pending), Bestätigt (confirmed), Eingecheckt (checked_in), Ausgecheckt (checked_out), Storniert (cancelled), Abgelehnt (declined)
- **Pagination**: 50 items per page with "Zurück" / "Weiter" buttons
- **Row click**: Navigate to `/bookings/{id}` detail page
- **Loading state**: Spinner with "Lade Buchungen..."
- **Error states**:
  - 401: "Session abgelaufen. Bitte melden Sie sich erneut an."
  - 403: "Zugriff verweigert. Sie haben keine Berechtigung, Buchungen anzuzeigen."
  - 503: "Service vorübergehend nicht verfügbar. Bitte versuchen Sie es später erneut."
- **Empty state**: "Keine Buchungen gefunden" with hint about Public Booking Requests / Channel Manager Sync

**API Endpoint**: `GET /api/v1/bookings?limit=50&offset=0`

**Response Format**: Supports both array and `{ items: [], total: number, limit: number, offset: number }`

### Properties List Page

**URL**: `https://admin.fewo.kolibri-visions.de/properties`

**Requires**: JWT authentication

**Features**:
- **Table view** with columns: Name, Interner Name, Status, ID, Erstellt
- **Search** (client-side): Filters by internal_name, name, title, or id
- **Pagination**: 50 items per page
- **Loading/Error/Empty states**: Same pattern as bookings list
- **No detail page link**: Properties detail page not yet implemented

**API Endpoint**: `GET /api/v1/properties?limit=50&offset=0`

**Response Format**: Same as bookings (supports array or object with items)

### Booking Detail Page

**URL**: `https://admin.fewo.kolibri-visions.de/bookings/{id}`

**Features**:
- **Status badges** with color coding:
  - "requested" → Blue (bg-blue-100 text-blue-800)
  - "under_review" → Purple (bg-purple-100 text-purple-800)
  - "confirmed" → Green
  - "pending" → Yellow
  - "cancelled" → Red
  - "checked_in" → Indigo
  - "checked_out" → Gray
- **Grid layout** with sections: Aufenthaltsinformationen, Preisinformationen, IDs und Referenzen, Zeitstempel, Notizen
- **Guest link**: "Zum Gast →" button if guest_id exists and guest record found
- **Error handling**: 401/403/404/503 with clear German messages

**API Endpoint**: `GET /api/v1/bookings/{id}`

### Browser Verification Steps

**Step 1: Login to Admin UI**

```bash
# Open browser
open https://admin.fewo.kolibri-visions.de/login

# Login with admin credentials
# Email: test1@example.com
# Password: 12345678
```

**Step 2: Navigate to Properties**

```bash
# Click "Objekte" in sidebar navigation
# Or directly: https://admin.fewo.kolibri-visions.de/properties

# Expected:
# - Table loads with property list (not "Objekte kommt bald" placeholder)
# - Columns show: Name, Interner Name, Status, ID, Erstellt
# - Search field works (type partial name, table filters client-side)
# - Pagination buttons enabled if > 50 properties
```

**Step 3: Navigate to Bookings**

```bash
# Click "Buchungen" in sidebar navigation
# Or directly: https://admin.fewo.kolibri-visions.de/bookings

# Expected:
# - Table loads with booking list (not "Buchungen kommt bald" placeholder)
# - Columns show: Referenz, Check-in, Check-out, Status, Preis, Erstellt
# - Status filter dropdown works (select "Angefragt" → shows only requested bookings)
# - Search field works (type booking_reference → filters results)
# - Pagination buttons enabled if > 50 bookings
```

**Step 4: Click on a Booking Row**

```bash
# Click any booking row in the table

# Expected:
# - Navigate to /bookings/{id} detail page
# - Page loads booking details (no "Failed to fetch" error)
# - Status badge displays correctly (e.g., "requested" shows blue badge)
# - All sections render: Aufenthaltsinformationen, Preisinformationen, etc.
# - "Zurück zur Buchungsliste" link works
```

### Troubleshooting

**Problem**: Bookings or Properties page shows "Keine ... gefunden" (empty state) despite data in database

**Root Causes**:
1. **API returns empty array**: Backend query filters results (e.g., agency_id mismatch, RLS policy)
2. **Client-side filters too restrictive**: Search query or status filter excludes all results
3. **Pagination offset too high**: Requesting offset=500 when only 50 records exist

**Solution**:
```bash
# 1. Check browser DevTools Network tab
# Look for: GET /api/v1/bookings?limit=50&offset=0
# Response should be: { items: [...], total: N } or [...]
# If items is empty, issue is backend

# 2. Verify JWT agency_id claim matches data in database
# Open browser console: localStorage.getItem('supabase.auth.token')
# Decode JWT (jwt.io): Check agency_id claim
# Query database: SELECT COUNT(*) FROM bookings WHERE agency_id = '<claim-value>'

# 3. Check RLS policies
# Database console:
SELECT * FROM pg_policies WHERE tablename = 'bookings';
# Ensure policy allows current user's role and agency_id

# 4. Reset filters in UI
# Clear search field (click X button)
# Set status filter to "Alle Status"
# Reset pagination to page 1
```

**Problem**: Booking detail page returns HTTP 500 with ResponseValidationError

**Root Cause**: Booking status value in database not in allowed Literal types (e.g., "requested" not in schema)

**Solution**: Fixed in backend commit cb8da7f - Extended BookingStatus Literal to include "requested" and "under_review"

**Verification**:
```bash
# Check deployed commit
curl -s https://api.fewo.kolibri-visions.de/api/v1/ops/version | jq -r .source_commit

# Should be cb8da7f or later (2026-01-07+)
# If earlier commit, trigger deployment or wait for auto-deploy
```

**Problem**: Properties or Bookings page shows "Session abgelaufen" (401 error)

**Root Cause**: JWT token expired or not present in request

**Solution**:
```bash
# 1. Check if cookies are set
# Browser DevTools → Application → Cookies → admin.fewo.kolibri-visions.de
# Should see: sb-*-auth-token cookies

# 2. If no cookies, re-login via /login page
# Login sets new session cookies

# 3. If cookies exist but still 401, check token validity
# Network tab → Request Headers → Authorization: Bearer <token>
# If no Authorization header, check that apiClient uses accessToken from useAuth()

# TOKEN SANITY CHECK:
# - TOKEN must be access_token (not refresh_token)
# - Expected length: ~616 characters
# - JWT parts: 3 (header.payload.signature)
# - When calling Kong auth endpoints (e.g., Supabase token endpoint at sb-pms.kolibri-visions.de):
#   Include "apikey" header with anon/service_role key
# - Verify JWT: echo "$TOKEN" | cut -d. -f2 | base64 -d 2>/dev/null | jq .
#   Should include: sub (user_id), email, role, agency_id (for multi-tenant)

# 4. Check CORS configuration
# Network tab → Failed request → Response Headers
# If missing Access-Control-Allow-Origin, backend CORS misconfigured
# See: [CORS Errors](#cors-errors-admin-console-blocked)
```

**Problem**: Properties or Bookings page shows "Service vorübergehend nicht verfügbar" (503 error)

**Root Cause**: Backend database unavailable or schema drift

**Solution**:
```bash
# 1. Check backend health
curl -s https://api.fewo.kolibri-visions.de/health/ready | jq .

# Expected: {"status": "up", "components": {"db": {"status": "up"}, ...}}
# If db: "down", check [DB DNS / Degraded Mode](#db-dns--degraded-mode)

# 2. Check backend logs
docker logs pms-backend --tail 100 | grep -i "error\|503"

# Look for: "Database unavailable", "Schema not installed", "Relation does not exist"
# If schema errors, see [Schema Drift](#schema-drift)

# 3. Verify backend container is running
docker ps | grep pms-backend

# If not running, restart via Coolify UI or docker start
```

### PROD Verified (2026-01-07)

**Deployed Commit:** 17448496c88810a32be44bc76b2ca36dac87f072

**Verification Evidence:**
```bash
# HOST-SERVER-TERMINAL
export API_BASE_URL="https://api.fewo.kolibri-visions.de"

# Verify deployed commit
curl -s "$API_BASE_URL/api/v1/ops/version" | jq -r .source_commit
# Output: 17448496c88810a32be44bc76b2ca36dac87f072

# Verify bookings list endpoint
curl -k -sS -i -H "Authorization: Bearer $TOKEN" \
  "$API_BASE_URL/api/v1/bookings?limit=1&offset=0" | head -20
# Output: HTTP/2 200
# Body includes: items array with booking objects

# Verify properties list endpoint
curl -k -sS -i -H "Authorization: Bearer $TOKEN" \
  "$API_BASE_URL/api/v1/properties?limit=1&offset=0" | head -20
# Output: HTTP/2 200
# Body includes: items array with property objects

# Verify CORS headers present
curl -sS -i -H "Origin: https://admin.fewo.kolibri-visions.de" \
  -H "Authorization: Bearer $TOKEN" \
  "$API_BASE_URL/api/v1/bookings?limit=1&offset=0" | grep -i access-control-allow-origin
# Output: access-control-allow-origin: https://admin.fewo.kolibri-visions.de
```

**Backend Started At:** 2026-01-07T19:13:03.928023+00:00

**Browser Verification:**
- ✅ https://admin.fewo.kolibri-visions.de/bookings shows real table (not "Buchungen kommt bald" placeholder)
- ✅ https://admin.fewo.kolibri-visions.de/properties shows real table (not "Objekte kommt bald" placeholder)
- ✅ Search, filtering, and pagination work as expected
- ✅ Booking detail page displays status='requested' with blue badge

**Result:** ✅ Admin UI list pages verified in production - real tables with full functionality

### Related Sections

- [Admin UI Authentication Verification](#admin-ui-authentication-verification) - For cookie-based SSR auth checks
- [CORS Errors (Admin Console Blocked)](#cors-errors-admin-console-blocked) - For CORS configuration
- [Booking Status Validation Error (500)](#booking-status-validation-error-500) - For status field validation errors
- [Schema Drift](#schema-drift) - For database schema issues


### Smoke Scripts

**Location**: `/app/scripts/` (in container)

The Phase 23 smoke script (`pms_phase23_smoke.sh`) provides quick confidence checks for post-deployment validation. It includes two **opt-in tests** for advanced inventory/availability validation:

#### AVAIL_BLOCK_TEST (Availability Block Conflict Test)

**What it does:**
- Creates a future availability block (today+30 days, 3-day duration)
- Verifies block appears in `/api/v1/availability` response
- Attempts to create overlapping booking (expects 409 `inventory_overlap`)
- Deletes the block for cleanup

**Usage:**
```bash
# In Coolify container terminal or via docker exec
export ENV_FILE=/root/pms_env.sh
export AVAIL_BLOCK_TEST=true
bash /app/scripts/pms_phase23_smoke.sh
```

**Safety:**
- Uses future dates (30+ days out) to avoid production conflicts
- Always cleans up (block deletion via trap, runs even on failure)

#### B2B_TEST (Back-to-Back Booking Boundary Test)

**What it does:**
- Scans for a 4-day free gap in the future (today+60 to today+150)
- Creates booking A: D → D+2 (2 nights)
- Creates booking B: D+2 → D+4 (2 nights, check-in = A's check-out)
- Expects both HTTP 201 (confirms end-exclusive date semantics)
- Cancels both bookings via PATCH `status=cancelled`

**Usage:**
```bash
# In Coolify container terminal or via docker exec
export ENV_FILE=/root/pms_env.sh
export B2B_TEST=true
bash /app/scripts/pms_phase23_smoke.sh
```

**Safety:**
- Uses far-future dates (60+ days out) to avoid production conflicts
- Always cleans up (booking cancellation via trap, runs even on failure)
- Uses PATCH cancel instead of DELETE (DELETE /bookings returns 405)

**When to Use:**
- After schema migrations affecting availability/inventory tables
- After deployment of conflict detection logic changes
- Pre-production validation before go-live
- NOT recommended for CI/CD or frequent monitoring (creates/deletes data)

#### Phase 23 Status Summary (2026-01-04)

**Test Status:** PASS (all required + optional tests)

**Core Tests (Always Run):**
- Health endpoints (`/health`, `/health/ready`) - HEAD and GET methods
- OpenAPI schema availability (`/openapi.json`)
- JWT authentication (token fetch from auth service)
- Authenticated API access (properties, bookings, availability)

**Optional Test 8 (AVAIL_BLOCK_TEST=true):**
- **Status:** PASS
- **Expectation:** Availability block overlap MUST return HTTP 409 with `conflict_type=inventory_overlap`
- **Enforcement:** Script FAILS (exit 1) if wrong conflict_type, even when HTTP 409 is correct
- **Semantic rule:** Block overlap → `inventory_overlap`, booking overlap → `double_booking`

**Optional Test 9 (B2B_TEST=true):**
- **Status:** PASS
- **Validation:** Back-to-back bookings allowed (check-in = previous check-out)
- **Confirms:** End-exclusive date semantics `[check_in, check_out)` working correctly

**Known Issues Resolved:**
- ✓ Conflict type detection: Fixed NameError (undefined `conn` variable) that caused HTTP 500
  - **Symptom:** Booking creation returned 500 instead of 409 when overlapping block
  - **Root cause:** Wrong database handle in pre-check (`conn` instead of `self.db`)
  - **Fix:** Use `self.db.fetchrow()` for availability block queries
- ✓ Frontend auto-detect: Connection selection now auto-derives Platform and Property fields
- ✓ Batch details: Duration display shows `duration_ms` with fallback logic

**API Response Shape Notes:**
- Some list endpoints return raw JSON array: `[{...}, {...}]`
- Others return object with items: `{"items": [...], "total": N, "has_more": bool}`
- **Best practice:** Always verify JSON shape before parsing in shell scripts
- **Example:** Use `python3 -c 'data = json.load(sys.stdin); items = data if isinstance(data, list) else data.get("items", [])'`

**Redirect/Trailing Slash:**
- Some endpoints may redirect (307/302) on trailing slash mismatch
- **Best practice:** Use `curl -L` in scripts to follow redirects automatically
- **Avoid:** Parsing empty/non-JSON bodies from redirect responses

**Full Documentation**: `/app/scripts/README.md` (in container)

#### Ops Note (2026-01-10): Phase 23 smoke script executable + PROD evidence

**Context:** Fixed `pms_phase23_smoke.sh` permission issues that caused rc=126 "Permission denied" on fresh checkouts.

**PROD Deployment:**
- **Commit:** `1aeb740bfe676a7a148be5ef17910755c3630b99`
- **Started at:** `2026-01-10T13:54:04.070027+00:00`
- **Deploy verification:** `pms_verify_deploy.sh` rc=0 with commit prefix match
- **API endpoint:** `https://api.fewo.kolibri-visions.de/api/v1/ops/version`

**Smoke Test Results:**
```bash
# HOST-SERVER-TERMINAL
./backend/scripts/pms_phase23_smoke.sh
# rc=0 (all tests PASS)
```

**Optional Tests (enabled):**
- `AVAIL_BLOCK_TEST=true` PASS
  - Availability block successfully prevents overlapping booking
  - Returns HTTP 409 with `conflict_type=inventory_overlap` as expected
- `B2B_TEST=true` PASS
  - Back-to-back bookings succeed (check-in = previous check-out)
  - Confirms end-exclusive date semantics `[check_in, check_out)` working correctly

**Fix Details:**
- **Issue:** File mode was 100644 (non-executable), causing rc=126 when running `./backend/scripts/pms_phase23_smoke.sh`
- **Solution:** Changed git file mode to 100755 (executable)
- **Now supported:** Direct execution without `bash` prefix:
  ```bash
  ./backend/scripts/pms_phase23_smoke.sh
  ```
- **Fallback (old checkouts):** If you still see rc=126, use:
  ```bash
  bash ./backend/scripts/pms_phase23_smoke.sh
  # OR
  chmod +x ./backend/scripts/pms_phase23_smoke.sh
  ```

**Verification Commands:**
```bash
# Check deployed commit
curl -sS https://api.fewo.kolibri-visions.de/api/v1/ops/version | jq -r '.source_commit, .started_at'

# Run deploy verification
export API_BASE_URL="https://api.fewo.kolibri-visions.de"
./backend/scripts/pms_verify_deploy.sh
# Expected: rc=0 with commit match

# Run smoke test with optional tests
export ENV_FILE=/root/pms_env.sh
export AVAIL_BLOCK_TEST=true
export B2B_TEST=true
./backend/scripts/pms_phase23_smoke.sh
echo "rc=$?"
# Expected: rc=0 (all tests PASS including opt-in tests)
```

#### Ops Note (2026-01-10): Expired JWT_TOKEN Auto-Refresh

**Context:** Smoke scripts now automatically detect and refresh expired JWT tokens to prevent auth failures.

**How It Works:**
1. When `JWT_TOKEN` is provided (3-part JWT), scripts extract and check the `exp` claim
2. If token is expired or near-expired (≤30 seconds):
   - **With fallback credentials** (`EMAIL`, `PASSWORD`, `SB_URL`, `ANON_KEY`): Automatically fetches fresh token
   - **Without fallback credentials**: Fails with actionable error message
3. Logs show token status for debugging:
   - Valid token: `Auth: JWT_TOKEN (provided, seconds_left=3600)`
   - Expired with refresh: `Auth: JWT_TOKEN expired/near-expired (seconds_left=-120) → fetching fresh token via EMAIL/PASSWORD`
   - Missing exp claim: `JWT_TOKEN missing exp claim, treating as valid`

**Benefits:**
- Prevents auth failures due to stale tokens in long-running automation
- Allows seamless operation without manual token refresh
- Clear error messages when auto-refresh not possible

**Affected Scripts:**
- `pms_phase20_final_smoke.sh`
- `pms_phase21_inventory_hardening_smoke.sh`
- `pms_phase23_smoke.sh`

**Error Handling:**
If you see "JWT_TOKEN is expired/near-expired" error:
```bash
# Option 1: Unset JWT_TOKEN to use EMAIL/PASSWORD auth
unset JWT_TOKEN
./backend/scripts/pms_phase23_smoke.sh

# Option 2: Provide fallback credentials for auto-refresh
export EMAIL="your-email@example.com"
export PASSWORD="your-password"
export SB_URL="https://your-project.supabase.co"
export ANON_KEY="your-anon-key"
export JWT_TOKEN="your-expired-token"
./backend/scripts/pms_phase23_smoke.sh  # Will auto-refresh
```

**Fix (2026-01-10):** The exp check was updated to be nounset-safe (bash `set -u` compatible) by using a python3 helper that receives the full JWT token as an argument and returns either "NOEXP" (if exp claim missing/invalid) or the integer seconds_left. Tokens without exp claim are accepted with a warning. This prevents "unbound variable" crashes in scripts using `set -u`.

### Other Resources

- **Inventory Contract** (Single Source of Truth): `/app/docs/domain/inventory.md` (date semantics, API contracts, edge cases, DB guarantees, test evidence)
- **Inventory & Availability Rules**: `/app/docs/database/exclusion-constraints.md` (conflict rules, EXCLUSION constraints, overlap prevention)
- **Modular Monolith Architecture**: `/app/docs/architecture/modules.md` (module system, registry, dependency management)
- **Architecture Docs**: `/app/docs/architecture/` (in container)
- **Supabase Dashboard**: Check database health, logs, network
- **Coolify Dashboard**: Application logs, environment variables, networks

---


---


## Admin UI Visual Style (Backoffice Theme v1)

### Overview

The Admin UI uses a modern "Backoffice Theme v1" inspired by Paperpillar dashboard design. The theme features a soft neutral background (#E8EFEA), white cards with generous radius, icon-only sidebar with dark active states, and a cohesive green-purple-beige color system.

### Theme v1 Palette

**Base Colors**:
- #121212 #201F23 #45515C #596269 #FFFFFF

**Green Palette** (Primary actions, success states):
- #395917 (dark green) #4C6C5A (primary) #617C6C #A4C8AE #E8EFEA (background)

**Purple Palette** (Accents, borders):
- #595D75 (accent) #BBBED5 (light) #E3E4EA (borders)

**Additional Accents**:
- Beige: #A39170 #E5D6B8
- Tosca: #C1DBDA
- Red: #9B140B (danger)

### Theme Tokens

**CSS Variables** (defined in `frontend/app/globals.css`):
- `--bo-bg`: #E8EFEA (soft neutral background - lightest green)
- `--bo-card`: #FFFFFF (white cards)
- `--bo-border`: #E3E4EA (subtle borders - light purple)
- `--bo-text`: #121212 (primary text - darkest base)
- `--bo-text-muted`: #596269 (muted text)
- `--bo-primary`: #4C6C5A (primary green for actions)
- `--bo-success`: #A4C8AE (success states)
- `--bo-danger`: #9B140B (danger/error states)
- `--bo-accent`: #595D75 (purple accents)
- `--bo-shadow-soft`: Soft shadows for pills/buttons
- `--bo-shadow`: Standard card shadows
- `--bo-shadow-md`: Medium elevation shadows
- `--bo-radius-lg`: 1.5rem (24px) for main cards
- `--bo-radius-full`: 9999px for pills/circles

**Typography**:
- Headings: Plus Jakarta Sans (via `font-heading` class) - fallback for General Sans
- Body: Inter (via `font-sans` class)
- Hierarchy: H1 = 2xl-3xl, H2 = xl, Body = base/sm

### Design Patterns

**Shell Layout**:
- Background: Soft neutral (`bg-bo-bg` - #E8EFEA, lightest green)
- Sidebar: Icon-only by default, pill container with `rounded-bo-lg`, white background, soft shadow
- Topbar: Transparent background with greeting header ("Hello, User!"), pill search input, circular icon buttons
- Content area: Generous padding (p-6 to p-8), cards with large radius

**Navigation**:
- Icon backgrounds: Circular (`w-10 h-10 rounded-full`) with light purple background
- Active state: Dark circle (#121212 - darkest base) with white icon and shadow
- Inactive state: Light purple background, hover transitions to lighter purple
- Expandable sidebar: Shows labels when expanded, icon-only when collapsed

**Cards & Tables**:
- Cards: White (`bg-bo-card`), `rounded-bo-lg` (24px), subtle border (#E3E4EA), soft shadow
- Table rows: Hover effect with `hover:bg-bo-surface-2`
- Status badges: `rounded-full` pills with semantic colors
- Card spacing: Generous internal padding (p-6 to p-12)

**Form Elements**:
- Inputs: Pill style (`rounded-full`), white background, light purple border, soft shadow
- Buttons: Primary uses green (`bg-bo-primary`), rounded-xl to rounded-2xl, soft shadows
- Focus rings: Use primary green color
- Search bar: Pill-shaped with icon, integrated in topbar

### Browser Verification

**Visual Checklist**:
```bash
# Navigate to Admin UI
open https://admin.fewo.kolibri-visions.de/login

# After login, verify Theme v1 styling:
1. Background is soft neutral (#E8EFEA - lightest green)
2. Sidebar is icon-only vertical layout (left side)
3. Navigation icons have circular backgrounds (w-10 h-10 rounded-full)
4. Active nav icon has dark circle (#121212) with white icon
5. Inactive nav icons have light background (bg-bo-surface-2)
6. Topbar has white background with pill-shaped search input
7. Topbar shows round icon buttons (notifications, profile)
8. Cards are white (#FFFFFF) with soft shadows (shadow-bo-soft)
9. Cards have large rounded corners (rounded-bo-lg or rounded-bo-xl)
10. Status badges are pill-shaped with semantic colors
11. Buttons are rounded-full with primary green (#4C6C5A)
12. Text colors: primary #121212, muted #596269
13. All text uses Inter font (next/font/google optimization)
14. Good contrast on all interactive elements

# Test pages for Theme v1:
- /dashboard         → White cards on soft green background
- /bookings          → Table with new color palette
- /bookings/{id}     → Detail page with info cards
- /properties        → Table with search filter
- /properties/{id}   → Detail page with multiple sections
- /channel-sync      → Sync dashboard with connection cards
- /guests            → Guest list
- /connections       → Connection management
```

### Troubleshooting

**Problem**: Fonts not loading or fallback to system fonts

**Solution**:
```bash
# 1. Check browser network tab for font download
# Inter font should load from fonts.gstatic.com

# 2. Hard refresh to clear cache
# Browser: Cmd+Shift+R (Mac) / Ctrl+Shift+R (Windows)

# 3. Verify font variables in DevTools
# Elements → <body> → Should see --font-inter variable
# Theme v1 uses Inter for all text (body and headings)

# 4. Check Next.js font optimization
# Inter is loaded via next/font/google with automatic preloading
```

**Problem**: CSS variables not applied (colors look wrong)

**Solution**:
```bash
# 1. Check if globals.css is loaded
# DevTools → Network → Filter CSS → Should see globals.css

# 2. Verify Theme v1 CSS variable values in DevTools
# DevTools → Elements → :root → Styles panel
# Should see Backoffice Theme v1 variables:
# --bo-bg: #E8EFEA
# --bo-card: #FFFFFF
# --bo-text: #121212
# --bo-primary: #4C6C5A
# ... (all Theme v1 palette variables)

# 3. Hard refresh browser cache
# Cmd+Shift+R (Mac) / Ctrl+Shift+R (Windows)

# 4. Check Tailwind config extension
# Ensure tailwind.config.ts extends theme.colors.bo with new utilities
```

**Problem**: Components still show old styling

**Solution**:
```bash
# 1. Check if page was updated to Theme v1
# View source → Search for "bg-bo-card", "text-bo-text", "rounded-bo-lg"
# Should NOT see old classes like "bg-white", "text-gray-*"

# 2. Clear Next.js cache and rebuild
cd frontend && rm -rf .next && npm run dev

# 3. Verify deployment includes Theme v1 commit
# Check git log for "ui: backoffice theme v1 (dashboard style)"
```

**Problem**: Rounded corners too aggressive / not matching design

**Solution**:
```bash
# Adjust CSS variables in frontend/app/globals.css
# --bo-radius-xl: 2rem → Reduce to 1.5rem for less rounding
# --bo-radius: 1rem → Adjust for standard cards

# Then rebuild frontend
cd frontend && npm run build
```

### Related Sections

- [Admin UI Authentication Verification](#admin-ui-authentication-verification)
- [Admin UI: Bookings & Properties Lists](#admin-ui-bookings--properties-lists)
- [Admin UI: Booking & Property Detail Pages](#admin-ui-booking--property-detail-pages)


## Admin UI Layout Polish v2.1 (Profile + Language + Sidebar Polish)

### Overview

Layout v2.1 adds comprehensive polish to the Admin UI with language switcher (RTL support), profile dropdown, and improved sidebar collapsed state. This builds on Theme v2 (blue/indigo palette, Lucide icons).

### Key Features

**Language Switcher**:
- Flag icons for DE/EN/AR in topbar (right side)
- Dropdown shows on HOVER (not just click)
- Persists selection in localStorage (`bo_lang` key)
- Sets `document.documentElement.lang` to 'de'|'en'|'ar'
- Sets `document.documentElement.dir` to 'rtl' for Arabic, 'ltr' for others
- Supports internationalization scaffolding for future i18n

**Profile Dropdown**:
- User icon button in topbar (right side)
- Shows user name (extracted from email if needed) and role
- Menu links:
  - Profil (`/profile`)
  - Profil bearbeiten (`/profile/edit`)
  - Sicherheit (`/profile/security`)
  - Abmelden (`performLogout()` - client-side signOut with redirect to /login)
- Stub pages created with AdminShell layout

**Sidebar Collapsed Polish**:
- Logo properly centered when collapsed (no clipping)
- Toggle button more visible: border, background, shadow
- All icons consistently centered in collapsed mode
- Tooltips show labels on hover when collapsed
- Scrollbar hidden but scroll functional (`scrollbar-hide` utility)
- No animation jank on route changes (no transitions on desktop)

### Implementation Details

**Files Modified**:
- `frontend/app/components/AdminShell.tsx` - Main shell component
  - Added `useEffect` to set document.lang and dir based on language state
  - Changed language dropdown from onClick to onMouseEnter/onMouseLeave
  - Centered logo in collapsed mode with conditional justify-center
  - Enhanced toggle button styling with border/bg/shadow

**Files Created**:
- `frontend/app/profile/page.tsx` - Profile stub page
- `frontend/app/profile/edit/page.tsx` - Edit profile stub page
- `frontend/app/profile/security/page.tsx` - Security settings stub page
- `frontend/app/profile/layout.tsx` - Profile section layout with auth

### Browser Verification

**Language Switcher**:
```bash
# 1. Open Admin UI in browser
open https://admin.fewo.kolibri-visions.de/dashboard

# 2. Verify language dropdown
# - Topbar right side shows current flag (🇩🇪 DE by default)
# - HOVER over flag button → dropdown appears with DE/EN/AR options
# - Click different language → page updates, selection persists on reload

# 3. Check RTL support for Arabic
# DevTools → Elements → <html>
# Should see: lang="ar" dir="rtl" when Arabic selected
# Should see: lang="de" dir="ltr" when German selected
# Should see: lang="en" dir="ltr" when English selected

# 4. Check localStorage persistence
# DevTools → Application → Local Storage
# Should see: bo_lang = "de"|"en"|"ar"
```

**Profile Dropdown**:
```bash
# 1. Click user icon in topbar (right side, after notifications)
# Dropdown appears with user info header (name + role)

# 2. Verify menu items
# - "Profil" link → navigates to /profile
# - "Profil bearbeiten" link → navigates to /profile/edit
# - "Sicherheit" link → navigates to /profile/security

# 3. Check profile pages
# All pages should:
# - Use AdminShell layout with sidebar
# - Show "Demnächst verfügbar" placeholder
# - Require authentication (redirect to /login if not logged in)
```

**Sidebar Collapsed State**:
```bash
# 1. Collapse sidebar using toggle button at bottom
# DevTools → Application → Local Storage
# Should see: sidebar-collapsed = "true"

# 2. Verify collapsed appearance
# - Logo centered in header (not clipped on sides)
# - All navigation icons centered (40px × 40px containers)
# - Toggle button has visible border and background
# - Sidebar width reduced to w-24 (96px)

# 3. Test tooltips
# Hover over any nav icon when collapsed
# Should see tooltip with item label (e.g., "Dashboard", "Buchungen")

# 4. Verify no scrollbar visible
# Sidebar should scroll if content exceeds height
# But scrollbar should be hidden (scrollbar-hide utility)

# 5. Test route changes
# Navigate between pages (Dashboard → Bookings → Properties)
# Sidebar should NOT animate/transition (no jank)
# Only content area updates
```

### Troubleshooting

**Problem**: Language dropdown doesn't show on hover

**Solution**:
```bash
# 1. Check JavaScript enabled in browser
# 2. Verify React hydration completed (no console errors)
# 3. Test with onClick as fallback:
# - Click flag button → dropdown should appear
# 4. Check browser event listeners in DevTools
# Elements → Language dropdown div → Event Listeners
# Should see: mouseenter, mouseleave
```

**Problem**: RTL not working for Arabic

**Solution**:
```bash
# 1. Verify document.documentElement.dir is set
# DevTools → Elements → <html> → Should see dir="rtl"

# 2. Check if CSS supports RTL
# Most Tailwind utilities are LTR-only by default
# May need to add RTL-specific CSS or use logical properties

# 3. Verify language state is "ar"
# DevTools → React DevTools → AdminShell component
# Should see: language = "ar"
```

**Problem**: Logo clipped in collapsed sidebar

**Solution**:
```bash
# 1. Check if conditional centering is applied
# DevTools → Elements → Brand header div
# When collapsed: should have "justify-center" class
# When expanded: should have "gap-3" class

# 2. Verify sidebar width
# Collapsed: w-24 (96px) - Logo is 48px, fits with padding
# Expanded: w-72 (288px)

# 3. Check logo flexbox
# Logo container should have: flex-shrink-0
```

**Problem**: Profile pages return 404

**Solution**:
```bash
# 1. Verify pages exist
ls -la frontend/app/profile/
# Should see: page.tsx, edit/, security/, layout.tsx

# 2. Rebuild Next.js
cd frontend && rm -rf .next && npm run dev

# 3. Check authentication
# Profile pages require auth via layout.tsx
# If not logged in → redirects to /login with returnTo parameter
```

### Related Sections

- [Admin UI Visual Style (Backoffice Theme v1)](#admin-ui-visual-style-backoffice-theme-v1)
- [Admin UI Authentication Verification](#admin-ui-authentication-verification)


## Admin UI Static Verification (Smoke Test)

### Overview

Automated smoke test that verifies Admin UI deployment and expected UI content without requiring authentication. Checks that critical UI components (language switcher, logout menu) are present in the deployed JavaScript bundles.

### Script

**Location**: `backend/scripts/pms_admin_ui_static_smoke.sh`

**Execution**: HOST-SERVER-TERMINAL (where Coolify/Docker is running)

**Purpose**: Verify that:
1. Admin UI container is running with expected SOURCE_COMMIT
2. `/login` endpoint returns HTTP 200
3. Next.js static chunks contain expected UI strings (Abmelden, Deutsch, English, العربية)

### Usage

```bash
# Basic usage (default: https://admin.fewo.kolibri-visions.de)
./backend/scripts/pms_admin_ui_static_smoke.sh

# With custom base URL
ADMIN_BASE_URL=https://admin.example.com ./backend/scripts/pms_admin_ui_static_smoke.sh

# With expected commit verification
EXPECTED_COMMIT=abc1234567 ./backend/scripts/pms_admin_ui_static_smoke.sh

# Increase chunk crawl limit if expected strings not found
MAX_CHUNKS=150 ./backend/scripts/pms_admin_ui_static_smoke.sh

# Debug mode: preserve temp directory with downloaded chunks
KEEP_TEMP=true ./backend/scripts/pms_admin_ui_static_smoke.sh

# Full configuration
ADMIN_BASE_URL=https://admin.example.com CONTAINER_NAME=pms-admin EXPECTED_COMMIT=abc1234567 MAX_CHUNKS=80 KEEP_TEMP=false ./backend/scripts/pms_admin_ui_static_smoke.sh
```

### Environment Variables

| Variable | Default | Description |
|----------|---------|-------------|
| `ADMIN_BASE_URL` | `https://admin.fewo.kolibri-visions.de` | Base URL of Admin UI |
| `CONTAINER_NAME` | `pms-admin` | Docker container name to inspect |
| `EXPECTED_COMMIT` | (empty) | Expected SOURCE_COMMIT; accepts short prefix (e.g., 7 chars) or full 40-char SHA |
| `MAX_CHUNKS` | `80` | Maximum number of JS chunks to download during crawling |
| `KEEP_TEMP` | `false` | Set to `true` to preserve temp directory for debugging |

**Notes**:
- `EXPECTED_COMMIT` supports short SHA prefixes (e.g., `18d76f2`) for convenience; full 40-char SHA also accepted
- Script checks numeric HTTP status code (`%{http_code}`), so HTTP/2 200 responses work correctly
- **Scan modes**: Script uses container-scan mode when docker is available (scans built Next.js assets inside container at `.next/static`); this covers UI strings in authenticated routes not referenced by /login. Falls back to http-crawl mode (downloads chunks from public site) when docker unavailable.
- Script crawls chunk graph: downloads initial chunks from /login HTML, parses them for additional chunk URLs, downloads up to `MAX_CHUNKS` limit
- If expected strings not found, try increasing `MAX_CHUNKS` or use `KEEP_TEMP=true` to inspect downloaded chunks

### Expected Output (PASS)

```
======================================================================
Admin UI Static Smoke Test
======================================================================

[INFO] Configuration:
[INFO]   ADMIN_BASE_URL: https://admin.fewo.kolibri-visions.de
[INFO]   CONTAINER_NAME: pms-admin
[INFO]   EXPECTED_COMMIT: 0572b72f059a71dc280c564a194dd279d9a7ab6d
[INFO]   MAX_CHUNKS: 80
[INFO]   KEEP_TEMP: false

[INFO] Step 1: Checking Docker container...
[INFO]   Container SOURCE_COMMIT: 0572b72f059a71dc280c564a194dd279d9a7ab6d
[INFO]   ✓ Commit matches expected: 0572b72f059a71dc280c564a194dd279d9a7ab6d

[INFO] Step 2: Checking https://admin.fewo.kolibri-visions.de/login ...
[INFO]   ✓ HTTP 200 OK

[INFO] Step 3: Searching for UI strings (mode: container-scan)...
[INFO]   Found Next.js assets at: /app/.next/static
[INFO]   ✓ Found 'Abmelden' in container:/app/.next/static/chunks/app-layout-1a2b3c4d5e.js
[INFO]   ✓ Found 'Deutsch' in container:/app/.next/static/chunks/app-layout-1a2b3c4d5e.js
[INFO]   ✓ Found 'English' in container:/app/.next/static/chunks/app-layout-1a2b3c4d5e.js
[INFO]   ✓ Found 'العربية' in container:/app/.next/static/chunks/app-layout-1a2b3c4d5e.js

[INFO] Summary: Found 4/4 expected strings

======================================================================
[INFO] ✓ PASS: All checks passed
======================================================================
```

Exit code: `0`

### Production Verification Procedure

To mark Admin UI features as **VERIFIED** in project_status.md:

1. **Collect commit hash** from Coolify deployment or Docker:
   ```bash
   docker inspect pms-admin --format '{{range .Config.Env}}{{println .}}{{end}}' | grep '^SOURCE_COMMIT='
   ```

2. **Run smoke script** with expected commit:
   ```bash
   EXPECTED_COMMIT=abc1234567 ./backend/scripts/pms_admin_ui_static_smoke.sh
   ```

3. **Verify exit code** is `0` and output shows all checks passing

4. **Update project_status.md**: Change status from "IMPLEMENTED (NOT VERIFIED)" to "VERIFIED" and add verification evidence:
   ```markdown
   **Status**: ✅ VERIFIED

   **Verification Evidence** (HOST-SERVER-TERMINAL):
   - Date: 2026-01-08
   - Container: pms-admin
   - SOURCE_COMMIT: 0572b72f059a71dc280c564a194dd279d9a7ab6d
   - Smoke script: pms_admin_ui_static_smoke.sh rc=0
   - All expected UI strings found in static bundles
   ```

### Troubleshooting

**Problem**: Script fails with "No chunk URLs found in HTML"

**Solution**:
```bash
# 1. Check if Next.js is using different build output structure (find chunk URLs manually)
curl -k -sS https://admin.fewo.kolibri-visions.de/login | grep -oE '/_next/static/[^"]+\.js' | head -5

# 2. Download first chunk and verify content structure
CHUNK="$(curl -k -sS https://admin.fewo.kolibri-visions.de/login | grep -oE '/_next/static/[^"]+\.js' | head -1)" && echo "Chunk: $CHUNK" && curl -k -sS "https://admin.fewo.kolibri-visions.de${CHUNK}" | head -20

# 3. Check if Next.js version changed (different chunk structure)
docker exec pms-admin sh -c "cat package.json | grep '\"next\"'"
```

**Problem**: Script fails with "Missing expected strings"

**Solution**:
```bash
# 1. Download first chunk manually and search for expected strings
CHUNK="$(curl -k -sS https://admin.fewo.kolibri-visions.de/login | grep -oE '/_next/static/[^"]+\.js' | head -1)" && curl -k -sS "https://admin.fewo.kolibri-visions.de${CHUNK}" -o /tmp/chunk.js && grep -E 'Abmelden|Deutsch|English|العربية' /tmp/chunk.js

# 2. If strings not found, check if deployment is correct commit
docker inspect pms-admin --format '{{range .Config.Env}}{{println .}}{{end}}' | grep '^SOURCE_COMMIT='

# 3. Check if UI code was actually deployed (cache issue) - force browser hard refresh: Cmd+Shift+R (Mac) or Ctrl+Shift+R (Windows)

# 4. If still missing, check if feature was actually merged
git log --oneline | grep -Ei 'logout|language|abmelden' | head -20
```

**Problem**: Commit mismatch error

**Solution**:
```bash
# 1. Check what commit is actually deployed
docker inspect pms-admin --format '{{range .Config.Env}}{{println .}}{{end}}' | grep '^SOURCE_COMMIT='

# 2. Check Coolify deployment logs (via Coolify UI or docker logs pms-admin)

# 3. If Coolify hasn't picked up latest commit yet: trigger manual redeploy in Coolify or wait for automatic deployment

# 4. Verify local repo is up to date
git fetch origin main && git log origin/main --oneline -5
```

### Related Sections

- [Admin UI Layout Polish v2.1 (Profile + Language + Sidebar Polish)](#admin-ui-layout-polish-v21-profile--language--sidebar-polish)
- [Admin UI Authentication Verification](#admin-ui-authentication-verification)

---

## Phase 21 — Availability Hardening Verification

### Overview

Phase 21 production hardening for Availability API endpoints. Validates availability query, block creation/deletion, overlap conflict detection (409), and proper error handling.

### Script

**Location**: `backend/scripts/pms_availability_phase21_smoke.sh`

**Execution**: HOST-SERVER-TERMINAL

**Purpose**: Verify that:
1. Availability query works (GET /api/v1/availability)
2. Block creation succeeds with 201 (POST /api/v1/availability/blocks)
3. Overlapping block returns 409 conflict (DB EXCLUSION constraint working)
4. Block read works (GET /api/v1/availability/blocks/{block_id})
5. Block deletion succeeds with 204 (DELETE /api/v1/availability/blocks/{block_id})
6. Deleted block returns 404 (verification)

### Usage

```bash
# Basic usage (requires JWT_TOKEN and PID)
JWT_TOKEN="eyJhbG..." PID="550e8400-e29b-..." ./backend/scripts/pms_availability_phase21_smoke.sh

# With custom API URL
API_BASE_URL=https://api.test.example.com JWT_TOKEN="eyJhbG..." PID="550e8400-..." ./backend/scripts/pms_availability_phase21_smoke.sh

# With custom date range
JWT_TOKEN="eyJhbG..." PID="550e8400-..." AVAIL_FROM=2026-02-01 AVAIL_TO=2026-02-08 ./backend/scripts/pms_availability_phase21_smoke.sh
```

### Environment Variables

| Variable | Default | Description |
|----------|---------|-------------|
| `JWT_TOKEN` | (required) | Valid JWT token with admin or manager role |
| `PID` | (required) | Property ID (UUID) to test with |
| `API_BASE_URL` | `https://api.fewo.kolibri-visions.de` | API base URL |
| `AVAIL_FROM` | tomorrow (YYYY-MM-DD) | Block start date |
| `AVAIL_TO` | tomorrow + 7 days (YYYY-MM-DD) | Block end date |

**Notes**:
- Script uses future dates by default to avoid past-date validation errors
- Property must exist and belong to authenticated user's agency
- Script creates test block, verifies overlap protection (409), then cleans up via DELETE
- Safe to run multiple times (idempotent: creates + deletes block each run)
- All HTTP status codes validated: 200 (query/read), 201 (create), 204 (delete), 404 (not found), 409 (conflict)

### Expected Output (PASS)

```
======================================================================
Availability API Phase 21 - Smoke Test
======================================================================

Configuration:
  API_BASE_URL: https://api.fewo.kolibri-visions.de
  PID: 550e8400-e29b-41d4-a716-446655440000
  AVAIL_FROM: 2026-01-09
  AVAIL_TO: 2026-01-16
  JWT_TOKEN: <set (hidden)>

[TEST 1] Query availability for property
  ✓ PASS: HTTP 200 - availability query successful
  ✓ PASS: Response has valid structure (property_id, from_date, to_date, ranges)

[TEST 2] Create availability block
  ✓ PASS: HTTP 201 - block created successfully
  ✓ PASS: Block ID extracted: 123e4567-e89b-12d3-a456-426614174000

[TEST 3] Create overlapping block (expect 409 conflict)
  ✓ PASS: HTTP 409 - overlap correctly rejected

[TEST 4] Read single availability block
  ✓ PASS: HTTP 200 - block retrieved successfully
  ✓ PASS: Block ID matches created block

[TEST 5] Delete availability block (cleanup)
  ✓ PASS: HTTP 204 - block deleted successfully

[TEST 6] Verify block deletion (expect 404)
  ✓ PASS: HTTP 404 - block no longer exists (verified)

======================================================================
Test Summary
======================================================================

Tests run:    6
Tests passed: 6
Tests failed: 0

  ✓ PASS: ✓ ALL TESTS PASSED
```

Exit code: `0`

### Troubleshooting

**Problem**: Test 1 fails with 401 Unauthorized

**Solution**:
```bash
# 1. Verify JWT_TOKEN is valid and not expired
echo $JWT_TOKEN | cut -d'.' -f2 | base64 -d 2>/dev/null | jq .exp

# 2. Check JWT has admin or manager role
echo $JWT_TOKEN | cut -d'.' -f2 | base64 -d 2>/dev/null | jq .role

# 3. Get fresh token from /api/v1/auth/login
curl -X POST https://api.fewo.kolibri-visions.de/api/v1/auth/login -H "Content-Type: application/json" -d '{"email":"admin@example.com","password":"..."}'
```

**Problem**: Test 1 fails with 404 Property not found

**Solution**:
```bash
# 1. Verify property exists and belongs to your agency
curl -H "Authorization: Bearer $JWT_TOKEN" https://api.fewo.kolibri-visions.de/api/v1/properties

# 2. Use a valid PID from the response
PID=<valid-property-uuid> JWT_TOKEN="..." ./backend/scripts/pms_availability_phase21_smoke.sh
```

**Problem**: Test 3 fails - overlap not rejected (expected 409, got 201)

**Cause**: Database EXCLUSION constraint not working or migration not applied

**Solution**:
```bash
# 1. Verify btree_gist extension enabled
psql $DATABASE_URL -c "SELECT * FROM pg_extension WHERE extname='btree_gist';"

# 2. Verify EXCLUSION constraint exists
psql $DATABASE_URL -c "SELECT conname, pg_get_constraintdef(oid) FROM pg_constraint WHERE conname='inventory_ranges_no_overlap';"

# 3. Re-apply migration if missing
psql $DATABASE_URL -f supabase/migrations/20251225190000_availability_inventory_system.sql
```

**Problem**: Test 2 fails with 422 Validation Error (invalid date range)

**Cause**: AVAIL_FROM is in the past or AVAIL_TO <= AVAIL_FROM

**Solution**:
```bash
# Use explicit future dates
AVAIL_FROM=2026-06-01 AVAIL_TO=2026-06-08 JWT_TOKEN="..." PID="..." ./backend/scripts/pms_availability_phase21_smoke.sh
```

**Problem**: Test 5 fails with 503 Service Unavailable

**Cause**: Database temporarily unavailable or connection pool exhausted

**Solution**:
```bash
# 1. Check database connectivity
psql $DATABASE_URL -c "SELECT 1;"

# 2. Check API health
curl https://api.fewo.kolibri-visions.de/health

# 3. Wait 30 seconds and retry (automatic retry with exponential backoff already in API)
```

### Production Verification Procedure

To mark Phase 21 as **VERIFIED** in project_status.md:

1. **Get JWT token** from authenticated session or login endpoint
2. **Get valid PID** from properties list:
   ```bash
   curl -H "Authorization: Bearer $JWT_TOKEN" https://api.fewo.kolibri-visions.de/api/v1/properties | jq -r '.items[0].id'
   ```
3. **Run smoke script**:
   ```bash
   JWT_TOKEN="eyJhbG..." PID="550e8400-..." ./backend/scripts/pms_availability_phase21_smoke.sh
   ```
4. **Verify exit code** is `0` and all 6 tests pass
5. **Update project_status.md**: Change status from "IMPLEMENTED" to "VERIFIED" and add verification evidence

### Related Sections

- [Availability API Documentation](#availability-api) (if exists)
- [Database Schema - Availability Tables](#database-schema) (if exists)

**Auth Note (2026-01-10)**: Smoke scripts (`pms_phase20_final_smoke.sh`, `pms_phase21_inventory_hardening_smoke.sh`, `pms_phase23_smoke.sh`) now support `JWT_TOKEN` directly (preferred) OR `EMAIL`+`PASSWORD` (auto-fetch JWT). If `JWT_TOKEN` is set (must be 3-part JWT), scripts use it directly without requiring Supabase auth credentials.

**Ops Note (2026-01-10)**: `pms_phase23_smoke.sh` is executable in git (mode 100755). If you see "Permission denied" (rc=126), use fallback: `bash ./backend/scripts/pms_phase23_smoke.sh` or fix permissions: `chmod +x ./backend/scripts/pms_phase23_smoke.sh`.

---

## Admin UI: Booking & Property Detail Pages

✅ **Verified in PROD on 2026-01-07** (source_commit a22da6660b7ad24a309429249c1255e575be37bc, smoke script exit code 0)

**Autodiscovery Note**:
- Autodiscovery requires list endpoints (`GET /api/v1/bookings?limit=1&offset=0`, `GET /api/v1/properties?limit=1&offset=0`) to return valid JSON with at least one item.
- If autodiscovery fails (empty database, auth error, session termination), bypass with explicit IDs:
  ```bash
  BID=your-booking-id PID=your-property-id TOKEN=... ./backend/scripts/pms_admin_detail_endpoints_smoke.sh
  ```
- For troubleshooting details, see [Scripts README: Troubleshooting Autodiscovery](../../scripts/README.md#troubleshooting-autodiscovery).

### Overview

The Admin UI provides detail pages for individual bookings and properties. These pages fetch full entity data via single-item GET endpoints and display comprehensive information.

### Booking Detail Page

**URL**: `https://admin.fewo.kolibri-visions.de/bookings/{id}`

**API Endpoint**: `GET /api/v1/bookings/{id}`

**Requires**: JWT authentication (session cookie or Authorization header)

**Features**:
- **Header**: booking_reference, status badge (includes "requested" and "under_review")
- **Dates & Stay**: check_in, check_out, num_nights
- **Guest Info**: guest_id with link to `/guests/{guest_id}` (handles null guest gracefully)
- **Price Breakdown**: nightly_rate, subtotal, cleaning_fee, service_fee, tax, total_price, currency
- **IDs**: booking id, property_id, guest_id, channel_booking_id (if present)
- **Metadata**: created_at, updated_at
- **Special Requests / Internal Notes**: displayed if present
- **Navigation**: "← Zurück zur Buchungsliste" link
- **Error States**: German messages for 401, 403, 404, 503 with retry button

### Property Detail Page

**URL**: `https://admin.fewo.kolibri-visions.de/properties/{id}`

**API Endpoint**: `GET /api/v1/properties/{id}`

**Requires**: JWT authentication

**Features**:
- **Header**: internal_name/name/title, status badge (aktiv/inaktiv/gelöscht)
- **Address**: address_line1/2, postal_code, city, country
- **Capacity**: max_guests, bedrooms, beds, bathrooms
- **Times**: check_in_time, check_out_time
- **Pricing**: base_price, cleaning_fee, currency, min_stay, booking_window_days
- **IDs**: property id, agency_id
- **Metadata**: created_at, updated_at, deleted_at (if soft-deleted)
- **Navigation**: "← Zurück zur Objektliste" link
- **Error States**: German messages for 401, 403, 404, 503 with retry button

### Browser Verification Steps

**Step 1: Verify Booking Detail**

```bash
# Login to Admin UI
open https://admin.fewo.kolibri-visions.de/login
# Login with admin credentials

# Navigate to bookings list
open https://admin.fewo.kolibri-visions.de/bookings

# Click on any booking row

# Expected:
# - Navigates to /bookings/{id} detail page
# - Page loads without "Failed to fetch" error
# - Status badge shows correct color:
#   - "requested" → blue
#   - "under_review" → purple
#   - "confirmed" → green
#   - "pending" → yellow
#   - "cancelled" → red
# - Guest section handles null guest gracefully (shows guest_id + link, no crash)
# - Price breakdown shows all fields with correct formatting
# - Retry button appears if error occurs
```

**Step 2: Verify Property Detail**

```bash
# Navigate to properties list
open https://admin.fewo.kolibri-visions.de/properties

# Click on any property row

# Expected:
# - Navigates to /properties/{id} detail page
# - Page loads without error
# - Status badge shows: Aktiv (green) / Inaktiv (gray) / Gelöscht (red)
# - Address section shows all available address fields
# - Capacity and pricing sections display available data
# - "—" shown for missing optional fields
# - Retry button appears if error occurs
```

### Troubleshooting

**Problem**: Detail page returns "Session abgelaufen. Bitte melden Sie sich erneut an." (401)

**Root Cause**: JWT token expired or missing

**Solution**:
```bash
# 1. Check if session cookie exists
# Browser DevTools → Application → Cookies → admin.fewo.kolibri-visions.de
# Should see: sb-*-auth-token cookies

# 2. Re-login via /login page if cookies missing or expired
open https://admin.fewo.kolibri-visions.de/login

# 3. If cookies exist but still 401, check token in Authorization header
# Network tab → Request Headers → Authorization: Bearer <token>
# TOKEN must be access_token (not refresh_token)
# Expected length: ~616 characters
# JWT parts: 3 (header.payload.signature)

# 4. Verify JWT claims:
# Open browser console:
const token = localStorage.getItem('supabase.auth.token');
# Decode at jwt.io - should include: sub, email, role, agency_id
```

**Problem**: Detail page returns "Keine Berechtigung, dieses Objekt/diese Buchung anzuzeigen." (403)

**Root Cause**: User role lacks permission or agency_id mismatch

**Solution**:
```bash
# 1. Check user role in JWT claims
# Network tab → Response Headers from any API call
# Or decode TOKEN and check "role" claim

# 2. Verify RLS policies allow access
# Database console:
SELECT * FROM pg_policies WHERE tablename IN ('bookings', 'properties');
# Ensure policy allows current user's role and agency_id

# 3. Check agency_id in JWT matches entity's agency_id
# Decode JWT → check agency_id claim
# Compare with entity: SELECT agency_id FROM bookings WHERE id = '...';
```

**Problem**: Detail page returns "Objekt/Buchung nicht gefunden." (404)

**Root Cause**: Entity doesn't exist or was soft-deleted

**Solution**:
```bash
# 1. Verify entity exists in database
# Database console:
SELECT id, deleted_at FROM bookings WHERE id = '...';
SELECT id, deleted_at FROM properties WHERE id = '...';

# 2. If soft-deleted (deleted_at NOT NULL), entity is hidden from detail view
# Admin UI doesn't show deleted entities by design

# 3. Check if ID was copied correctly from list page
# Compare URL: /bookings/{id} with database ID
```

**Problem**: Detail page returns "Service vorübergehend nicht verfügbar." (503)

**Root Cause**: Backend database unavailable or schema drift

**Solution**:
```bash
# 1. Check backend health
curl -s https://api.fewo.kolibri-visions.de/health/ready | jq .

# Expected: {"status": "up", "components": {"db": {"status": "up"}, ...}}
# If db: "down", see [DB DNS / Degraded Mode](#db-dns--degraded-mode)

# 2. Check backend logs for schema errors
docker logs pms-backend --tail 100 | grep -i "error\|503"

# Look for: "Relation does not exist", "column ... does not exist"
# If schema errors, see [Schema Drift](#schema-drift)

# 3. Verify detail endpoint works via curl
export TOKEN="..."
export API_BASE_URL="https://api.fewo.kolibri-visions.de"

curl -sS -i -H "Authorization: Bearer $TOKEN" \
  "$API_BASE_URL/api/v1/bookings/{id}" | head -20
# Expected: HTTP/2 200

curl -sS -i -H "Authorization: Bearer $TOKEN" \
  "$API_BASE_URL/api/v1/properties/{id}" | head -20
# Expected: HTTP/2 200
```

**Problem**: Booking detail returns "500 Internal Server Error" with "ResponseValidationError: cancelled_by"

✅ **Fixed and Verified in PROD on 2026-01-07** (source_commit a22da6660b7ad24a309429249c1255e575be37bc, smoke script exit code 0)

**Root Cause**: Legacy data in database has UUID values in `cancelled_by` field instead of expected actor enum ('guest', 'host', 'platform', 'system').

**Solution**:
```bash
# This is now fixed via backward-compatible normalization.
# The service layer automatically maps:
# - UUID values → actor='host', cancelled_by_user_id=<uuid>
# - Valid actors → preserved as-is
# - Invalid values → actor='system' (safe fallback)

# Verify fix is deployed:
curl -sS -H "Authorization: Bearer $TOKEN" \
  "$API_BASE_URL/api/v1/bookings/{id}" | jq -r '.cancelled_by, .cancelled_by_user_id'

# Expected output for legacy UUID data:
# "host"
# "8036f477-1234-5678-9abc-def012345678"

# Expected output for standard actor data:
# "guest"
# null

# If still seeing 500 errors, check backend logs for ResponseValidationError
docker logs pms-backend --tail 50 | grep -i "validationerror"
```

**Prevention**: All new cancellations should use actor enum values. The `cancelled_by_user_id` field preserves user identity when needed.

### Verification (SERVER-SIDE)

**Automated Smoke Test**:

```bash
# HOST-SERVER-TERMINAL

# Set TOKEN (obtain from Admin UI login or Supabase dashboard)
export TOKEN="eyJhbGc..."

# Run smoke test (auto-discovers booking and property IDs)
./backend/scripts/pms_admin_detail_endpoints_smoke.sh

# Expected output:
# [INFO] All tests passed! ✓
# [INFO] Booking detail endpoint: OK
# [INFO] Property detail endpoint: OK
# [INFO] CORS headers: OK

# Exit code 0 = success
# Exit code 1 = failure (404, 401, 403, or missing data)
# Exit code 2 = server error (500 - regression!)
```

**Manual API Verification**:

```bash
# HOST-SERVER-TERMINAL
export API_BASE_URL="https://api.fewo.kolibri-visions.de"
export TOKEN="..."

# Get booking ID from list
BID=$(curl -sS -H "Authorization: Bearer $TOKEN" \
  "$API_BASE_URL/api/v1/bookings?limit=1&offset=0" | jq -r '.items[0].id // .[0].id')

echo "Testing booking ID: $BID"

# Test booking detail endpoint
curl -sS -i -H "Authorization: Bearer $TOKEN" \
  "$API_BASE_URL/api/v1/bookings/$BID" | head -30

# Expected: HTTP/2 200
# Body includes: booking_reference, status, total_price, etc.

# Get property ID from list
PID=$(curl -sS -H "Authorization: Bearer $TOKEN" \
  "$API_BASE_URL/api/v1/properties?limit=1&offset=0" | jq -r '.items[0].id // .[0].id')

echo "Testing property ID: $PID"

# Test property detail endpoint
curl -sS -i -H "Authorization: Bearer $TOKEN" \
  "$API_BASE_URL/api/v1/properties/$PID" | head -30

# Expected: HTTP/2 200
# Body includes: internal_name, address fields, capacity, pricing, etc.
```

**Deploy Verification**:

```bash
# HOST-SERVER-TERMINAL

# Verify deployed commit
curl -s "$API_BASE_URL/api/v1/ops/version" | jq -r '.source_commit, .started_at'

# Run deploy verification script (checks commit match + modules)
EXPECT_COMMIT=<commit-sha> ./backend/scripts/pms_verify_deploy.sh

# Exit code 0 = commit matches and backend healthy
```

### Related Sections

- [Admin UI: Bookings & Properties Lists](#admin-ui-bookings--properties-lists) - List pages that link to these detail pages
- [Booking Status Validation Error (500)](#booking-status-validation-error-500) - For status field validation issues
- [Admin UI Authentication Verification](#admin-ui-authentication-verification) - For cookie-based SSR auth checks
- [CORS Errors (Admin Console Blocked)](#cors-errors-admin-console-blocked) - For CORS configuration

## Full Sync Batching (batch_id)

**Purpose:** Full Sync operations trigger 3 concurrent tasks (availability_update, pricing_update, bookings_sync) grouped by a shared `batch_id` for easier tracking and verification.

**Migration:** `supabase/migrations/20260101150000_add_batch_id_to_channel_sync_logs.sql`

### How It Works

When triggering a Full Sync via:
```bash
curl -X POST https://api.fewo.kolibri-visions.de/api/v1/channel-connections/{connection_id}/sync \
  -H "Authorization: Bearer YOUR_JWT_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"sync_type": "full"}'
```

**Response includes batch_id:**
```json
{
  "status": "triggered",
  "message": "Manual full sync triggered successfully",
  "task_ids": ["task_1", "task_2", "task_3"],
  "batch_id": "70bce471-d82a-4cd9-8ad3-8c9f2e5f4a11"
}
```

All 3 operations (availability_update, pricing_update, bookings_sync) share the same `batch_id`.

### Verification via API

**List all sync logs (includes batch_id):**
```bash
curl https://api.fewo.kolibri-visions.de/api/v1/channel-connections/{connection_id}/sync-logs \
  -H "Authorization: Bearer YOUR_JWT_TOKEN"
```

**Expected response:**
```json
{
  "connection_id": "...",
  "logs": [
    {
      "id": "...",
      "operation_type": "availability_update",
      "status": "success",
      "batch_id": "70bce471-d82a-4cd9-8ad3-8c9f2e5f4a11",
      "created_at": "2026-01-01T12:00:00Z",
      "task_id": "..."
    },
    {
      "id": "...",
      "operation_type": "pricing_update",
      "status": "success",
      "batch_id": "70bce471-d82a-4cd9-8ad3-8c9f2e5f4a11",
      "created_at": "2026-01-01T12:00:00Z",
      "task_id": "..."
    },
    {
      "id": "...",
      "operation_type": "bookings_sync",
      "status": "success",
      "batch_id": "70bce471-d82a-4cd9-8ad3-8c9f2e5f4a11",
      "created_at": "2026-01-01T12:00:00Z",
      "task_id": "..."
    }
  ]
}
```

**Filter logs by batch_id:**
```bash
curl "https://api.fewo.kolibri-visions.de/api/v1/channel-connections/{connection_id}/sync-logs?batch_id=70bce471-d82a-4cd9-8ad3-8c9f2e5f4a11" \
  -H "Authorization: Bearer YOUR_JWT_TOKEN"
```

### Verification via Database

**Count operations per batch:**
```sql
SELECT
  batch_id,
  COUNT(*) as operation_count,
  ARRAY_AGG(DISTINCT operation_type ORDER BY operation_type) as operations,
  ARRAY_AGG(DISTINCT status ORDER BY status) as statuses,
  MIN(created_at) as first_created,
  MAX(created_at) as last_created
FROM channel_sync_logs
WHERE batch_id IS NOT NULL
GROUP BY batch_id
ORDER BY first_created DESC
LIMIT 10;
```

**Expected output for Full Sync:**
```
              batch_id              | operation_count |                   operations                   | statuses | first_created | last_created
------------------------------------+-----------------+------------------------------------------------+----------+---------------+--------------
 70bce471-d82a-4cd9-8ad3-8c9f2e5f4a11 |               3 | {availability_update,bookings_sync,pricing_update} | {success}| 2026-01-01... | 2026-01-01...
```

**Find incomplete batches (not all 3 operations succeeded):**
```sql
SELECT
  batch_id,
  COUNT(*) as total_ops,
  COUNT(*) FILTER (WHERE status = 'success') as success_count,
  COUNT(*) FILTER (WHERE status = 'failed') as failed_count,
  ARRAY_AGG(operation_type || ':' || status) as op_statuses
FROM channel_sync_logs
WHERE batch_id IS NOT NULL
GROUP BY batch_id
HAVING COUNT(*) FILTER (WHERE status = 'success') < 3
ORDER BY MAX(created_at) DESC
LIMIT 10;
```

### UI Display

The Admin UI (`/connections` page) automatically groups Full Sync operations by `batch_id`:

**Batch Header Features:**
- **Collapsible indigo card** with expand/collapse arrow icon
- **Overall batch status badge** (Success/Failed/Running/Pending):
  - Green "Success" = all 3 operations succeeded
  - Red "Failed" = any operation failed
  - Blue "Running" = operations in progress
  - Gray "Pending" = operations queued/triggered
- **Batch ID display** with copy button (click to copy full UUID)
  - Shows shortened ID: `70bce471...`
  - Copies full UUID on click
- **Timestamp** from newest operation
- **Operation count** with filter indicator (e.g., "2 operations (filtered)" if filters active)
- **Operation badges** showing each operation with status-coded color:
  - Full labels without truncation (e.g., "availability update", "pricing update", "bookings sync")
  - Color matches status (green/red/blue/gray)

**Expanded Batch View:**
- Click batch header to expand/collapse
- Shows table with: Operation, Status, Error, Actions
- "Details" button opens log details modal
- Expansion state preserved across auto-refresh

**Filtering Behavior:**
- Filters (status/type) work with batched logs
- Batch visible if ANY operation matches filter
- Shows only matching operations inside batch
- Displays "(filtered)" indicator if fewer than 3 operations shown

**Unbatched Logs:**
- Logs without `batch_id` (old logs, manual single operations) appear in standard flat table
- Backward compatible with pre-migration logs

### Troubleshooting

**Problem:** API `/sync-logs` does not include `batch_id` field

**Diagnosis:**
```bash
# Check if migration applied
docker exec -it pms-db psql -U postgres -d postgres \
  -c "\d channel_sync_logs" | grep batch_id
```

**Expected:** `batch_id | uuid |`

**If missing:**
```bash
# Apply migration
docker exec -i pms-db psql -U postgres -d postgres \
  < supabase/migrations/20260101150000_add_batch_id_to_channel_sync_logs.sql
```

**Restart API after migration:**
```bash
docker restart pms-api
```

**Problem:** Old logs show `batch_id: null` in API response

**Cause:** Logs created before migration have `NULL` batch_id (expected, backward compatible)

**Verification:** Only logs created after migration + restart will have batch_id populated

---

## Batch Status Aggregation

**Purpose:** Query aggregated batch status for monitoring and UI display without fetching individual log entries.

**Endpoint:**
```
GET /api/v1/channel-connections/{connection_id}/sync-batches/{batch_id}
```

**Use Cases:**

- **Admin UI:** Display batch progress card with overall status badge (green/red/blue)
- **Monitoring:** Quick health check for batch completion without parsing logs
- **Debugging:** Identify which operation(s) in a batch failed
- **Dashboards:** Show batch completion rate (success vs. failed)

**Request Example:**

```bash
# Production
curl -k -sS https://api.fewo.kolibri-visions.de/api/v1/channel-connections/abc-123-def-456/sync-batches/70bce471-d82a-4cd9-8ad3-8c9f2e5f4a11 \
  -H "Authorization: Bearer $TOKEN" \
  | jq .

# Local (via Supabase auth)
TOKEN=$(curl -sX POST "$SB_URL/auth/v1/token?grant_type=password" \
  -H "apikey: $ANON_KEY" \
  -H "Content-Type: application/json" \
  -d '{"email":"admin@example.com","password":"password"}' \
  | jq -r '.access_token')

curl -sS "$API/api/v1/channel-connections/$CID/sync-batches/$BATCH_ID" \
  -H "Authorization: Bearer $TOKEN" \
  | jq .
```

**Response Example (Success):**

```json
{
  "batch_id": "70bce471-d82a-4cd9-8ad3-8c9f2e5f4a11",
  "connection_id": "abc-123-def-456",
  "batch_status": "success",
  "status_counts": {
    "triggered": 0,
    "running": 0,
    "success": 3,
    "failed": 0,
    "other": 0
  },
  "created_at_min": "2026-01-01T12:00:00Z",
  "updated_at_max": "2026-01-01T12:05:30Z",
  "operations": [
    {
      "operation_type": "availability_update",
      "status": "success",
      "updated_at": "2026-01-01T12:03:15Z"
    },
    {
      "operation_type": "pricing_update",
      "status": "success",
      "updated_at": "2026-01-01T12:04:20Z"
    },
    {
      "operation_type": "bookings_sync",
      "status": "success",
      "updated_at": "2026-01-01T12:05:30Z"
    }
  ]
}
```

**Response Example (Failed):**

```json
{
  "batch_id": "70bce471-d82a-4cd9-8ad3-8c9f2e5f4a11",
  "connection_id": "abc-123-def-456",
  "batch_status": "failed",
  "status_counts": {
    "triggered": 0,
    "running": 0,
    "success": 2,
    "failed": 1,
    "other": 0
  },
  "created_at_min": "2026-01-01T12:00:00Z",
  "updated_at_max": "2026-01-01T12:05:30Z",
  "operations": [
    {
      "operation_type": "availability_update",
      "status": "success",
      "updated_at": "2026-01-01T12:03:15Z"
    },
    {
      "operation_type": "pricing_update",
      "status": "failed",
      "updated_at": "2026-01-01T12:04:20Z"
    },
    {
      "operation_type": "bookings_sync",
      "status": "success",
      "updated_at": "2026-01-01T12:05:30Z"
    }
  ]
}
```

**Batch Status Logic:**

The `batch_status` field is derived from status counts:

| Condition | batch_status | Meaning |
|-----------|--------------|---------|
| `failed > 0` | `failed` | At least one operation failed (batch incomplete) |
| `running > 0` OR `triggered > 0` | `running` | Operations still in progress (no failures yet) |
| `success == total` AND `total > 0` | `success` | All operations completed successfully |
| None of above | `unknown` | No operations found or unexpected state |

**Priority:** Failed > Running > Success > Unknown

**Status Semantics: Queued vs. Triggered**

**Queued Status:**

Operations with `status = 'queued'` are **counted under the "Triggered" bucket** in `status_counts.triggered`. This ensures correct batch status when the Celery worker is offline or overloaded.

| Individual Status | Bucket in status_counts | Meaning |
|-------------------|------------------------|---------|
| `triggered` | `triggered` | Sync task created, waiting for worker pickup |
| `queued` | `triggered` | Task enqueued in Celery, worker not processing yet |
| `running` | `running` | Worker actively processing task |
| `success` | `success` | Task completed successfully |
| `failed` | `failed` | Task failed (exhausted retries or unrecoverable error) |
| Other | `other` | Unknown/unexpected status (should not occur) |

**Why Queued = Triggered:**

- **Semantically:** Both `queued` and `triggered` represent "in-progress but not yet running"
- **User Expectation:** Users expect batch_status="running" when sync is triggered, even if worker is offline
- **Prior Bug:** When worker was stopped, `queued` counted as `other`, causing batch_status="unknown" (misleading)

**Reproduction Steps (Worker Offline Scenario):**

1. **Stop worker:** `docker stop pms-worker-v2` (or systemctl stop)
2. **Trigger full sync:** POST `/api/v1/channel-connections/{id}/sync?sync_type=full`
   - API returns 200 with batch_id
   - 3 operations created with status="queued" (Celery task enqueued but not picked up)
3. **Check batch status:** GET `/api/v1/channel-connections/{id}/sync-batches/{batch_id}`
   - **Expected:** `batch_status="running"`, `status_counts.triggered=3` (queued counted as triggered)
   - **Before fix:** `batch_status="unknown"`, `status_counts.other=3` (misleading)
4. **Start worker:** `docker start pms-worker-v2`
   - Worker picks up queued tasks
   - Operations transition: `queued` → `running` → `success`/`failed`
5. **Final check:** Batch status becomes `success` (if all ops succeed) or `failed` (if any fail)

**SQL Implementation:**

```sql
-- Triggered count includes both triggered and queued
COUNT(*) FILTER (WHERE status IN ('triggered', 'queued')) AS triggered_count

-- Other count excludes queued
COUNT(*) FILTER (WHERE status NOT IN ('triggered', 'queued', 'running', 'success', 'failed')) AS other_count

-- Batch status derivation treats triggered>0 as running
CASE
  WHEN COUNT(*) FILTER (WHERE status = 'failed') > 0 THEN 'failed'
  WHEN COUNT(*) FILTER (WHERE status = 'running') > 0
    OR COUNT(*) FILTER (WHERE status IN ('triggered', 'queued')) > 0 THEN 'running'
  WHEN COUNT(*) FILTER (WHERE status = 'success') = COUNT(*) AND COUNT(*) > 0 THEN 'success'
  ELSE 'unknown'
END AS batch_status
```

**Use in Admin UI:**

```javascript
// Fetch batch status
const response = await fetch(
  `/api/v1/channel-connections/${connectionId}/sync-batches/${batchId}`,
  { headers: { Authorization: `Bearer ${token}` } }
);
const batch = await response.json();

// Display badge based on batch_status
const badgeColor = {
  success: 'green',
  failed: 'red',
  running: 'blue',
  unknown: 'gray'
}[batch.batch_status];

// Show progress: "2/3 operations completed"
const completed = batch.status_counts.success + batch.status_counts.failed;
const total = Object.values(batch.status_counts).reduce((a, b) => a + b, 0);
```

**Performance:**

- **Single SQL query** with CTEs (batch_aggregation + operations_list)
- **Efficient aggregation** using `COUNT(*) FILTER (WHERE ...)` (PostgreSQL 9.4+)
- **Scoped by connection_id** (prevents cross-tenant leaks)
- **Indexed columns:** `batch_id`, `connection_id`, `status`, `created_at`

**Error Responses:**

```bash
# 404 - Batch not found
{
  "error": "batch_not_found",
  "message": "No sync operations found for batch_id=... and connection_id=..."
}

# 503 - Schema not installed
{
  "error": "service_unavailable",
  "message": "Channel sync logs schema not installed..."
}

# 401 - Not authenticated
{
  "detail": "Not authenticated"
}
```

**Monitoring Examples:**

```bash
# Check if batch completed successfully
BATCH_STATUS=$(curl -sS "$API/api/v1/channel-connections/$CID/sync-batches/$BATCH_ID" \
  -H "Authorization: Bearer $TOKEN" | jq -r '.batch_status')

if [[ "$BATCH_STATUS" == "success" ]]; then
  echo "✅ Batch completed successfully"
  exit 0
elif [[ "$BATCH_STATUS" == "failed" ]]; then
  echo "❌ Batch failed - check logs"
  exit 1
elif [[ "$BATCH_STATUS" == "running" ]]; then
  echo "⏳ Batch still in progress"
  exit 2
else
  echo "❓ Unknown batch status"
  exit 3
fi

# Get failed operations count
FAILED_COUNT=$(curl -sS "$API/api/v1/channel-connections/$CID/sync-batches/$BATCH_ID" \
  -H "Authorization: Bearer $TOKEN" | jq -r '.status_counts.failed')

if [[ "$FAILED_COUNT" -gt 0 ]]; then
  echo "⚠️  $FAILED_COUNT operation(s) failed in batch"
fi
```

**Related:**

- See [Full Sync Batching (batch_id)](#full-sync-batching-batch_id) for batch grouping concept
- See [GET /sync-logs](#get-sync-logs) for individual log entries
- See [Admin UI - Channel Manager Operations](#admin-ui--channel-manager-operations) for UI usage

---

## List Sync Batches

**Purpose:** List recent sync batches for a connection with pagination and optional status filtering. Used by Admin UI "Sync history" page.

**Endpoint:**
```
GET /api/v1/channel-connections/{connection_id}/sync-batches
```

**Query Parameters:**
- `limit` (int, default: 50, max: 200): Number of batches to return
- `offset` (int, default: 0): Offset for pagination
- `status` (optional string): Filter by batch status
  - Omit or `any`: Return all batches regardless of status
  - `running`: Return batches where any operation is triggered or running (and none failed)
  - `failed`: Return batches where any operation failed
  - `success`: Return batches where all operations are success (and none failed/running/triggered)

**Sorting:**
- Newest first by `updated_at_max` (most recently updated batch)
- Falls back to `created_at_min` for batches with no updates

**Use Cases:**

- **Admin UI:** Display sync history with pagination
- **Monitoring:** Find recent failed batches for alerting
- **Debugging:** Track sync operations over time
- **Dashboards:** Show sync success rate and trends

**Request Examples:**

```bash
# Production - List first 20 batches
curl -k -sS https://api.fewo.kolibri-visions.de/api/v1/channel-connections/abc-123-def-456/sync-batches?limit=20&offset=0 \
  -H "Authorization: Bearer $TOKEN" \
  | jq .

# Production - List only failed batches
curl -k -sS https://api.fewo.kolibri-visions.de/api/v1/channel-connections/abc-123-def-456/sync-batches?status=failed \
  -H "Authorization: Bearer $TOKEN" \
  | jq .

# Production - Pagination (second page, 50 items per page)
curl -k -sS https://api.fewo.kolibri-visions.de/api/v1/channel-connections/abc-123-def-456/sync-batches?limit=50&offset=50 \
  -H "Authorization: Bearer $TOKEN" \
  | jq .

# Local (via Supabase auth)
TOKEN=$(curl -sX POST "$SB_URL/auth/v1/token?grant_type=password" \
  -H "apikey: $ANON_KEY" \
  -H "Content-Type: application/json" \
  -d '{"email":"admin@example.com","password":"password"}' \
  | jq -r '.access_token')

curl -sS "$API/api/v1/channel-connections/$CID/sync-batches?status=running" \
  -H "Authorization: Bearer $TOKEN" \
  | jq .
```

**Response Example:**

```json
{
  "items": [
    {
      "batch_id": "70bce471-d82a-4cd9-8ad3-8c9f2e5f4a11",
      "connection_id": "abc-123-def-456",
      "batch_status": "success",
      "status_counts": {
        "triggered": 0,
        "running": 0,
        "success": 3,
        "failed": 0,
        "other": 0
      },
      "created_at_min": "2026-01-01T12:00:00Z",
      "updated_at_max": "2026-01-01T12:05:30Z",
      "operations": [
        {
          "operation_type": "availability_update",
          "status": "success",
          "updated_at": "2026-01-01T12:03:15Z"
        },
        {
          "operation_type": "pricing_update",
          "status": "success",
          "updated_at": "2026-01-01T12:04:20Z"
        },
        {
          "operation_type": "bookings_sync",
          "status": "success",
          "updated_at": "2026-01-01T12:05:30Z"
        }
      ]
    },
    {
      "batch_id": "a1b2c3d4-e5f6-7890-abcd-ef1234567890",
      "connection_id": "abc-123-def-456",
      "batch_status": "failed",
      "status_counts": {
        "triggered": 0,
        "running": 0,
        "success": 2,
        "failed": 1,
        "other": 0
      },
      "created_at_min": "2026-01-01T10:00:00Z",
      "updated_at_max": "2026-01-01T10:08:45Z",
      "operations": [
        {
          "operation_type": "availability_update",
          "status": "success",
          "updated_at": "2026-01-01T10:03:20Z"
        },
        {
          "operation_type": "pricing_update",
          "status": "failed",
          "updated_at": "2026-01-01T10:05:15Z"
        },
        {
          "operation_type": "bookings_sync",
          "status": "success",
          "updated_at": "2026-01-01T10:08:45Z"
        }
      ]
    }
  ],
  "limit": 50,
  "offset": 0
}
```

**Status Filter Mapping:**

| Filter Value | SQL Logic | Description |
|--------------|-----------|-------------|
| Omit or `any` | No filter applied | Return all batches |
| `running` | `failed_count = 0 AND (running_count > 0 OR triggered_count > 0)` | At least one operation in progress, none failed |
| `failed` | `failed_count > 0` | At least one operation failed |
| `success` | `failed_count = 0 AND running_count = 0 AND triggered_count = 0 AND success_count = total_count AND total_count > 0` | All operations succeeded |

**Performance:**

- **Single SQL query** with CTEs (batch_aggregation + operations_per_batch)
- **Efficient aggregation** using `COUNT(*) FILTER (WHERE ...)` and `json_agg()`
- **Status filter in SQL** (WHERE clause on derived batch_status)
- **Scoped by connection_id** (prevents cross-tenant leaks)
- **Indexed columns:** `batch_id`, `connection_id`, `status`, `created_at`, `updated_at`

**Error Responses:**

```bash
# 400 - Invalid status parameter
{
  "error": "invalid_status",
  "message": "Status must be one of: any, running, failed, success (got 'invalid')"
}

# 503 - Schema not installed
{
  "error": "service_unavailable",
  "message": "Channel sync logs schema not installed..."
}

# 401 - Not authenticated
{
  "detail": "Not authenticated"
}
```

**Monitoring Examples:**

```bash
# Count total batches (all statuses)
curl -sS "$API/api/v1/channel-connections/$CID/sync-batches?limit=200" \
  -H "Authorization: Bearer $TOKEN" \
  | jq '.items | length'

# Find recent failed batches (last 10)
curl -sS "$API/api/v1/channel-connections/$CID/sync-batches?status=failed&limit=10" \
  -H "Authorization: Bearer $TOKEN" \
  | jq '.items[] | {batch_id, created_at_min, failed_count: .status_counts.failed}'

# Check if any batches are currently running
RUNNING_COUNT=$(curl -sS "$API/api/v1/channel-connections/$CID/sync-batches?status=running&limit=1" \
  -H "Authorization: Bearer $TOKEN" \
  | jq '.items | length')

if [[ "$RUNNING_COUNT" -gt 0 ]]; then
  echo "⏳ Sync operations in progress"
else
  echo "✅ No active syncs"
fi

# Pagination example - fetch all batches (handling large result sets)
OFFSET=0
LIMIT=50
while true; do
  RESPONSE=$(curl -sS "$API/api/v1/channel-connections/$CID/sync-batches?limit=$LIMIT&offset=$OFFSET" \
    -H "Authorization: Bearer $TOKEN")

  COUNT=$(echo "$RESPONSE" | jq '.items | length')

  if [[ "$COUNT" -eq 0 ]]; then
    break
  fi

  echo "$RESPONSE" | jq -c '.items[]'

  OFFSET=$((OFFSET + LIMIT))
done
```

**Use in Admin UI:**

```javascript
// Fetch recent batches with status filter
const fetchBatches = async (connectionId, status = 'any', limit = 50, offset = 0) => {
  const params = new URLSearchParams({ limit, offset });
  if (status !== 'any') {
    params.append('status', status);
  }

  const response = await fetch(
    `/api/v1/channel-connections/${connectionId}/sync-batches?${params}`,
    { headers: { Authorization: `Bearer ${token}` } }
  );

  return await response.json();
};

// Display batches with pagination
const { items: batches, limit, offset } = await fetchBatches(connectionId, 'failed');

batches.forEach(batch => {
  console.log(`Batch ${batch.batch_id}: ${batch.batch_status}`);
  console.log(`  Operations: ${batch.operations.length}`);
  console.log(`  Success: ${batch.status_counts.success}, Failed: ${batch.status_counts.failed}`);
});

// Infinite scroll / pagination
const nextPage = await fetchBatches(connectionId, 'any', limit, offset + limit);
```

**Related:**

- See [Batch Status Aggregation](#batch-status-aggregation) for single batch status details
- See [Full Sync Batching (batch_id)](#full-sync-batching-batch_id) for batch grouping concept
- See [GET /sync-logs](#get-sync-logs) for individual log entries

---

## Admin UI - Sync History Integration

**Purpose:** The Admin UI provides a user-friendly interface for viewing and managing channel sync history using the batch list and batch detail endpoints.

**Location:** `/connections` page → Connection Details modal → **Sync History** section

**Features:**

1. **Batch List View:**
   - Displays recent sync batches in a paginated table
   - Status filter dropdown (Any, Running, Failed, Success)
   - Table columns:
     - **Updated**: Most recent update timestamp (updated_at_max)
     - **Status**: Visual badge (green/red/blue/gray) indicating batch_status
     - **Counts**: Icon-based summary (✓ success, ✗ failed, ⟳ running, ⋯ triggered)
     - **Operations**: Emoji indicators for operation types (📅 availability, 💰 pricing, 🔄 bookings)
     - **Batch ID**: Truncated UUID with copy button
   - Pagination: Previous/Next buttons (20 batches per page)

2. **Batch Detail Modal:**
   - Click any batch row to open detailed view
   - Nested modal (z-60) overlays connection details modal
   - Sections:
     - **Batch Summary**: Status, created/updated timestamps, total operations
     - **Status Breakdown**: Visual grid showing counts for each status
     - **Operations List**: Individual operations with status badges and timestamps
     - **Troubleshooting Hint**: Yellow warning box for failed batches with runbook reference

**API Integration:**

```javascript
// Fetch batch list
GET /api/v1/channel-connections/{connectionId}/sync-batches?limit=20&offset=0&status=any

// Fetch batch detail
GET /api/v1/channel-connections/{connectionId}/sync-batches/{batchId}
```

**Auto-Refresh Behavior:**

- Batch list refreshes when:
  - Connection details modal is opened
  - Status filter is changed
  - Pagination buttons are clicked
- No auto-polling for batch list (user must manually refresh)
- Batch detail loaded on-demand when row is clicked

**Error Handling:**

| Error | UI Behavior |
|-------|-------------|
| 401/403 | Modal remains open, error logged to console (no user-facing alert) |
| 503 (DB schema drift) | Empty batch list, error logged to console |
| Network error | Empty batch list, error logged to console |

**User Flow Example:**

1. Admin opens connection details for connection `abc-123-def-456`
2. Scrolls to **Sync History** section (below Sync Logs)
3. Selects "Failed" from status filter dropdown → Only failed batches shown
4. Clicks a batch row → Batch detail modal opens
5. Reviews status breakdown → Sees 2/3 operations succeeded, 1 failed
6. Reads troubleshooting hint → "Check worker logs / runbook.md"
7. Closes batch detail modal → Returns to batch list
8. Clicks "Next" → Loads next 20 batches (offset=20)

**Production Access:**

- **URL**: `https://fewo.kolibri-visions.de/connections`
- **Auth**: Requires valid JWT token (admin role recommended for full access)
- **Browser Console**: Check network tab for API requests/responses if batches don't load

**Troubleshooting:**

**Problem:** Batch list is empty despite recent syncs

**Possible Causes:**
- No batches match current status filter (try "Any")
- All batches are on later pages (click "Next")
- Database schema drift (check API response in network tab for 503 errors)
- Connection has no sync history (batches only created after batch_id feature deployed)

**Solution:**
```bash
# Check if batches exist via API
curl -sS "https://api.fewo.kolibri-visions.de/api/v1/channel-connections/$CID/sync-batches?status=any&limit=100" \
  -H "Authorization: Bearer $TOKEN" \
  | jq '.items | length'

# Expected: > 0 if batches exist
```

**Problem:** Batch detail modal shows "Loading batch details..." indefinitely

**Possible Causes:**
- 404 error (batch_id not found or doesn't belong to connection_id)
- 503 error (DB schema drift)
- Network timeout

**Solution:**
- Check browser console for errors
- Verify batch_id exists: `curl ... /sync-batches/{batch_id}` via API
- Check backend logs for database connection issues

**Related:**

- See [List Sync Batches](#list-sync-batches) for API endpoint documentation
- See [Batch Status Aggregation](#batch-status-aggregation) for single batch detail endpoint
- Frontend code: `/frontend/app/connections/page.tsx`

---

## Admin UI - Batch Details & Live Status

**Purpose:** Enhanced batch detail view with real-time updates for running batches and quick access from log details.

**Features:**

### 1. Live Status Updates

When viewing a batch that is currently running:

- **Auto-Polling**: Batch details refresh every 3 seconds automatically
- **Live Indicator**: Blue "Live" badge with pulsing dot appears in header
- **Last Updated**: Timestamp shows when data was last refreshed
- **Auto-Stop**: Polling stops automatically when:
  - Batch status becomes `success`, `failed`, or `unknown`
  - All operations complete (no triggered/running operations)
  - Maximum polling time reached (60 seconds / 20 polls)

**Visual Cues:**

```
┌─────────────────────────────────────────────────┐
│ Batch Details  [🔵 Live]                        │
│ Batch ID: 70bce471-... │ Last updated: 14:23:45 │
└─────────────────────────────────────────────────┘
```

**When Polling Is Active:**

- Running batches (batch_status = "running")
- Batches with triggered or running operations (counts.triggered + counts.running > 0)

**Implementation Details:**

```typescript
// Polling logic (frontend)
useEffect(() => {
  if (!selectedBatchDetail) return;

  const isRunning = selectedBatchDetail.batch_status === "running" ||
                   (selectedBatchDetail.status_counts.triggered +
                    selectedBatchDetail.status_counts.running) > 0;

  if (!isRunning) return;

  // Poll every 3s for up to 60s (20 polls max)
  const interval = setInterval(() => {
    refreshBatchDetail();
  }, 3000);

  return () => clearInterval(interval);
}, [selectedBatchDetail]);
```

### 2. Open Batch from Log Details

Users can navigate from individual log entries to the full batch view:

**Location:** Log Details modal → Summary section

**When Visible:** Only when log entry has a `batch_id` field

**Behavior:**
1. Log Details modal shows "Batch ID" field in summary
2. "Open Batch Details →" button appears below summary
3. Click button → Log Details modal closes, Batch Details modal opens
4. Full batch status, operations list, and counts displayed

**Use Case Example:**

```
User Flow:
1. Admin clicks "Details" on a log entry in Sync Logs
2. Sees batch_id: "70bce471-d82a-4cd9-8ad3-8c9f2e5f4a11"
3. Clicks "Open Batch Details →" button
4. Batch Details modal opens showing:
   - All 3 operations in the batch
   - Overall batch status (e.g., 2/3 success, 1 failed)
   - Status breakdown chart
   - If running: Live indicator + auto-refresh
```

**Button Appearance:**

```html
┌────────────────────────────────┐
│ Operation Type: availability   │
│ Status: success                │
│ Batch ID: 70bce471-...         │
│                                │
│      [Open Batch Details →]   │
└────────────────────────────────┘
```

### 3. Back Navigation from Batch to Log

**Purpose:** Seamless back navigation from Batch Details to Log Details modal.

**Location:** Batch Details modal → Header (left side, icon-only button)

**When Visible:** Only when batch was opened from a log entry (not standalone)

**Behavior:**
1. Batch Details header shows left arrow icon (←) button on left side (no text label)
2. Click arrow → Batch Details closes, Log Details remains visible underneath
3. Log Details shows the exact same log that opened the batch
4. If batch was opened standalone (e.g., from sync history), no back arrow appears

**Modal Stack:**
- Log Details remains open in background (z-index 60) when Batch Details opens
- Batch Details renders on top (z-index 70) with darker overlay
- Back arrow simply closes Batch Details layer, revealing Log Details underneath
- No re-fetching or state restoration needed - log modal stays in memory

**User Flow Example:**

```
Navigation Path:
1. Connections → Open → Sync Logs → Click "Details" on a log entry
2. Log Details modal opens (z-60)
3. Click "Open Batch Details →" → Batch Details overlays on top (z-70)
4. Log Details remains open underneath (not closed)
5. Click [←] arrow icon → Batch Details closes
6. Log Details is immediately visible with same content
7. Close Log Details → returns to Connection Details
```

**Visual Appearance:**

```html
┌──────────────────────────────────────────────────┐
│ [←]  Batch Details  [🔵 Live]  [×]               │
│      (icon only - no text)                       │
│ Batch ID: 70bce471-... │ Last updated: 14:23:45  │
└──────────────────────────────────────────────────┘
```

**Testing Navigation:**

1. Open Admin UI → Connections → Select connection → "Open"
2. Navigate to Sync Logs table
3. Click "Details" on any log entry with a batch_id
4. In Log Details, click "Open Batch Details →"
5. Verify: Batch Details shows [←] arrow icon (top left, no text)
6. Click the arrow icon
7. Verify: Batch Details closes, Log Details is visible immediately
8. Verify: Same log content as before (no re-fetch)
9. Alternative: Click "X" in Batch Details → closes batch, reveals log

**Implementation Notes:**

- Modal stack approach: both modals coexist in state
- Log Details NOT closed when opening Batch Details
- Source log ID tracked for conditional rendering of back arrow
- aria-label="Back to log" for accessibility (screen readers)
- Escape key and X button work as normal close actions
- Navigation is purely UI state-based (no URL routing)

### 4. Failed Batch Guidance

When viewing a batch with failed operations:

**Yellow Warning Box:**
```
⚠️ Troubleshooting
This batch has failed operations. Check worker logs for detailed error messages.
See backend/docs/ops/runbook.md for common issues and solutions.
```

**Where to Check Logs:**

```bash
# Production worker logs (Docker)
docker logs pms-worker-v2 --tail 100 --follow

# Filter for specific batch
docker logs pms-worker-v2 2>&1 | grep "70bce471-d82a-4cd9-8ad3-8c9f2e5f4a11"

# Check Celery task failures
docker logs pms-worker-v2 2>&1 | grep "Task.*failed"
```

**Common Failure Patterns:**

| Error Pattern | Likely Cause | Runbook Section |
|---------------|-------------|-----------------|
| `Database unavailable` | DB connection pool exhausted or DNS failure | [DB DNS / Degraded Mode](#db-dns--degraded-mode) |
| `Schema drift` | channel_sync_logs table missing or constraint out of date | [Schema Drift](#schema-drift) |
| `Channel adapter error` | External API timeout or rate limit | Check platform-specific sections |
| `Task timeout` | Operation exceeded soft/hard time limits | [Celery Configuration](#celery-configuration) |

### 4. Performance Characteristics

**Polling Overhead:**

- **Network**: 1 API request every 3 seconds (max 20 requests)
- **Backend**: No additional load (read-only query, indexed columns)
- **Browser**: Minimal CPU/memory (React state updates only)

**Automatic Cleanup:**

- Polling stops when batch completes → no infinite loops
- Modal close clears polling interval → no background requests
- Last updated timestamp prevents stale data confusion

**Best Practices:**

- ✅ **Do**: Let polling run for active batches (provides real-time feedback)
- ✅ **Do**: Close batch detail modal when done (stops polling immediately)
- ❌ **Don't**: Open batch details for old/completed batches expecting updates (polling won't start)
- ❌ **Don't**: Keep multiple batch detail modals open simultaneously (only one can be open at a time)

### 5. Troubleshooting Auto-Refresh & Live Updates

**Problem:** "Live" badge doesn't appear for running batch

**Possible Causes:**
- Batch status is not "running" (check batch_status field)
- All operations already completed (counts.triggered + counts.running = 0)
- Batch detail loaded before operations started (rare race condition)

**Solution:**
- Use manual Refresh button to fetch latest data
- Verify batch is actually running via API: `curl .../sync-batches/{batch_id}`

**Problem:** Auto-refresh not updating "Last refreshed" timestamp

**Possible Causes:**
- Auto-refresh toggle is OFF (unchecked)
- Network error preventing API calls (check browser console)
- Interval cleared unexpectedly (browser throttling, component error)

**Solution:**
1. Verify "Auto refresh" checkbox is checked in modal header
2. Check browser console for API errors (401/403/503)
3. Try manual Refresh button to verify API connectivity
4. If manual refresh works but auto-refresh doesn't: close and reopen modal
5. If issue persists: hard refresh page (Ctrl+F5 / Cmd+Shift+R)

**Problem:** Auto-refresh updates too slowly for running batch

**Expected Behavior:**
- Running batches: 3-second interval
- Completed batches: 10-second interval

**Debugging Steps:**
1. Check batch status in Batch Summary (should show "running" for 3s interval)
2. Verify operations list has triggered/running items
3. If batch shows "success" but has running operations: data inconsistency, use manual Refresh

**Problem:** Auto-refresh doesn't stop when toggled OFF

**Solution:**
- Expected: Interval clears immediately when unchecking toggle
- If "Last refreshed" keeps updating: component state issue, close and reopen modal
- Check browser console for React errors

**Problem:** Modal flickers/resizes during auto-refresh

**Root Cause:**
- UI was clearing data arrays (setLogs([]) / setHistory([])) before re-fetching
- Caused layout collapse → modal shrinks → data loads → modal expands
- Loading states (logsLoading / syncHistoryLoading) replaced content with "Loading..." text

**Solution (Implemented):**
- **Stale-while-revalidate pattern:** Keep existing data visible during refresh
- Separate loading states: `logsRefreshing` / `syncHistoryRefreshing` for auto-refresh vs `logsLoading` / `syncHistoryLoading` for initial load
- Stable layout heights: Added `min-h-[200px]` to content containers
- Errors during refresh don't clear existing data (graceful degradation)
- Only show "Loading..." on initial load, not on refresh

**Implementation:**
- `fetchSyncLogs(connectionId, isRefresh)` - auto-refresh calls with isRefresh=true
- `fetchSyncHistory(connectionId, isRefresh)` - auto-refresh calls with isRefresh=true
- Frontend code: `/frontend/app/connections/page.tsx:399-437` (fetchSyncLogs), `page.tsx:516-554` (fetchSyncHistory)

**Related:**

- See [Admin UI - Sync History Integration](#admin-ui---sync-history-integration) for batch list view
- See [Batch Status Aggregation](#batch-status-aggregation) for batch detail API endpoint
- See [Admin UI - Batch Details Refresh Semantics](#admin-ui---batch-details-refresh-semantics) for timestamp meanings
- Frontend code: `/frontend/app/connections/page.tsx` (refreshBatchDetail, auto-refresh useEffect, toggle state)

---

## Admin UI - Batch Details Refresh Semantics

**Purpose:** Clarify timestamp meanings and refresh behavior in Batch Details modal to avoid confusion between data timestamps and UI refresh times.

**Problem (Fixed):**

Previously, the Batch Details modal showed "Last updated" in the header, which was ambiguous:
- Did it mean when the data was last updated on the server (updated_at_max)?
- Or when the UI last fetched the data (local refresh time)?

The header timestamp didn't update while the modal stayed open, requiring users to close and reopen to see refresh updates.

**Solution:**

Two distinct timestamps with clear labels:

### 1. Data Updated At (Server Timestamp)

**Location:** Batch Summary section

**Label:** "Data Updated At"

**Value:** `updated_at_max` from API response (most recent operation update in the batch)

**Format:** Full date/time (e.g., "1/1/2026, 2:23:45 PM")

**Meaning:** When the batch data was last modified on the server

**Updates:** Only changes when backend operations complete/update

### 2. Last Refreshed (UI Timestamp)

**Location:** Modal header (below Batch ID)

**Label:** "Last refreshed"

**Value:** Local browser time of last successful API fetch

**Format:** Time only (e.g., "14:23:45")

**Meaning:** When the UI last successfully fetched fresh data from the server

**Updates:** Changes on every refresh (auto-poll or manual button click)

**Visual Example:**

```
┌────────────────────────────────────────────────────┐
│ Batch Details  [🔵 Live]          [Refresh]  [×]  │
│ Batch ID: 70bce471-... │ Last refreshed: 14:23:45 │ ← UI refresh time
├────────────────────────────────────────────────────┤
│ Batch Summary                                      │
│ Created At: 1/1/2026, 2:20:00 PM                   │
│ Data Updated At: 1/1/2026, 2:23:45 PM              │ ← Server data timestamp
│ Total Operations: 3                                │
└────────────────────────────────────────────────────┘
```

### 3. Manual Refresh Button

**Location:** Modal header (between title and close button)

**Appearance:** Indigo button with refresh icon

**States:**
- Normal: "Refresh" with circular arrow icon
- Loading: "Refreshing..." with spinning icon (disabled)

**Behavior:**
- Click → Immediately calls `GET /sync-batches/{batch_id}`
- Updates both batch data and "Last refreshed" timestamp
- Works for both running and completed batches

### 4. Auto-Refresh Behavior

**Auto-Refresh Toggle:**
- **Location:** Modal header (between Refresh button and Close button)
- **Default:** ON (checked)
- **Appearance:** Checkbox with "Auto refresh" label
- **Behavior:** User can toggle auto-refresh ON/OFF at any time

**Refresh Intervals (when toggle ON):**
- **Running Batches:** Auto-refreshes every 3 seconds
  - Batch status is "running" OR any operations are in "triggered"/"running" status
- **Completed Batches:** Auto-refreshes every 10 seconds
  - All operations completed (success/failed)

**Stable Interval Architecture:**
- Interval set exactly once when modal opens with toggle ON
- Runs continuously until modal closes or toggle turned OFF
- Updates "Last refreshed" timestamp on each automatic refresh
- Interval ID stored in ref for reliable cleanup

**Cleanup:**
- Interval cleared when modal closes
- Interval cleared when toggle turned OFF
- Auto-refresh resets to ON (default) when opening a new batch

### 5. Timestamp Semantics Table

| Timestamp | Location | Label | Source | Format | Updates When |
|-----------|----------|-------|--------|--------|--------------|
| Server | Summary | "Data Updated At" | `updated_at_max` | Full datetime | Backend operation updates |
| Client | Header | "Last refreshed" | `new Date()` | Time only | Every API fetch |

**Related:**

- See [Admin UI - Batch Details & Live Status](#admin-ui---batch-details--live-status) for auto-polling behavior
- Frontend code: `/frontend/app/connections/page.tsx` (refreshBatchDetail, polling useEffect, manual refresh button)

---

## Emergency Contacts

- **Primary On-Call**: [Add contact info]
- **Database Admin**: [Add contact info]
- **Supabase Support**: https://supabase.com/support

---

## Admin UI — Channel Manager (Connections + Sync Logs)

**Purpose:** Backoffice Console UI for managing channel connections, viewing sync logs, and triggering sync operations.

**URL:** `https://admin.fewo.kolibri-visions.de/connections`

**RBAC:** Admin and Manager roles only

---

### Overview

The Connections page provides a comprehensive interface for:
1. **Viewing channel connections** - Table of all configured platform connections
2. **Viewing sync logs** - Per-connection sync operation history with search and filters
3. **Triggering syncs** - Manual sync triggers (availability, pricing, bookings, full)
4. **Monitoring status** - Real-time status badges, error messages, and auto-refresh
5. **Batch tracking** - Grouped view of Full Sync operations with batch_id

---

### Connections Quick Actions

Each connection row in the table provides inline quick actions:

**Test Connection:**
- **Button:** "Test" (inline per row)
- **Action:** POST `/api/v1/channel-connections/{id}/test`
- **Response:** Health status, platform API connectivity check
- **Display:** Shows notification banner with pass/fail result
- **Disable state:** Button disabled during test (shows "Testing...")

**Sync Quick Actions:**
- **Buttons:** A (Availability), P (Pricing), B (Bookings), F (Full)
- **Action:** POST `/api/v1/channel-connections/{id}/sync` with `{"sync_type": "availability"|"pricing"|"bookings"|"full"}`
- **Response:** Returns `batch_id`, `task_ids` array, `status`, `message`
- **Display:** Shows success notification with task count/batch ID
- **Disable state:** Individual button disabled while that sync type is in progress (shows "...")
- **Optimistic update:** Connections list refetched after trigger to update `last_sync_at`

**View Logs:**
- **Button:** "View Logs" (link-style button)
- **Action:** Sets `localStorage.setItem("channelSync:lastConnectionId", connection_id)` and navigates to `/channel-sync`
- **Result:** Channel Sync page loads with connection preselected and logs displayed immediately (no Auto-detect click needed)
- **Note:** Does NOT auto-open sync log details modal (modal opens only on explicit row click)

**Last Sync Age Display:**
- **Column:** "Last Sync"
- **Format:** Relative time (e.g., "3m ago", "2h ago", "1d ago", "never")
- **Helper:** `formatRelativeTime()` converts ISO timestamp to human-friendly age
- **Precision:**
  - < 60s: "Xs ago"
  - < 60m: "Xm ago"
  - < 24h: "Xh ago"
  - < 7d: "Xd ago"
  - < 30d: "Xw ago"
  - < 365d: "Xmo ago"
  - ≥ 365d: "Xy ago"

---

### Sync History (Batches)

The Connections page includes a "Sync History" section that displays recent sync batches for the selected connection.

**Data Source:**
- Endpoint: `GET /api/v1/channel-connections/{connection_id}/sync-batches?limit={N}&offset={M}&status={filter}`
- Returns: List of batches with operations, status counts, and timestamps

**UI Features:**

1. **Batch Table Columns:**
   - **Updated:** Most recent timestamp (updated_at_max or created_at_min)
   - **Status:** Batch status badge (success/failed/running/unknown)
   - **Counts:** Visual summary of operation statuses (✓ success, ✗ failed, ⟳ running, ⋯ triggered)
   - **Operations:** Pills showing operation types with direction indicators (📅 → for availability_update outbound, 💰 → for pricing_update outbound, 🔄 ← for bookings_sync inbound)
   - **Batch ID:** Truncated ID with click-to-copy functionality

2. **Status Filter Dropdown:**
   - **Options:** All Status | Running | Failed | Success
   - **Behavior:** Updates `?status=` query parameter
   - **"All Status"**: Omits status param (shows all batches)
   - **Other values**: Filters batches by batch_status field

3. **Pagination Controls:**
   - **Buttons:** Previous / Next
   - **Default limit:** 50 batches per page
   - **Offset tracking:** Advances by limit value on each page change
   - **Display:** Shows "Showing X - Y" range indicator

4. **Batch Detail Modal:**
   - **Trigger:** Click any batch row in the table
   - **Action:** Fetches `GET /sync-batches/{batch_id}` for detailed operations
   - **Displays:** Batch summary, status breakdown, operations list with direction/task_id/error fields
   - **Auto-refresh:** Optional polling checkbox for running batches
   - **Back navigation:** Returns to connection details or log details (if opened from log)

**Direction Display:**
- **Outbound (→):** availability_update, pricing_update (PMS → Channel Manager)
- **Inbound (←):** bookings_sync (Channel Manager → PMS)
- Derived from operation_type if DB field is NULL (defensive fallback)

**Empty State:**
- Message: "No batches found" when items array is empty
- Occurs when: No sync operations triggered yet, or filter excludes all batches

**Error Handling:**
- Inline error banner for API failures
- Graceful degradation: Does not crash the page
- Retry button available via manual refresh

---

### Batch Details (Full Sync)

When a Full Sync is triggered (sync_type=full), the backend creates a batch containing 3 operations:
1. **availability_update** (outbound)
2. **pricing_update** (outbound)
3. **bookings_sync** (inbound)

All operations in the batch share the same `batch_id`.

**Accessing Batch Details:**
- **From logs table:** Click the blue "Batch" button in any log row that has a `batch_id` (visible for full sync operations)
- **From success panel:** After triggering a full sync, click the green "View Batch" button in the success notification

**Batch Details Modal:**
- **Header:**
  - Title: "Full Sync Batch Details"
  - Copy buttons for Batch ID and Connection ID (📋 with truncated IDs)
  - Refresh button (⟳) to reload batch details
  - Close button (×)

- **Table Columns:**
  - **Operation:** operation_type (e.g., availability_update, pricing_update, bookings_sync)
  - **Status:** Status badge (triggered/running/success/failed) with color coding
  - **Direction:** outbound or inbound
  - **Task ID:** Clickable truncated task_id (click to copy full ID)
  - **Duration:** Calculated from started_at/finished_at or created_at/updated_at
  - **Updated At:** Localized timestamp
  - **Error:** "View Error" button (if error exists) → opens detail drawer and closes batch modal

- **States:**
  - **Loading:** Spinner with "Loading batch details..." message
  - **Error:** Red error banner with API error message
  - **Empty:** "No operations found for this batch." (shouldn't happen for valid full sync batches)
  - **Success:** Table showing all 3 operations

**UX Rules:**
- Clicking "View Error" button transitions from batch modal to detail drawer (closes modal, opens drawer)
- Closing batch modal clears any active toasts (`setToast(null)`)
- Toast lifecycle follows standard rules (6s auto-dismiss, cleared on navigation)

**Backend Endpoints:**

**1. List Recent Batches (Discovery):**
```bash
GET /api/v1/channel-connections/{connection_id}/sync-batches?limit=5&offset=0
```

**Example curl:**
```bash
curl -L -X GET "https://api.fewo.kolibri-visions.de/api/v1/channel-connections/{CID}/sync-batches?limit=5&offset=0" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Accept: application/json"
```

**Response:** Array of batch summaries with `batch_id`, `batch_status`, `status_counts`, `operations` (detailed)

**2. Get Batch Details (Detailed Operations):**
```bash
GET /api/v1/channel-connections/{connection_id}/sync-batches/{batch_id}
```

**Example curl:**
```bash
curl -L -X GET "https://api.fewo.kolibri-visions.de/api/v1/channel-connections/{CID}/sync-batches/{BATCH_ID}" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Accept: application/json"
```

**Response Fields (per operation):**
- `operation_type`: availability_update | pricing_update | bookings_sync
- `status`: triggered | running | success | failed
- `direction`: outbound | inbound (derived from operation_type if NULL: availability_update/pricing_update → outbound, bookings_sync → inbound)
- `task_id`: Celery task UUID (nullable)
- `error`: Error message string (nullable, present only if status=failed)
- `duration_ms`: Duration in milliseconds (nullable, null if started_at/finished_at not available)
- `updated_at`: Timestamp of last update
- `log_id`: Sync log UUID (for UI "View Error" drawer)

**Note:** Both list (`GET /sync-batches`) and details (`GET /sync-batches/{batch_id}`) endpoints return operations with all fields above. The list endpoint aggregates all logs per batch, while the details endpoint shows a single batch.

**Duration Computation Fallback:**

The `duration_ms` field is computed using the following fallback logic (implemented in SQL):
1. **Preferred:** If `started_at` and `finished_at` columns exist and are populated, use those timestamps
2. **Fallback:** If `started_at`/`finished_at` are not available but `created_at` and `updated_at` exist and `updated_at >= created_at`, compute duration as: `EXTRACT(EPOCH FROM (updated_at - created_at)) * 1000` milliseconds
3. **Null:** If neither timestamps are available or `updated_at < created_at`, `duration_ms` is `null`

**UI Display:**
- When `duration_ms` is present: Display as human-friendly format (e.g., "2.35s" for 2350ms)
- When `duration_ms` is `null`: Display as "—" (em dash) to indicate no duration available
- **Hover Tooltip:** When duration is available, hovering over the duration shows raw milliseconds (e.g., "453 ms")
- **Format:** Seconds with 2 decimal places (e.g., "0.45s"), tooltip shows integer milliseconds

**Response Example:**
```json
{
  "batch_id": "64c93985-f61b-4b95-856c-dae0baf35efc",
  "connection_id": "abc-123-def-456",
  "batch_status": "success",
  "status_counts": {
    "triggered": 0,
    "running": 0,
    "success": 3,
    "failed": 0,
    "other": 0
  },
  "created_at_min": "2026-01-03T10:00:00Z",
  "updated_at_max": "2026-01-03T10:05:00Z",
  "operations": [
    {
      "operation_type": "availability_update",
      "status": "success",
      "direction": "outbound",
      "task_id": "550e8400-e29b-41d4-a716-446655440001",
      "error": null,
      "duration_ms": null,
      "updated_at": "2026-01-03T10:03:00Z",
      "log_id": "log-uuid-001"
    },
    {
      "operation_type": "pricing_update",
      "status": "success",
      "direction": "outbound",
      "task_id": "550e8400-e29b-41d4-a716-446655440002",
      "error": null,
      "duration_ms": null,
      "updated_at": "2026-01-03T10:04:00Z",
      "log_id": "log-uuid-002"
    },
    {
      "operation_type": "bookings_sync",
      "status": "success",
      "direction": "inbound",
      "task_id": "550e8400-e29b-41d4-a716-446655440003",
      "error": null,
      "duration_ms": null,
      "updated_at": "2026-01-03T10:05:00Z",
      "log_id": "log-uuid-003"
    }
  ]
}
```

**Alternative Endpoint (Logs-based):**
```bash
GET /api/v1/channel-connections/{connection_id}/sync-logs?batch_id={batch_id}&limit=100
```

**Response:** Standard sync-logs response with `logs` array filtered to the specified batch_id

**Use Case:**
Operators can track the progress of all 3 operations in a Full Sync batch from a single view, quickly identifying which operation succeeded/failed and drilling into errors via the detail drawer. The `/sync-batches/{batch_id}` endpoint provides detailed per-operation fields (direction, task_id, error, duration_ms) in a single API call.

---

### API Endpoints Used

The Admin UI consumes the following Channel Manager API endpoints:

#### 1. List Connections
```
GET /api/v1/channel-connections/?limit=50&offset=0
```

**Response Shape:** Array
```json
[
  {
    "id": "uuid",
    "tenant_id": "uuid",
    "property_id": "uuid",
    "platform_type": "airbnb|booking_com|expedia|fewo_direkt|google",
    "platform_listing_id": "string",
    "status": "active|inactive|error",
    "platform_metadata": {},
    "last_sync_at": "2025-01-03T12:00:00Z" | null,
    "created_at": "2025-01-03T10:00:00Z",
    "updated_at": "2025-01-03T12:00:00Z"
  }
]
```

**curl Example:**
```bash
curl -L -X GET "https://api.fewo.kolibri-visions.de/api/v1/channel-connections/?limit=50&offset=0" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Accept: application/json"
```

**Notes:**
- Returns **array directly** (not wrapped in object)
- UI uses `normalizeConnections()` helper to handle both array and object responses
- Trailing slash + query params required to avoid 307 redirects

#### 2. Get Sync Logs
```
GET /api/v1/channel-connections/{connection_id}/sync-logs?limit=50&offset=0
```

**Response Shape:** Object with `logs` array
```json
{
  "connection_id": "uuid",
  "logs": [
    {
      "id": "uuid",
      "connection_id": "uuid",
      "operation_type": "availability_update|pricing_update|bookings_sync",
      "direction": "outbound|inbound",
      "status": "triggered|running|success|failed",
      "details": ["<json-string>", "<json-string>"] | {},
      "error": null | "error message",
      "task_id": "celery-task-uuid",
      "batch_id": "batch-uuid" | null,
      "created_at": "2025-01-03T12:00:00Z",
      "updated_at": "2025-01-03T12:05:00Z"
    }
  ],
  "limit": 50,
  "offset": 0,
  "batch_id": null
}
```

**curl Example:**
```bash
CID="your-connection-uuid"
curl -L -X GET "https://api.fewo.kolibri-visions.de/api/v1/channel-connections/$CID/sync-logs?limit=50&offset=0" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Accept: application/json"
```

**Notes:**
- Returns **object** with `logs` array (not array directly)
- UI uses `normalizeLogs()` helper to extract array from `data.logs || data.items || data.data`
- `details` field can be:
  - Array of JSON strings: `["{\\"key\\": \\"value\\"}"]`
  - Already parsed object: `{"key": "value"}`
  - UI uses `parseLogDetails()` to safely parse JSON strings

#### 3. Trigger Sync
```
POST /api/v1/channel-connections/{connection_id}/sync
```

**Request Body:**
```json
{
  "sync_type": "availability|pricing|bookings|full"
}
```

**Response Shape:**
```json
{
  "status": "triggered",
  "message": "Sync triggered successfully",
  "task_ids": ["task-uuid-1", "task-uuid-2", "task-uuid-3"],
  "batch_id": "batch-uuid"
}
```

**curl Example:**
```bash
CID="your-connection-uuid"
curl -L -X POST "https://api.fewo.kolibri-visions.de/api/v1/channel-connections/$CID/sync" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"sync_type": "availability"}'
```

**Notes:**
- Full sync returns `batch_id` + `task_ids` array (3 operations)
- Single sync (availability/pricing) may return single `task_id` or array
- UI displays batch_id with copy button for easy tracking

---

### UI Features

#### Connections Table
- **Columns:** Platform, Property ID, Status, Last Sync, Updated At
- **Search:** Client-side search by ID, platform, or status
- **Actions:**
  - **View Logs** - Opens detail modal with sync logs for that connection
  - **Trigger Sync** - Dropdown selector (availability, pricing, bookings, full)

#### Sync Logs Panel (in Connection Detail Modal)
- **Search:** Free-text search across all log fields:
  - IDs: id, task_id, batch_id, connection_id
  - Fields: operation_type, status, direction, error
  - Details: JSON-stringified details
  - Timestamps: created_at, updated_at
- **Filters:**
  - Status: All / Triggered / Running / Success / Failed
  - Sync Type: All / Full / Availability / Pricing / Bookings
- **Sorting:** Newest first (created_at DESC)
- **Batch Grouping:** Full Sync operations grouped by `batch_id` in collapsible indigo cards
- **Auto-Refresh:**
  - Logs: 10 seconds (when checkbox enabled)
  - Batch logs: 3 seconds (when batch is active)
- **Detail Drawer:** Click any log row to view:
  - Parsed details JSON (pretty-printed)
  - Error messages (if failed)
  - Full log record (expandable)

#### Polling After Trigger
When user triggers a sync:
1. API returns `batch_id` and `task_ids`
2. UI displays notification banner with batch_id (copy button)
3. UI starts polling sync logs endpoint every 3 seconds for up to 60 seconds
4. Matches logs by:
   - `batch_id` (for Full Sync)
   - Any `task_id` in returned array
5. Highlights matching rows in logs table
6. Stops polling when all matching logs reach terminal status (`success` or `failed`) OR timeout

---

### Manual Verification with curl

#### Step 1: List all connections
```bash
export TOKEN="your-jwt-token"
export API="https://api.fewo.kolibri-visions.de"

curl -L -X GET "$API/api/v1/channel-connections/?limit=50&offset=0" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Accept: application/json" | jq
```

**Expected:** Array of connection objects

#### Step 2: Get sync logs for a connection
```bash
CID="connection-uuid-from-step-1"

curl -L -X GET "$API/api/v1/channel-connections/$CID/sync-logs?limit=50&offset=0" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Accept: application/json" | jq
```

**Expected:** Object with `logs` array

#### Step 3: Trigger a sync
```bash
curl -L -X POST "$API/api/v1/channel-connections/$CID/sync" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"sync_type": "availability"}' | jq
```

**Expected:** Object with `status`, `task_ids`, `batch_id`

#### Step 4: Poll logs to verify sync created log entries
```bash
# Wait 2-3 seconds, then fetch logs again
sleep 3
curl -L -X GET "$API/api/v1/channel-connections/$CID/sync-logs?limit=50&offset=0" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Accept: application/json" | jq '.logs[] | select(.batch_id == "batch-uuid-from-step-3")'
```

**Expected:** Log entries with matching `batch_id` or `task_id`

---

### Response Shape Handling (Resilience)

The UI uses normalization helpers to handle API response variations:

```typescript
// Handle connections endpoint (array response)
function normalizeConnections(data: any): any[] {
  if (Array.isArray(data)) return data;
  if (data && typeof data === "object") {
    return data.connections || data.data || data.items || [];
  }
  return [];
}

// Handle sync logs endpoint (object with logs array)
function normalizeLogs(data: any): any[] {
  if (Array.isArray(data)) return data;
  if (data && typeof data === "object") {
    return data.logs || data.items || data.data || [];
  }
  return [];
}

// Parse details field (can be JSON string, array of strings, or object)
function parseLogDetails(details: any): any {
  if (!details) return null;
  if (typeof details === 'object' && !Array.isArray(details)) return details;

  // Array of JSON strings
  if (Array.isArray(details)) {
    return details.map(item =>
      typeof item === 'string' ? JSON.parse(item) : item
    );
  }

  // Single JSON string
  if (typeof details === 'string') {
    try { return JSON.parse(details); }
    catch (e) { return details; }
  }

  return details;
}
```

**Why this matters:**
- Connections endpoint returns **array directly**
- Sync logs endpoint returns **object with `logs` array**
- Details field can be **JSON string, array of strings, or already parsed**
- Normalization prevents crashes when API shape changes

---

### Troubleshooting

#### No connections shown
**Check:**
1. User has admin/manager role (403 if insufficient permissions)
2. Database migrations applied (`channel_connections` table exists)
3. Browser console for API errors
4. curl command works (test token validity)

#### Sync logs empty after trigger
**Check:**
1. Celery worker is running (`docker ps | grep celery`)
2. Redis is accessible (`redis-cli ping`)
3. Check sync log status in DB: `SELECT * FROM channel_sync_logs WHERE batch_id = 'your-batch-id'`
4. Worker logs for errors: `docker logs pms-celery-worker`

#### Polling doesn't find new logs
**Check:**
1. `batch_id` or `task_id` matches between trigger response and log entries
2. Auto-refresh checkbox is enabled
3. Logs endpoint returns matching entries (test with curl)
4. Browser network tab shows polling requests every 3s

#### Error: "Failed to fetch connections (HTTP 503)"
**Fix:** Database schema out of date, run migrations:
```bash
cd supabase/migrations
supabase db push
```

#### Frontend deploy - Admin UI doesn't update / /guests 404 after commit
**Symptoms:**
- New admin UI features don't appear after git push
- Routes like `/guests` return 404
- Coolify shows "Build failed" or old image still running

**Common Causes:**
1. Frontend build failed due to ESLint not installed (requires `eslint` package or `ignoreDuringBuilds: true`)
2. TypeScript compilation errors (type mismatches, missing properties)
3. Build process exited before completion (OOM, timeout)

**Fix Checklist:**
1. Check Coolify build logs for errors (ESLint, TypeScript, out-of-memory)
2. Add `eslint: { ignoreDuringBuilds: true }` to `next.config.js` if ESLint not installed
3. Fix TypeScript errors (especially Supabase query response types - array vs object handling)
4. Verify routes exist in correct locations (`app/guests/page.tsx`, `app/guests/layout.tsx`)
5. Trigger manual redeploy in Coolify after fixes are pushed
6. Verify new routes are accessible after successful build

**Example Fix (TypeScript array/object handling):**
```typescript
// Bad: assumes object
const agencyName = teamMember?.agency?.name || 'PMS';

// Good: handles both array and object
const agency = (teamMember as any)?.agency;
const agencyName = (Array.isArray(agency) ? agency?.[0]?.name : agency?.name) ?? 'PMS';
```

**Specific Issue: Nullable Props to AdminShell (2026-01-05)**

**Symptom:**
```
Type error: Type 'string | null' is not assignable to type 'string'.
  ./app/settings/branding/layout.tsx:121:17
```

**Root Cause:**
- AdminShell expects `userRole: string` (non-nullable required)
- Layout files pass nullable values: `resolvedRole: string | null`, `session.user.email: string | null`

**Fix:**
Normalize to safe strings before passing to AdminShell:
```typescript
// Normalize nullable auth/session values
const safeUserName = (userEmail ?? "").trim() || "—";
const safeUserRole = (resolvedRole ?? "").trim() || "staff";

<AdminShell userRole={safeUserRole} userName={safeUserName} agencyName={agencyName}>
```

**Diagnostic Commands:**
```bash
# Check Coolify build logs for TypeScript errors
# Look for "Type 'string | null' is not assignable to type 'string'"

# Verify docker image tag after deploy
docker ps | grep pms-admin

# If old image still running, check build succeeded
docker logs <container-id> 2>&1 | grep -i error
```

#### Guests Page Shows HTTP 404 / List Not Loading (2026-01-05)

**Symptoms:**
- Navigate to `/guests` → page renders but shows "Fehler beim Laden — HTTP 404"
- No guest list displayed
- Browser console shows: `GET /api/v1/guests?limit=20&offset=0 404 (Not Found)`
- Search and pagination don't work (list never loads)

**Root Causes:**
1. **Wrong API base URL** - Frontend using relative URL `/api/v1/guests` instead of absolute backend API URL
   - Resolves to `https://admin.<domain>/api/v1/guests` instead of `https://api.<domain>/api/v1/guests`
   - Cause: fetch() call without proper base URL configuration
2. **Missing API route** - Guests router not properly mounted (less common after Phase 19)
3. **CORS/credentials** - API rejects requests without proper credentials or origin headers

**Quick Diagnostic Checks:**

1. **Verify API endpoint exists:**
```bash
# Check OpenAPI schema lists guests routes
curl -s https://api.<domain>/openapi.json | grep -i guests

# Test API directly
curl -H "Cookie: sb-access-token=..." \
     https://api.<domain>/api/v1/guests?limit=5
# Should return 200 with {items: [...], total: N}
```

2. **Check frontend API base URL:**
```bash
# In browser console on /guests page:
console.log(process.env.NEXT_PUBLIC_API_BASE)
# Should show: https://api.<domain>

# Check if getApiBase() is being used:
grep -n "getApiBase\|apiClient" frontend/app/guests/page.tsx
```

3. **Check browser network tab:**
   - Failed request URL should be `https://api.<domain>/api/v1/guests` (correct)
   - If showing `https://admin.<domain>/api/v1/guests` → frontend base URL issue

**Fix Summary (2026-01-05):**
- Exported `getApiBase()` from `frontend/app/lib/api-client.ts`
- Updated `frontend/app/guests/page.tsx` to use `getApiBase()` for constructing full API URL
- Before: `fetch('/api/v1/guests?...')`
- After: `fetch('${getApiBase()}/api/v1/guests?...')`

**Verification After Fix:**
```bash
# 1. Build should pass
cd frontend && npm run build

# 2. Deploy and check /guests loads
curl -H "Cookie: ..." https://admin.<domain>/guests
# Should render page without 404 errors

# 3. Run smoke test
export API_BASE_URL="https://api.<domain>"
export JWT_TOKEN="..."
bash backend/scripts/pms_guests_smoke.sh
# Should pass: list, search, pagination tests
```

---

#### Admin UI API Base URL Resolution (2026-01-05)

**Overview:**
The frontend admin UI needs to communicate with the backend API, which is hosted on a different subdomain. The API client uses a smart resolution strategy to determine the correct API base URL.

**Resolution Strategy (in order):**

1. **NEXT_PUBLIC_API_BASE environment variable (preferred)**
   - Explicitly set in `.env` or deployment config
   - Example: `NEXT_PUBLIC_API_BASE=https://api.fewo.kolibri-visions.de`
   - This is the recommended approach for production deployments

2. **Automatic hostname derivation (fallback)**
   - If `NEXT_PUBLIC_API_BASE` is not set, derives from current hostname
   - Pattern: `admin.*` → `api.*`
   - Examples:
     - `admin.fewo.kolibri-visions.de` → `https://api.fewo.kolibri-visions.de`
     - `admin.localhost:3000` → `http://api.localhost:3000`
   - Preserves protocol and port
   - Logs to console: `[api-client] Derived API base from hostname: <url>`

3. **Mixed content protection**
   - If frontend is HTTPS but env var specifies HTTP, automatically upgrades to HTTPS
   - Prevents browser mixed-content blocking
   - Logs warning: `[api-client] Upgrading HTTP API base to HTTPS...`

**Common Symptom: UI Shows 404 While API is Healthy**

**Diagnosis Steps:**

1. **Check browser network tab:**
   ```
   Expected: Request to https://api.<domain>/api/v1/...
   Wrong:    Request to https://admin.<domain>/api/v1/...
   ```
   - If seeing `admin.*` in API request URLs → API base URL resolution failed

2. **Check browser console:**
   ```javascript
   // Should show API base URL being used
   // Look for: [api-client] Derived API base from hostname: ...
   ```

3. **Verify environment variable:**
   ```bash
   # In Coolify or deployment config
   echo $NEXT_PUBLIC_API_BASE
   # Should show: https://api.<domain>

   # If empty, auto-derivation should work if hostname pattern is admin.*
   ```

4. **Test API endpoint directly:**
   ```bash
   # Verify API is actually reachable
   curl -H "Cookie: sb-access-token=..." \
        https://api.<domain>/api/v1/guests?limit=5
   # Should return 200 with data
   ```

**Fix Actions:**

1. **Set NEXT_PUBLIC_API_BASE explicitly (recommended):**
   ```bash
   # In Coolify environment variables or .env
   NEXT_PUBLIC_API_BASE=https://api.fewo.kolibri-visions.de
   ```
   - Rebuild and redeploy frontend
   - Verify in browser console that env var is set

2. **Verify hostname pattern (if using auto-derivation):**
   - Frontend must be hosted on `admin.*` subdomain
   - If using different pattern (e.g., `app.*`), auto-derivation won't work
   - Solution: Set `NEXT_PUBLIC_API_BASE` explicitly

3. **Check for typos in code:**
   ```bash
   # Verify all pages use getApiBase() or apiClient
   grep -r "fetch.*api/v1" frontend/app/
   # Should NOT find hardcoded relative URLs like fetch("/api/v1/...")

   # All should import and use:
   import { getApiBase } from "../lib/api-client";
   // OR
   import { apiClient } from "../lib/api-client";
   ```

**Code Reference:**
- Implementation: `frontend/app/lib/api-client.ts` → `getApiBase()` function
- Used by: All admin pages (guests, connections, settings, etc.)

---

#### Unified Admin UI Baseline (2026-01-05)

**Overview:**
All admin pages now use the unified AdminShell component for consistent layout, navigation, and branding.

**Pages using Admin UI baseline:**
- /guests - Guest CRM (list, detail, timeline)
- /settings/branding - Branding settings
- /connections - Channel connections management
- /channel-sync - Sync monitoring console
- /ops/status - System health checks
- /ops/runbook - Operations documentation

**Layout Components:**
- `AdminShell` (frontend/app/components/AdminShell.tsx) - Unified shell with sidebar navigation
- Individual `layout.tsx` files per route handle auth, role checks, and wrap children in AdminShell

**Navigation Structure:**
- Übersicht: Dashboard
- Betrieb: Objekte, Buchungen, Verfügbarkeit, **Systemstatus** (admin-only), **Runbook** (admin-only)
- Channel Manager: Verbindungen, Sync-Protokoll
- CRM: Gäste
- Einstellungen: Branding, Rollen & Rechte, Plan & Abrechnung

**Finding Ops Pages:**
- **Systemstatus** (/ops/status): Real-time system health monitoring - admin role required
- **Runbook** (/ops/runbook): Operations troubleshooting guide - admin role required
- Both pages accessible via sidebar navigation under "Betrieb" section
- Requires NEXT_PUBLIC_ENABLE_OPS_CONSOLE=1 environment variable

**Design Consistency:**
- All pages use German language for UI text
- Platform names replaced with neutral labels (Plattform A/B/C)
- Consistent typography, spacing (4/8/12/16/24 px rhythm)
- Indigo primary palette with gray neutrals
- No third-party brand names in user-facing UI

**Finding Admin Pages:**
All admin routes now show sidebar navigation. Access via:
- Direct URL: `/guests`, `/connections`, `/channel-sync`, `/ops/status`, `/ops/runbook`
- Sidebar navigation from any admin page
- Login redirect preserves destination path

**UI Troubleshooting Quick Checks:**

If UI shows 404 errors or empty data when API is healthy:

1. **Check API Base URL Resolution:**
   ```javascript
   // In browser console:
   console.log(process.env.NEXT_PUBLIC_API_BASE)
   // Should show: https://api.<domain>
   ```
   - If undefined, check environment variables or hostname derivation (admin.* → api.*)
   - See "Admin UI API Base URL Resolution" section above for details

2. **Check Browser Network Tab:**
   ```
   Expected: Request to https://api.<domain>/api/v1/...
   Wrong:    Request to https://admin.<domain>/api/v1/... (404)
   ```
   - If seeing admin.* in API URLs → API base URL misconfigured

3. **Check JWT Session:**
   ```javascript
   // In browser console:
   document.cookie.split(';').find(c => c.trim().startsWith('sb-access-token'))
   // Should exist and not be expired
   ```
   - If missing → session expired, logout and login again
   - Check browser dev tools → Application → Cookies → sb-access-token

4. **Check Admin Role (for /ops pages):**
   - /ops/status and /ops/runbook require admin role
   - Non-admin users see "Access Denied" message (not a bug)
   - Verify user has role='admin' in team_members table

5. **Common UI Issues:**
   - **Sidebar doesn't show Ops pages** → User role is not admin
   - **Page shows blank/empty** → JWT expired, refresh or re-login
   - **404 on all API calls** → NEXT_PUBLIC_API_BASE not set or wrong
   - **Mixed content errors** → Frontend HTTPS but API configured as HTTP

---

## Admin UI Sidebar Architecture (Single Source of Truth)

**Purpose:** Ensure consistent navigation across ALL authenticated routes in the frontend admin UI.

**Problem Solved:** Previously, some pages used cookie-based authentication while others used Supabase server-side auth, leading to inconsistent user data (role, agency name) and navigation visibility across routes.

**Architecture Overview:**

1. **Navigation Configuration (Single Source):**
   - File: `frontend/app/components/AdminShell.tsx`
   - Constant: `NAV_GROUPS` (lines 34-73)
   - All navigation groups and items are defined here
   - Supports role-based visibility via `roles: ["admin"]` property
   - Supports plan-gating via `planLocked: true` property

2. **Server-Side Authentication (Shared Utility):**
   - File: `frontend/app/lib/server-auth.ts`
   - Function: `getAuthenticatedUser(currentPath)`
   - ALL authenticated routes use this utility for consistent:
     - Supabase session validation with redirect
     - Database role lookup from `team_members` table
     - Agency name resolution from `agencies` table
     - Normalized user data for AdminShell props

3. **Layout Pattern (Standardized):**
   - Every authenticated route has a `layout.tsx` that:
     - Calls `getAuthenticatedUser('/route-path')`
     - Wraps children in `<AdminShell>` with user data
     - Uses `dynamic = 'force-dynamic'` export config
   - Examples:
     - `frontend/app/guests/layout.tsx`
     - `frontend/app/properties/layout.tsx`
     - `frontend/app/connections/layout.tsx`
     - `frontend/app/settings/*/layout.tsx`

**Navigation Groups:**

Current sidebar groups (as of implementation):
1. **Übersicht** (Overview): Dashboard
2. **Betrieb** (Operations): Properties, Bookings, Availability, Systemstatus*, Runbook*
3. **Channel Manager**: Connections, Sync-Protokoll
4. **CRM**: Guests
5. **Einstellungen** (Settings): Branding, Roles*, Billing (plan-locked)

*Items marked with asterisk are admin-only (visible when `userRole === "admin"`)

**How to Add a New Sidebar Item:**

1. **Add Route Files:**
   ```bash
   mkdir -p frontend/app/my-feature
   touch frontend/app/my-feature/layout.tsx
   touch frontend/app/my-feature/page.tsx
   ```

2. **Create Layout (Use Shared Auth):**
   ```typescript
   // frontend/app/my-feature/layout.tsx
   import AdminShell from "../components/AdminShell";
   import { getAuthenticatedUser } from "../lib/server-auth";

   export const dynamic = 'force-dynamic';
   export const revalidate = 0;
   export const fetchCache = 'force-no-store';

   export default async function MyFeatureLayout({ children }) {
     const userData = await getAuthenticatedUser('/my-feature');
     return (
       <AdminShell
         userRole={userData.role}
         userName={userData.name}
         agencyName={userData.agencyName}
       >
         {children}
       </AdminShell>
     );
   }
   ```

3. **Add Nav Item to AdminShell:**
   ```typescript
   // frontend/app/components/AdminShell.tsx
   // Find the appropriate NAV_GROUP and add:
   {
     label: "Betrieb",
     items: [
       // ... existing items ...
       { label: "My Feature", href: "/my-feature", icon: "🎯" },
       // Optional: restrict to admins
       { label: "Admin Feature", href: "/admin-feature", icon: "🔒", roles: ["admin"] },
     ],
   }
   ```

4. **Verify Build:**
   ```bash
   cd frontend && npm run build
   # Should compile successfully
   ```

**Troubleshooting:**

1. **Sidebar item not showing:**
   - Check `NAV_GROUPS` in AdminShell.tsx includes the item
   - Check `roles` property - if set to `["admin"]`, only admins see it
   - Verify `userRole` is correctly passed from layout to AdminShell
   - Check browser console for any errors

2. **Different sidebar on different pages:**
   - This should NOT happen with unified architecture
   - Verify all layouts use `getAuthenticatedUser()` utility
   - Check that no layouts are reading cookies directly
   - Ensure all layouts import AdminShell from same file

3. **User role not resolving:**
   - Check Supabase session is valid (not expired)
   - Verify `team_members` table has active row for user
   - Check `profiles.last_active_agency_id` points to correct agency
   - Review `server-auth.ts` role resolution logic

**Architecture Benefits:**

- ✅ **Consistent UX:** Same navigation visible on all pages
- ✅ **Single Source of Truth:** Nav config in one place (AdminShell.tsx)
- ✅ **Secure:** Server-side Supabase auth with database role lookup
- ✅ **Maintainable:** Add new pages by following standard pattern
- ✅ **Role-Based:** Supports admin-only items via `roles` property
- ✅ **Plan-Gating:** Supports locked features via `planLocked` property

**Related Files:**
- `frontend/app/components/AdminShell.tsx` - Navigation config + UI component
- `frontend/app/lib/server-auth.ts` - Shared authentication utility
- `frontend/app/*/layout.tsx` - Individual route layouts (all use shared auth)

### Admin UI API Calls - Prevent Relative URL Bugs

**Symptom:** Admin UI page shows HTTP 404 in browser console when calling API:
```
GET https://admin.fewo.kolibri-visions.de/api/v1/guests/<id> → 404
```

**Root Cause:**
- Page uses relative API URL: `fetch("/api/v1/guests/...")`
- Browser resolves this against current domain (admin.*) instead of API backend (api.*)
- Backend API lives at `https://api.fewo.kolibri-visions.de`

**Fix Pattern (Mandatory):**

ALL client-side API calls MUST use the centralized API helper:

```typescript
// CORRECT - Use getApiBase() for fetch calls
import { getApiBase } from "../../lib/api-client";

const apiBase = getApiBase();
const response = await fetch(`${apiBase}/api/v1/guests/${id}`, {
  credentials: "include",
});

// OR use apiClient wrapper (preferred when token-based auth):
import { apiClient } from "../../lib/api-client";

const data = await apiClient.get(`/api/v1/guests/${id}`, accessToken);
```

```typescript
// WRONG - Never use relative URLs
const response = await fetch("/api/v1/guests/${id}"); // ❌ BUG!
```

**How getApiBase() Works:**

Resolution order (see `frontend/app/lib/api-client.ts`):
1. **NEXT_PUBLIC_API_BASE** env var (preferred, explicit)
2. **Hostname derivation:** `admin.fewo.* → api.fewo.*`
3. **HTTPS upgrade:** If frontend is HTTPS, upgrades HTTP API base to HTTPS (prevent mixed content)

**Quick Debugging Checklist:**

When investigating API 404 errors in Admin UI:

1. **Check browser DevTools Network tab:**
   - Look at the request URL hostname
   - Should be `api.fewo.kolibri-visions.de`, NOT `admin.fewo.kolibri-visions.de`

2. **Check source code:**
   ```bash
   cd frontend
   # Find all relative API fetch calls (should return EMPTY)
   rg -n --fixed-strings 'fetch("/api/' app/
   rg -n --fixed-strings "fetch('/api/" app/
   ```

3. **Verify env var (if set):**
   ```bash
   # In Coolify deployment settings:
   NEXT_PUBLIC_API_BASE=https://api.fewo.kolibri-visions.de
   ```

4. **Check runtime API base:**
   - Open browser console on admin page
   - Run: `fetch('/_next/static/...')` (check if HTTPS)
   - Derivation should work: admin.* → api.*

**Prevention:**

- Code review: Reject any `fetch("/api/v1/...")` patterns
- Use ESLint rule (future): Warn on relative `/api/` URLs in fetch calls
- Always import and use `getApiBase()` or `apiClient` from `lib/api-client.ts`

**Related Files:**
- `frontend/app/lib/api-client.ts` - API base URL resolution + apiClient wrapper
- `frontend/app/guests/[id]/page.tsx` - Example: Guest detail using `apiClient.get()` with auth
- `frontend/app/guests/page.tsx` - Example: Guest list using `apiClient.get()` with auth

#### 403 Forbidden - Missing Authorization Header

**Symptom:** Admin UI page loads but API calls return HTTP 403:
```
GET https://api.fewo.kolibri-visions.de/api/v1/guests/<id> → 403
Response: {"detail": "Not authenticated"}
```

**Root Causes:**

1. **Missing Authorization header** in client-side fetch call
2. **Missing access token** from Supabase session
3. **Using raw fetch()** instead of authenticated API client

**Fix Pattern (Mandatory):**

ALL authenticated API calls MUST:
1. Use `useAuth()` hook to get the access token
2. Pass token to `apiClient` methods OR add Authorization header manually
3. Handle 401/403 errors with actionable user message

```typescript
// CORRECT - Use apiClient with auth token
import { useAuth } from "../../lib/auth-context";
import { apiClient, ApiError } from "../../lib/api-client";

export default function MyPage() {
  const { accessToken } = useAuth();

  useEffect(() => {
    const fetchData = async () => {
      if (!accessToken) {
        setError("Session expired. Please log in again.");
        return;
      }

      try {
        const data = await apiClient.get(`/api/v1/guests/${id}`, accessToken);
        // Use data...
      } catch (err) {
        if (err instanceof ApiError && (err.status === 401 || err.status === 403)) {
          setError("Session expired or not authenticated. Please log in again.");
        }
      }
    };
    fetchData();
  }, [accessToken]);
}
```

```typescript
// WRONG - Raw fetch without auth
const response = await fetch(`${apiBase}/api/v1/guests/${id}`); // ❌ NO AUTH!
```

**How apiClient Handles Auth:**

See `frontend/app/lib/api-client.ts`:
```typescript
export async function apiRequest(endpoint, options = {}) {
  const { token, ...fetchOptions } = options;
  const headers = new Headers();

  // Automatically adds Authorization header if token provided
  if (token) {
    headers.set("Authorization", `Bearer ${token}`);
  }

  const url = endpoint.startsWith("http") ? endpoint : `${API_BASE}${endpoint}`;
  return fetch(url, { ...fetchOptions, headers });
}
```

**Debug Checklist for 403 Errors:**

1. **Check browser DevTools Network tab:**
   - Request Headers should include: `Authorization: Bearer eyJ...`
   - If missing → auth token not passed to apiClient

2. **Check React DevTools / Console:**
   ```javascript
   // In browser console on admin page:
   console.log("Access token:", localStorage.getItem('supabase.auth.token'))
   ```
   - If null/empty → Supabase session expired, user needs to login

3. **Check source code:**
   ```bash
   cd frontend/app
   # Find pages using raw fetch without auth (should return EMPTY)
   rg -n 'fetch.*api/v1' --type tsx | grep -v apiClient
   ```

4. **Verify useAuth() is called:**
   - Page must import: `import { useAuth } from "../../lib/auth-context";`
   - Component must call: `const { accessToken } = useAuth();`
   - Effect must check: `if (!accessToken) return;`

**Common Mistakes:**

- ❌ Using `credentials: "include"` without Authorization header (backend expects JWT, not cookies)
- ❌ Forgetting to pass `accessToken` to `apiClient.get()`
- ❌ Not handling 401/403 errors with user-friendly message
- ❌ Using raw `fetch()` instead of `apiClient` wrapper

**Prevention:**

- Code review: ALL API calls must use `apiClient` with `accessToken`
- Error boundaries: Show "Session expired" message on 401/403
- Consistent pattern: Follow guests list page example (frontend/app/guests/page.tsx)

#### Next.js 404 (Missing Route) vs API 404

**Symptom:** Clicking a link in Admin UI opens a page that shows Next.js 404 error:
```
404 | This page could not be found
```

**Browser network tab shows:**
```
GET /bookings/<uuid>?_rsc=... → 404
```

**Root Cause:**

This is a **Next.js route 404**, NOT an API 404. The difference:
- **Next route missing** → Browser requests `/bookings/<id>` from Next.js app, but no `app/bookings/[id]/page.tsx` exists
- **API 404** → Route exists, but backend API returns 404 for resource

**How to Distinguish:**

1. **Check URL in browser address bar:**
   - If URL is `https://admin.fewo.../bookings/<id>` → Next route issue
   - Network tab shows `?_rsc=...` parameter → Next.js RSC (React Server Component) request

2. **Check browser DevTools Network tab:**
   - Request URL ends with `?_rsc=...` → Next.js routing, NOT backend API
   - No `Authorization` header in request → Not an API call
   - Response is HTML/text, not JSON → Next.js 404 page

3. **Backend API 404 would show:**
   - Request URL: `https://api.fewo.../api/v1/bookings/<id>` (API domain)
   - Authorization header present
   - JSON response: `{"detail": "Not found"}`

**Fix Pattern:**

Create the missing Next.js route:

1. **Create route directory:**
   ```bash
   mkdir -p frontend/app/bookings/[id]
   ```

2. **Create page component:**
   ```typescript
   // frontend/app/bookings/[id]/page.tsx
   "use client";

   import { useState, useEffect } from "react";
   import { useParams } from "next/navigation";
   import { useAuth } from "../../lib/auth-context";
   import { apiClient, ApiError } from "../../lib/api-client";

   export default function BookingDetailPage() {
     const params = useParams();
     const bookingId = params?.id as string;
     const { accessToken } = useAuth();

     useEffect(() => {
       const fetchData = async () => {
         if (!accessToken) return;
         const data = await apiClient.get(
           `/api/v1/bookings/${bookingId}`,
           accessToken
         );
         // Render data...
       };
       fetchData();
     }, [bookingId, accessToken]);

     return <div>Booking Details...</div>;
   }
   ```

3. **IMPORTANT - Use correct import paths:**
   - From `app/bookings/[id]/page.tsx`: `import { ... } from "../../lib/..."`
   - NOT `../../../lib/...` (will cause webpack module not found error)

4. **Use apiClient with auth** (avoid relative URLs):
   - ✅ `apiClient.get(\`/api/v1/bookings/${id}\`, accessToken)`
   - ❌ `fetch(\`/api/v1/bookings/${id}\`)` (no auth)

**Verification:**

```bash
# Build must succeed
cd frontend && npm run build
# ✓ Compiled successfully

# Check route exists
ls frontend/app/bookings/[id]/page.tsx
```

**Common Mistakes:**

- ❌ Thinking it's an API issue when it's a Next route issue
- ❌ Wrong import paths (too many `../`)
- ❌ Forgetting to add auth to API calls in new route
- ❌ Not creating a layout if auth is needed

**Related Files:**
- `frontend/app/bookings/[id]/page.tsx` - Booking detail route
- `frontend/app/guests/[id]/page.tsx` - Similar pattern (guest detail)

#### Guest Booking History Count Badge Shows 0

**Symptom:** Admin UI → Guests → Guest Detail → Tab shows "Buchungshistorie (0)" but the booking history list renders multiple booking cards (e.g., 4 entries).

**Root Cause:**

The tab count badge used `guest.total_bookings` from the guest record, but the actual booking history is fetched separately via the timeline API. If `guest.total_bookings` is 0 or stale, the badge shows 0 even though timeline items are rendered.

**Fix Pattern:**

Derive the count from the actual timeline data fetched, not from the guest record:

```typescript
// Store timeline total from API response
const [timelineTotal, setTimelineTotal] = useState<number>(0);

// When fetching timeline
const timelineData: TimelineResponse = await apiClient.get(
  `/api/v1/guests/${guestId}/timeline?limit=10&offset=0`,
  accessToken
);
setTimeline(timelineData.bookings);
setTimelineTotal(timelineData.total ?? 0);

// In tab label - use max to handle cases where API total is 0 but items exist
<button>
  Buchungshistorie ({Math.max(timelineTotal, timeline.length)})
</button>
```

**Why `Math.max(timelineTotal, timeline.length)`:**
- `timelineTotal` is the API's total count (may be paginated total)
- `timeline.length` is the actual items fetched (limited to 10)
- If API returns `total: 0` but items exist → show items count (prevents 0 badge with visible items)
- If API returns correct total → show total (e.g., 15 when showing 10 items)

**Prevention:**
- Always derive UI counts from the actual data being rendered, not from separate/stale fields
- For paginated data, display either `items.length` or `max(apiTotal, items.length)`

**Related Files:**
- `frontend/app/guests/[id]/page.tsx:218` - Booking history tab count

#### Booking → Zum Gast Navigation (Guard Against 404)

**Symptom:** Admin UI → Booking Details → Clicking "Zum Gast →" button navigates to `/guests/<guest_id>` but returns 404 with `{"detail":"Guest with ID '...' not found"}`.

**Root Cause:**

Booking records may have a `guest_id` that references a non-existent guest record (orphaned reference, deleted guest, or data inconsistency). The UI previously rendered the "Zum Gast" link without verifying the guest exists, causing 404 navigation.

**Fix Pattern:**

Before rendering the guest link, verify the guest exists:

```typescript
const [guestExists, setGuestExists] = useState<boolean | null>(null);

// After fetching booking
if (bookingData.guest_id) {
  try {
    await apiClient.get(`/api/v1/guests/${bookingData.guest_id}`, accessToken);
    setGuestExists(true);
  } catch (guestErr) {
    if (guestErr instanceof ApiError && guestErr.status === 404) {
      setGuestExists(false);
    }
  }
}

// Conditional rendering
{booking.guest_id && guestExists === true && (
  <Link href={`/guests/${booking.guest_id}`}>Zum Gast →</Link>
)}
{booking.guest_id && guestExists === false && (
  <div>Gast nicht verknüpft</div>
)}
```

**UI Behavior:**
- If guest exists (200) → Show "Zum Gast →" link
- If guest missing (404) → Show "Gast nicht verknüpft" text (no link)
- In IDs section → Show "Gast-ID (nicht verknüpft)" label when guest doesn't exist

**Prevention:**
- Always verify foreign key references before navigation
- Use guard checks for optional relationships to prevent 404 user experience
- For orphaned references, show inline status ("nicht verknüpft") instead of broken links

**Related Files:**
- `frontend/app/bookings/[id]/page.tsx:72-89,194-206,313` - Guest existence check and conditional rendering

#### Booking Details Shows "NaN €"

**Symptom:** Admin UI → Booking Details → Preisinformationen section shows "Steuer: NaN €" (or other fields like "Subtotal: NaN €", "Cleaning Fee: NaN €").

**Root Cause:**

Monetary fields from the API (`tax`, `subtotal`, `cleaning_fee`, etc.) are strings (e.g., `"0.00"`) but may be `null`, `undefined`, or empty strings in some cases. The `formatCurrency()` function called `parseFloat(amount)` directly without validation, causing:
- `parseFloat(null)` → `NaN`
- `parseFloat(undefined)` → `NaN`
- `parseFloat("")` → `NaN`
- `Intl.NumberFormat().format(NaN)` → `"NaN €"`

**Fix Pattern:**

Add a `safeNumber` helper to coerce invalid values to 0 before formatting:

```typescript
const safeNumber = (value: string | null | undefined): number => {
  if (value === null || value === undefined || value === "") return 0;
  const num = parseFloat(value);
  return isNaN(num) ? 0 : num;
};

const formatCurrency = (amount: string | null | undefined) => {
  return new Intl.NumberFormat("de-DE", {
    style: "currency",
    currency: "EUR",
  }).format(safeNumber(amount));
};
```

**Result:**
- Null/undefined/empty monetary fields → display as `"0,00 €"`
- Invalid strings → display as `"0,00 €"`
- Valid strings like `"42.50"` → display as `"42,50 €"`
- Never renders `"NaN €"`

**Prevention:**
- Always validate/coerce monetary values before formatting
- Use safe parsers for all numeric fields that may be null/undefined
- Add regression guards: `isNaN(num) ? 0 : num`

**Related Files:**
- `frontend/app/bookings/[id]/page.tsx:117-128` - safeNumber helper and formatCurrency

#### DSGVO / Guest vs Booking Linkage (Best Practice)

**Data Model Philosophy:**

The PMS follows DSGVO-minimal best practices for guest-booking relationships:
- **Guest is optional**: Bookings can exist without a linked guest (guest_id=NULL is valid)
- **Booking is standalone**: Preserves business records even after guest deletion (DSGVO right to erasure)
- **When guest_id is set, it MUST reference a valid guest**: Foreign key constraint prevents orphaned references
- **Never use Auth User UUID as guest_id**: Guests are CRM entities, not authentication entities

**Database Constraints:**

```sql
-- Foreign key with ON DELETE SET NULL
ALTER TABLE bookings
ADD CONSTRAINT fk_bookings_guest_id
FOREIGN KEY (guest_id) REFERENCES guests(id) ON DELETE SET NULL;
```

**ON DELETE SET NULL behavior:**
- When guest deleted (DSGVO erasure request) → booking.guest_id becomes NULL
- Booking history preserved for business/accounting purposes
- Guest data (PII) removed from system
- Booking shows "Gast nicht verknüpft" in UI

**API Validation (Create/Update):**

When creating/updating bookings:
1. **If `guest_id` provided:** Validate guest exists in same agency, else 422 error
2. **If guest data provided** (email/phone/name): Upsert guest, then set guest_id
3. **If neither:** guest_id remains NULL (booking without CRM linkage)

```python
# In booking service create_booking():
if guest_id_input is not None:
    guest_exists = await db.fetchval(
        "SELECT EXISTS(SELECT 1 FROM guests WHERE id = $1 AND agency_id = $2)",
        guest_id_input, agency_id
    )
    if not guest_exists:
        raise ValidationException(
            f"Guest with ID '{guest_id_input}' not found or does not belong to this agency"
        )
```

**Migration Safety:**

Migration `20260105150000_enforce_booking_guest_fk.sql` handles existing bad data:
1. **Cleanup orphaned references:** `UPDATE bookings SET guest_id=NULL WHERE guest_id NOT IN (SELECT id FROM guests)`
2. **Add FK constraint:** Safe after cleanup, no data loss
3. **Add index:** Speeds up FK checks and guest-based queries

**Troubleshooting:**

| Symptom | Root Cause | Resolution |
|---------|-----------|------------|
| 422 "Guest with ID '...' not found" on booking creation | Provided guest_id doesn't exist in guests table | Verify guest exists, or provide guest data for upsert instead |
| Booking shows "Gast nicht verknüpft" | Guest was deleted (DSGVO erasure) or guest_id was NULL | Expected behavior - booking preserved, guest link cleared |
| Cannot create booking with guest_id | Guest belongs to different agency | Use guest from same agency, or create new guest |

**Related Files:**
- `backend/app/services/booking_service.py:540-553` - Guest existence validation
- `supabase/migrations/20260105150000_enforce_booking_guest_fk.sql` - FK constraint migration

**Production Issue: 500 ResponseValidationError After FK ON DELETE SET NULL**

**Symptom:** After applying FK constraint migration, `GET /api/v1/bookings/{id}` returns 500 error:
```json
{
  "detail": [
    {
      "loc": ["response", "guest_id"],
      "msg": "UUID input should be a string/bytes/UUID object",
      "input": null
    }
  ]
}
```

**Root Cause:**

After FK constraint with `ON DELETE SET NULL`, bookings can have `guest_id=NULL` (guest deleted or booking created without guest). However, `BookingResponse` schema defined `guest_id: UUID` (non-nullable), causing FastAPI response validation to fail when serializing bookings with NULL guest_id.

**Fix:**

Change `guest_id` in `BookingResponse` schema to nullable:
```python
# Before (breaks with NULL guest_id)
guest_id: UUID = Field(description="Guest making the booking")

# After (allows NULL per DSGVO design)
guest_id: Optional[UUID] = Field(
    default=None,
    description="Guest making the booking (nullable - guest optional per DSGVO design)"
)
```

**Prevention:**
- When adding FK constraints with `ON DELETE SET NULL`, ensure response schemas allow NULL
- Test API responses with NULL foreign keys before deploying constraints
- Align schema nullability with database column constraints

**Related Files:**
- `backend/app/schemas/bookings.py:662` - BookingResponse.guest_id now Optional[UUID]

#### Guest Booking History Consistency

**Symptom:** Guests list shows `total_bookings=0` but guest detail timeline displays multiple bookings (e.g., 4 entries shown).

**Root Cause:**

Inconsistency between two booking count queries:
- **Timeline API** (`GET /api/v1/guests/{id}/timeline`): Counts ALL bookings WHERE `bookings.guest_id = guest.id`
- **Old trigger** (`update_guest_statistics`): Filtered by status, excluded cancelled/declined/no_show bookings
- Result: UX confusion - "0 bookings listed but 4 shown in history"

**DSGVO/Business Rule:**

Guest booking history follows FK-based linkage ONLY:
- **Source of truth**: `bookings.guest_id` (FK to guests.id)
- **Counts ALL bookings** linked to guest (including cancelled)
- **Does NOT count**: Bookings with `guest_id=NULL` (guest optional by design)
- **Does NOT infer**: By auth_user_id, email, or other heuristics

**Fix:**

Align `total_bookings` computation with timeline query:
```sql
-- Updated trigger (migration 20260105160000)
total_bookings = (
  SELECT COUNT(*)
  FROM bookings
  WHERE guest_id = NEW.guest_id
    -- No status filter - count ALL bookings
    -- Aligns with timeline API behavior
)
```

**Timeline Query (unchanged, correct):**
```sql
-- backend/app/services/guest_service.py:675-680
SELECT COUNT(*)
FROM bookings b
WHERE b.guest_id = $1 AND b.agency_id = $2 AND b.deleted_at IS NULL
```

**Why Count Cancelled Bookings:**
- Part of guest's complete history (business record)
- Aligns with timeline display (shows all bookings regardless of status)
- Consistent UX: count matches what user sees

**Troubleshooting:**

| Symptom | Root Cause | Resolution |
|---------|-----------|------------|
| total_bookings=0 but timeline shows bookings | Old trigger had status filter | Apply migration 20260105160000 to align trigger |
| Expected bookings not shown in timeline | Bookings have guest_id=NULL | Link bookings to guest via booking create/update with valid guest_id |
| Timeline shows 0 for guest with bookings in agency | Bookings linked to different guest_id | Verify correct guest linkage in bookings table |

**Prevention:**
- Use same WHERE clause for counts and list queries
- Document filtering rules in API comments
- Test count endpoints against list endpoints in integration tests

**Related Files:**
- `supabase/migrations/20260105160000_align_guest_total_bookings_with_timeline.sql` - Fixed trigger
- `backend/app/services/guest_service.py:675-680` - Timeline count query (source of truth)

---

## Admin UI — Visual QA Checklist (Layout v2)

**Purpose:** Browser-based verification checklist for Admin UI layout and visual quality after Theme v2 updates.

**What Changed in v2:**
- Primary palette: Blue/indigo (#2563eb) instead of green (trustworthy, modern)
- Icons: Lucide React icons (professional, consistent) instead of emojis
- Sidebar scrollbar: Hidden (scrollbar-hide utility) but scroll still works
- Header overlap: Fixed with sticky + blur background (no content overlap)
- Sidebar animation: Removed width transitions to eliminate jank on navigation
- Brand header: Improved with gradient avatar + divider
- Collapsed state: Better tooltips, rounded-2xl icon containers, professional spacing

**Browser Verification Checklist:**

```bash
# Navigate to Admin UI
open https://admin.fewo.kolibri-visions.de/login

# After login, verify Layout v2:
□ Background is very light slate (#f8fafc)
□ Sidebar uses Lucide icons (not emojis)
□ Sidebar scrollbar is hidden (but scroll works if needed)
□ Sidebar has NO width animation jank when navigating between pages
□ Brand header shows gradient avatar + agency name with divider below
□ Active nav item has blue background (#2563eb) with white icon
□ Inactive nav items have rounded-2xl light backgrounds
□ Topbar is sticky with blur background (bg-bo-bg/80 backdrop-blur-md)
□ Topbar does NOT overlap content when scrolling
□ Primary buttons use blue (#2563eb)
□ Status badges use semantic colors (green=success, red=danger, blue=info)
□ Collapsed sidebar shows tooltips on hover
□ Collapsed sidebar icons are in rounded-2xl containers (not circles)
□ Search bar in topbar uses Lucide Search icon
□ Notification buttons use Lucide icons (MessageSquare, Bell)

# Test navigation between pages (NO sidebar jank):
- Click Dashboard → no sidebar width animation
- Click Buchungen → no sidebar width animation
- Click Objekte → no sidebar width animation
- Click Verbindungen → no sidebar width animation
- Sidebar should feel stable, only color changes on active item

# Test scrolling (header doesn't overlap):
- Go to /bookings (list page with table)
- Scroll down → header stays at top with blur, table rows visible below header
- Header NEVER covers table content

# Test collapsed mode:
- Click collapse button (ChevronLeft icon)
- Sidebar width changes to narrow (w-24)
- Only icons visible, text hidden
- Hover over nav icons → tooltips appear with labels
- No visible scrollbar in collapsed mode
- Icons in rounded-2xl containers with proper spacing

# Test palette consistency:
- Check buttons: should use blue primary (#2563eb)
- Check active states: blue not green
- Check status chips: green for confirmed, red for cancelled, blue for requested
```

**Common Issues:**

**Problem:** Sidebar still shows scrollbar

**Solution:**
```bash
# Check if scrollbar-hide class is applied
# DevTools → Inspect nav element → Should have class "scrollbar-hide"
# CSS should have:
#   -ms-overflow-style: none;
#   scrollbar-width: none;
#   ::-webkit-scrollbar { display: none; }

# If missing, rebuild frontend:
cd frontend && rm -rf .next && npm run dev
```

**Problem:** Sidebar still animates/janks on navigation

**Solution:**
```bash
# Check sidebar aside element in DevTools
# Should NOT have "transition-all" or "transition-width" classes
# Only collapse state changes width: isCollapsed ? "w-24" : "w-72"
# No duration/animation on width change

# Verify AdminShell.tsx line ~287-291:
# Should be: className={`hidden lg:block flex-shrink-0 ${isCollapsed ? "w-24" : "w-72"}`}
# NOT: className="... transition-all duration-300 ..."
```

**Problem:** Header overlaps content when scrolling

**Solution:**
```bash
# Check header element in DevTools
# Should have: "sticky top-0 z-30 bg-bo-bg/80 backdrop-blur-md"
# Ensure main content is NOT using negative margin or absolute positioning
# Content should flow naturally below header

# Verify layout structure:
# <div flex flex-col>
#   <header sticky> ... header content
#   <main flex-1> ... page content (starts AFTER header, not under it)
```

**Problem:** Icons still showing as emojis

**Solution:**
```bash
# Check if lucide-react is installed:
cd frontend && npm list lucide-react
# Should show: lucide-react@x.x.x

# If not installed:
npm install lucide-react

# Check AdminShell.tsx imports (lines 6-25):
# Should import from "lucide-react" (LayoutDashboard, Home, Calendar, etc.)

# Check NAV_GROUPS icon properties (lines 54-93):
# Should be: icon: LayoutDashboard (not icon: "📊")

# Rebuild:
rm -rf .next && npm run dev
```

**Problem:** Primary color still green instead of blue

**Solution:**
```bash
# Check globals.css Theme v2 palette (lines 24-60)
# Should have:
#   --bo-primary: #2563eb;  /* Primary blue (trustworthy) */
#   --bo-primary-hover: #1e3a8a;

# NOT:
#   --bo-primary: #4C6C5A;  /* Primary green */

# Check DevTools → Elements → :root → Styles
# Verify --bo-primary is #2563eb

# If wrong, ensure latest commit is deployed
# Hard refresh browser: Cmd+Shift+R
```

**Related Sections:**
- [Admin UI Sidebar Architecture (Single Source of Truth)](#admin-ui-sidebar-architecture-single-source-of-truth)
- [Admin UI Visual Style (Backoffice Theme v1)](#admin-ui-visual-style-backoffice-theme-v1)

---

## Admin UI — Header: Language Switch + Profile Dropdown

**Purpose:** Document the header changes that replaced search with language dropdown and added profile menu.

**What Changed:**
- **Search field removed** - Previously occupied center of topbar
- **Language dropdown added** - Shows current language flag (🇩🇪/🇬🇧/🇸🇦) with dropdown to switch
- **Profile dropdown added** - User icon in top-right opens menu with profile links
- **Page title simplified** - Left side shows only page name (e.g. "Verbindungen"), removed "Hello, email!" greeting

**Language Switcher:**
- **Location:** Top-right area, before notification icons
- **Display:** Current language flag + code (e.g. 🇩🇪 DE)
- **Dropdown:** Click to show all languages (Deutsch, English, العربية)
- **Persistence:** Selection saved in localStorage with key `bo_lang`
- **Supported languages:**
  - `de` - Deutsch (🇩🇪)
  - `en` - English (🇬🇧)
  - `ar` - العربية (🇸🇦)

**Profile Dropdown:**
- **Location:** Far right of topbar, after notifications
- **Icon:** User icon (Lucide `User` component)
- **Dropdown content:**
  - User display name (extracted from email if needed)
  - Role badge (e.g. "Admin", "User")
  - Links:
    - Profil → `/profile`
    - Profil bearbeiten → `/profile/edit`
    - Sicherheit → `/profile/security`

**Profile Routes:**
All profile routes use AdminShell layout with authentication:
- `/profile` - Profile overview (stub page)
- `/profile/edit` - Edit profile settings (stub page)
- `/profile/security` - Security settings (stub page)

Note: Profile pages are currently minimal stubs showing "Demnächst verfügbar" (Coming soon). Full implementation planned for future phase.

**localStorage Keys:**
- `bo_lang` - Stores selected language code (de/en/ar)
- `sidebar-collapsed` - Stores sidebar collapse state (unchanged)

**Verification Checklist:**
```bash
# Open Admin UI
open https://admin.fewo.kolibri-visions.de/dashboard

# Visual checks:
□ Header left shows only page title (e.g. "Dashboard")
□ No "Hello, email!" greeting visible
□ Language dropdown visible (flag + code)
□ Click language dropdown → shows all 3 languages
□ Select language → persists after page reload
□ Profile icon visible (User icon)
□ Click profile → dropdown opens with name, role, and 3 links
□ Profile links navigate to correct routes
□ Profile pages load (even if showing "Coming soon")

# localStorage verification:
# DevTools → Application → localStorage → Check for "bo_lang" key after switching language
```

**Related Sections:**
- [Admin UI — Visual QA Checklist (Layout v2)](#admin-ui--visual-qa-checklist-layout-v2)
- [Admin UI Sidebar Architecture (Single Source of Truth)](#admin-ui-sidebar-architecture-single-source-of-truth)

---

## Admin UI Header Verification (Automated)

**Purpose:** Automated headless browser testing of Admin UI header features using Playwright in Docker.

**Smoke Script:** `backend/scripts/pms_admin_ui_header_smoke.sh`

**What It Verifies:**
- Login flow with credentials
- Header page title (no "Hello, email!" greeting)
- Language switcher functionality (DE → EN, localStorage persistence)
- Profile dropdown (display name, role, menu items)
- Profile stub pages navigation and content

**Usage:**
```bash
# HOST-SERVER-TERMINAL
export ADMIN_EMAIL="admin@example.com"
export ADMIN_PASSWORD="your-password"
./backend/scripts/pms_admin_ui_header_smoke.sh
# Expected: rc=0 (all tests pass)
```

**Common Issues:**

### Login Fails (Timeout or Wrong Credentials)

**Symptom:** Test 1 fails with timeout or "Login unsuccessful" message.

**Root Causes:**
1. **Wrong credentials** - ADMIN_EMAIL/ADMIN_PASSWORD incorrect
2. **2FA enabled** - Test user requires two-factor authentication
3. **Session already active** - User logged in elsewhere causing redirect
4. **Slow page load** - Login page takes longer than PW_TIMEOUT_MS

**How to Debug:**
```bash
# Run with visible browser (requires X11/display for container)
export HEADLESS=false
export ADMIN_EMAIL="admin@example.com"
export ADMIN_PASSWORD="your-password"
./backend/scripts/pms_admin_ui_header_smoke.sh

# Increase timeout
export PW_TIMEOUT_MS=60000
export ADMIN_EMAIL="admin@example.com"
export ADMIN_PASSWORD="your-password"
./backend/scripts/pms_admin_ui_header_smoke.sh
```

**Solution:**
- Verify credentials are correct for test user
- Use test user without 2FA enabled (create dedicated smoke test account if needed)
- Check screenshots in /tmp/pms_admin_ui_header_smoke/ for visual clues
- Ensure ADMIN_URL is correct and accessible from host

### Selectors Changed (UI Refactor)

**Symptom:** Tests fail with "Timeout waiting for selector" or "Element not found" errors after UI changes.

**Root Cause:** AdminShell component refactored, changing DOM structure or class names.

**Where Selectors Are Used:**
- **Login page**: `input[type="email"]`, `input[type="password"]`, `button` with text matching /login|anmelden/i
- **Header title**: `header h1`
- **Language button**: `button` containing 🇩🇪/🇬🇧 or DE/EN text
- **Language options**: `button` containing "English" or 🇬🇧
- **Profile button**: Last `header button` with `svg` child (User icon)
- **Profile menu items**: `text="Profil"`, `text="Profil bearbeiten"`, `text="Sicherheit"`
- **Stub pages**: `text="Demnächst verfügbar"`

**How to Debug:**
```bash
# Run with visible browser to see what's happening
export HEADLESS=false
export ADMIN_EMAIL="admin@example.com"
export ADMIN_PASSWORD="your-password"
./backend/scripts/pms_admin_ui_header_smoke.sh

# Check screenshots on failure
ls -lh /tmp/pms_admin_ui_header_smoke/
```

**Solution:**
- Update selectors in script heredoc (PLAYWRIGHT_EOF section)
- Add data-testid attributes to AdminShell for more stable selectors
- Consider using role-based selectors when possible

### Timeout on Slow Pages

**Symptom:** Tests fail with "Timeout exceeded" on navigation steps.

**Root Cause:** Production environment slower than expected (cold start, DB query latency, network).

**Solution:**
```bash
# Increase timeout to 60 seconds
export PW_TIMEOUT_MS=60000
export ADMIN_EMAIL="admin@example.com"
export ADMIN_PASSWORD="your-password"
./backend/scripts/pms_admin_ui_header_smoke.sh
```

### Docker Not Available

**Symptom:** Script fails with "docker: command not found" or "Cannot connect to Docker daemon".

**Root Cause:** Docker not installed or not running on host.

**Solution:**
```bash
# Check Docker status
docker --version
docker ps

# Start Docker daemon if needed (macOS)
open -a Docker

# Linux: ensure docker service running
sudo systemctl start docker
```

### Screenshots Not Saved

**Symptom:** Failure occurs but no screenshots in PW_SCREENSHOTS_DIR.

**Root Cause:** Directory not writable or mount failed in container.

**Solution:**
```bash
# Check directory permissions
ls -ld /tmp/pms_admin_ui_header_smoke
mkdir -p /tmp/pms_admin_ui_header_smoke
chmod 755 /tmp/pms_admin_ui_header_smoke

# Custom screenshots directory
export PW_SCREENSHOTS_DIR=/home/user/screenshots
mkdir -p "$PW_SCREENSHOTS_DIR"
export ADMIN_EMAIL="admin@example.com"
export ADMIN_PASSWORD="your-password"
./backend/scripts/pms_admin_ui_header_smoke.sh
```

**Related Sections:**
- [Admin UI — Header: Language Switch + Profile Dropdown](#admin-ui--header-language-switch--profile-dropdown)
- [Admin UI Static Verification (Smoke Test)](#admin-ui-static-verification-smoke-test)

---

## Frontend Build Failures (TSX/JSX Syntax Errors)

**Problem:** Coolify deployment fails with TypeScript/JSX syntax errors like:
```
Expected ',', got '{'
```

**Common Causes:**
1. **Extra closing tags** - Premature closure of JSX container
2. **Mismatched braces** - Stray `}` or `]` breaking JSX context
3. **JSX comments outside JSX** - Comments in array/object literal context

**Diagnosis:**
```bash
# Run build locally to reproduce
cd frontend && npm run build

# Check TypeScript compilation
npx tsc --noEmit --jsx preserve app/path/to/file.tsx
```

**Root Cause (connections/page.tsx Example):**
The connections page had an extra `</div>` at line 1221 that prematurely closed the main container `<div className="space-y-6">`, leaving modals outside the JSX tree.

Structure was:
```tsx
return (
  <div className="space-y-6">  {/* Line 1037 - Main container */}
    {/* Search + Table cards */}
    <div className="bg-white...">  {/* Line 1102 - Table card */}
      {/* Table content */}
    </div>  {/* Line 1220 - Closes table card */}
    </div>  {/* Line 1221 - EXTRA! Closes main container */}

    {/* Connection Details Modal */}  {/* Line 1223 - NOW OUTSIDE JSX! */}
```

**Fix:**
Remove the extra closing tag so modals remain inside the JSX tree:
```tsx
return (
  <div className="space-y-6">
    {/* Search + Table */}
    <div className="bg-white...">
      {/* Table content */}
    </div>  {/* Closes table card only */}

    {/* Modals still inside main container */}
    {selectedConnection && (<div>...</div>)}

  </div>  {/* Main container closes at end */}
);
```

**Verification:**
```bash
# Build must succeed
npm run build
# ✓ Compiled successfully
```

**Related:**
- Prerender errors about missing Supabase env vars are expected in local builds
- Coolify deployment has env vars configured via settings

---

## Frontend Build Failures (AdminShell Icon Typing)

**Problem:** Coolify deployment fails during `npm run build` with TypeScript error in AdminShell.tsx:
```
./app/components/AdminShell.tsx:169:37
Type error: Property 'strokeWidth' does not exist on type '{ className?: string }'
<Icon className="w-5 h-5" strokeWidth={1.75} />
```

**Root Cause:**
The `icon` property in `NavItem` interface was typed too narrowly:
```ts
// WRONG - only accepts className prop
icon: React.ComponentType<{ className?: string }>;
```

This prevented passing `strokeWidth` and other Lucide icon props like `size`, `color`, etc.

**Fix:**
Import and use the proper `LucideIcon` type from lucide-react:
```ts
// Correct typing
import { type LucideIcon } from "lucide-react";

interface NavItem {
  label: string;
  href: string;
  icon: LucideIcon;  // ✓ Accepts all Lucide props including strokeWidth
  roles?: string[];
  planLocked?: boolean;
}
```

**Verification:**
```bash
# Local build test
cd frontend && npm run build

# Should compile successfully with no strokeWidth errors
# All icon usages with strokeWidth={1.75} will now type-check correctly
```

**Files Changed:**
- `frontend/app/components/AdminShell.tsx` - Updated NavItem interface to use LucideIcon type

**Related:**
- All icons in AdminShell are Lucide React icons (LayoutDashboard, Home, Calendar, etc.)
- strokeWidth is a valid prop for all Lucide icons (controls line thickness)

---

## Deploy Verification + Implemented vs Verified

### Overview
This section documents the "Implemented vs Verified" best-practice workflow for production deployments.

**Status Semantics**:
- **Implemented**: Feature code merged to main branch, deployed to staging/production
- **Verified**: Automatic production verification succeeded (deploy script + health checks)

**Key Principle**: Only mark features as "Verified" after automated deploy verification succeeds in production environment.

### Deploy Verification Script

**Script**: `backend/scripts/pms_verify_deploy.sh`

**Purpose**: Automatically verify successful deployment and commit hash in production/staging.

**Usage**:
```bash
# On HOST-SERVER-TERMINAL (production/staging server)
# Basic verification
API_BASE_URL=https://api.example.com ./backend/scripts/pms_verify_deploy.sh

# With commit verification (recommended for CI/CD)
API_BASE_URL=https://api.example.com \
EXPECT_COMMIT=$(git rev-parse HEAD) \
./backend/scripts/pms_verify_deploy.sh
```

**Endpoints Checked**:

**EXPECT_COMMIT - Short SHA Support**:

The `EXPECT_COMMIT` parameter accepts both full (40-char) and short (7+ char) commit SHAs. The script uses intelligent prefix matching:

- **Short SHA** (recommended): `EXPECT_COMMIT=5767b15` - more readable, typical git convention
- **Full SHA**: `EXPECT_COMMIT=5767b154906f9edf037fc9bbc10312126698cc29` - exact match

Verification passes if the deployed `source_commit` starts with the expected prefix (case-insensitive). Output indicates "prefix match" or "exact match" for clarity.

1. `GET /health` - Basic health check
2. `GET /health/ready` - Readiness check (database connectivity)
3. `GET /api/v1/ops/version` - Deployment version metadata

**Exit Codes**:
- `0`: Success (all checks passed)
- `1`: Missing configuration (API_BASE_URL not set)
- `2`: Endpoint failure (non-200 status or parse error)
- `3`: Commit mismatch (EXPECT_COMMIT set but source_commit doesn't match)

**Example Output**:
```
╔════════════════════════════════════════════════════════════╗
║ PMS Deploy Verification                                    ║
╚════════════════════════════════════════════════════════════╝
API Base URL: https://api.example.com

[1/3] GET /health
✅ Status: 200 OK

[2/3] GET /health/ready
✅ Status: 200 OK

[3/3] GET /api/v1/ops/version
✅ Status: 200 OK
📦 Service: pms-backend
🌍 Environment: production
🔖 API Version: 0.1.0
📝 Source Commit: abc123def456
⏰ Started At: 2024-01-05T10:30:00Z

╔════════════════════════════════════════════════════════════╗
║ ✅ All checks passed!                                      ║
╚════════════════════════════════════════════════════════════╝
```

### Version Endpoint

**Marking Features as VERIFIED**:

After running `pms_verify_deploy.sh` with `EXPECT_COMMIT` and receiving exit code 0 (all checks passed):

1. Capture the script output as evidence
2. Update the corresponding entry in `backend/docs/project_status.md`:
   - Change header from `✅` to `✅ VERIFIED`
   - Add a "Verification (PROD)" subsection with:
     - Date and environment
     - Commit hash verified
     - Command executed
     - Verification results (endpoints, commit match, exit code)
     - Evidence summary

**Example Entry Update**:
```markdown
### Feature Name ✅ VERIFIED

**Date Completed:** 2026-01-05
[... implementation details ...]

**Verification (PROD)** ✅ VERIFIED

**Date**: 2026-01-05 (post-deployment)
**Environment**: Production
**Commit**: abc123def456

**Command Executed** (HOST-SERVER-TERMINAL):
```bash
API_BASE_URL=https://api.production.example.com \
EXPECT_COMMIT=abc123def456 \
./backend/scripts/pms_verify_deploy.sh
```

**Verification Results**:
- ✅ GET /health → 200 OK
- ✅ GET /health/ready → 200 OK
- ✅ GET /api/v1/ops/version → 200 OK
  - source_commit: abc123def456
- ✅ Commit verification: PASSED
- ✅ Script exit code: 0

**Evidence**: All checks passed - feature operational in production.
```


**Endpoint**: `GET /api/v1/ops/version`

**Purpose**: Returns deployment metadata for automated verification and monitoring.

**Authentication**: None required (public endpoint, safe metadata only)

**Response Schema**:
```json
{
  "service": "pms-backend",
  "source_commit": "abc123def456",
  "environment": "production",
  "api_version": "0.1.0",
  "started_at": "2024-01-05T10:30:00Z"
}
```

**Response Fields**:
- `service`: Service name (always "pms-backend")
- `source_commit`: Git commit SHA (from SOURCE_COMMIT env var, null if not set)
- `environment`: Environment name (development/staging/production)
- `api_version`: FastAPI application version
- `started_at`: ISO 8601 timestamp of process start time

**Environment Variables**:
- `SOURCE_COMMIT`: Git commit SHA (set by CI/CD pipeline)
- `ENVIRONMENT`: Environment name (development/staging/production)

**Characteristics**:
- **No database calls**: Always fast, suitable for health checks
- **No authentication**: Safe metadata only, no secrets exposed
- **Cheap**: Suitable for frequent monitoring/alerting polls

**Admin UI Version Endpoint**: `GET /api/ops/version`

The admin UI (pms-admin) also exposes a `/api/ops/version` endpoint for deployment verification.

**Response Schema**:
```json
{
  "service": "pms-admin",
  "source_commit": "81b848f...",
  "started_at": "2026-01-15T01:11:03.457058+00:00",
  "environment": "production"
}
```

**Response Fields**:
- `service`: Service name (always "pms-admin")
- `source_commit`: Git commit SHA (from SOURCE_COMMIT env var, null if not set)
- `started_at`: ISO 8601 timestamp of Node.js process start time (computed from process.uptime(), never null)
- `environment`: Environment name (from NODE_ENV)

**Key Differences from Backend**:
- `started_at` is computed per Node.js process (not from env var), ensuring it's always a valid ISO timestamp
- `started_at` represents when the Next.js server process started (may change after redeploy/restart)
- No `api_version` field (frontend doesn't version API separately)

**Usage in pms_verify_deploy.sh**:
The deploy verification script checks both backend and admin `/ops/version` endpoints when `ADMIN_BASE_URL` is provided, ensuring both deployments match the expected commit.

### Workflow: Implemented → Verified

**Step 1: Implementation**
1. Merge feature code to main branch
2. CI/CD pipeline builds and deploys to staging/production
3. Mark feature status as "Implemented" in project_status.md

**Step 2: Verification**
1. Run deploy verification script on production server:
   ```bash
   API_BASE_URL=https://api.production.example.com \
   EXPECT_COMMIT=$(git rev-parse HEAD) \
   ./backend/scripts/pms_verify_deploy.sh
   ```
2. If script exits with code 0 (success):
   - All endpoints returned 200 OK
   - Commit hash matches expected value
   - Mark feature status as "Verified" in project_status.md

**Step 3: Evidence**
- Save script output as deployment evidence
- Include in deployment log/changelog
- Reference commit SHA in verification notes

**Example project_status.md Entry**:
```markdown
### API - Deploy Verification Endpoint ✅ VERIFIED

**Date**: 2024-01-05  
**Status**: Implemented + Verified in production  
**Commit**: abc123def456

**Issue**: Need automated way to verify production deployments

**Implementation**:
- Added GET /api/v1/ops/version endpoint (no DB, no auth)
- Created backend/scripts/pms_verify_deploy.sh
- Updated runbook with verification workflow

**Verification**:
- ✅ Deployed to production (2024-01-05 15:30 UTC)
- ✅ Script passed: all endpoints 200 OK, commit verified
- ✅ Monitoring polling /ops/version successfully

**Expected Result**:
- Deploy verification script exits 0 on successful deployment
- Commit hash verification prevents wrong-version deploys
- Monitoring can track deployments via source_commit field
```

### Troubleshooting

**Problem**: Script exits with code 2 (endpoint failure)

**Diagnosis**:
- Check script output for which endpoint failed (health, ready, or version)
- Verify API_BASE_URL is correct and server is reachable
- Check server logs for errors

**Solution**:
```bash
# Test endpoints manually
curl -v https://api.example.com/health
curl -v https://api.example.com/health/ready
curl -v https://api.example.com/api/v1/ops/version
```

---

**Problem**: Script exits with code 3 (commit mismatch)

**Diagnosis**:
- SOURCE_COMMIT env var not set in deployment
- Wrong commit deployed (e.g., stale Docker image)

**Solution**:
```bash
# Check what's deployed
curl https://api.example.com/api/v1/ops/version

# Verify CI/CD sets SOURCE_COMMIT in deployment
# Example Dockerfile:
ARG SOURCE_COMMIT
ENV SOURCE_COMMIT=${SOURCE_COMMIT}

# Example docker-compose.yml:
environment:
  SOURCE_COMMIT: ${GITHUB_SHA}
```

---

**Problem**: source_commit field returns null

**Diagnosis**:
- SOURCE_COMMIT environment variable not set in deployment
- Deployment process doesn't pass git commit SHA

**Solution**:
1. Update CI/CD pipeline to set SOURCE_COMMIT:
   ```bash
   # In CI/CD script
   export SOURCE_COMMIT=$(git rev-parse HEAD)
   # or
   docker build --build-arg SOURCE_COMMIT=$(git rev-parse HEAD) .
   ```
2. Update Dockerfile to accept and use SOURCE_COMMIT:
   ```dockerfile
   ARG SOURCE_COMMIT
   ENV SOURCE_COMMIT=${SOURCE_COMMIT}
   ```
3. Verify deployment:
   ```bash
   curl https://api.example.com/api/v1/ops/version | grep source_commit
   ```

---


## Change Log
## Race-Safe Bookings (DB Exclusion Constraint)

### Overview

This section documents the database-level exclusion constraint that prevents overlapping bookings for the same property, ensuring inventory safety even under concurrent API requests.

**Problem**: Application-level availability checks are not race-safe (TOCTOU: time-of-check-time-of-use). Multiple concurrent POST /api/v1/bookings can create overlapping dates, resulting in double-bookings.

**Solution**: PostgreSQL EXCLUSION constraint with btree_gist extension provides atomic, database-level guarantees that no overlaps can occur.

### What It Prevents

The `bookings_no_overlap_exclusion` constraint prevents overlapping bookings for the same property when the booking status is **inventory-occupying**:

**Blocking Statuses** (inventory-occupying):
- `confirmed`: Booking is confirmed and occupies property
- `checked_in`: Guest is currently in the property

**Non-blocking Statuses** (do NOT occupy inventory):
- `cancelled`, `declined`, `no_show`: Booking is terminated
- `checked_out`: Guest has left
- `inquiry`, `pending`: Not yet confirmed

### Constraint Details

**Constraint Name**: `bookings_no_overlap_exclusion`

**Definition**:
```sql
ALTER TABLE bookings
ADD CONSTRAINT bookings_no_overlap_exclusion
EXCLUDE USING gist (
    property_id WITH =,
    daterange(check_in, check_out, '[)') WITH &&
)
WHERE (
    check_in IS NOT NULL
    AND check_out IS NOT NULL
    AND status IN ('confirmed', 'checked_in')
    AND deleted_at IS NULL
);
```

**Date Range Semantics**:
- `[)` means [check_in, check_out) - inclusive start, exclusive end
- Check-in day: included
- Check-out day: excluded (guest leaves, property available)
- Example: [2024-01-01, 2024-01-05) = nights of Jan 1-4, free on Jan 5

### How to Apply in Production

**Step 1: Verify btree_gist Extension**

```sql
-- In Supabase SQL Editor (or psql)
SELECT * FROM pg_extension WHERE extname = 'btree_gist';
```

If not installed, the migration will create it automatically.

**Step 2: Check for Existing Overlaps**

Before applying the migration, check if existing data has overlaps:

```sql
-- Find overlapping bookings
SELECT b1.id, b1.property_id, b1.check_in, b1.check_out, b1.status,
       b2.id AS id2, b2.check_in AS check_in2, b2.check_out AS check_out2, b2.status AS status2
FROM bookings b1
INNER JOIN bookings b2 ON (
  b1.property_id = b2.property_id
  AND b1.id < b2.id
  AND daterange(b1.check_in, b1.check_out, '[)') && daterange(b2.check_in, b2.check_out, '[)')
  AND b1.status IN ('confirmed', 'checked_in')
  AND b2.status IN ('confirmed', 'checked_in')
  AND b1.deleted_at IS NULL
  AND b2.deleted_at IS NULL
);
```

**Step 3: Resolve Conflicts (if any)**

If overlaps found, resolve before migration:

```sql
-- Option 1: Cancel one booking
UPDATE bookings
SET status = 'cancelled',
    cancellation_reason = 'Overlap resolution before constraint',
    cancelled_by = 'system',
    cancelled_at = NOW()
WHERE id = '<conflicting-booking-id>';

-- Option 2: Adjust dates
UPDATE bookings
SET check_out = '2024-01-05'  -- Adjust to not overlap
WHERE id = '<conflicting-booking-id>';
```

**Step 4: Apply Migration**

```bash
# Via Supabase Dashboard -> SQL Editor
# Paste contents of: supabase/migrations/20260105170000_race_safe_bookings_exclusion.sql
# Run migration
```

Or via CLI:
```bash
supabase db push
```

**Step 5: Verify Constraint**

```sql
-- Check constraint exists
SELECT conname, contype, pg_get_constraintdef(oid)
FROM pg_constraint
WHERE conname = 'bookings_no_overlap_exclusion';

-- Should return:
-- conname: bookings_no_overlap_exclusion
-- contype: x (exclusion)
-- pg_get_constraintdef: EXCLUDE USING gist (property_id WITH =, ...
```

### Testing

**Run Concurrency Smoke Test** (HOST-SERVER-TERMINAL):

```bash
# On production server or with production API access
export API_BASE_URL="https://api.production.example.com"
export TOKEN="$(./backend/scripts/get_fresh_token.sh)"

./backend/scripts/pms_booking_concurrency_smoke.sh
```

**Expected Output**:
```
╔════════════════════════════════════════════════════════════╗
║ PMS Booking Concurrency Smoke Test                        ║
╚════════════════════════════════════════════════════════════╝
API: https://api.production.example.com
Property: <uuid>
Dates: 2024-01-15 → 2024-01-17
Concurrency: 10 parallel requests

Results:
────────────────────────────────────────────────────────────
Total requests:     10
201 Created:        1 ✅
409 Conflict:       9 🚫
500 Server Error:   0 ⚠️
Other:              0
════════════════════════════════════════════════════════════

╔════════════════════════════════════════════════════════════╗
║ ✅ TEST PASSED                                             ║
╚════════════════════════════════════════════════════════════╝

Race-safe booking validation successful:
  - Exactly 1 booking created (201)
  - Exactly 9 requests rejected with 409 Conflict
  - Database exclusion constraint working correctly
  - No 500 errors (API properly maps constraint to 409)
```

**Important Note on Test Reuse**:
When rerunning the concurrency smoke test (e.g., after redeployment or for periodic verification), **always use a fresh date window** that doesn't overlap with previous test runs. If you reuse the same dates from a previous test, the property will already have a confirmed booking for those dates, causing all 10 concurrent requests to return 409 Conflict (10x409 instead of 1x201 + 9x409). This is a false negative—the constraint is working, but you won't see the expected "exactly 1 success" pattern.

**Best Practice**: Either:
- Set `CHECK_IN_DATE` and `CHECK_OUT_DATE` to future dates that haven't been tested yet
- Let the script use default dates (+14 days), which automatically advances with calendar time
- Cancel any bookings created by previous test runs before reusing the same date range

**Verification Requirements for Production Evidence**:
When documenting concurrency smoke test results for production verification, always capture the following from the running deployment:
- `/api/v1/ops/version` response: `source_commit` (full hash) and `started_at` timestamp
- `pms_verify_deploy.sh EXPECT_COMMIT=<hash>` result: must show rc=0 with commit match confirmation
- `pms_booking_concurrency_smoke.sh` exit code: must be rc=0 (PASS: 1×201, 9×409, 0×500)
- Evidence must reference the **currently deployed commit** (not a previous or local commit)

### Troubleshooting

---

**Problem**: Migration fails with "existing overlapping bookings found"

**Diagnosis**: Existing data has overlapping confirmed/checked_in bookings

**Solution**:
1. Query to find conflicts (see Step 2 above)
2. Review conflicting bookings with business team
3. Resolve: cancel one booking or adjust dates
4. Re-run migration

**Sample Error**:
```
ERROR:  Cannot create exclusion constraint: 2 existing overlapping bookings found.

Sample conflicts:
  property=abc-123: booking_id=xyz-1 [2024-01-01 to 2024-01-05] OVERLAPS booking_id=xyz-2 [2024-01-03 to 2024-01-07]
```

---

**Problem**: Concurrent booking requests return 500 Server Error

**Diagnosis**: API not properly catching ExclusionViolationError

**Solution**:
1. Check backend logs for `asyncpg.exceptions.ExclusionViolationError`
2. Verify booking_service.py catches exception and raises ConflictException
3. Verify exception handler maps ConflictException to 409
4. Check code at:
   - `backend/app/services/booking_service.py:766-829` (create_booking)
   - `backend/app/services/booking_service.py:1565-1576` (update_booking)

---

**Problem**: Concurrency smoke test shows multiple 201 (successes)

**Diagnosis**: Exclusion constraint not active or dates not overlapping

**Solution**:
1. Verify constraint exists: `SELECT * FROM pg_constraint WHERE conname = 'bookings_no_overlap_exclusion'`
2. Check if dates overlap: `SELECT daterange('2024-01-01', '2024-01-05', '[)') && daterange('2024-01-03', '2024-01-07', '[)')` (should be `t`)
3. Check booking status: constraint only blocks `confirmed` and `checked_in`
4. Check deleted_at: constraint excludes soft-deleted bookings

---

**Problem**: Need to temporarily disable constraint for data migration

**Diagnosis**: Bulk data import or migration needs to bypass constraint

**Solution** (use with caution):
```sql
-- Disable constraint (destructive - use only for migrations)
ALTER TABLE bookings DROP CONSTRAINT bookings_no_overlap_exclusion;

-- Perform data migration
-- ...

-- Re-enable constraint (check for overlaps first!)
-- Run pre-migration overlap check query from Step 2
-- Then re-apply constraint creation from migration file
```

---

**Problem**: Concurrency smoke test returns 0×201 and all 10×409 (all conflicts, no successes)

**Diagnosis**: Date window already booked (property has existing confirmed/checked_in booking for those dates)

**Symptoms**:
- `pms_booking_concurrency_smoke.sh` shows: 0 Created (201), 10 Conflict (409), 0 Server Errors
- Script prints "All requests returned 409 Conflict (0 successes)"
- Test is failing but API behavior is correct (constraint is working)

**Root Cause**:
- The date window being tested already has a booking
- All 10 concurrent requests correctly receive 409 Conflict
- This is NOT a test failure—it's expected behavior for already-booked dates

**Solution**:
1. **Use DATE_FROM/DATE_TO overrides** with known-free dates:
   ```bash
   export DATE_FROM="2026-12-01"
   export DATE_TO="2026-12-03"
   ./backend/scripts/pms_booking_concurrency_smoke.sh
   ```

2. **Rely on auto-shift** (default behavior as of 2026-01-05):
   - Script automatically detects "all 409s" and shifts window by `SHIFT_DAYS` (default 7)
   - Retries up to `MAX_WINDOW_TRIES` (default 10) times
   - First free window will yield expected 1×201 + 9×409

3. **Cancel existing booking** for the window:
   ```bash
   # Find booking for the window
   curl "$API_BASE_URL/api/v1/bookings?property_id=<uuid>&check_in=2026-09-14" \
     -H "Authorization: Bearer $TOKEN"

   # Cancel it
   curl -X POST "$API_BASE_URL/api/v1/bookings/<booking-id>/cancel" \
     -H "Authorization: Bearer $TOKEN" \
     -H "Content-Type: application/json" \
     -d '{"cancelled_by":"host","cancellation_reason":"Smoke test cleanup"}'
   ```

**Expected Behavior After Fix**:
- ✅ Script auto-shifts to free window and passes (1×201 + 9×409)
- ✅ OR manual override with DATE_FROM/DATE_TO succeeds
- ✅ Exit code 0 on success, exit code 2 if all retries exhausted

---

**Problem**: Concurrent booking requests return 500 Server Error with "foreign key constraint" violation

**Diagnosis**: API attempting to create bookings with invalid guest_id (not in guests table)

**Symptoms**:
- Backend logs show: `asyncpg.exceptions.ForeignKeyViolationError: insert or update on table "bookings" violates foreign key constraint "fk_bookings_guest_id"`
- Error message: `Key (guest_id)=<uuid> is not present in table "guests"`
- Concurrency smoke test: all 10 requests return 500

**Root Cause**:
- guest_id provided in request doesn't match any existing guest record
- Auth user ID (JWT sub) used as guest_id instead of actual guests table ID
- Concurrent guest upserts failing under load

**Solution**:
1. **Use existing guest_id**: Ensure guest exists before creating booking
   ```bash
   # Verify guest exists
   curl -H "Authorization: Bearer $TOKEN" \
        "$API_BASE_URL/api/v1/guests/$GUEST_ID"

   # If not found (404), create guest first:
   curl -X POST "$API_BASE_URL/api/v1/guests" \
        -H "Authorization: Bearer $TOKEN" \
        -H "Content-Type: application/json" \
        -d '{"email":"guest@example.com","first_name":"John","last_name":"Doe"}'
   ```

2. **For smoke test**: Use `GUEST_ID` env var or let script auto-pick/create
   ```bash
   export API_BASE_URL="https://api.example.com"
   export TOKEN="$(./backend/scripts/get_fresh_token.sh)"
   export GUEST_ID="<existing-guest-uuid>"  # Optional, auto-picked if not set
   ./backend/scripts/pms_booking_concurrency_smoke.sh
   ```

3. **Verify FK violation returns 422 (not 500)**: After fix, API should return:
   ```json
   {
     "error": "validation_error",
     "message": "guest_id does not reference an existing guest. Create the guest first or omit guest_id to create booking without guest."
   }
   ```

**Expected Behavior**:
- ✅ FK violations return 422 Unprocessable Entity (not 500)
- ✅ Exclusion violations return 409 Conflict
- ✅ Concurrency smoke test: 1 success (201), 9 conflicts (409), 0 errors (500)

**Code Reference**:
- FK violation handling: `backend/app/services/booking_service.py:833-855`
- Smoke test: `backend/scripts/pms_booking_concurrency_smoke.sh`

---

**Problem**: Smoke test script shows "syntax error in expression (error token is '0')" or "unbound variable" errors

**Diagnosis**: Bash parsing issues with counter variables under `set -euo pipefail`

**Symptoms**:
- Script output shows: `bash: 0: syntax error in expression (error token is "0")`
- Or: `bash: ERROR_COUNT: unbound variable`
- Test results show correct HTTP codes (1x201, 9x409) but script exits rc=1 or rc=2 instead of rc=0
- Counter values appear as "0\n0" (two lines) instead of single integer

**Root Cause**:
- Pattern `COUNT=$(grep -c ... || echo "0")` can produce "0\n0" when grep fails (outputs to stdout, then `|| echo "0"` also executes)
- Arithmetic evaluation `$((...))` on "0\n0" triggers "syntax error in expression"
- `set -u` requires all variables initialized before use, but counters were parsed directly without initialization

**Solution**:
1. **Initialize all counters to 0** before parsing:
   ```bash
   SUCCESS_COUNT=0
   CONFLICT_COUNT=0
   ERROR_COUNT=0
   ```

2. **Use robust parsing pattern**:
   ```bash
   # Safe pattern: grep || true, strip non-digits, default to 0
   SUCCESS_COUNT=$(grep -c "^201$" "$RESPONSES_FILE" 2>/dev/null || true)
   SUCCESS_COUNT=${SUCCESS_COUNT//[^0-9]/}  # strip non-digits
   [[ -n "$SUCCESS_COUNT" ]] || SUCCESS_COUNT=0
   ```

3. **Verify script version**: Ensure using latest version with counter parsing fix (commit 405d3f0 or later)

**Expected Behavior After Fix**:
- ✅ Script returns rc=0 when test passes (1 success, 9 conflicts, 0 errors)
- ✅ No "syntax error in expression" or "unbound variable" errors
- ✅ All counters are valid integers (0-10 range for 10 concurrent requests)

**Code Reference**:
- Counter parsing: `backend/scripts/pms_booking_concurrency_smoke.sh:262-291`

---

**Problem**: How to verify guest_id FK hardening in production

**Purpose**: Confirm that booking creation API correctly handles guest_id foreign key violations without returning 500 errors.

**When to Run**:
- After deploying guest_id FK hardening fix (commit with booking_service.py FK error handling)
- During routine production health checks
- Before marking feature as VERIFIED in project_status.md

**Steps**:
1. **Verify deployment commit**:
   ```bash
   export API_BASE_URL="https://api.example.com"
   export EXPECT_COMMIT="<commit-sha>"  # Expected production commit
   ./backend/scripts/pms_verify_deploy.sh
   ```

2. **Run guest_id FK smoke test**:
   ```bash
   export API_BASE_URL="https://api.example.com"
   export TOKEN="$(./backend/scripts/get_fresh_token.sh)"
   export PROPERTY_ID="<uuid>"  # Optional, auto-picks if not set
   ./backend/scripts/pms_booking_guest_id_fk_smoke.sh
   ```

3. **Expected results**:
   - Test 1 (guest_id omitted): 201 Created, guest_id=null
   - Test 2 (guest_id invalid): 422 Unprocessable Entity with actionable message
   - Exit code: rc=0
   - Auto-shift: Script automatically retries with shifted date window if 409 conflict encountered


**Success Criteria**:
- ✅ pms_verify_deploy.sh: Commit match + rc=0
- ✅ pms_booking_guest_id_fk_smoke.sh: Both tests pass + rc=0
- ✅ No 500 errors in either test case

**Failure Scenarios**:
- Test 1 returns 500: Booking service may be using auth user ID as guest_id fallback (check booking_service.py:555-558)
- Test 2 returns 500: FK violation not caught (check booking_service.py:833-855)
- Exit code rc=2: FK hardening broken, 500 errors present

**Code Reference**:
- Smoke test: `backend/scripts/pms_booking_guest_id_fk_smoke.sh`
- FK error handling: `backend/app/services/booking_service.py:833-855`
- Script docs: `backend/scripts/README.md` (search "guest_id FK Hardening")

---


### API Error Response

When exclusion constraint is triggered, API returns:

**HTTP 409 Conflict**:
```json
{
  "error": "conflict",
  "message": "Property is already booked for these dates",
  "conflict_type": "double_booking",
  "path": "/api/v1/bookings"
}
```

**Client Handling**:
- Do NOT retry automatically (conflict is permanent for same dates)
- Show user-friendly message: "Property unavailable for selected dates"
- Suggest alternative dates or properties

### Performance Considerations

**Index**: The EXCLUSION constraint creates a GIST index automatically.

**Advisory Lock**: Booking service acquires advisory lock per property to serialize concurrent requests and prevent deadlocks:

```sql
SELECT pg_advisory_xact_lock(hashtextextended($1::text, 0))
```

This ensures concurrent bookings for the same property are processed sequentially, preventing potential deadlock scenarios from overlapping constraint checks.

**Query Impact**: Minimal - GIST index lookups are O(log N).

---


## Smoke User Lifecycle

**Purpose**: Document the lifecycle of smoke test users and how to clean them up safely.

### User Creation

Smoke test users are created during API testing via the admin users endpoint:

**Typical creation pattern**:
```bash
# Create smoke test user
curl -X POST "$API_BASE_URL/api/v1/admin/users" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "email": "pms-smoke-20260106-abc123@example.com",
    "password": "<generated-password>",
    "email_confirm": true
  }'
```

**Email naming convention**:
- Prefix: `pms-smoke-`
- Timestamp: YYYYMMDD or YYYYMMDD-HHMMSS
- Random suffix: 6-character hex
- Domain: `example.com` (test domain)
- Example: `pms-smoke-20260106-a3f7b2@example.com`

### Agency Membership

Smoke users may be linked to an agency via `team_members` table:

**Membership creation**:
```sql
INSERT INTO public.team_members (
  user_id,
  agency_id,
  role,
  is_active
) VALUES (
  '<smoke-user-id>',
  '<agency-id>',
  'staff',
  true
);
```

**Active membership** enables:
- Access to agency properties
- Booking creation/management
- Guest operations
- Channel Manager operations (if enabled)

### Cleanup Procedure

Use `pms_smoke_user_cleanup.sh` for safe cleanup:

**Step 1: Dry-run (default, safe)**:
```bash
# Show what would be cleaned up (no changes)
./backend/scripts/pms_smoke_user_cleanup.sh
```

**Step 2: Apply cleanup (deactivate membership)**:
```bash
# Deactivate membership in team_members (preserves auth user)
DRY_RUN=0 CONFIRM=1 ./backend/scripts/pms_smoke_user_cleanup.sh
```

**Step 3: Optional - Delete auth user**:
```bash
# Full cleanup: deactivate membership AND delete auth user
DRY_RUN=0 CONFIRM=1 CONFIRM_DELETE_USER=1 ./backend/scripts/pms_smoke_user_cleanup.sh
```

**What the script does**:
1. Finds the latest `pms-smoke-*@example.com` user (or uses `USER_ID` override)
2. Deactivates membership: `UPDATE team_members SET is_active=false WHERE user_id=...`
3. Optionally deletes auth user: `DELETE /auth/v1/admin/users/{id}` (requires explicit flag)

**Safety features**:
- Default `DRY_RUN=1` (no changes)
- Requires `CONFIRM=1` to apply changes
- Requires `CONFIRM_DELETE_USER=1` to delete auth user
- Never prints service keys or secrets to stdout
- Shows clear plan before executing destructive actions

### Security Notes

**CRITICAL - Never paste service keys in chat/logs**:
- The cleanup script requires `SB_SERVICE_KEY` (Supabase service role key)
- Script auto-detects from docker if not set: `docker exec supabase-kong printenv SUPABASE_SERVICE_KEY`
- Never echo or log the service key
- Only print key length for confirmation (e.g., "length: 274")

**Service key usage**:
- Used only for GoTrue Admin API calls (list/delete users)
- Passed via Authorization header (not in URL)
- Never logged or stored by the script

**Best practices**:
1. Always run dry-run first to verify target user
2. Use `USER_ID` override if you know the specific user to clean
3. Prefer membership deactivation over auth user deletion (less destructive)
4. Keep service keys in docker environment or 1Password (never in shell history)

### Related Documentation

- [Smoke User Cleanup Script](../scripts/README.md#smoke-user-cleanup-pms_smoke_user_cleanupsh) - Full script documentation
- [GoTrue Admin API](https://supabase.com/docs/reference/cli/global-flags#admin-api) - User management endpoints
- [Project Status](../docs/project_status.md) - Implementation status

---

## Direct Booking (Public) v0

**Purpose**: Public direct booking flow without authentication or payment integration.

**Production Status**: Verified in production on 2026-01-06 (commit d9db091, verify_rc=0, smoke_rc=0).

### What is included (v0)

**Endpoints** (no auth required):
1. GET /api/v1/public/availability - Check property availability for date range
2. POST /api/v1/public/booking-requests - Create public booking request

**Features**:
- No JWT/auth required (truly public endpoints)
- Guest creation/lookup by email (case-insensitive)
- Booking created with status="requested" (pending approval)
- Auto-detects agency via property_id
- Currency support: defaults to EUR if not specified; accepts 3-letter ISO codes (EUR, USD, GBP, etc.)
- Stub pricing (v0): nightly_rate, subtotal, and total_price all set to 0.00 to satisfy NOT NULL constraints; pricing engine integration comes in future version
- Proper error mapping:
  - 409 conflict_type=double_booking for overlapping bookings
  - 422 for FK violations/validation errors
  - Never returns 500 on constraint/validation errors

**What is NOT included (v0)**:
- ❌ Payment processing
- ❌ Email notifications
- ❌ Booking confirmation workflow (manual approval required)
- ❌ Availability calendar UI
- ❌ Price calculation

### How to run smoke test on HOST

**Prerequisites**:
- No token/auth required
- Need property ID (PID)

**Basic usage**:
```bash
export API_BASE_URL="https://api.fewo.kolibri-visions.de"
export PID="<property-uuid>"
./backend/scripts/pms_direct_booking_public_smoke.sh
```

**With date override**:
```bash
export DATE_FROM="2037-01-01"
export DATE_TO="2037-01-03"
./backend/scripts/pms_direct_booking_public_smoke.sh
```

**Script behavior**:
1. Calls GET /api/v1/public/availability
2. Calls POST /api/v1/public/booking-requests
3. Auto-shifts date window on 409 conflicts (similar to other smoke scripts)
4. Exit 0 on success, 1 on unexpected codes, 2 on 500 errors

**Auto-retry**: If booking creation returns 409 (double_booking), automatically shifts window by SHIFT_DAYS (default 7) and retries up to MAX_WINDOW_TRIES (default 10) times.

### Troubleshooting

**Problem**: "PID (property ID) environment variable required"

**Solution**: Set PID explicitly (do NOT auto-pick in prod):
```bash
export PID="abc-123-uuid"
./backend/scripts/pms_direct_booking_public_smoke.sh
```

---

**Problem**: Booking creation returns 500 error

**Diagnosis**: Validation/FK/constraint errors not properly handled OR unhandled exception

**Solution**: Check backend logs for specific error type. Expected error mappings:
- Exclusion violation (double booking) → 409 conflict_type=double_booking
- FK violation (property/guest not found) → 422 with actionable message
- Validation error (invalid dates, etc.) → 422 with details
- Schema drift (UndefinedColumn/UndefinedTable) → 503 with migration guidance (see below)

If 500 persists, check logs for unexpected exceptions and file incident report.

---

**Problem**: Booking creation returns 503 "Database schema not installed or out of date"

**Diagnosis**: Schema drift - code references DB column/table/function that doesn't exist or has ambiguous signature

**Common Examples**:
```
# Case 1: Missing column
Backend logs: "column 'notes' of relation 'bookings' does not exist"
Response: 503 {"error":"service_unavailable","message":"Database schema not installed or out of date: column notes..."}

# Case 2: Ambiguous function
Backend logs: "function generate_booking_reference() is not unique"
Response: 503 {"error":"service_unavailable","message":"Database schema/function definitions out of date or duplicated: generate_booking_reference is ambiguous..."}
```

**Root Causes**:
- **Missing column**: Public booking endpoint tries to INSERT into column that doesn't exist (e.g., bookings.notes)
- **Ambiguous function**: Multiple `generate_booking_reference()` function signatures exist without proper type disambiguation
- **Schema drift**: DB migrations not run or schema out of sync with code

**Solution**:
1. Check if migration exists for the missing column/table/function
2. If migration exists, run it:
   ```bash
   cd /path/to/supabase
   supabase db push
   # OR manually apply migration SQL
   ```
3. For ambiguous function error:
   - Check for duplicate function definitions in database:
     ```sql
     SELECT proname, proargtypes, prosrc FROM pg_proc WHERE proname = 'generate_booking_reference';
     ```
   - Drop duplicate/old function signatures
   - Keep only one signature with explicit types (e.g., `generate_booking_reference(text)`)
   - Code uses explicit type cast: `public.generate_booking_reference($1::text)`
4. If migration does NOT exist:
   - Code should not reference the missing column/function (bug in code)
   - Public booking endpoint intentionally does NOT persist optional fields (like notes)
   - Verify `backend/app/api/routes/public_booking.py` generates booking_reference before INSERT
   - If code was reverted accidentally, redeploy correct version

**Prevention**: Public booking endpoint explicitly generates booking_reference with type cast (`$1::text`) before INSERT to avoid ambiguity. It does not rely on database DEFAULT values that may have ambiguous function calls.

---

**Problem**: Booking creation returns 422 "Property is missing agency assignment (agency_id)"

**Diagnosis**: Property tenant assignment missing - property exists but agency_id column is NULL

**Root Cause**:
- Property row exists in database but agency_id field is not populated
- This violates the NOT NULL constraint on bookings.agency_id
- Public booking endpoint resolves agency_id from property before creating booking

**SQL Diagnostic**:
```sql
-- Check if property exists and has agency_id populated
SELECT id, agency_id FROM public.properties WHERE id='<property-uuid>';

-- If agency_id is NULL, property needs tenant assignment
```

**Solution**:
1. If agency_id is NULL for the property:
   - Backfill agency_id for the property (assign it to correct agency/tenant)
   - Run migrations if agency_id column doesn't exist in properties table
   ```sql
   UPDATE public.properties SET agency_id = '<agency-uuid>' WHERE id = '<property-uuid>';
   ```
2. If agency_id column doesn't exist:
   - Run pending migrations to add agency_id to properties table
   ```bash
   cd /path/to/supabase
   supabase db push
   ```
3. Verify property now has agency_id:
   ```sql
   SELECT id, agency_id FROM public.properties WHERE id='<property-uuid>';
   ```

**Prevention**: Ensure all properties have agency_id populated before creating public bookings. Use database constraints or backfill scripts to enforce agency_id NOT NULL on properties table.

---

**Problem**: Booking creation returns 422 "Booking creation failed: currency is required"

**Diagnosis**: Currency field missing or NULL in booking request (should not happen with current API schema)

**Root Cause**:
- Public booking endpoint requires currency field (defaults to EUR)
- If currency is not provided or is NULL, booking INSERT fails NOT NULL constraint
- This error indicates request validation bypassed or schema mismatch

**Solution**:
1. Ensure currency is included in booking request payload:
   ```json
   {
     "property_id": "<uuid>",
     "date_from": "2026-06-01",
     "date_to": "2026-06-03",
     "adults": 2,
     "children": 0,
     "currency": "EUR",
     "guest": { ... }
   }
   ```
2. If omitted, API defaults to "EUR" (no need to explicitly set)
3. Currency must be 3-letter uppercase ISO code (EUR, USD, GBP, etc.)
4. Smoke script supports `PUBLIC_CURRENCY` env var (default: EUR):
   ```bash
   export PUBLIC_CURRENCY="USD"
   ./backend/scripts/pms_direct_booking_public_smoke.sh
   ```

**Validation Rules**:
- Must be exactly 3 alphabetic characters
- Automatically uppercased and trimmed
- Validated against pattern `^[A-Z]{3}$`

**Prevention**: Public booking endpoint now always sets currency (default EUR). This error should not occur in normal operation unless request schema validation is bypassed.

---

**Problem**: Booking creation returns 422 "null value in column 'nightly_rate' (or subtotal/total_price) violates not-null constraint"

**Diagnosis**: Pricing fields not set in booking INSERT (should not happen with current v0 implementation)

**Root Cause**:
- Bookings table has NOT NULL constraints on pricing fields: nightly_rate, subtotal, total_price
- Public booking v0 must provide stub pricing values (0.00) to satisfy these constraints
- Error indicates pricing stub values were not included in INSERT

**Solution**:
1. Verify public booking endpoint sets stub pricing:
   - `nightly_rate = Decimal("0.00")`
   - `subtotal = Decimal("0.00")`
   - `total_price = Decimal("0.00")`
2. Check INSERT statement includes these columns and bindings
3. Verify no schema drift (columns exist in bookings table):
   ```sql
   SELECT column_name, is_nullable, data_type
   FROM information_schema.columns
   WHERE table_name = 'bookings'
   AND column_name IN ('nightly_rate', 'subtotal', 'total_price');
   ```

**Expected Behavior (v0)**:
- All public bookings created with pricing = 0.00 (stub values)
- Status = "requested" (pending manual pricing/approval)
- Pricing engine integration comes in future version

**Prevention**: Public booking endpoint v0 always sets stub pricing defaults. This error should not occur unless code was reverted or INSERT statement modified incorrectly.

---

**Problem**: All window attempts exhausted

**Diagnosis**: Every tested date window already has bookings

**Solution**: Use DATE_FROM/DATE_TO with known-free dates:
```bash
export DATE_FROM="2037-01-01"
export DATE_TO="2037-01-03"
./backend/scripts/pms_direct_booking_public_smoke.sh
```

---

**Problem**: GET /api/v1/public/availability returns 404 Not Found, OpenAPI docs show zero /api/v1/public paths

**Diagnosis**: Public booking router not mounted (module system failed and failsafe not triggered)

**Root Cause**: Router not mounted via module registry AND failsafe explicit mounting in app factory did not execute

**Solution**:
1. Check backend logs for failsafe mounting messages:
   - ✅ Expected: "Public booking router already mounted via module system"
   - ⚠️  Fallback: "Public booking router not found in mounted routes, applying failsafe mounting"
   - ❌ Neither message → app factory not reached or error during startup
2. Verify module registration:
   - `backend/app/modules/public_booking.py` exists with ModuleSpec
   - `backend/app/modules/bootstrap.py:98` includes `from . import public_booking`
3. Verify failsafe mounting in app factory:
   - `backend/app/main.py:142-154` has failsafe include_router after mount_modules()
4. Check OpenAPI docs at `/docs` - should show "Public Direct Booking" tag
5. Use preflight check: `curl https://api.example.com/api/v1/public/ping` (should return 200 {"status": "ok"})

**Architecture**: Two-layer mounting guarantees router availability:
- **Layer 1 (Correct)**: Module system via `mount_modules()` registers public_booking module
- **Layer 2 (Failsafe)**: Explicit `include_router()` in main.py after module mounting if Layer 1 failed

**Prevention**: Smoke script includes /ping preflight check with OpenAPI diagnostics to detect unmounted router early

### Public API Anti-Abuse (Rate Limiting + Honeypot)

**Verification (PROD)**: ✅ Verified 2026-01-06 (commit f85efb9, smoke rc=0, observed 429 responses)

**Coverage**: All /api/v1/public/* endpoints

**Protection Mechanisms**:

1. **IP-Based Rate Limiting**:
   - Ping endpoint: 60 requests per 10-second window (per IP)
   - Availability endpoint: 30 requests per 10-second window (per IP + property_id when available)
   - Booking requests: 10 requests per 10-second window (per IP + property_id when available)
   - Redis-backed with atomic INCR+EXPIRE operations
   - Fail-open design: allows requests if Redis unavailable (logs warning)

2. **Honeypot Field** (Booking Requests Only):
   - Field name: `website` (configurable via PUBLIC_HONEYPOT_FIELD)
   - Behavior: If field is present and non-empty, request blocked with 429
   - Does not reveal honeypot reason in response
   - OpenAPI documents field as "Anti-bot honeypot field (must be empty)"

**Response Headers** (on success):
- `X-RateLimit-Limit`: Max requests allowed in window
- `X-RateLimit-Remaining`: Requests remaining
- `X-RateLimit-Window`: Window duration in seconds

**Response on Limit Exceeded**:
- Status: 429 Too Many Requests
- Detail: "Too many requests. Please try again later."
- Header: `Retry-After` (seconds until window resets)

**Environment Variables**:

| Variable | Default | Description |
|----------|---------|-------------|
| `PUBLIC_ANTI_ABUSE_ENABLED` | `true` | Master toggle for anti-abuse protection |
| `PUBLIC_RATE_LIMIT_ENABLED` | `true` | Enable rate limiting |
| `PUBLIC_RATE_LIMIT_WINDOW_SECONDS` | `10` | Rate limit window duration |
| `PUBLIC_RATE_LIMIT_PING_MAX` | `60` | Max ping requests per window |
| `PUBLIC_RATE_LIMIT_AVAIL_MAX` | `30` | Max availability requests per window |
| `PUBLIC_RATE_LIMIT_BOOKING_MAX` | `10` | Max booking requests per window |
| `PUBLIC_RATE_LIMIT_REDIS_URL` | (auto) | Redis URL (defaults to REDIS_URL or CELERY_BROKER_URL) |
| `PUBLIC_RATE_LIMIT_PREFIX` | `public_rl` | Redis key prefix |
| `PUBLIC_HONEYPOT_FIELD` | `website` | Honeypot field name |

**Testing**:

The public booking smoke script (`pms_direct_booking_public_smoke.sh`) includes a rate limit test:
- Sends burst of ping requests (limit + 5 or 80 if header not present)
- Verifies at least one 429 response observed
- Honors Retry-After header if already rate-limited at start
- PASS condition: Test runs without fatal errors (informational test)

**Logging**:

Rate limit decisions logged with structured context:
- `decision`: allowed / limited / honeypot
- `ip`: Client IP address
- `path`: Request path
- `method`: HTTP method
- `bucket`: Rate limit bucket (ping/availability/booking_requests)
- `property_id`: Property ID if available
- `user_agent`: User-Agent header
- `retry_after`: Seconds until limit resets (on 429 only)

**Troubleshooting**:

**Problem**: All public requests return 429

**Solution**:
1. Check if rate limits configured too low for your traffic
2. Verify Redis is operational: `redis-cli -u $REDIS_URL PING`
3. Increase limits via environment variables (e.g., `PUBLIC_RATE_LIMIT_PING_MAX=120`)
4. Check if multiple IPs sharing same public IP (NAT/proxy) - consider property-scoped limits

**Problem**: Rate limiting not working (no 429s observed)

**Solution**:
1. Check `PUBLIC_RATE_LIMIT_ENABLED=true` in environment
2. Verify Redis connection: check app logs for "Rate limit Redis pool created"
3. If Redis unavailable, limiter fails open (allows all requests with warning)
4. Test manually: `for i in {1..70}; do curl -s -o /dev/null -w "%{http_code}\n" https://api.example.com/api/v1/public/ping; done | grep 429`

**Problem**: Legitimate requests blocked by honeypot

**Solution**:
1. Ensure frontend does NOT populate `website` field (or configured honeypot field)
2. Check POST payload: field should be absent or empty string
3. Verify field name matches `PUBLIC_HONEYPOT_FIELD` setting

### Related Documentation

- [Public Booking Smoke Script](../scripts/README.md#public-direct-booking-smoke-test-pms_direct_booking_public_smokesh) - Full script documentation
- [Public Booking Router](../app/api/routes/public_booking.py) - API implementation
- [Project Status](../docs/project_status.md) - Implementation status

### P1 Booking Request Review Workflow

**Scope**: Internal review workflow for public booking requests (submitted → under_review → approved/declined)

**Architecture Note**: Booking requests are stored directly in the `bookings` table (booking_request_id == bookings.id). The P1 workflow operates on existing bookings table columns:
- `status`: requested → under_review → confirmed OR cancelled
- `confirmed_at`: Set when approved (maps to API field `approved_at`)
- `cancelled_at`, `cancelled_by`, `cancellation_reason`: Set when declined (map to API fields `reviewed_at`, `reviewed_by`, `decline_reason`)
- `internal_notes`: Append-only log of review actions with timestamps
- No separate workflow-specific columns required (uses existing bookings schema)

**Status Mapping (API ↔ DB)**: The API exposes `under_review` status for P1 workflow semantics, but the database stores this as `inquiry` for backward compatibility with existing PROD schema constraints. The mapping layer transparently converts:
- API `under_review` ↔ DB `inquiry`
- All other statuses (requested, confirmed, cancelled) pass through unchanged
- Clients always see API status `under_review` in responses, never `inquiry`

**Admin UI Redirect Note**: The admin booking form at `/buchung` has been replaced with a server-side redirect to the public booking form `https://fewo.kolibri-visions.de/buchung` to avoid duplicate booking request forms. The public form is the single source of truth. Query parameters are preserved during redirect.

**Verification**:
```bash
# Test redirect with query params
curl -k -I "https://admin.fewo.kolibri-visions.de/buchung?foo=bar"
# Expected: 307/308 redirect with Location: https://fewo.kolibri-visions.de/buchung?foo=bar
```

**Implementation Detail (2026-01-12)**: Redirect is implemented via Next.js middleware (`frontend/middleware.ts`) with host-based routing. Only triggers when `host.startsWith("admin.")`. Public `/buchung` on `fewo.kolibri-visions.de` renders the actual booking form (no redirect). Returns HTTP 308 Permanent Redirect with `Cache-Control: no-store`.

**Verification Commands**:
```bash
# Admin host should redirect (308)
curl -k -I "https://admin.fewo.kolibri-visions.de/buchung?foo=bar"
# Expected: 308 redirect with Location: https://fewo.kolibri-visions.de/buchung?foo=bar

# Public host should render form (200)
curl -k -I "https://fewo.kolibri-visions.de/buchung"
# Expected: 200 OK (HTML page)
```

**Endpoints** (authenticated, requires manager/admin role):

1. **List Booking Requests**:
   - `GET /api/v1/booking-requests?status=requested&limit=50&offset=0`
   - Filter by status: requested, under_review, confirmed, cancelled
   - Returns paginated list with confirmation/cancellation timestamps

2. **Get Booking Request Detail**:
   - `GET /api/v1/booking-requests/{id}`
   - Returns full booking request details including internal notes, decline reason (cancellation_reason)

3. **Review Booking Request**:
   - `POST /api/v1/booking-requests/{id}/review`
   - Transitions: requested → under_review, under_review → under_review (update note)
   - Body: `{"internal_note": "optional note"}`
   - Sets: status=under_review, internal_notes (appends timestamped note)

4. **Approve Booking Request**:
   - `POST /api/v1/booking-requests/{id}/approve`
   - Transitions: requested/under_review → confirmed
   - Body: `{"internal_note": "optional note"}`
   - Sets: status=confirmed, confirmed_at, cancelled_by (as actor field), internal_notes
   - Idempotent: re-approving returns 200 with existing booking_id

5. **Decline Booking Request**:
   - `POST /api/v1/booking-requests/{id}/decline`
   - Transitions: requested/under_review → cancelled
   - Body: `{"decline_reason": "required reason", "internal_note": "optional note"}`
   - Sets: status=cancelled, cancelled_at, cancelled_by (as actor field), cancellation_reason, internal_notes
   - Idempotent: re-declining returns 200 with existing state

**Status Lifecycle**:
```
requested → under_review → confirmed (approved)
         ↘               ↘ cancelled (declined)
```

**Error Codes**:
- **401 Unauthorized**: Missing or invalid JWT token
- **403 Forbidden**: User lacks manager/admin role or agency access
- **404 Not Found**: Booking request not found or deleted
- **409 Conflict**: Invalid status transition (e.g., cannot approve cancelled request)
- **422 Validation**: Missing required fields (e.g., decline_reason)
- **500 Internal Server Error**: Database error (check logs for column availability issues)

**Troubleshooting**:

**Problem**: Cannot approve booking request (409 Conflict - Invalid Transition)

**Solution**:
1. Check current status: `GET /api/v1/booking-requests/{id}` → verify status is requested/under_review
2. If status=cancelled: cannot approve cancelled requests (invalid transition)
3. If status=confirmed: already approved (idempotent, returns 200 with booking_id)

---

**Problem**: Approve returns 409 Conflict - Booking Overlap (no_double_bookings)

**Cause**: The requested date range overlaps with an existing confirmed booking for the same property. The database exclusion constraint `no_double_bookings` prevents double bookings.

**Solution**:
1. Check for existing bookings in the date range:
   ```sql
   SELECT id, check_in, check_out, status
   FROM bookings
   WHERE property_id = '<property_id>'
   AND daterange(check_in, check_out, '[)') && daterange('<check_in>', '<check_out>', '[)')
   AND status = 'confirmed'
   AND deleted_at IS NULL;
   ```
2. Options:
   - Choose a different date range (modify the booking request before approving)
   - Cancel the conflicting booking first (if appropriate)
   - Decline this booking request with appropriate reason

**Related**: The smoke test (`pms_public_booking_requests_workflow_smoke.sh`) automatically finds available windows using `/api/v1/public/availability` with configurable `MAX_WINDOW_TRIES` and `SHIFT_DAYS` to avoid this issue.

---

**Problem**: Decline fails with 422 Validation

**Solution**:
1. Ensure `decline_reason` is provided and non-empty in request body
2. Example: `{"decline_reason": "Property unavailable", "internal_note": "..."}`

**Problem**: Endpoints return 500 Internal Server Error

**Solution**:
1. Check backend logs for asyncpg errors (column not found, relation not found)
2. Verify bookings table has required columns: confirmed_at, cancelled_at, cancelled_by, cancellation_reason, internal_notes
3. P1 workflow uses EXISTING bookings columns (no separate workflow table/columns required)
4. If columns missing: check that Phase 17B migration was applied (initial bookings schema)

**Problem**: All endpoints return 404 Not Found (router not mounted)

**Solution**:
1. Verify router is mounted: Check `/openapi.json` for `/api/v1/booking-requests` paths
2. If missing from OpenAPI: router not registered in module system
3. Verify: `backend/app/modules/booking_requests.py` exists and is imported
4. Verify: `backend/app/modules/bootstrap.py` imports booking_requests module
5. Check logs at startup for "Booking Requests module not available" warnings
6. If MODULES_ENABLED=false in production, verify fallback router mounting in main.py


---

**Problem**: Approve returns 422 Unprocessable Entity (Field required)

**Symptom**: POST /api/v1/booking-requests/{id}/approve returns 422 with error: "Field required" or "Body is required"

**Root Cause** (before fix): Approve endpoint required body with `internal_note` field, but many clients call approve without body.

**Solution** (after fix):
- Approve endpoint now accepts empty body (body is optional)
- `internal_note` is optional (defaults to None if body empty)
- Call approve without body:
  ```bash
  curl -X POST "https://api.fewo.kolibri-visions.de/api/v1/booking-requests/{id}/approve" \
    -H "Authorization: Bearer $JWT_TOKEN" \
    -H "Content-Type: application/json"
  # Expected: 200 OK (no body required)
  ```
- Call approve with body:
  ```bash
  curl -X POST "https://api.fewo.kolibri-visions.de/api/v1/booking-requests/{id}/approve" \
    -H "Authorization: Bearer $JWT_TOKEN" \
    -H "Content-Type: application/json" \
    -d '{"internal_note": "Approved by manager"}'
  # Expected: 200 OK
  ```

---

**Problem**: Approve returns 500 Internal Server Error (TypeError: ConflictException)

**Symptom**: POST /api/v1/booking-requests/{id}/approve returns 500 when booking dates conflict. Backend logs show: `TypeError: ConflictException.__init__() got an unexpected keyword argument 'message'`

**Root Cause** (before fix): Conflict handler called `ConflictException(message=...)` but exception only accepted `detail=` kwarg, causing TypeError and 500 response instead of 409 Conflict.

**Solution** (after fix):
- ConflictException now accepts both `detail=` and `message=` kwargs (backward compatible)
- Booking conflicts now correctly return 409 Conflict instead of 500
- Example 409 response:
  ```json
  {
    "detail": "Booking conflict: dates overlap with existing booking",
    "conflict_type": "booking_overlap",
    "existing_resource_id": "<existing-booking-uuid>"
  }
  ```
- How to handle 409 in client code:
  1. On 409, retrieve conflict details from response body
  2. Shift booking dates by +7 days and retry approval
  3. Or decline the booking request with appropriate reason
  4. Smoke test (`pms_p1_booking_request_smoke.sh` Step E) implements automatic retry with date shifting

---

**Problem**: Approve returns 500 Internal Server Error (AttributeError: 'NoneType' object has no attribute 'internal_note')

**Symptom**: POST /api/v1/booking-requests/{id}/approve returns 500 when client sends request without JSON body (Content-Length: 0) or with empty body {}. Backend logs show: `AttributeError: 'NoneType' object has no attribute 'internal_note'` at audit event emission.

**Root Cause** (before fix): The approve endpoint accepted optional body (`input: ApproveInput | None = Body(default=None)`), but audit event metadata directly accessed `input.internal_note` without checking if `input` was None, causing AttributeError crash after successful DB transaction.

**Impact**: Database transition succeeded (booking approved) but response crashed with 500, leaving client uncertain about approval status.

**Solution** (after fix):
- Audit event now guards field access: `"internal_note": input.internal_note if input else None`
- Approve accepts missing/empty body without crashing (internal_note defaults to None)
- Idempotent approve already handled (re-approving confirmed booking returns 200)
- Improved logging: catch-all exception handler in database.py no longer mislabels application errors as "database error"

**How to test approve endpoint:**
```bash
# Approve without body (recommended - simpler request)
curl -X POST "https://api.fewo.kolibri-visions.de/api/v1/booking-requests/{id}/approve" \
  -H "Authorization: Bearer $JWT_TOKEN"
# Expected: 200 OK with status=confirmed, or 409 if already cancelled/dates conflict

# Approve with empty body (also valid)
curl -X POST "https://api.fewo.kolibri-visions.de/api/v1/booking-requests/{id}/approve" \
  -H "Authorization: Bearer $JWT_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{}'

# Approve with internal note (optional)
curl -X POST "https://api.fewo.kolibri-visions.de/api/v1/booking-requests/{id}/approve" \
  -H "Authorization: Bearer $JWT_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"internal_note": "Approved after availability check"}'
```

**Idempotency behavior:**
- Already confirmed: Returns 200 with message "Booking request already approved (idempotent)"
- Already cancelled/declined: Returns 409 with "Cannot approve cancelled booking request"
- Fresh approval: Returns 200 with status=confirmed, sets confirmed_at timestamp

**PROD Verified (2026-01-14):** Approve endpoint accepts missing/empty body without 500 error. Smoke test (`pms_public_booking_requests_workflow_smoke.sh`) rc=0 with commit 29c1f99. See project_status.md for full evidence.

---

#### Update (2026-01-12): P1 Smoke Script Hardened for Public Properties Response

**Issue**: The P1 smoke script (`pms_p1_booking_request_smoke.sh`) failed in production (commit 66ca49e) when discovering properties via GET `/api/v1/public/properties`. The script assumed the response shape was always `{items: [...]}`, but the public properties endpoint returns a plain array `[...]` by default (backward compatible), and only returns `{items: [...], total: N}` when `paginated=true` query parameter is used.

**Error**: `jq: error: Cannot index array with string "items"` at Step A property discovery.

**Fix**: Updated the smoke script to handle both response shapes:
- Array (default): `[ {...}, {...} ]`
- Paginated object: `{ "items": [...], "total": N }`

The script now uses a jq filter similar to Epic C public website smoke test:
```bash
PROPERTY_ID=$(echo "$RESPONSE" | jq -r '
    if type == "array" then
        if length > 0 then .[0].id else empty end
    elif type == "object" and has("items") then
        if (.items | length) > 0 then .items[0].id else empty end
    else
        empty
    end
')
```

Additionally, the script now includes tenant resolution headers for public endpoints (matching Epic C approach):
- `X-Forwarded-Host: ${PUBLIC_HOST}`
- `X-Forwarded-Proto: https`
- `Origin: https://${PUBLIC_HOST}`

**Verification Commands** (HOST-SERVER-TERMINAL):
```bash
# 1. Pull latest code
cd /data/repos/pms-webapp
git fetch origin main && git reset --hard origin/main

# 2. Optional: Verify deployment
export API_BASE_URL="https://api.fewo.kolibri-visions.de"
./backend/scripts/pms_verify_deploy.sh

# 3. Run P1 smoke test with required env vars
export HOST="api.fewo.kolibri-visions.de"
export PUBLIC_HOST="fewo.kolibri-visions.de"
export JWT_TOKEN="<manager-or-admin-jwt-token>"
# Optional: export AGENCY_ID="<agency-uuid>"
./backend/scripts/pms_p1_booking_request_smoke.sh
echo "rc=$?"

# Expected: All 5 tests pass (A-E), rc=0
```

**Expected Output**:
```
ℹ Step A: Discovering published property...
✅ Step A PASSED: Found property 'Property Name' (ID: <uuid>)
ℹ Step B: Creating first booking request (for decline test)...
✅ Step B PASSED: Booking request created (ID: <uuid>, status: requested)
ℹ Step C: Retrieving booking requests (authenticated)...
✅ Step C PASSED: Found booking request in list (ID: <uuid>)
ℹ Step D: Declining booking request with reason...
✅ Step D PASSED: Booking request declined (status: cancelled, reason: ...)
ℹ Step E: Creating second booking request (for approve test)...
✅ Created second booking request (ID: <uuid>, status: requested)
ℹ Approving booking request...
✅ Step E PASSED: Booking request approved (status: confirmed, booking_id: <uuid>)
✅ ALL TESTS PASSED ✅
```

**Troubleshooting**:
- **Still fails at Step A with jq error**: Script not updated; pull latest code
- **Step A returns empty property list**: No public properties published; check `/api/v1/properties` (authenticated) vs `/api/v1/public/properties` (public)
- **Step B/C/D/E auth errors**: JWT_TOKEN invalid or missing manager/admin role
- **Step E returns 409 Conflict**: Property already has bookings for test dates; smoke test logs this as WARNING (expected in production)
- **Step E fails with "curl: (6) Could not resolve host: <UUID>"**: Header quoting issue (pre-fix). The AGENCY_ID was treated as a hostname due to unquoted command substitution. Fixed in commit > c1cb753 by using array-based headers. Pull latest code: `git fetch origin main && git reset --hard origin/main`

**Smoke Test**:
```bash
# Run workflow smoke test (requires JWT_TOKEN with manager/admin role)
HOST=https://api.example.com \
JWT_TOKEN=<token> \
./backend/scripts/pms_public_booking_requests_workflow_smoke.sh
```

**Related Documentation**:
- [Booking Request Workflow Smoke Script](../scripts/README.md#p1-booking-request-workflow-smoke-test) - Full script documentation
- [Booking Requests API](../app/api/routes/booking_requests.py) - API implementation
- [Migration 20260106120000](../../supabase/migrations/20260106120000_add_booking_request_workflow.sql) - Database schema

**PROD Verification (2026-01-10)**:

Verified in production with the following commands:

```bash
# [HOST-SERVER-TERMINAL] Check API version
curl -sS "https://api.fewo.kolibri-visions.de/api/v1/ops/version" | jq .
# Output: source_commit: eb033bf8c48ad3e7b9270c536932a7f0c512b419
#         started_at: 2026-01-10T18:27:04.685372+00:00

# [HOST-SERVER-TERMINAL] Verify deployment
cd /data/repos/pms-webapp
export API_BASE_URL="https://api.fewo.kolibri-visions.de"
./backend/scripts/pms_verify_deploy.sh
# Output: rc=0, commit match eb033bf8c48ad3e7b9270c536932a7f0c512b419

# [HOST-SERVER-TERMINAL] Run P1 workflow smoke test
export HOST="https://api.fewo.kolibri-visions.de"
export JWT_TOKEN="<manager/admin JWT>"
./backend/scripts/pms_public_booking_requests_workflow_smoke.sh
echo "rc=$?"
# Output: rc=0 (all tests passed)
```

**Observed State Machine** (from smoke test):
- **requested → under_review**: Review action sets status to under_review (DB: inquiry)

#### Update (2026-01-12): Hardened Step C - Booking Requests List Parsing

**Issue**: The production smoke script (`pms_p1_booking_request_smoke.sh`)
failed at Step C (GET `/api/v1/booking-requests`) with:

- `jq: error: Cannot iterate over null (null)`

The script assumed response shape `.items[]` and did not robustly handle
alternative shapes or transient errors.

**Fix**: Step C has been hardened with:
- **HTTP Status Code Checking**: Separate HTTP code extraction via `curl -w "%{http_code}"`, expects 200, retries on 502/503
- **Multiple Response Shape Support**: Handles:
  - array `[...]`
  - `{ "items": [...] }`
  - `{ "data": [...] }`
  - `{ "data": { "items": [...] } }`
- **Retry Logic**: 5 attempts with 1s sleep for transient errors or delayed list updates
- **Tenant Headers**: Sets `x-agency-id` header when `AGENCY_ID` env var is provided
- **Better Diagnostics**: On failure, outputs first 5 booking request IDs/statuses,
  response shape, and truncated body (600 chars)

**Verification Commands** (HOST-SERVER-TERMINAL):
```bash
# 1) Pull latest code
cd /data/repos/pms-webapp
git fetch origin main && git reset --hard origin/main

# 2) Export required env vars
export API_BASE_URL="https://api.fewo.kolibri-visions.de"
export PUBLIC_HOST="fewo.kolibri-visions.de"
export JWT_TOKEN="<manager-or-admin-jwt-token>"
export AGENCY_ID="<agency-uuid>"  # Optional but recommended

# 3) Run smoke test
./backend/scripts/pms_p1_booking_request_smoke.sh
echo "rc=$?"

# Expected: All steps pass (A-E), rc=0
```

**Expected Output at Step C**:
```
ℹ Step C: Retrieving booking requests (authenticated)...
ℹ GET https://api.fewo.kolibri-visions.de/api/v1/booking-requests
ℹ Found 10 booking request(s) in response
✅ Step C PASSED: Found booking request in list (ID: <uuid>)
```

**Troubleshooting Step C**:
- **"Cannot iterate over null"**: Script not updated; pull latest code
- **HTTP 401/403**: JWT_TOKEN invalid or lacks manager/admin role
- **HTTP 500**: Backend error; check logs for SQL/constraint issues
- **"Could not find booking request after 5 attempts"**: Booking request not visible;
  check agency scoping (ensure AGENCY_ID matches JWT claims and booking request agency)

- **under_review → confirmed**: Approve action sets status to confirmed, sets confirmed_at timestamp
- **under_review → cancelled**: Decline action sets status to cancelled, stores decline_reason in cancellation_reason
- **Idempotency**: Re-approving confirmed booking returns 200 with same booking_id (no-op)
- **Booking ID**: booking_id equals request_id (by design, no separate booking record created)

**Additional Verification**:
- `GET /api/v1/bookings/831336ed-8d86-46ed-8a5a-494c5c831e79` returned HTTP 200 (confirmed booking exists after approval)

---

#### P1: Admin UI and Common Operations

**Admin UI Location**: `https://admin.fewo.kolibri-visions.de/booking-requests`

**Page Features:**
- Responsive table view with booking requests sorted by creation date (newest first)
- Status filter dropdown: All, Requested, Under Review, Confirmed, Cancelled
- Search functionality (future: by guest name, email, reference)
- Real-time action buttons: Review, Approve, Decline
- Decline modal with required reason field and validation
- Toast notifications for success/error feedback
- Loading states during API calls
- Empty state: "Keine Buchungsanfragen gefunden" with helpful guidance

**Access Control:**
- Requires valid JWT token (obtained via Supabase Auth login)
- Role requirement: `manager` or `admin` in `team_members` table
- Agency scoping: Users only see booking requests for their agency_id
- RLS policies enforce database-level isolation

**Common Operations:**

1. **View Pending Booking Requests:**
   ```bash
   # List all requested booking requests
   curl -sS -H "Authorization: Bearer $JWT_TOKEN" \
     "https://api.fewo.kolibri-visions.de/api/v1/booking-requests?status=requested&limit=50" | jq '.'
   ```

2. **Review a Booking Request:**
   ```bash
   # Mark as under_review with internal note
   curl -sS -X POST \
     -H "Authorization: Bearer $JWT_TOKEN" \
     -H "Content-Type: application/json" \
     -d '{"internal_note": "Checking property availability"}' \
     "https://api.fewo.kolibri-visions.de/api/v1/booking-requests/{request-id}/review" | jq '.'
   ```

3. **Approve a Booking Request:**
   ```bash
   # Approve and create confirmed booking
   curl -sS -X POST \
     -H "Authorization: Bearer $JWT_TOKEN" \
     -H "Content-Type: application/json" \
     -d '{"internal_note": "Approved - property available"}' \
     "https://api.fewo.kolibri-visions.de/api/v1/booking-requests/{request-id}/approve" | jq '.'

   # Response includes booking_id for newly confirmed booking
   ```

4. **Decline a Booking Request:**
   ```bash
   # Decline with required reason
   curl -sS -X POST \
     -H "Authorization: Bearer $JWT_TOKEN" \
     -H "Content-Type: application/json" \
     -d '{"decline_reason": "Property not available for requested dates", "internal_note": "Declined"}' \
     "https://api.fewo.kolibri-visions.de/api/v1/booking-requests/{request-id}/decline" | jq '.'
   ```

**Database Queries for Operations:**

```sql
-- Count pending booking requests by status
SELECT status, COUNT(*)
FROM booking_requests
WHERE agency_id = '<your-agency-id>'
GROUP BY status;

-- Find oldest unreviewed requests
SELECT id, created_at, date_from, date_to, status
FROM booking_requests
WHERE agency_id = '<your-agency-id>'
  AND status = 'requested'
ORDER BY created_at ASC
LIMIT 10;

-- Check for requests awaiting approval (under_review)
SELECT id, reviewed_at, date_from, date_to, internal_notes
FROM booking_requests
WHERE agency_id = '<your-agency-id>'
  AND status = 'under_review'
ORDER BY reviewed_at ASC
LIMIT 10;

-- Find approved requests with booking_ids
SELECT br.id, br.booking_id, br.confirmed_at, b.status AS booking_status
FROM booking_requests br
LEFT JOIN bookings b ON br.booking_id = b.id
WHERE br.agency_id = '<your-agency-id>'
  AND br.status = 'confirmed'
ORDER BY br.confirmed_at DESC
LIMIT 10;
```

---

#### P1: Additional Troubleshooting Scenarios

**Scenario: Admin UI Shows "Session Expired" (401 Error)**

**Symptom:** /booking-requests page shows "Session abgelaufen" banner and redirects to login

**Root Cause:** JWT token expired (Supabase Auth tokens have 1-hour default TTL)

**Solution:**
1. Re-login via `/login` page to obtain fresh JWT token
2. Token is automatically refreshed by Supabase client if refresh token is valid
3. For API testing: Generate fresh token using `pms_fresh_jwt.sh` script

**Verification:**
```bash
# Check token expiration
echo $JWT_TOKEN | cut -d'.' -f2 | base64 -d 2>/dev/null | jq '.exp'
# Compare with current time: date +%s
```

---

**Scenario: Cannot Decline Request (422 Validation Error)**

**Symptom:** POST /api/v1/booking-requests/{id}/decline returns 422 with "decline_reason is required"

**Root Cause:** Request body missing required `decline_reason` field

**Solution:**
- Ensure request body includes non-empty `decline_reason` string
- Example: `{"decline_reason": "Property unavailable", "internal_note": "Optional note"}`
- Admin UI decline modal enforces this validation client-side

---

**Scenario: Booking Request List Empty Despite Database Records**

**Symptom:** GET /api/v1/booking-requests returns empty array but database has records

**Root Cause:** Agency_id mismatch or RLS policy filtering records

**Solution:**
1. Verify JWT token agency_id claim:
   ```bash
   echo $JWT_TOKEN | cut -d'.' -f2 | base64 -d 2>/dev/null | jq '.agency_id'
   ```

2. Check booking_requests agency_id:
   ```sql
   SELECT agency_id, COUNT(*)
   FROM booking_requests
   GROUP BY agency_id;
   ```

3. Verify RLS policies allow SELECT:
   ```sql
   SELECT * FROM pg_policies
   WHERE tablename = 'booking_requests'
     AND cmd = 'SELECT';
   ```

---

**Scenario: Approve Action Hangs or Times Out**

**Symptom:** POST /api/v1/booking-requests/{id}/approve never returns, times out after 30s

**Root Cause:** Database deadlock or slow query (checking for overlapping bookings)

**Solution:**
1. Check for long-running queries:
   ```sql
   SELECT pid, now() - query_start AS duration, state, query
   FROM pg_stat_activity
   WHERE state != 'idle'
     AND query LIKE '%booking_requests%'
   ORDER BY duration DESC;
   ```

2. Kill long-running query if necessary:
   ```sql
   SELECT pg_terminate_backend(<pid>);
   ```

3. Check for database locks:
   ```sql
   SELECT * FROM pg_locks
   WHERE NOT granted
     AND relation::regclass::text = 'booking_requests';
   ```

---

**Scenario: Idempotent Approval Returns Different booking_id**

**Symptom:** Re-approving same booking request returns different booking_id each time

**Root Cause:** Idempotency logic not working (should return cached response for 24h)

**Solution:**
1. Check idempotency_keys table:
   ```sql
   SELECT key, endpoint, response_body, created_at
   FROM idempotency_keys
   WHERE endpoint LIKE '%approve%'
   ORDER BY created_at DESC
   LIMIT 5;
   ```

2. Verify `Idempotency-Key` header was sent in original request
3. Check Redis connectivity (if using Redis for idempotency cache)
4. TTL is 24 hours - requests older than 24h will create new bookings

---

**Scenario: Admin UI Actions Don't Update Status**

**Symptom:** Clicked Approve/Decline button but status still shows same value after refresh

**Root Cause:** JavaScript error in frontend, API error not displayed, or state update failure

**Solution:**
1. Open browser DevTools Console tab and check for errors
2. Open Network tab and inspect API response status code
3. If 200 OK: Check response body for actual status value

**Scenario: GET /api/v1/booking-requests Returns 500 (guest_id ValidationError)**

**Symptom:** GET /api/v1/booking-requests returns 500 Internal Server Error. Backend logs show Pydantic ValidationError for BookingRequestListItem: field guest_id expected UUID but got None.

**Root Cause:** Database contains booking request rows with guest_id = NULL (legacy data or data migration gaps). Pydantic schema expected required UUID field, causing ValidationError when mapping DB rows to response models.

**Fix (after deploy):**
- Schema changed: `BookingRequestListItem.guest_id` and `BookingRequestDetail.guest_id` now `Optional[UUID] = None`
- Endpoint tolerates NULL guest_id and returns 200 with guest_id=null in JSON response
- Defensive per-row validation with warning log as fallback (non-fatal)

**Diagnostic Commands:**
```bash
# Check backend logs for guest_id validation errors
docker logs --since 30m pms-backend | grep -nE "booking-requests|ValidationError|BookingRequestListItem|guest_id"

# Find booking requests with NULL guest_id (Supabase SQL Editor)
SELECT id, status, guest_id, guest_email, created_at
FROM public.booking_requests
WHERE guest_id IS NULL
ORDER BY created_at DESC
LIMIT 50;
```

**Verification After Deploy:**
```bash
# [HOST-SERVER-TERMINAL] Pull latest code and verify deployment
cd /data/repos/pms-webapp
git fetch origin main && git reset --hard origin/main

export API_BASE_URL="https://api.fewo.kolibri-visions.de"
./backend/scripts/pms_verify_deploy.sh

# Run P1 smoke test (should pass with rc=0)
export API_BASE_URL="https://api.fewo.kolibri-visions.de"
export PUBLIC_HOST="fewo.kolibri-visions.de"
export JWT_TOKEN="<manager-jwt>"
export AGENCY_ID="<agency-uuid>"
./backend/scripts/pms_p1_booking_request_smoke.sh
echo "rc=$?"
# Expected: rc=0
```

---
4. If 4xx/5xx: Check error message in response body
5. Hard refresh page (Cmd+Shift+R / Ctrl+F5) to clear stale state

---

### P2 Pricing v1 Foundation

**Scope**: Rate plans, seasonal pricing overrides, and quote calculation for booking requests

**Architecture Note**: Pricing is stored in two tables:
- `rate_plans`: Base pricing configuration for properties (agency-wide or property-specific)
- `rate_plan_seasons`: Date-range specific pricing overrides (seasonal rates, min stay)

All pricing fields are nullable/optional for gradual adoption. If property_id is NULL, the rate plan applies agency-wide.

**Currency Fallback Hierarchy**: rate_plan.currency → property.currency → agency.currency → EUR

**Database Schema**:

1. **rate_plans**:
   - `id`: UUID primary key
   - `agency_id`: UUID (required, FK to agencies)
   - `property_id`: UUID (nullable, FK to properties, NULL = agency-wide)
   - `name`: TEXT (required, rate plan display name)
   - `currency`: TEXT (nullable, ISO 4217, fallback to property/agency)
   - `base_nightly_cents`: INT (nullable, base nightly rate in cents)
   - `min_stay_nights`: INT (nullable, minimum stay requirement)
   - `max_stay_nights`: INT (nullable, maximum stay allowed)
   - `active`: BOOLEAN (default true)
   - Constraints: FK to agencies (CASCADE), FK to properties (CASCADE)

2. **rate_plan_seasons**:
   - `id`: UUID primary key
   - `rate_plan_id`: UUID (required, FK to rate_plans)
   - `date_from`: DATE (required, season start inclusive)
   - `date_to`: DATE (required, season end exclusive)
   - `nightly_cents`: INT (nullable, override nightly rate)
   - `min_stay_nights`: INT (nullable, override min stay)
   - `active`: BOOLEAN (default true)
   - Constraints: FK to rate_plans (CASCADE), CHECK (date_from < date_to)

**Endpoints**:

1. **List Rate Plans** (manager/admin):
   - `GET /api/v1/pricing/rate-plans?property_id={uuid}`
   - Returns all rate plans for agency with seasonal overrides
   - Optional filter by property_id

2. **Create Rate Plan** (manager/admin):
   - `POST /api/v1/pricing/rate-plans`
   - Body: `{"property_id": "uuid|null", "name": "...", "currency": "USD", "base_nightly_cents": 15000, "min_stay_nights": 1, "active": true, "seasons": [...]}`
   - Creates rate plan with optional seasonal overrides
   - Returns 201 with created rate plan including all seasons

3. **Calculate Quote** (authenticated):
   - `POST /api/v1/pricing/quote`
   - Body: `{"property_id": "uuid", "check_in": "2026-01-10", "check_out": "2026-01-13"}`
   - Calculates pricing for date range using active rate plan
   - Seasonal override takes precedence over base rate if date range overlaps
   - Returns quote with nightly_cents, total_cents, nights, currency, rate_plan details
   - If no pricing configured: returns quote with message and null amounts

**Quote Calculation Logic**:
1. Find active rate plan for property (property-specific first, then agency-wide)
2. Check for seasonal override that applies to check_in date
3. Use seasonal override nightly_cents if found, otherwise base_nightly_cents
4. Calculate: total_cents = nightly_cents × nights
5. Return quote with all pricing details

**Error Codes**:
- **400 Bad Request**: Invalid dates (check_out must be after check_in)
- **401 Unauthorized**: Missing or invalid JWT token
- **403 Forbidden**: User lacks required role or agency access
- **404 Not Found**: Property not found in agency
- **422 Validation**: Invalid input (e.g., negative nightly_cents, invalid currency)
- **500 Internal Server Error**: Database error

**Troubleshooting**:

**Problem**: Quote returns null pricing (nightly_cents=null, total_cents=null)

**Solution**:
1. Check if rate plan exists: `GET /api/v1/pricing/rate-plans?property_id={uuid}`
2. If no rate plans: Create one with `POST /api/v1/pricing/rate-plans`
3. If rate plan exists but has null base_nightly_cents: No seasonal override found for dates
4. Add seasonal override or set base_nightly_cents in rate plan

---

**Problem**: Quote uses wrong rate (expected seasonal rate, got base rate)

**Solution**:
1. Check seasonal override date ranges: `GET /api/v1/pricing/rate-plans`
2. Verify check_in date falls within season date_from (inclusive) to date_to (exclusive)
3. Verify seasonal override has active=true and nightly_cents is not null
4. Season selection uses check_in date only (not check_out)

---

**Problem**: Quote uses wrong rate plan (expected newest plan, got older plan)

**Symptoms**:
- Multiple active rate plans exist for property
- Quote returns rate_plan_id that doesn't match most recently created/updated plan
- Smoke test fails with "Rate plan ID mismatch"
- Nondeterministic behavior: sometimes gets new plan, sometimes old plan

**Root Cause**:
- Before fix (commit <43c122a): Quote selection had no ORDER BY for updated_at/created_at
- PostgreSQL returned arbitrary row when multiple plans matched filter
- Race condition: which plan gets selected depended on database internal ordering

**Solution (Fixed)**:
Quote endpoint now uses deterministic ordering:
```sql
ORDER BY property_id NULLS LAST,        -- Prefer property-specific over agency-wide
         updated_at DESC NULLS LAST,    -- Then newest updated
         created_at DESC NULLS LAST,    -- Then newest created
         id DESC                        -- Final tiebreaker
LIMIT 1
```

List endpoint also uses same ordering (without property_id preference since already filtered):
```sql
ORDER BY updated_at DESC NULLS LAST, created_at DESC NULLS LAST, id DESC
```

**Expected Behavior**:
- Quote always selects the most recently updated/created active rate plan
- Property-specific plans always preferred over agency-wide
- First rate plan in list matches the one used for quotes (consistent ordering)

**Verification**:
```bash
# Create new rate plan
curl -X POST "$HOST/api/v1/pricing/rate-plans" \
  -H "Authorization: Bearer $JWT_TOKEN" \
  -H "x-agency-id: $AGENCY_ID" \
  -H "Content-Type: application/json" \
  -d '{"property_id": "...", "name": "New Plan", ...}' | jq '.id'
# → new_plan_id

# Get quote - should use new_plan_id
curl -X POST "$HOST/api/v1/pricing/quote" \
  -H "Authorization: Bearer $JWT_TOKEN" \
  -H "x-agency-id: $AGENCY_ID" \
  -H "Content-Type: application/json" \
  -d '{"property_id": "...", "check_in": "...", "check_out": "..."}' | jq '.rate_plan_id'
# → should match new_plan_id
```

---

**Problem**: Create rate plan fails with 404 Property Not Found

**Solution**:
1. Verify property_id exists in agency: `GET /api/v1/properties`
2. Verify property belongs to current agency (agency scoping enforced)
3. For agency-wide rate plan: set property_id to null in request body

---

**Problem**: Create rate plan returns 500 Internal Server Error - Pydantic ValidationError on created_at/updated_at

**Symptoms**:
- POST /api/v1/pricing/rate-plans creates database row successfully
- Logs show: "Created rate plan: id=9c15fd7e-..."
- Then 500 error with ValidationError: "Input should be a valid string, got datetime.datetime(...)"
- Stack trace points to RatePlanResponse serialization (pricing.py line ~209)
- Same issue affects GET /api/v1/pricing/rate-plans if returned

**Root Cause**:
- Database returns created_at/updated_at as datetime.datetime objects
- Pydantic schema (RatePlanResponse, RatePlanSeasonResponse) typed these as `str`
- Validation fails when trying to serialize response with datetime objects

**Solution**:
Schema was fixed to use `datetime` type instead of `str`:
```python
# backend/app/schemas/pricing.py
from datetime import date, datetime  # Added datetime import

class RatePlanResponse(BaseModel):
    created_at: datetime  # Changed from str
    updated_at: datetime  # Changed from str
```

Pydantic v2 automatically serializes datetime to ISO 8601 strings in JSON responses.

**Verification**:
```bash
# Create rate plan should return 201 (not 500)
curl -X POST "$HOST/api/v1/pricing/rate-plans" \
  -H "Authorization: Bearer $JWT_TOKEN" \
  -H "x-agency-id: $AGENCY_ID" \
  -H "Content-Type: application/json" \
  -d '{"property_id": "...", "name": "Test", "currency": "USD", "base_nightly_cents": 15000, "active": true}' \
  | jq '.created_at'  # Should show ISO string like "2026-01-06T12:34:56.789Z"

# Smoke script should pass
HOST=$HOST JWT_TOKEN=$JWT_TOKEN AGENCY_ID=$AGENCY_ID ./backend/scripts/pms_pricing_quote_smoke.sh
```

**Prevention**:
- Always match Pydantic schema types to actual database column types
- Use `datetime` for TIMESTAMP columns, not `str`
- Let Pydantic handle serialization (no manual .isoformat() needed)

---

**Problem**: Smoke script fails with "Cannot index object with number" or JSONDecodeError during property auto-pick

**Solution**:
1. Properties list endpoint returns paginated response: `{items: [...], total: N, ...}`
2. Script auto-pick uses: `GET /api/v1/properties/?limit=1&offset=0` → parses `.items[0].id`
3. If you see 307 redirects: Script already uses `curl -L` to follow redirects automatically
4. If no properties exist: Create one first or export `PROPERTY_ID=<uuid>` and rerun (script exits with rc=2)
5. Verify API is reachable: `curl -L "$HOST/api/v1/properties/?limit=1" -H "Authorization: Bearer <token>"`

---

**Problem**: Endpoints return 404 Not Found (router not mounted)

**Solution**:
1. Check ACTUAL mounted routes (authoritative): `GET /api/v1/ops/modules` → verify:
   - `mounted_has_pricing: true`
   - `pricing_paths: ["/api/v1/pricing/rate-plans", "/api/v1/pricing/quote"]`
   - `pricing` appears in `modules` list with prefixes `["/api/v1/pricing"]`
2. If `mounted_has_pricing: false`:
   - Check if pricing module is in registry modules list
   - If missing from registry: pricing module failed to load/register
     - Root cause: `backend/app/api/routes/__init__.py` must import pricing
     - Verify: `from . import ... pricing` exists in routes/__init__.py
     - Check logs for "Pricing module not available" ImportError warnings
   - If in registry but not mounted: check MODULES_ENABLED setting
3. Check OpenAPI schema: `GET /openapi.json` → verify `/api/v1/pricing/*` paths exist
4. Check deploy commit: `GET /api/v1/ops/version` → verify `source_commit` matches expected SHA
5. If still failing: restart app (stale process or module import cached failure)

**Smoke Test**:
```bash
# Run pricing smoke test (requires JWT_TOKEN with manager/admin role)
HOST=https://api.example.com \
JWT_TOKEN=<token> \
./backend/scripts/pms_pricing_quote_smoke.sh
```

**Related Documentation**:
- [Pricing Quote Smoke Script](../scripts/README.md#p2-pricing-quote-smoke-test) - Full script documentation
- [Pricing API](../app/api/routes/pricing.py) - API implementation
- [Pricing Schemas](../app/schemas/pricing.py) - Pydantic models
- [Migration 20260106150000](../../supabase/migrations/20260106150000_add_pricing_v1.sql) - Database schema

**Ops Note (2026-01-10)**: Pricing quote smoke script is PROD-safe (idempotent + delta verification). Safe to re-run unlimited times in non-empty PROD environments. SMOKE-P2 artifacts (rate plan, fee, tax) are reused by name; verification uses baseline+delta instead of absolute totals. Tax delta verification accounts for (a) existing taxes applied to taxable delta and (b) newly-added SMOKE tax on full taxable amount. See script README for full details.

---

---

**Problem**: GET /api/v1/ops/modules doesn't show channel-connections routes (hyphenated prefixes missing)

**Symptoms**:
- `/api/v1/ops/modules` returns `mounted_prefixes` list that's missing `/api/v1/channel-connections`
- `mounted_has_channel_connections: false` even though routes exist in OpenAPI schema
- Hyphenated path segments (e.g., `channel-connections`, `rate-plans`) not detected in prefix extraction
- Module registry shows routes but actual mounted paths are incomplete

**Root Cause**:
- Old prefix extraction used word-boundary regex `\w+` which doesn't match hyphens
- Pattern `^(/api/v1/\w+)` only matched alphanumeric + underscore (not hyphens)
- Routes like `/api/v1/channel-connections/*` were skipped during inspection
- Result: ops/modules output was incomplete and not authoritative

**Solution**:
Fixed prefix extraction to use robust regex: `^(/api/v1/[^/]+)`
- Matches any character except forward slash (includes hyphens, underscores, alphanumeric)
- Extracts first three path segments: `/api/v1/<prefix>`
- Handles hyphenated prefixes: `channel-connections`, `booking-requests`, etc.
- Deduplicates paths automatically (uses set internally)

**New Fields (added 2026-01-07)**:
- `mounted_has_channel_connections: bool` - True if `/api/v1/channel-connections/*` routes exist
- `channel_connections_paths: list[str]` - All channel-connections paths (sorted, deduplicated)
- `pricing_paths: list[str]` - Now deduplicated (previously could contain duplicates)

**Verification**:
```bash
# Check channel-connections detection
curl https://api.example.com/api/v1/ops/modules | jq '{
  mounted_has_channel_connections,
  channel_connections_paths,
  mounted_prefixes: .mounted_prefixes | map(select(contains("channel")))
}'

# Expected output (if channel-connections routes exist):
# {
#   "mounted_has_channel_connections": true,
#   "channel_connections_paths": [
#     "/api/v1/channel-connections/sync",
#     "/api/v1/channel-connections/availability",
#     "/api/v1/channel-connections/pricing"
#   ],
#   "mounted_prefixes": ["/api/v1/channel-connections"]
# }
```

**Use Cases**:
1. **Deploy Verification**: Confirm channel-connections module is mounted after deploy
2. **Troubleshooting 404s**: Check if routes are actually mounted vs just registered
3. **Module Registry vs Reality**: Compare registry metadata with actual app.routes
4. **Smoke Tests**: Verify specific route families exist before running tests

**Related Helpers** (internal, tested):
- `extract_mounted_prefixes(routes)` - Extract unique API prefixes with regex `^(/api/v1/[^/]+)`
- `extract_paths_by_prefix(routes, prefix)` - Get all paths matching prefix (deduplicated, sorted)
- Unit tests: `tests/unit/test_ops_helpers.py`

**Important Notes**:
- `/ops/modules` is **authoritative** - reads from `request.app.routes` (not registry)
- No database calls, no authentication required (safe metadata only)
- Response reflects actual FastAPI routing table at request time
- If `modules_enabled=false` but routes exist: routes were mounted directly (bypass module system)
- Use this endpoint for ops verification, not `/openapi.json` (OpenAPI may lag behind reality)

---

### P2 Pricing v1 Extension (Fees and Taxes)

**Scope**: Comprehensive pricing breakdown with fees and taxes for booking quotes

**Architecture Note**: Extends P2 Foundation with two additional tables:
- `pricing_fees`: Fixed or percentage-based fees added to booking cost (cleaning, service, etc.)
- `pricing_taxes`: Percentage taxes applied to taxable amounts (subtotal + taxable fees)

All fields nullable/optional for gradual adoption. If property_id is NULL, the fee/tax applies agency-wide.

**Database Schema**:

1. **pricing_fees**:
   - `id`: UUID primary key
   - `agency_id`: UUID (required, FK to agencies)
   - `property_id`: UUID (nullable, FK to properties, NULL = agency-wide)
   - `name`: TEXT (required, fee display name)
   - `type`: TEXT (required, one of: per_stay, per_night, per_person, percent)
   - `value_cents`: INT (nullable, value in cents for fixed fees)
   - `value_percent`: NUMERIC(5,2) (nullable, percentage for percent type)
   - `taxable`: BOOLEAN (default false, whether fee is included in tax calculation)
   - `active`: BOOLEAN (default true)
   - Constraints: FK to agencies (CASCADE), FK to properties (CASCADE)
   - Validation: percent type requires value_percent, others require value_cents

2. **pricing_taxes**:
   - `id`: UUID primary key
   - `agency_id`: UUID (required, FK to agencies)
   - `property_id`: UUID (nullable, FK to properties, NULL = agency-wide)
   - `name`: TEXT (required, tax display name)
   - `percent`: NUMERIC(5,2) (required, tax rate percentage 0-100)
   - `active`: BOOLEAN (default true)
   - Constraints: FK to agencies (CASCADE), FK to properties (CASCADE)

**Endpoints**:

1. **List Fees** (manager/admin):
   - `GET /api/v1/pricing/fees?property_id={uuid}&active={bool}&limit={int}&offset={int}`
   - Returns all fees for agency with optional filters
   - Pagination support with limit (default 100) and offset (default 0)

2. **Create Fee** (manager/admin):
   - `POST /api/v1/pricing/fees`
   - Body: `{"property_id": "uuid|null", "name": "Cleaning Fee", "type": "per_stay", "value_cents": 5000, "taxable": true, "active": true}`
   - Type validation: percent → value_percent required; others → value_cents required
   - Returns 201 with created fee

3. **List Taxes** (manager/admin):
   - `GET /api/v1/pricing/taxes?property_id={uuid}&active={bool}&limit={int}&offset={int}`
   - Returns all taxes for agency with optional filters
   - Pagination support with limit (default 100) and offset (default 0)

4. **Create Tax** (manager/admin):
   - `POST /api/v1/pricing/taxes`
   - Body: `{"property_id": "uuid|null", "name": "Sales Tax", "percent": 7.5, "active": true}`
   - Returns 201 with created tax

5. **Calculate Quote** (authenticated) - EXTENDED:
   - `POST /api/v1/pricing/quote`
   - Body: `{"property_id": "uuid", "check_in": "2026-01-10", "check_out": "2026-01-13", "adults": 2, "children": 1}`
   - Calculates comprehensive pricing breakdown including fees and taxes
   - Returns QuoteResponse with:
     - `nightly_cents`: Nightly rate (unchanged from Foundation)
     - `subtotal_cents`: Accommodation subtotal (nightly_cents × nights)
     - `fees`: Array of FeeLineItem (name, type, amount_cents, taxable)
     - `fees_total_cents`: Sum of all fees
     - `taxable_amount_cents`: Subtotal + taxable fees
     - `taxes`: Array of TaxLineItem (name, percent, amount_cents)
     - `taxes_total_cents`: Sum of all taxes
     - `total_cents`: Grand total (subtotal + fees + taxes)
   - Backward compatible: if no fees/taxes configured, returns empty arrays and totals=0

**Quote Calculation Logic (Extended)**:
1. Find active rate plan for property (unchanged from Foundation)
2. Calculate accommodation subtotal: `subtotal_cents = nightly_cents × nights`
3. Fetch active fees for property (property-specific first, then agency-wide)
4. Calculate each fee:
   - `per_stay`: fee.value_cents (once per booking)
   - `per_night`: fee.value_cents × nights
   - `per_person`: fee.value_cents × (adults + children) × nights
   - `percent`: (subtotal_cents × fee.value_percent) / 100
5. Calculate taxable amount: `taxable_amount_cents = subtotal_cents + sum(taxable_fees)`
6. Fetch active taxes for property (property-specific first, then agency-wide)
7. Calculate each tax: `tax_amount_cents = (taxable_amount_cents × tax.percent) / 100`
8. Calculate grand total: `total_cents = subtotal_cents + fees_total_cents + taxes_total_cents`

**Fee Type Examples**:
- **per_stay**: Cleaning fee charged once per booking (e.g., $50.00 = 5000 cents)
- **per_night**: Resort fee charged per night (e.g., $20.00/night = 2000 cents)
- **per_person**: Linen fee charged per person per night (e.g., $5.00/person/night = 500 cents)
- **percent**: Service fee as percentage of subtotal (e.g., 10% = 10.0)

**Error Codes**:
- **400 Bad Request**: Invalid fee/tax creation (e.g., percent type without value_percent)
- **401 Unauthorized**: Missing or invalid JWT token
- **403 Forbidden**: User lacks manager/admin role
- **404 Not Found**: Property not found in agency
- **422 Validation**: Invalid input (e.g., negative value_cents, percent > 100)
- **500 Internal Server Error**: Database error

**Troubleshooting**:

**Problem**: Quote returns empty fees/taxes arrays (fees=[], taxes=[])

**Solution**:
1. Check if fees exist: `GET /api/v1/pricing/fees?property_id={uuid}`
2. Check if taxes exist: `GET /api/v1/pricing/taxes?property_id={uuid}`
3. If no fees/taxes found: This is expected behavior, quote returns empty arrays
4. If fees/taxes exist but not in quote: Check active=true and property_id matches
5. For agency-wide fees/taxes: Create with property_id=null in request body

---

**Problem**: Fee calculation incorrect (expected $50, got different amount)

**Solution**:
1. Verify fee type matches expected calculation:
   - per_stay: Should be constant regardless of nights/guests
   - per_night: Should multiply by nights
   - per_person: Should multiply by (adults + children) × nights
   - percent: Should calculate as percentage of subtotal_cents
2. Check fee value_cents or value_percent in database
3. Verify fee is active=true and property_id matches quote property

---

**Problem**: Tax calculation incorrect (expected 7.5% of subtotal, got different amount)

**Solution**:
1. Taxes are calculated on taxable_amount_cents = subtotal + taxable fees (NOT just subtotal)
2. Check which fees have taxable=true in database
3. Formula: tax_amount = (subtotal + sum(taxable_fees)) × tax_percent / 100
4. Verify tax percent value in database (stored as NUMERIC, e.g., 7.5 for 7.5%)
5. Integer rounding: Tax calculation uses int() truncation, not round()

---

**Problem**: Create fee fails with 400 "value_percent is required for percent type fees"

**Solution**:
1. For type=percent: Must provide value_percent, value_cents must be null
2. For type=per_stay/per_night/per_person: Must provide value_cents, value_percent must be null
3. Check request body matches type requirements
4. Example percent fee: `{"type": "percent", "value_percent": 10.0, "value_cents": null}`
5. Example fixed fee: `{"type": "per_stay", "value_cents": 5000, "value_percent": null}`

---

**Problem**: Quote total doesn't match expected calculation

**Symptoms**:
- Frontend shows different total than backend quote
- Manual calculation: subtotal + fees + taxes ≠ total_cents
- Smoke test fails with "Grand total mismatch"

**Root Cause**:
- Floating point precision issues in fee/tax percentage calculations
- Client-side calculation differs from server-side int() truncation
- Missing taxable fees in tax base calculation

**Solution**:
1. Backend uses integer arithmetic with int() truncation (not round())
2. Always use backend total_cents as source of truth
3. Verify calculation manually:
   ```bash
   subtotal_cents = nightly_cents × nights
   fees_total_cents = sum(all fee amounts)
   taxable_amount_cents = subtotal_cents + sum(taxable fee amounts)
   taxes_total_cents = sum((taxable_amount_cents × tax.percent / 100) for each tax)
   total_cents = subtotal_cents + fees_total_cents + taxes_total_cents
   ```
4. Example: $150/night × 3 nights = $450 subtotal, $50 cleaning (taxable), $37.50 tax (7.5%) = $537.50 total
   - subtotal_cents = 15000 × 3 = 45000
   - fees_total_cents = 5000
   - taxable_amount_cents = 45000 + 5000 = 50000
   - taxes_total_cents = int(50000 × 7.5 / 100) = 3750
   - total_cents = 45000 + 5000 + 3750 = 53750

---

### Smoke Testing P2 Extension

**Script**: `backend/scripts/pms_pricing_quote_smoke.sh`

**Tests**:
1. Create rate plan with base pricing
2. Create cleaning fee (per_stay, taxable)
3. Create sales tax (7.5%)
4. Calculate quote and verify comprehensive breakdown
5. Verify: subtotal = nightly × nights
6. Verify: fees_total = cleaning_fee
7. Verify: taxable_amount = subtotal + cleaning_fee
8. Verify: taxes_total = taxable_amount × 7.5%
9. Verify: total = subtotal + fees + taxes

**Usage**:
```bash
# HOST-SERVER-TERMINAL
HOST="https://api.fewo.kolibri-visions.de" \
JWT_TOKEN="<manager-or-admin-token>" \
AGENCY_ID="<uuid>" \
./backend/scripts/pms_pricing_quote_smoke.sh
```

**Expected Output**:
```
✅ Created rate plan: <uuid>
✅ Created fee: <uuid> (Cleaning Fee: 5000 cents)
✅ Created tax: <uuid> (Sales Tax: 7.5%)
✅ Quote calculated:
  Subtotal: 45000 cents
  Fees Total: 5000 cents
  Taxable Amount: 50000 cents
  Taxes Total: 3750 cents
  Grand Total: 53750 cents
✅ Quote calculation verified (subtotal + fees + taxes = total)
✅ All P2 Pricing + Extension smoke tests passed! 🎉
```

---

## P2.2 Rate Plan Seasons Editor

**Overview:** Manual seasons editing interface for property-scoped rate plans.

**Purpose:** Allow staff to create/edit/delete seasonal pricing overrides directly on rate plans, with template application as a convenience feature.

**Architecture:**
- **UI Location**: `/pricing/rate-plans` → "Seasons" button per rate plan
- **API Endpoints**: CRUD operations on `/api/v1/pricing/rate-plans/{rate_plan_id}/seasons`
- **Database**: `rate_plan_seasons` table with label, date ranges, nightly_cents, archived_at
- **Validation**: Server-side overlap detection (422), date range validation, price validation
- **Soft Delete**: DELETE endpoint sets `archived_at` timestamp instead of hard delete

**API Endpoints:**

Staff (manager/admin):
- `GET /api/v1/pricing/rate-plans/{rate_plan_id}/seasons?include_archived=` - List seasons for rate plan
- `POST /api/v1/pricing/rate-plans/{rate_plan_id}/seasons` - Create new season
- `PATCH /api/v1/pricing/rate-plans/{rate_plan_id}/seasons/{season_id}` - Update existing season
- `DELETE /api/v1/pricing/rate-plans/{rate_plan_id}/seasons/{season_id}` - Soft delete season (204)

**Validation Rules:**
- No overlapping date ranges between active seasons (422 error if overlap detected)
- `date_from` must be before `date_to` (422 error)
- `nightly_cents` must be positive if provided (422 error)
- All operations scoped by agency_id (tenant isolation)
- Rate plan must exist and belong to agency (404 error)

**Verification Commands:**

```bash
# HOST-SERVER-TERMINAL
export HOST="https://api.fewo.kolibri-visions.de"
export MANAGER_JWT_TOKEN="<manager/admin JWT>"
# Optional: export PROPERTY_ID="<uuid>"

# Run smoke test
./backend/scripts/pms_rate_plan_seasons_smoke.sh
echo "rc=$?"

# Expected output: All 8 tests pass, rc=0
```

**Common Issues:**

### Season Overlaps with Existing Season (422)

**Symptom:** POST or PATCH returns 422 with error: "Season overlaps with existing season. Date range YYYY-MM-DD to YYYY-MM-DD conflicts with another active season."

**Root Cause:** New or updated season date range overlaps with an existing active, non-archived season for the same rate plan.

**How to Debug:**
```bash
# List all seasons for the rate plan
curl -X GET "$HOST/api/v1/pricing/rate-plans/{rate_plan_id}/seasons" \
  -H "Authorization: Bearer $MANAGER_JWT_TOKEN"

# Check for overlaps in date ranges
# Overlap logic: (start1 < end2) AND (start2 < end1)
```

**Solution:**
- Check existing seasons and adjust date ranges to avoid overlap
- OR: Set conflicting season to `active=false` via PATCH
- OR: Delete conflicting season via DELETE endpoint

### Invalid Date Range (422)

**Symptom:** POST or PATCH returns 422 with error: "date_from must be before date_to".

**Root Cause:** `date_from` is greater than or equal to `date_to`.

**How to Debug:**
```bash
# Check the payload dates
echo "date_from: YYYY-MM-DD"
echo "date_to: YYYY-MM-DD"
# date_from must be strictly less than date_to
```

**Solution:**
- Ensure `date_from < date_to` in the request payload
- Verify date format is YYYY-MM-DD

### Season Not Found (404)

**Symptom:** PATCH or DELETE returns 404 with error: "Season not found".

**Root Cause:** Season ID doesn't exist, doesn't belong to the specified rate plan, or rate plan doesn't belong to agency.

**How to Debug:**
```bash
# Verify season exists and belongs to rate plan
curl -X GET "$HOST/api/v1/pricing/rate-plans/{rate_plan_id}/seasons" \
  -H "Authorization: Bearer $MANAGER_JWT_TOKEN"

# Check if season_id is in the response
```

**Solution:**
- Verify season_id is correct
- Verify rate_plan_id is correct
- Ensure JWT token has correct agency_id claim

### Template Apply Fails

**Symptom:** POST to `/api/v1/pricing/rate-plans/{rate_plan_id}/apply-template` fails with 422 or other error.

**Root Cause:** Template periods cause overlaps in "merge" mode, or template doesn't exist.

**How to Debug:**
- Check template exists: GET `/api/v1/pricing/season-templates?active=true`
- Check existing seasons: GET seasons endpoint
- Try "replace" mode instead of "merge"

**Solution:**
- Use "replace" mode to clear existing seasons before applying template
- OR: Manually delete conflicting seasons before applying template in "merge" mode

### Restart Loop on Deploy: NameError in pricing.py

**Symptom:** pms-backend container enters restart loop after deploying P2.2. Runtime logs show:
```
NameError: name 'RatePlanSeasonCreate' is not defined
  File "/app/app/api/routes/pricing.py", line 523, in <module>
    season_input: RatePlanSeasonCreate,
```
Container exits with code 1, preventing uvicorn from importing app.main.

**Root Cause:** Type annotation in endpoint signature references schema class that is not imported at module import time (Python 3.12 strict type checking).

**How to Debug:**
```bash
# Check backend logs for import errors
docker logs pms-backend 2>&1 | grep -A5 "NameError"

# Verify schema exists in schemas/pricing.py
cd backend
python3 -c "from app.schemas.pricing import RatePlanSeasonCreate; print('OK')"

# Verify import in routes/pricing.py
rg -n "RatePlanSeasonCreate" backend/app/api/routes/pricing.py
rg -n "from.*schemas.pricing import" backend/app/api/routes/pricing.py
```

**Solution:**
- Add missing schema to import statement in `backend/app/api/routes/pricing.py`:
  ```python
  from ...schemas.pricing import (
      ...
      RatePlanSeasonCreate,  # Add this line
      RatePlanSeasonResponse,
      ...
  )
  ```
- Optionally add `from __future__ import annotations` at top of file for forward compatibility
- Verify: `python3 -m py_compile backend/app/api/routes/pricing.py`
- Commit and push
- Backend container should start successfully after redeploy

**Prevention:**
- Always verify endpoint signature types are imported when adding new endpoints
- Run `python3 -m py_compile` on modified route files before committing
- Use IDE with type checking enabled (Pyright, mypy)

---

## P2.3 Rate Plans Admin UI — Archived Items Hidden by Default

### UI: Archived Items Hidden by Default

**Behavior**: Rate Plans admin UI (`/pricing/rate-plans`) hides archived items by default.

**Toggle Control**: "Archivierte anzeigen" checkbox in the page header
- **Default**: OFF (archived items hidden)
- **When ON**: Shows both active and archived items
- **Visual Distinction**: Archived items display gray "archiviert" badge

**Applies To**:
- Property-scoped rate plans tab (Tarifpläne)
- Agency-scoped templates tab (Vorlagen)

**API Behavior**:
- `include_archived=false` by default (excludes archived)
- `include_archived=true` when toggle enabled (includes archived)

**Note**: Archived items still exist in database and can be restored by staff if needed. Toggle only controls visibility in admin UI.

---

## Pricing v1 Rate Plans MVP

**Overview:** Rate Plans API with CRUD endpoints and quote integration for property pricing configuration.

**Purpose:** Allow property managers to create and manage rate plans with seasonal overrides, and integrate them into quote calculations for accurate pricing.

**Architecture:**
- **Database**: `rate_plans` and `rate_plan_seasons` tables (migration: `20260106150000_add_pricing_v1.sql`)
- **Agency Resolution**: Agency determined by `get_current_agency_id()` dependency (domain/x-agency-id header + DB validation)
- **RBAC**: Endpoints use `require_agency_roles("manager", "admin")` dependency (checks team_members.role in DB, not Supabase JWT role claim which is often only "authenticated")
- **Property Scope**: Rate plans can be property-specific (property_id) or agency-wide (property_id IS NULL)
- **Quote Integration**: Quote endpoint accepts optional rate_plan_id or auto-selects active rate plan

**API Endpoints:**

Manager/Admin:
- `GET /api/v1/pricing/rate-plans?property_id=&active=&limit=&offset=` - List rate plans
- `POST /api/v1/pricing/rate-plans` - Create rate plan (with optional seasons)
- `PATCH /api/v1/pricing/rate-plans/{id}` - Update rate plan fields
- `DELETE /api/v1/pricing/rate-plans/{id}` - Delete rate plan (cascade deletes seasons)
- `POST /api/v1/pricing/quote` - Calculate quote (optional rate_plan_id parameter)

**Database Tables:**
- `rate_plans` - Rate plan configurations with base pricing
- `rate_plan_seasons` - Seasonal overrides for date-range specific pricing

**Verification Commands:**

```bash
# [HOST-SERVER-TERMINAL] Pull latest code
cd /data/repos/pms-webapp
git fetch origin main && git reset --hard origin/main

# [HOST-SERVER-TERMINAL] Run rate plans smoke test
export API_BASE_URL="https://api.fewo.kolibri-visions.de"
export JWT_TOKEN="<<<manager/admin JWT>>>"
# Optional:
# export PROPERTY_ID="23dd8fda-59ae-4b2f-8489-7a90f5d46c66"
# export AGENCY_ID="ffd0123a-10b6-40cd-8ad5-66eee9757ab7"
./backend/scripts/pms_pricing_rate_plans_smoke.sh
echo "rc=$?"

# Expected output: All 10 tests pass, rc=0
```

**Common Issues:**

### PATCH Rate Plan Returns 404

**Symptom:** PATCH /api/v1/pricing/rate-plans/{id} returns 404 for existing rate plan.

**Root Cause:** Rate plan does not belong to user's agency, or rate_plan_id is invalid.

**How to Debug:**
```bash
# Check rate plan exists and agency (Supabase SQL Editor)
SELECT id, agency_id, property_id, name
FROM rate_plans
WHERE id = '<rate_plan_id>';

# Verify user has manager/admin role in team_members
SELECT role FROM team_members
WHERE agency_id = '<agency_id>' AND user_id = '<user_id>';
# Must return 'manager' or 'admin'
```

**Solution:**
- Verify rate plan UUID is correct
- Ensure user's agency (via get_current_agency_id) matches rate_plans.agency_id
- Check user has manager or admin role in team_members table for the agency

### Quote Does Not Use Specified rate_plan_id

**Symptom:** POST /api/v1/pricing/quote with rate_plan_id returns 404 or uses different rate plan.

**Root Cause:** Specified rate plan not found, or not applicable to property (wrong agency or property mismatch).

**How to Debug:**
```bash
# Test quote with specific rate_plan_id
curl -X POST "$API_BASE_URL/api/v1/pricing/quote" \
  -H "Authorization: Bearer $JWT_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "property_id": "<property_id>",
    "check_in": "2026-07-15",
    "check_out": "2026-07-18",
    "adults": 2,
    "rate_plan_id": "<rate_plan_id>"
  }' | jq '.rate_plan_id, .rate_plan_name, .message'

# Check if rate plan applies to property (Supabase SQL Editor)
SELECT id, name, property_id, active, agency_id
FROM rate_plans
WHERE id = '<rate_plan_id>';
# property_id must match quote property_id OR be NULL (agency-wide)
```

**Solution:**
- Verify rate_plan_id belongs to same agency as property
- Ensure rate plan property_id matches quote property_id OR is NULL (agency-wide)
- If 404 returned: rate plan does not exist or does not belong to agency

### Seasonal Override Not Applied

**Symptom:** Quote returns base_nightly_cents instead of seasonal override rate.

**Root Cause:** Seasonal override date range does not include check_in date, or season is inactive.

**How to Debug:**
```bash
# Check seasons for rate plan (Supabase SQL Editor)
SELECT id, date_from, date_to, nightly_cents, active
FROM rate_plan_seasons
WHERE rate_plan_id = '<rate_plan_id>'
ORDER BY date_from;

# Season applies if: check_in >= date_from AND check_in < date_to AND active = true
# Example: check_in='2026-07-15', date_from='2026-06-01', date_to='2026-08-31' → MATCH
```

**Solution:**
- Verify check_in >= date_from AND check_in < date_to
- Ensure season active = true
- Check nightly_cents is not NULL in seasonal override
- If multiple seasons overlap check_in, most recent date_from wins (ORDER BY date_from DESC LIMIT 1)

### Cannot Set Rate Plan Field to NULL

**Symptom:** PATCH request to clear optional field (e.g., {"currency": null}) does not update database.

**Root Cause:** Field not provided in request body (omitted vs explicitly null).

**How to Debug:**
```bash
# Test PATCH with explicit NULL value
curl -X PATCH "$API_BASE_URL/api/v1/pricing/rate-plans/<id>" \
  -H "Authorization: Bearer $JWT_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"currency": null}' | jq '.currency'

# Should return null, not previous value
```

**Solution:**
- Backend uses Pydantic model_fields_set to detect provided fields (including explicit null)
- Verify PATCH payload includes field with null value: `{"currency": null}`
- Omitting field entirely leaves it unchanged; null value clears it

### Smoke Test — DELETE Step Shows Blank HTTP Code

**Symptom:** `pms_pricing_rate_plans_smoke.sh` Test 6 (DELETE) shows blank HTTP code or fails with "HTTP code: (empty)".

**Root Cause:** One of:
1. **curl HTTP code capture bug**: Old script didn't properly separate HTTP code from response body
2. **Redirects not followed**: Missing `-L` flag caused 307 redirects to be treated as final response
3. **Invalid JWT token**: Token expired, malformed, or missing (curl fails silently)
4. **404 after DELETE re-run**: Rate plan was already deleted in previous test run (idempotent cleanup not handled)

**How to Debug:**
```bash
# Verify JWT token is valid (3 parts, length > 100 chars)
echo $JWT_TOKEN | tr '.' '\n' | wc -l  # Should output: 3
echo ${#JWT_TOKEN}  # Should be > 100

# Manual DELETE with verbose output
curl -L -v -X DELETE "$API_BASE_URL/api/v1/pricing/rate-plans/<rate_plan_id>" \
  -H "Authorization: Bearer $JWT_TOKEN" \
  -w "\nHTTP_CODE: %{http_code}\n" \
  -o /tmp/delete_response.json

# Check response body
cat /tmp/delete_response.json

# If 404: Verify rate plan is absent (idempotent cleanup)
curl -X GET "$API_BASE_URL/api/v1/pricing/rate-plans?limit=100" \
  -H "Authorization: Bearer $JWT_TOKEN" | jq '.items[] | select(.id=="<rate_plan_id>")'
# Should return empty (rate plan confirmed deleted)
```

**Solution:**
- **Updated script available** (commit 71e1ce2+): Run latest `pms_pricing_rate_plans_smoke.sh` with enhanced DELETE handling:
  - JWT token preflight validation (checks length and structure before running tests)
  - Robust HTTP code capture using `-D` (headers file), `-o` (body file), `-w "%{http_code}"` (status)
  - Always prints captured HTTP code to console: "DELETE returned HTTP 204"
  - curl `-k` flag (ignore SSL errors), `-L` flag (follow redirects)
  - DELETE idempotent handling: 404 treated as success if resource confirmed absent via GET list
  - If status capture fails or unexpected: Prints first 120 lines of headers, first 200 lines of body
- **Idempotent DELETE semantics**: Test 6 accepts 204/200 (success) or 404 (already deleted, verified absent)
- **Manual verification**: Use `-k -L -D <headers_file> -o <body_file> -w "%{http_code}"` flags in curl for reliable HTTP status capture
- **JWT refresh**: If token expired, obtain new manager/admin JWT from Supabase auth

### Cannot Archive Default Rate Plan (409 Conflict)

**Symptom:** DELETE /api/v1/pricing/rate-plans/{id} returns 409 Conflict for default rate plan.

**Root Cause:** Backend prevents archiving default rate plans to maintain at least one active default per property/agency. User must set another plan as default before archiving.

**How to Debug:**
```bash
# Check if rate plan is default (Supabase SQL Editor)
SELECT id, name, is_default, archived_at
FROM rate_plans
WHERE id = '<rate_plan_id>';
# If is_default=true and archived_at IS NULL, DELETE will return 409

# List other rate plans for same property/agency
SELECT id, name, is_default, active
FROM rate_plans
WHERE property_id = '<property_id>' AND archived_at IS NULL;
# OR for agency-wide:
SELECT id, name, is_default, active
FROM rate_plans
WHERE property_id IS NULL AND agency_id = '<agency_id>' AND archived_at IS NULL;
```

**Solution:**
- Set another rate plan as default first via PATCH with is_default=true
- Then archive the previous default plan via DELETE (will return 204)
- Or use Admin UI "Make Default" button on another plan before archiving

### Default Rate Plan Not Auto-Unset

**Symptom:** Creating or updating a rate plan with is_default=true does not unset other default plans.

**Root Cause:** Backend logic should auto-unset other defaults when is_default=true, but this may fail if transaction rollback occurs or logic is not applied.

**How to Debug:**
```bash
# Check how many defaults exist for property/agency (Supabase SQL Editor)
SELECT id, name, is_default, property_id
FROM rate_plans
WHERE property_id = '<property_id>' AND is_default = true AND archived_at IS NULL;
# Should return at most 1 row per property (partial unique index enforces this)

# Test POST with is_default=true
curl -X POST "$API_BASE_URL/api/v1/pricing/rate-plans" \
  -H "Authorization: Bearer $JWT_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "property_id": "<property_id>",
    "name": "New Default Plan",
    "currency": "EUR",
    "base_nightly_cents": 10000,
    "is_default": true
  }' | jq '.is_default'
# Should return true

# Verify old default was unset
curl -X GET "$API_BASE_URL/api/v1/pricing/rate-plans/<old_default_id>" \
  -H "Authorization: Bearer $JWT_TOKEN" | jq '.is_default'
# Should return false
```

**Solution:**
- Backend uses UPDATE query to set is_default=false for all other rate plans with same property_id/agency_id before setting new default
- Partial unique index enforces at most one default: `CREATE UNIQUE INDEX idx_rate_plans_default_per_property ON rate_plans (property_id, agency_id) WHERE is_default = true AND archived_at IS NULL`
- If multiple defaults exist: Migration 20260115000000 may not be applied, or index was dropped/disabled

### Archived Rate Plans Still Show in List

**Symptom:** GET /api/v1/pricing/rate-plans returns archived rate plans (archived_at NOT NULL).

**Root Cause:** List query should filter by archived_at IS NULL but may be missing this condition.

**How to Debug:**
```bash
# Check archived_at column exists (Supabase SQL Editor)
SELECT archived_at FROM rate_plans LIMIT 1;
# Should not error (column exists since migration 20260115000000)

# Test list endpoint
curl -X GET "$API_BASE_URL/api/v1/pricing/rate-plans?limit=100" \
  -H "Authorization: Bearer $JWT_TOKEN" | jq '.items[] | {id, name, archived_at}'
# archived_at should be null for all returned items

# Check if archived plans exist
SELECT id, name, archived_at FROM rate_plans WHERE archived_at IS NOT NULL;
```

**Solution:**
- Backend list query must include WHERE archived_at IS NULL filter (soft delete pattern)
- Verify pricing.py line ~97: `WHERE archived_at IS NULL` in SELECT query
- If archived plans appear: Check migration 20260115000000 applied, backend logic includes filter

### Admin UI Rate Plans Page Empty After Refresh

**Symptom:** Navigating to /pricing/rate-plans shows "Keine Tarifpläne vorhanden" (empty state) after browser refresh or direct URL access, but works after navigating from another page.

**Root Cause:** Auth context (Authorization header, x-agency-id) not initialized when page loads before Supabase session hydration completes. Page fetched data before token was available, resulting in failed request interpreted as empty list.

**How to Debug:**
```bash
# Check browser DevTools Network tab
# Look for GET /api/v1/pricing/rate-plans request
# Expected: 200 OK with rate plans array
# If 401/403/422: Auth headers missing or agency context not set
# If request missing entirely: Page didn't wait for auth to load

# Check browser Console for errors
# Look for: "No access token" or auth-related warnings
```

**Solution:**
- Frontend page must wait for authLoading state before fetching data
- Check frontend/app/pricing/rate-plans/page.tsx uses authLoading from useAuth() hook
- useEffect dependency must include authLoading: useEffect(() => {...}, [authLoading])
- Show loading state while authLoading is true (not empty state)
- Verify AuthContext initializes session before components mount
- Pattern: if (authLoading || loading) return <LoadingSpinner /> (lines 200+)

### Quote Totals Calculation Mismatch

**Symptom:** Quote endpoint returns totals where total_cents ≠ subtotal_nightly_cents + fees_total_cents + taxes_total_cents, or rounding appears incorrect.

**Root Cause:** Totals service not using ROUND_HALF_UP consistently, or fees/taxes configuration has invalid data causing calculation errors.

**How to Debug**:
```bash
# Test quote endpoint
curl -X POST "$API_BASE_URL/api/v1/pricing/quote" \
  -H "Authorization: Bearer $JWT_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "property_id": "<property_id>",
    "check_in": "2026-07-15",
    "check_out": "2026-07-17",
    "adults": 2,
    "children": 0
  }' | python3 -c "import sys, json; d=json.load(sys.stdin); print(f'subtotal={d.get(\"subtotal_nightly_cents\")} fees={d.get(\"fees_total_cents\")} taxes={d.get(\"taxes_total_cents\")} total={d.get(\"total_cents\")}')"

# Expected: total == subtotal + fees + taxes

# Check fees configuration (Supabase SQL Editor)
SELECT id, name, type, value_cents, value_percent, taxable
FROM pricing_fees
WHERE agency_id = '<agency_id>' AND active = true;

# Check taxes configuration
SELECT id, name, percent
FROM pricing_taxes
WHERE agency_id = '<agency_id>' AND active = true;
```

**Solution**:
- Verify backend/app/services/pricing_totals.py uses Decimal with ROUND_HALF_UP for all percent calculations
- Check fees/taxes configuration: percent fees must have value_percent set, fixed fees must have value_cents set
- Validate taxable flag: only taxable fees should increase tax base
- If mismatch persists: Check deployment of pricing_totals.py service, verify compute_totals() function exists
- Rounding rule: 12.5 → 13 (HALF_UP), 10.4 → 10 (down), 10.5 → 11 (HALF_UP)

### Quote Totals Fields Missing

**Symptom:** Quote endpoint response does not include subtotal_nightly_cents, fees_total_cents, taxes_total_cents, or total_cents fields.

**Root Cause:** Totals service not deployed, or quote endpoint not calling compute_totals() function.

**How to Debug**:
```bash
# Check API version
curl -sS "$API_BASE_URL/api/v1/ops/version" | jq '.source_commit, .started_at'

# Test quote endpoint raw response
curl -X POST "$API_BASE_URL/api/v1/pricing/quote" \
  -H "Authorization: Bearer $JWT_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "property_id": "<property_id>",
    "check_in": "2026-07-15",
    "check_out": "2026-07-17",
    "adults": 2
  }' | jq 'keys'

# Expected: Should include "subtotal_nightly_cents", "fees_total_cents", "taxes_total_cents", "total_cents"
```

**Solution**:
- Verify deployment includes backend/app/services/pricing_totals.py module
- Check backend/app/api/routes/pricing.py imports compute_totals and calls it in quote endpoint
- Verify quote response schema includes new totals fields (backward compatible, optional fields)
- Redeploy if pricing_totals service missing

---

## P2 Pricing – Full PROD Verification

**Overview:** Comprehensive P2 pricing verification using the wrapper script that runs all P2 smoke tests in order.

**Purpose:** Verify complete P2 pricing functionality (rate plans, seasons, fees, taxes, quotes, management UI) in production with a single command.

**Verification Script:** `backend/scripts/pms_p2_full_smoke.sh`

**Prerequisites:**
- Manager/admin JWT token with valid permissions
- Property ID for testing (existing property in PROD)
- Access to HOST-SERVER-TERMINAL or local workstation with network access to API

**Verification Checklist:**

WHERE: HOST-SERVER-TERMINAL
```bash
# 1. Pull latest code
cd /data/repos/pms-webapp
git fetch origin main && git reset --hard origin/main

# 2. Set environment variables
export HOST="https://api.fewo.kolibri-visions.de"
export JWT_TOKEN="<manager/admin JWT>"
export PROPERTY_ID="23dd8fda-59ae-4b2f-8489-7a90f5d46c66"

# 3. Run full P2 verification
./backend/scripts/pms_p2_full_smoke.sh
echo "rc=$?"

# Expected: All 4 tests pass, rc=0
```

**Tests Executed (in order):**
1. **Quote Calculation** (`pms_pricing_quote_smoke.sh`) - Fees/taxes breakdown
2. **Seasonal Rates** (`pms_pricing_seasons_smoke.sh`) - Rate overrides by date range
3. **Rate Plan CRUD** (`pms_pricing_rate_plans_smoke.sh`) - Create/update/delete operations
4. **Management UI** (`pms_pricing_management_ui_smoke.sh`) - Admin API flows

**Expected Output:**
- Each test displays clear banner with test name
- Progress messages for each step within tests
- ✅ PASSED or ❌ FAILED status after each test
- Final summary with counts (Tests Run/Passed/Failed)
- Exit code 0 if all passed, non-zero if any failed

**Common Issues:**

**JWT_TOKEN: unbound variable**
- **Cause:** JWT_TOKEN environment variable not set
- **Solution:** Export JWT_TOKEN before running script
- **Verification:** `echo ${JWT_TOKEN:0:20}` (should show first 20 chars)

**PROPERTY_ID: unbound variable**
- **Cause:** PROPERTY_ID environment variable not set
- **Solution:** Export PROPERTY_ID with valid UUID
- **Verification:** `curl -sS "$HOST/api/v1/properties/$PROPERTY_ID" -H "Authorization: Bearer $JWT_TOKEN"` (should return 200)

**Individual test failures**
- **Cause:** Specific P2 component issue (rate plans, fees, taxes, seasons)
- **Solution:** Check individual test output above failure for error details
- **Debugging:** Re-run failed test individually with verbose output

**HOST vs API_BASE_URL mismatch**
- **Cause:** Script expects HOST but some underlying scripts may use API_BASE_URL
- **Solution:** Wrapper sets both automatically; ensure HOST is correct
- **Verification:** `echo $HOST` and `echo $API_BASE_URL` (should match after script runs)

**Related Sections:**
- [P2 Pricing Seasonality](#p2-pricing-seasonality) - Season-specific verification
- [P2 Pricing Management UI](#p2-pricing-management-ui) - UI-specific verification
- [Scripts README: P2 Full Verification](../scripts/README.md) - Detailed script documentation

---

## P2 Pricing Seasonality

**Overview:** Seasonal pricing overrides within rate plans for date-range specific pricing.

**Purpose:** Enable property managers to configure seasonal pricing (e.g., high season rates) that automatically apply when booking dates fall within the season's date window.

**Implementation Facts (from pricing.py):**
- **Season creation**: Seasons can ONLY be created via `POST /api/v1/pricing/rate-plans` with `seasons` array
- **No season updates**: `PATCH /api/v1/pricing/rate-plans/{id}` has NO seasons field (per RatePlanUpdate schema)
- **No dedicated endpoints**: No separate POST/PATCH/DELETE endpoints for individual seasons
- **Modification strategy**: To change seasons, create a new rate plan or DELETE and recreate
- **Cascade delete**: DELETE rate plan automatically deletes all associated seasons (ON DELETE CASCADE)

**Season Fields:**
- `rate_plan_id`: Parent rate plan (FK to rate_plans.id)
- `date_from`: Season start date (inclusive, YYYY-MM-DD)
- `date_to`: Season end date (exclusive, YYYY-MM-DD)
- `nightly_cents`: Override rate in cents (replaces rate_plan.base_nightly_cents)
- `min_stay_nights`: Optional minimum stay override (NULL uses rate_plan.min_stay_nights)
- `active`: Boolean flag to enable/disable season

**Quote Calculation Logic (pricing.py:954-979):**

Current behavior:
```sql
SELECT nightly_cents
FROM rate_plan_seasons
WHERE rate_plan_id = $1
  AND $2 >= date_from
  AND $2 < date_to
  AND active = true
ORDER BY date_from DESC
LIMIT 1
```

- **Date matching**: `check_in >= date_from AND check_in < date_to` (date_to is EXCLUSIVE)
- **Active filter**: Only `active=true` seasons considered
- **Overlap resolution**: `ORDER BY date_from DESC LIMIT 1` (latest/highest date_from wins)
- **Rate application**: Quote uses `season.nightly_cents` for entire stay: `subtotal = nightly_cents * nights`

**Example:**
- Season A: 2026-07-01 to 2026-08-01, nightly 15000 cents
- Season B: 2026-07-15 to 2026-08-01, nightly 20000 cents (overlaps A)
- Quote check_in=2026-07-20: Uses Season B (20000 cents) due to ORDER BY date_from DESC

**Verification Commands:**

```bash
# Run seasonality smoke test
HOST=https://api.fewo.kolibri-visions.de \
JWT_TOKEN="<<<manager/admin JWT>>>" \
./backend/scripts/pms_pricing_seasons_smoke.sh
echo "rc=$?"
```

Expected: rc=0 (all 5 tests pass)

**Common Issues:**

### Quote Doesn't Use Seasonal Rate

**Symptom:** Quote returns base rate even though check_in falls within season window.

**Possible Causes:**
1. Season inactive (active=false in database)
2. Date window wrong: check_in must be >= date_from AND < date_to
3. Wrong rate_plan_id in quote request
4. Season not created (POST may have failed silently)

**How to Debug:**
```bash
# List rate plans with seasons
curl -X GET "$HOST/api/v1/pricing/rate-plans?limit=10" \
  -H "Authorization: Bearer $JWT_TOKEN" | jq '.items[] | {id, name, seasons}'

# Test quote with explicit rate_plan_id
curl -X POST "$HOST/api/v1/pricing/quote" \
  -H "Authorization: Bearer $JWT_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "property_id": "<property_id>",
    "check_in": "2026-07-20",
    "check_out": "2026-07-22",
    "adults": 2,
    "rate_plan_id": "<rate_plan_id>"
  }' | jq '.nightly_cents'
```

**Solution:**
1. Verify season exists in rate_plan.seasons array (GET /api/v1/pricing/rate-plans)
2. Check season.active = true
3. Verify date_from < date_to and check_in within range
4. If season missing: Create new rate plan with corrected seasons array

### Can't Update/Toggle Season Active Flag

**Symptom:** Need to deactivate season without deleting rate plan.

**Root Cause:** No season update endpoints. PATCH /api/v1/pricing/rate-plans/{id} has no seasons field.

**Workaround Options:**
1. **Direct DB update** (admin only, not recommended for production):
   ```sql
   UPDATE rate_plan_seasons SET active = false WHERE id = '<season_id>';
   ```
2. **DELETE and recreate rate plan** with updated seasons array
3. **Feature request**: Add dedicated season management endpoints

**NOTE:** This is a current limitation of the API design.

### Overlapping Seasons Unclear Priority

**Symptom:** Multiple active seasons overlap, unclear which rate applies.

**Current Behavior:** `ORDER BY date_from DESC LIMIT 1` (latest date_from wins)

**Example:**
- Season A: date_from=2026-07-01, nightly=15000
- Season B: date_from=2026-07-15, nightly=20000
- Quote check_in=2026-07-20: Season B wins (higher date_from)

**Best Practice:** Avoid overlapping seasons unless intentional tiered pricing.

### Smoke Script Stops at Test 1 (Pre-cleanup)

**Symptom:** `pms_pricing_seasons_smoke.sh` exits at Test 1 with rc=1 before reaching Test 2.

**Possible Causes:**
1. JWT token expired (HTTP 401)
2. User not manager/admin for agency (HTTP 403)
3. Missing or invalid x-agency-id header (HTTP 422/400)

**How to Debug:**
- Check error output for HTTP code and hints
- If HTTP 401: Refresh JWT_TOKEN (token expired)
- If HTTP 403: Verify user has manager/admin role for the agency_id
- If HTTP 422/400: Check x-agency-id header matches user's agency membership

**Solution:**
- Obtain fresh JWT token from Supabase auth
- Verify user role in team_members table: `SELECT role FROM team_members WHERE user_id = '<user_id>' AND agency_id = '<agency_id>'`
- Ensure AGENCY_ID env var matches user's agency (or omit for auto-detection)

**Note:** Pre-cleanup is a no-op when no prior smoke rate plans exist (deleted 0 is expected on first run).

---

## P2 Pricing Management UI

**Overview:** Admin UI for managing fees and taxes with create/list/toggle capabilities.

**Purpose:** Provide property managers and admins a web interface to configure pricing fees (cleaning, service, etc.) and taxes without backend access.

**UI Routes:**
- `/pricing` - Main pricing management page with fees/taxes tabs

**API Endpoints Used:**
- `GET /api/v1/properties` - List properties (for selector)
- `GET /api/v1/pricing/fees?property_id=&active=&limit=&offset=` - List fees
- `POST /api/v1/pricing/fees` - Create fee
- `PATCH /api/v1/pricing/fees/{id}` - Update fee (toggle active, edit values)
- `GET /api/v1/pricing/taxes?property_id=&active=&limit=&offset=` - List taxes
- `POST /api/v1/pricing/taxes` - Create tax
- `PATCH /api/v1/pricing/taxes/{id}` - Update tax (toggle active, edit values)
- `POST /api/v1/pricing/quote` - Calculate quote with fees/taxes (optional test)

**Database Tables:**
- `pricing_fees` - Property-specific or agency-wide fees (per_stay, per_night, per_person, percent types)
- `pricing_taxes` - Property-specific or agency-wide taxes (percent type)
- Migration: `20260104200000_add_pricing_fees_and_taxes.sql`

**Features:**
1. **Property Selector**: Auto-picks first property, allows manual selection
2. **Tabs**: Separate views for Fees and Taxes
3. **List View**: Paginated list with name, type, value, taxable, scope, active status
4. **Create Forms**:
   - Fee: name, type (per_stay/per_night/per_person/percent), value (cents or percent), taxable checkbox, scope (property/agency)
   - Tax: name, percent, scope (property/agency)
5. **Toggle Active**: Click status badge to toggle active=true/false using PATCH
6. **Validation**: Client-side validation for required fields, type-specific value fields
7. **Toast Notifications**: Success/error messages for CRUD operations

**Verification Commands:**

```bash
# [HOST-SERVER-TERMINAL] Pull latest code
cd /data/repos/pms-webapp
git fetch origin main && git reset --hard origin/main

# [HOST-SERVER-TERMINAL] Optional: Verify deploy after Coolify redeploy
export API_BASE_URL="https://api.fewo.kolibri-visions.de"
./backend/scripts/pms_verify_deploy.sh

# [HOST-SERVER-TERMINAL] Run pricing management UI smoke test
export HOST="https://api.fewo.kolibri-visions.de"
export JWT_TOKEN="<<<manager/admin JWT>>>"
# Optional:
# export PROPERTY_ID="23dd8fda-59ae-4b2f-8489-7a90f5d46c66"
# export AGENCY_ID="ffd0123a-10b6-40cd-8ad5-66eee9757ab7"
./backend/scripts/pms_pricing_management_ui_smoke.sh
echo "rc=$?"

# Expected output: All 8 tests pass, rc=0
```

**Common Issues:**

### PATCH Endpoints Return 404

**Symptom:** UI shows error when toggling active status: "Failed to update fee/tax".

**Cause:** PATCH endpoints not deployed yet (added in this release).

**How to Debug:**
```bash
# Check if PATCH endpoints exist in OpenAPI schema
curl -sS "$HOST/openapi.json" | jq '.paths["/api/v1/pricing/fees/{fee_id}"]'
curl -sS "$HOST/openapi.json" | jq '.paths["/api/v1/pricing/taxes/{tax_id}"]'

# Should return method "patch" with parameters
```

**Solution:** Ensure latest backend deployed with commit containing PATCH endpoints.

### PATCH Toggle Returns 500 Database Error

**Symptom:** Smoke test Test 5/6 fails with 500 Database error when calling PATCH `/api/v1/pricing/fees/{id}` or `/api/v1/pricing/taxes/{id}` to toggle active status. Response: `{"detail":{"error":"internal_server_error","message":"Database error occurred","path":null}}`

**Root Cause:** Bug in dynamic UPDATE query builder in `backend/app/api/routes/pricing.py`. The code incorrectly incremented `param_count` before adding `updated_at = NOW()`, causing parameter count mismatch. PostgreSQL expects N parameters but only N-1 were provided.

**Example Flow (Buggy Code):**
```python
# When only updating active field:
param_count = 1  # for active
update_fields = ["active = $1"]
params = [False]

# Bug: Increment param_count for NOW() (which needs no param!)
param_count = 2  # WRONG
update_fields.append("updated_at = NOW()")

# Add fee_id as final param
param_count = 3  # WRONG - should be 2
params.append(fee_id)

# Query: WHERE id = $3
# Params: [False, fee_id]  # Only 2 params!
# PostgreSQL error: $3 doesn't exist
```

**How to Debug:**
```bash
# Check backend logs for asyncpg parameter errors
docker logs pms-backend --tail 100 | grep -E "pricing|Parameter|asyncpg"

# Verify pricing.py has fix (param_count should NOT increment before NOW())
grep -A 2 "updated_at = NOW()" backend/app/api/routes/pricing.py
# Expected: No param_count += 1 on line before NOW()
```

**Solution:** Fixed in commit that removed incorrect `param_count += 1` on lines 464 and 673 (for fees and taxes respectively). The `NOW()` function doesn't use a parameter, so param_count should not be incremented. Ensure backend code is updated and redeployed.

**Verification:**
```bash
# Run smoke test to verify fix
HOST="$PROD_API" JWT_TOKEN="$TOKEN" ./backend/scripts/pms_pricing_management_ui_smoke.sh
# Expected: Test 5 PASSED, Test 6 PASSED
```

### UI Shows "Failed to load fees/taxes"

**Symptom:** UI displays error toast, browser console shows 401/403/500.

**Possible Causes:**
1. JWT token expired (401)
2. User lacks manager/admin role (403)
3. Migrations not applied - pricing_fees/pricing_taxes tables missing (500)

**How to Debug:**
```bash
# Check JWT expiration
echo $JWT_TOKEN | cut -d'.' -f2 | base64 -d | jq '.exp'
date -r $(echo $JWT_TOKEN | cut -d'.' -f2 | base64 -d | jq -r '.exp')

# Check JWT role
echo $JWT_TOKEN | cut -d'.' -f2 | base64 -d | jq '.role'
# Should be "manager" or "admin"

# Check tables exist
psql $DATABASE_URL -c "\dt pricing_*"
# Should show: pricing_fees, pricing_taxes

# Check backend logs
docker logs pms-backend --tail 50 | grep -E "pricing|fees|taxes"
```

**Solution:**
- Refresh JWT token if expired
- Verify user has manager/admin role
- Apply migration 20260104200000 if tables missing

### Create Fee/Tax Returns Validation Error

**Symptom:** UI shows "Failed to create fee: value_percent is required for percent type fees" or similar.

**Cause:** Client-side validation mismatch with backend requirements.

**Validation Rules:**
- **Fee (percent type)**: Must have `value_percent` (0-100), `value_cents` must be null
- **Fee (other types)**: Must have `value_cents` (≥0), `value_percent` must be null
- **Tax**: Must have `percent` (0-100)
- **All**: `name` required (1-255 chars), `active` defaults to true

**Solution:** Check form inputs match type-specific requirements, ensure UI sends correct payload shape.

### Active Toggle Doesn't Filter List

**Symptom:** After toggling fee/tax to inactive, it still appears in active=true filtered list.

**Cause:** Frontend may not be re-fetching list after PATCH, or active filter query param not working.

**How to Debug:**
```bash
# Verify PATCH worked
curl -X GET "$HOST/api/v1/pricing/fees?property_id=<uuid>" \
  -H "Authorization: Bearer $JWT_TOKEN" | jq '.[] | {id, name, active}'

# Verify active filter works
curl -X GET "$HOST/api/v1/pricing/fees?property_id=<uuid>&active=true" \
  -H "Authorization: Bearer $JWT_TOKEN" | jq 'length'

curl -X GET "$HOST/api/v1/pricing/fees?property_id=<uuid>&active=false" \
  -H "Authorization: Bearer $JWT_TOKEN" | jq 'length'
```

**Solution:** Ensure UI re-fetches list after PATCH, verify backend active filter logic.

### No Properties Found

**Symptom:** UI shows "No properties found. Create one to get started."

**Cause:** Agency has no properties, or JWT agency_id doesn't match any properties.

**Solution:**
- Create a property first via `/properties` UI or API
- Verify JWT agency_id matches property agency_id
- Check backend logs for tenant scoping issues

**Related Documentation:**
- [Scripts README: P2 Pricing Management UI](../../scripts/README.md#p2-pricing-management-ui-smoke-test-pms_pricing_management_ui_smokesh) - Smoke script usage
- [Project Status: P2 Pricing Management UI](../project_status.md) - Implementation status

---

## Season Templates (Agency-Wide Reusable Periods)

**Overview:** Agency-level reusable season date windows for applying to multiple rate plans.

**Purpose:** Define common season periods once (e.g., Hauptsaison, Mittelsaison, Nebensaison) and apply them to property-scoped rate plans. Each rate plan can then customize nightly_cents per season.

**Architecture:**
- Season templates are agency-scoped (not property-specific)
- Templates contain periods (label, date_from, date_to, sort_order)
- Apply action copies template periods into rate plan seasons
- Copied seasons inherit rate_plan.base_nightly_cents initially
- Quote resolution unchanged: uses ONLY property-scoped rate plan seasons

**API Endpoints:**
- GET /api/v1/pricing/season-templates (list agency templates)
- POST /api/v1/pricing/season-templates (create template with periods)
- PATCH /api/v1/pricing/season-templates/{id} (update template)
- DELETE /api/v1/pricing/season-templates/{id} (archive template)
- POST /api/v1/pricing/season-templates/{id}/periods (add period)
- PATCH /api/v1/pricing/season-templates/{id}/periods/{period_id} (edit period)
- DELETE /api/v1/pricing/season-templates/{id}/periods/{period_id} (remove period)
- POST /api/v1/pricing/rate-plans/{id}/apply-season-template (apply template to rate plan)
- GET /api/v1/pricing/rate-plans/{id} (fetch single rate plan with seasons)

**Admin UI:**
- Route: `/pricing/seasons` (Saisons (Agentur))
- Create templates with periods (label, date_from, date_to)
- Edit templates and periods (add/remove/update dates and labels)
- Archive templates
- Period count display in template list
- Note: Templates are agency-wide, NOT property-specific
- Application to rate plans: `/pricing/rate-plans` (per-property pricing)

**UI Operations (Bugfix 2026-01-16):**
- Update, Delete, and Rename operations now handle 204 No Content responses correctly
- Some endpoints (DELETE) return empty body; UI parses responses safely
- To verify operations: check browser console for no JSON parse errors, verify state refreshes

### Template Update - Adding Periods to Empty Template

**Context:** Templates can be created with zero periods and later edited to add periods.

**Behavior:** Update endpoint uses replace-all strategy:
- All existing periods are deleted
- New periods from request are inserted with fresh IDs
- Previous period IDs are not preserved across updates


### Saisonvorlagen (Jahreslogik)

**Konzept:** Saisonvorlagen mit Jahreszahl im Namen (z.B. "2026 - Hauptsaison") sind jahresgebunden.

**Validierung:**
- Vorlagenname beginnt mit 4-stelligem Jahr (2000-2099): `^(20\d{2})(\b|\s|[-_])`
- Alle Perioden müssen innerhalb dieses Jahres liegen
- Bei Verstoß: HTTP 422 "Diese Saisonvorlage ist für {year}. Zeitraum muss innerhalb {year} liegen."

**Beispiele:**
- ✅ "2026 - Hauptsaison" → Perioden nur in 2026 erlaubt
- ✅ "2026-Preise" → Perioden nur in 2026 erlaubt
- ✅ "Standard" → Keine Einschränkung (kein Jahr erkannt)
- ❌ "2026 - Hauptsaison" mit Periode bis 2027-03-31 → Validierungsfehler

**Vorlage geändert - Objekt zeigt alte Zeiträume:**

**Symptom:** Saisonvorlage wurde korrigiert (z.B. Periode verkürzt auf 2026), aber Objekt-Saisonzeiten reichen noch bis 2027.

**Ursache:** Saisonzeiten wurden früher importiert, bevor Vorlage korrigiert wurde.

**Lösung:** "Vorlage synchronisieren" im Objekt-Preiseinstellungen
- Repariert automatisch: Legacy-Saisonzeiten werden gekürzt/angepasst
- Verknüpft: Linkage zur Vorlage wird hergestellt (source_template_id, source_template_period_id)
- Sicher: Kein Duplikat, keine 422-Fehler

**Workflow:**
1. Objekt → Preiseinstellungen → "Aus Saisonvorlage importieren"
2. Vorlage auswählen
3. Klick "Vorlage synchronisieren" (nicht "Nur fehlende importieren")
4. Vorschau prüfen:
   - "Reparierte Saisonzeiten" zeigt gekürzte/angepasste Zeiten
   - "Konflikte" zeigt Überlappungen (werden übersprungen)
5. Bestätigen → Reparatur erfolgt

**Konflikte:**
- **Was:** UPDATE würde Überlappung mit ANDERER Saison erzeugen
- **Folge:** Betroffene Saison wird übersprungen (soft fail, kein 422)
- **Lösung:** Überlappende Saison manuell anpassen oder archivieren, dann erneut synchronisieren

**Bugfix (2026-01-16):** Prior version attempted to update periods by ID, causing "Period not found" errors when adding new periods. Replace-all strategy eliminates this issue.

**Apply Modes:**
- **replace**: Deletes existing rate plan seasons, inserts new ones from template
- **merge**: Adds missing periods, skips overlaps (basic implementation)

**Common Issues:**

### Apply Endpoint Returns 404 (Template Not Found)

**Symptom:** POST /api/v1/pricing/rate-plans/{id}/apply-season-template returns 404.

**Root Cause:** Template ID doesn't exist or belongs to different agency.

**Solution:**
- Verify template_id exists: GET /api/v1/pricing/season-templates
- Check agency_id matches (templates are agency-scoped)

### Seasons Not Appearing in Quote

**Symptom:** Applied seasons to rate plan but quote doesn't use seasonal rates.

**Root Cause:** Quote resolution uses check_in date to find matching season. Check date overlap.

**Solution:**
- Verify seasons cover desired date ranges
- Check season active=true
- Use GET /api/v1/pricing/rate-plans/{id} to inspect seasons

### 405 Method Not Allowed on Rate Plan Fetch

**Symptom:** GET request to rate plan returns 405.

**Root Cause:** Prior to 2026-01-16, no GET detail endpoint existed for single rate plan.

**Solution:** Use GET /api/v1/pricing/rate-plans/{id} endpoint added in commit after 12aa7d2.

---
## OPS endpoints: Auth & Zugriff

### Current Behavior (as deployed)

**Stand:** Nach Commit `ae589e4`:

- `/api/v1/ops/version` — **PUBLIC**
- `/api/v1/ops/modules` — **PUBLIC** (aktueller Stand)
- `/api/v1/ops/audit-log` — **AUTH REQUIRED** (JWT + ggf. Role/DB)

**PROD evidence (2026-01-07):**
source_commit=ae589e4266dd62085968eab0f76419865a7c423e
started_at=2026-01-07T14:55:04.858297+00:00

### Verification Commands (Current Behavior)

```bash
# HOST-SERVER-TERMINAL
export API_BASE_URL="https://api.fewo.kolibri-visions.de"

curl -k -sS -i "$API_BASE_URL/api/v1/ops/version" | sed -n '1,25p'
# Expected: HTTP 200

curl -k -sS -i "$API_BASE_URL/api/v1/ops/modules" | sed -n '1,25p'
# Expected: HTTP 200

curl -k -sS -i -H "Authorization: Bearer " "$API_BASE_URL/api/v1/ops/modules" | sed -n '1,25p'
# Expected: HTTP 200
```

---

### Hardening (Optional / Future)

Falls `/api/v1/ops/modules` künftig geschützt werden soll (JWT-Signaturprüfung, DB-frei):

- Ohne Authorization Header → HTTP 401/403
- Mit leerem Bearer (`Authorization: Bearer `) → HTTP 401/403
- Mit gültigem JWT → HTTP 200

```bash
# HOST-SERVER-TERMINAL
export API_BASE_URL="https://api.fewo.kolibri-visions.de"
export TOKEN="eyJhbGc..."  # valid JWT token

curl -k -sS -i "$API_BASE_URL/api/v1/ops/modules" | sed -n '1,25p'
# Expected after hardening: HTTP 401 or 403

curl -k -sS -i -H "Authorization: Bearer $TOKEN" "$API_BASE_URL/api/v1/ops/modules" | sed -n '1,25p'
# Expected after hardening: HTTP 200
```

**Rationale (warum /ops/modules ggf. schützen):**
- Sensible Ops-Metadaten (Routes, Module Registry)
- Reduziert API-Exposure für Unbefugte
- JWT-Check bleibt DB-frei (funktioniert auch wenn DB down)

**Rationale (warum /ops/version PUBLIC bleibt):**
- Deploy-Verify & Monitoring ohne Auth möglich
- Keine sensiblen Inhalte, nur Meta-Infos

---

**Problem**: Create rate plan fails with "Missing agency_id in token claims"

**Symptoms**:
- POST /api/v1/pricing/rate-plans returns 403 with `{"detail":"Missing agency_id in token claims"}`
- JWT is valid (authenticated successfully, len=616, parts=3)
- Endpoints are mounted correctly (pricing routes exist in OpenAPI)
- Issue occurs when JWT doesn't have agency_id claim

**Root Cause**:
- Pricing endpoints require agency context for multi-tenancy
- JWT token doesn't include agency_id claim (depends on auth provider/Supabase setup)
- Backend needs agency_id to scope rate plans to correct tenant

**Solution**:

Strategy 1: Provide agency_id via x-agency-id header (recommended for multi-agency users)
```bash
# Create rate plan with x-agency-id header
curl -X POST "$HOST/api/v1/pricing/rate-plans" \
  -H "Authorization: Bearer $JWT_TOKEN" \
  -H "x-agency-id: $AGENCY_ID" \
  -H "Content-Type: application/json" \
  -d '{"property_id": "...", "name": "...", ...}'

# Smoke script with AGENCY_ID
HOST=$HOST JWT_TOKEN=$JWT_TOKEN AGENCY_ID=$AGENCY_ID ./backend/scripts/pms_pricing_quote_smoke.sh
```

Strategy 2: Use single-agency fallback (if user belongs to exactly 1 agency)
- Backend automatically resolves to user's first/only agency membership
- Queries `team_members` table for user's active memberships
- Updates `profiles.last_active_agency_id` for future requests
- No header needed if user has only 1 agency

Strategy 3: Verify JWT includes agency_id claim (auth provider configuration)
```bash
# Decode JWT to check for agency_id claim
echo "$JWT_TOKEN" | cut -d. -f2 | base64 -d 2>/dev/null | jq '.'
# Should include: {"sub": "...", "agency_id": "...", "role": "..."}
```

**How Tenant Resolution Works** (backend/app/api/deps.py:get_current_agency_id):
1. Check x-agency-id header (if present, validate user membership)
2. Check JWT claim agency_id (if present)
3. Query user's last_active_agency_id from profiles table
4. Fallback to first agency membership from team_members table
5. If no memberships: return 404 with actionable error

**Prevention**:
- For multi-agency environments: Always send x-agency-id header in requests
- Update smoke scripts to support AGENCY_ID env var
- Document x-agency-id requirement for multi-tenant API clients

**Verification**:
```bash
# Verify user has agency membership
psql $DATABASE_URL -c "SELECT agency_id, role FROM team_members WHERE user_id = '$USER_ID' AND is_active = true;"

# Test with x-agency-id header
curl "$HOST/api/v1/pricing/rate-plans" \
  -H "Authorization: Bearer $JWT_TOKEN" \
  -H "x-agency-id: $AGENCY_ID"
```

---

**Problem**: PROD restart loop - ImportError: cannot import name 'get_current_agency_id' from 'app.core.auth'

**Symptoms**:
- Backend crashes at startup with ImportError
- 502 Bad Gateway (no healthy backend pods)
- Logs show: `ImportError: cannot import name 'get_current_agency_id' from 'app.core.auth'`
- Affects imports in pricing.py: `get_current_agency_id`, `get_current_role`, `require_roles`
- MODULES_ENABLED=false doesn't help (imports happen in main.py fallback)

**Root Cause**:
- `backend/app/api/routes/pricing.py` imports auth dependencies that don't exist in auth.py
- Missing symbols: `get_current_agency_id`, `get_current_role`, `require_roles`
- auth.py has `require_role` (singular) but pricing.py imports `require_roles` (plural)
- Import happens at startup (module load time), causing immediate crash

**Solution**:
1. Add missing auth dependencies to `backend/app/core/auth.py`:
   - `get_current_agency_id`: Extract agency_id from JWT claims
   - `get_current_role`: Extract role from JWT claims
   - `require_roles`: Alias to existing `require_role` function
2. Verify `get_current_user` extracts `agency_id` and `role` from JWT payload (lines 147-148)
3. Pattern: Follow same structure as `get_current_user_id` (lines 216-240)
4. Deploy and verify: Backend boots successfully, pricing routes respond

**Prevention**:
- Always verify imported symbols exist before deploying
- Test imports in isolation: `python -c "from app.core.auth import get_current_agency_id"`
- Add import smoke tests to CI/CD pipeline

**Verification**:
```bash
# Check if backend boots (should return version info, not 502)
curl https://api.example.com/api/v1/ops/version

# Verify pricing routes mounted
curl https://api.example.com/api/v1/ops/modules | jq '.mounted_has_pricing'  # should be true
```

**Related**:
- auth.py:243-273 - get_current_agency_id implementation
- auth.py:276-306 - get_current_role implementation
- auth.py:498 - require_roles alias
- pricing.py:22 - Import statement

---


| Date | Change | Author |
|------|--------|--------|
| 2025-12-26 | Initial runbook creation (Phase 24) | System |
| 2025-12-27 | Document smoke script opt-in tests (AVAIL_BLOCK_TEST, B2B_TEST) | System |
| 2025-12-27 | Phase 30 — Inventory Final validation results (block conflict, B2B boundary, end-exclusive semantics) | System |
| 2025-12-27 | Phase 30.5 — Inventory Contract documented (single source of truth for inventory semantics, edge cases, DB guarantees) | System |
| 2025-12-27 | Phase 31 — Modular Monolith architecture documented (module system, registry, router aggregation) | System |
| 2025-12-27 | Phase 31 — MODULES_ENABLED kill-switch added (ops safety, emergency fallback to explicit router mounting) | System |
| 2025-12-27 | Phase 33B — Split api_v1 into domain modules (properties, bookings, inventory) | System |
| 2025-12-27 | Phase 34 — Updated kill-switch docs with Phase 33B context (explicit fallback router list) | System |
| 2025-12-27 | Phase 35 — Module mounting hardening (router dedupe guard, improved logging) | System |
| 2025-12-27 | Phase 36 — Channel Manager module integration with feature flag (CHANNEL_MANAGER_ENABLED, default OFF) | System |
| 2025-12-27 | Phase 36 — Redis + Celery worker setup runbook (password encoding, deployment steps, verification, troubleshooting) | System |
| 2025-12-30 | Admin UI authentication verification - Cookie-based SSR login, curl checks for /ops/* access | System |
| 2026-01-01 | Full Sync batching (batch_id) - Group 3 operations, API exposure, UI grouping, verification queries | System |
| 2026-01-06 | P1 Booking Request Workflow - Fixed to operate on bookings table using existing columns (confirmed_at, cancelled_at, cancelled_by, cancellation_reason, internal_notes). Status: cancelled instead of declined. | System |
| 2026-01-06 | P1 Status Mapping - Added API-to-DB status mapping layer: API under_review maps to DB inquiry for PROD compatibility. Prevents 500 errors from unsupported status values. | System |
| 2026-01-06 | P2 Pricing v1 Foundation - rate_plans and rate_plan_seasons tables for pricing engine. All fields nullable/optional for gradual adoption. Endpoints: GET/POST /api/v1/pricing/rate-plans, POST /api/v1/pricing/quote. | System |
| 2026-01-06 | P3a: Idempotency + Audit Log - Idempotency-Key for public booking requests (dedupe) + best-effort audit log + smoke script | System |

## P3a: Idempotency + Audit Log (Public Booking Requests)

### Overview

P3a adds idempotency support and audit logging to the public direct booking endpoint (`POST /api/v1/public/booking-requests`). These features prevent duplicate booking creation from repeated requests and provide an audit trail for public booking requests.

### Idempotency-Key Support

**How It Works**:
1. Client sends `Idempotency-Key` header with request
2. First request with key creates booking and caches response (24h TTL)
3. Replay (same key + same payload) returns cached response without DB insert
4. Conflict (same key + different payload) returns 409 with actionable error

**Behavior**:
- Same key + same request → returns cached 201 response with same booking_id
- Same key + different request → returns 409 idempotency_conflict
- No key provided → standard behavior (no idempotency check)

**Tables**:
- `idempotency_keys`: Stores key, request hash, cached response, expires after 24h
- Indexed on `(agency_id, endpoint, method, idempotency_key)` for fast lookup

---

### Audit Log

**How It Works**:
- Best-effort logging for critical actions (must not break main request)
- Captures: actor type, action, entity, IP, user-agent, metadata
- Failed audit writes are logged but do NOT fail the request

**Tables**:
- `audit_log`: Stores event records (agency_id, actor_type, action, entity_type, entity_id, request_id, idempotency_key, ip, user_agent, metadata JSONB)
- Indexed on agency_id, entity, action, and actor for query performance

**Sample Events**:
- `public_booking_request_created`: Public booking request created via direct booking endpoint

---

### Troubleshooting

**Problem**: Idempotency replay creates duplicate booking instead of returning cached response

**Symptoms**:
- Same Idempotency-Key + same payload creates two bookings with different IDs
- Expected 201 with cached booking_id, got 201 with new booking_id

**Root Cause**:
- Idempotency check not running (missing dependency or import error)
- Race condition: two concurrent requests with same key before first commits
- Idempotency record expired (TTL > 24h since first request)

**Solution**:
1. Verify idempotency_keys table exists and has unique constraint:
   ```sql
   \d idempotency_keys
   -- Should show UNIQUE constraint: idempotency_keys_unique (agency_id, endpoint, method, idempotency_key)
   ```
2. Check if idempotency record exists:
   ```sql
   SELECT * FROM idempotency_keys 
   WHERE endpoint = '/api/v1/public/booking-requests' 
   AND idempotency_key = '<key>' 
   AND expires_at > NOW();
   ```
3. If missing: First request may have failed before storing idempotency record
4. If expired: Record was created >24h ago (normal behavior, retry will create new booking)
5. If race condition: Verify transaction isolation and uniqueness constraint enforcement

**Verification**:
```bash
# Run P3a smoke test
export API_BASE_URL="https://api.example.com"
export PID="<property-uuid>"
./backend/scripts/pms_p3a_idempotency_smoke.sh
# Should pass all 3 tests: first request (201), replay (cached 201), conflict (409)
```

---

**Problem**: Idempotency conflict (409) when replaying same request

**Symptoms**:
- Same Idempotency-Key + same payload returns 409 idempotency_conflict
- Expected cached 201 response

**Root Cause**:
- Request payload differs slightly (whitespace, field order, floating point precision)
- Request hash computation differs between first request and replay

**Solution**:
1. Verify payload is byte-for-byte identical (JSON canonical form):
   ```python
   import json, hashlib
   payload = {...}  # Your request dict
   canonical = json.dumps(payload, sort_keys=True, separators=(',', ':'))
   print(hashlib.sha256(canonical.encode('utf-8')).hexdigest())
   ```
2. Check stored request_hash in idempotency_keys table
3. Common causes:
   - Floating point precision: `2.5` vs `2.50` (use integers for cents/currency)
   - Date format: `2037-06-01` vs `2037-6-1` (use ISO 8601 with zero-padding)
   - Field order: JSON objects have stable field order in canonical form
4. If hashes differ: Client must send exact same payload or use new Idempotency-Key

---

**Problem**: Audit events not appearing in audit_log table

**Symptoms**:
- Booking created successfully but no audit_log record
- Expected `public_booking_request_created` event missing

**Root Cause**:
- Audit logging is best-effort (failures logged but don't break request)
- audit_log table doesn't exist or schema out of date
- Audit event emission failed (exception caught and logged)

**Solution**:
1. Verify audit_log table exists:
   ```sql
   \d audit_log
   -- Should show columns: id, created_at, agency_id, actor_user_id, actor_type, action, entity_type, entity_id, request_id, idempotency_key, ip, user_agent, metadata
   ```
2. Check application logs for audit emission errors:
   ```bash
   docker logs pms-backend 2>&1 | grep "Failed to emit audit event"
   # Should show non-fatal error if audit write failed
   ```
3. If table missing: Run migrations to create audit_log table
4. If schema mismatch: Run latest migrations to update schema
5. Verify audit event was called (check application logs):
   ```bash
   docker logs pms-backend 2>&1 | grep "Audit event emitted"
   # Should show: action=public_booking_request_created, entity=booking/<uuid>
   ```

**Verification**:
```bash
# Check audit log for recent public booking events
psql $DATABASE_URL -c "
SELECT created_at, action, entity_type, entity_id, actor_type, ip, metadata->>'property_id' as property_id
FROM audit_log
WHERE action = 'public_booking_request_created'
ORDER BY created_at DESC
LIMIT 5;
"
```

---

**Problem**: Migration 20260106160000 fails with ERROR 42P17 (functions in index predicate must be marked IMMUTABLE)

**Symptoms**:
- Running migration `20260106160000_add_idempotency_keys.sql` in Supabase SQL Editor fails
- Error: `42P17 - functions in index predicate must be marked IMMUTABLE`
- Error points to indexes `idx_idempotency_keys_lookup` or `idx_idempotency_keys_expires_at`

**Root Cause**:
- Original migration created partial indexes with `WHERE expires_at > NOW()` predicates
- PostgreSQL rejects this because `NOW()` is VOLATILE (not IMMUTABLE)
- Index predicates require IMMUTABLE functions only

**Solution**:
1. Apply hotfix migration `20260106180000_fix_idempotency_keys_indexes.sql`:
   - Drops problematic partial indexes
   - Recreates them without the `WHERE` predicate
   - Indexes still work correctly, just without the partial filter optimization
2. Run in Supabase SQL Editor:
   ```sql
   -- Verify indexes exist after migration
   SELECT indexname, indexdef
   FROM pg_indexes
   WHERE tablename = 'idempotency_keys';
   ```
3. Expected output: Two indexes without `WHERE expires_at > NOW()` predicates

**Prevention**:
- Never use VOLATILE functions (`NOW()`, `CURRENT_TIMESTAMP`, etc.) in index predicates
- Use IMMUTABLE functions only, or omit the `WHERE` clause entirely

---

**Problem**: Smoke script Test 3 fails to detect idempotency_conflict despite 409 response

**Symptoms**:
- `pms_p3a_idempotency_smoke.sh` Test 3 returns 409 Conflict (correct)
- Script reports: "Got 409 but error type is not idempotency_conflict"
- Test 3 fails even though API behavior is correct

**Root Cause**:
- API returns 409 with nested error structure: `{"detail": {"error": "idempotency_conflict", ...}}`
- Script was checking top-level `.error` field, not `.detail.error`

**Solution**:
- Updated smoke script to check multiple paths: `.detail.error`, `.error`, or grep fallback
- Script now handles nested FastAPI HTTPException detail dicts
- No API changes needed (response format is correct)

**Verification**:
```bash
# Run smoke test (should pass all 3 tests now)
export API_BASE_URL="https://api.fewo.kolibri-visions.de"
export PID="<property-uuid>"
./backend/scripts/pms_p3a_idempotency_smoke.sh
# Expected: rc=0, all tests PASS
```

---

**Related Documentation**:
- [P3a Idempotency Smoke Script](../../scripts/pms_p3a_idempotency_smoke.sh) - Smoke test for idempotency behavior
- [Idempotency Implementation](../../app/core/idempotency.py) - Idempotency check/store functions
- [Audit Implementation](../../app/core/audit.py) - Audit event emission (best-effort)
- [Public Booking Routes](../../app/api/routes/public_booking.py) - Integration point
- [Migration 20260106160000](../../../supabase/migrations/20260106160000_add_idempotency_keys.sql) - Idempotency keys table
- [Migration 20260106170000](../../../supabase/migrations/20260106170000_add_audit_log.sql) - Audit log table
- [Migration 20260106180000](../../../supabase/migrations/20260106180000_fix_idempotency_keys_indexes.sql) - Fix 42P17 index issue (hotfix)

---
## P3b: Domain Tenant Resolution + Host Allowlist + CORS (Public Endpoints)

### Overview

P3b adds multi-tenant domain mapping, host allowlist enforcement, and explicit CORS configuration for public direct booking endpoints. These features enable:
1. **Domain-based tenant resolution**: Each agency can have their own domain (e.g., customer.com → agency_id)
2. **Host allowlist**: Prevents unauthorized domains from accessing public endpoints
3. **Explicit CORS origins**: Fine-grained control over which browser origins can call the API

### Environment Configuration

**Required Settings:**
```bash
# Host allowlist (comma-separated domains)
ALLOWED_HOSTS="api.fewo.kolibri-visions.de,customer1.com,customer2.de"

# CORS allowed origins (explicit origins, no wildcards)
# Optional: Uses ALLOWED_ORIGINS if not set (backward compat)
CORS_ALLOWED_ORIGINS="https://app.customer1.com,https://www.customer2.de"

# Proxy headers (default: true)
# Set to true if behind reverse proxy/load balancer (respect X-Forwarded-Host)
TRUST_PROXY_HEADERS=true
```

**Safe Defaults:**
- `ALLOWED_HOSTS=""` (empty): In non-prod → allow all with warning; In prod → allow all but log error (fail-open for backward compat)
- `CORS_ALLOWED_ORIGINS` not set: Falls back to existing `ALLOWED_ORIGINS`
- `TRUST_PROXY_HEADERS=true`: Respects X-Forwarded-Host header (standard for proxied deployments)

---

### How to Add Customer Domain

**Prerequisites:**
- Customer domain DNS points to API server (via A/CNAME record or reverse proxy)
- Domain is routed to your API (Plesk, Nginx, etc.)

**Steps:**

1. **Add domain mapping in Supabase SQL Editor:**
   ```sql
   -- Insert domain → agency mapping
   INSERT INTO public.agency_domains (agency_id, domain, is_primary)
   VALUES (
       '<agency-uuid>',        -- Agency UUID
       'customer.com',          -- Domain (lowercase, no port, no protocol)
       true                     -- Primary domain (one per agency recommended)
   );
   ```

2. **Add domain to ALLOWED_HOSTS:**
   ```bash
   # Update environment variable (append to existing list)
   ALLOWED_HOSTS="api.fewo.kolibri-visions.de,customer.com"
   ```

3. **Add site origins to CORS_ALLOWED_ORIGINS (if browser calls API directly):**
   ```bash
   # Update environment variable
   CORS_ALLOWED_ORIGINS="https://app.customer.com,https://www.customer.com"
   ```

4. **Restart backend service:**
   ```bash
   docker restart pms-backend
   ```

5. **Verify domain works:**
   ```bash
   # Test with custom Host header
   curl -H "Host: customer.com" https://api.fewo.kolibri-visions.de/api/v1/public/ping
   # Expected: 200 OK
   ```

---

### Domain Tenant Resolution Flow

**For Public Endpoints** (`/api/v1/public/*`):

1. **Extract Host from request:**
   - If `TRUST_PROXY_HEADERS=true`: Prefer `X-Forwarded-Host` (first value if multiple)
   - Else: Use `Host` header
   - Normalize: lowercase, remove port, strip trailing dot

2. **Resolve agency_id:**
   - **Primary**: Query `agency_domains` table by domain → get `agency_id`
   - **Fallback**: Query `properties` table by `property_id` → get `agency_id`
   - **Cross-check**: Verify property belongs to resolved agency (prevent cross-tenant access)

3. **If no resolution:**
   - Return 422 with actionable message: "Could not resolve agency for booking request. Property may not exist, or domain mapping not configured."

---

### Troubleshooting

**Problem**: Request returns 403 host_not_allowed

**Symptoms**:
- `POST /api/v1/public/booking-requests` returns 403
- Error: `{"error": "host_not_allowed", "message": "Host '...' not allowed. Configure ALLOWED_HOSTS."}`

**Root Cause**:
- Request Host/X-Forwarded-Host is not in `ALLOWED_HOSTS` environment variable
- Domain not added to allowlist after configuring in `agency_domains`

**Solution**:
1. Check request Host header:
   ```bash
   curl -v https://api.example.com/api/v1/public/ping 2>&1 | grep "Host:"
   # Or check X-Forwarded-Host if behind proxy
   ```
2. Verify ALLOWED_HOSTS includes the domain:
   ```bash
   # Check environment variable
   docker exec pms-backend env | grep ALLOWED_HOSTS
   ```
3. Add domain to ALLOWED_HOSTS and restart:
   ```bash
   ALLOWED_HOSTS="api.fewo.kolibri-visions.de,customer.com"
   docker restart pms-backend
   ```

---

**Problem**: Domain mapping not working (wrong agency resolved)

**Symptoms**:
- Request to `customer.com` resolves to wrong agency or fails
- "Property not available for this domain" error

**Root Cause**:
- Domain not in `agency_domains` table
- Domain normalized incorrectly (case mismatch, port included, etc.)
- Property belongs to different agency than domain mapping

**Solution**:
1. Check if domain mapping exists:
   ```sql
   SELECT * FROM agency_domains WHERE domain = 'customer.com';
   -- Should return row with correct agency_id
   ```
2. If missing, add mapping:
   ```sql
   INSERT INTO agency_domains (agency_id, domain, is_primary)
   VALUES ('<agency-uuid>', 'customer.com', true);
   ```
3. Verify domain normalization matches:
   - Stored domain must be lowercase, no port, no protocol
   - Example: ✅ `customer.com` ❌ `Customer.com:443` ❌ `https://customer.com`
4. Cross-check property agency:
   ```sql
   SELECT id, agency_id FROM properties WHERE id = '<property-uuid>';
   -- agency_id must match agency_domains.agency_id for domain
   ```

---

**Problem**: Proxy strips X-Forwarded-Host header

**Symptoms**:
- Domain resolution uses wrong host (API domain instead of customer domain)
- Host allowlist check fails unexpectedly

**Root Cause**:
- Reverse proxy (Nginx, Caddy, etc.) not forwarding `X-Forwarded-Host` header
- Proxy configuration missing `proxy_set_header X-Forwarded-Host $host;`

**Solution**:
1. **Nginx**: Add to proxy block:
   ```nginx
   location / {
       proxy_pass http://backend:8000;
       proxy_set_header Host $host;
       proxy_set_header X-Forwarded-Host $host;
       proxy_set_header X-Forwarded-Proto $scheme;
       proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
   }
   ```
2. **Caddy**: Usually automatic, but verify:
   ```caddyfile
   reverse_proxy backend:8000 {
       header_up Host {host}
       header_up X-Forwarded-Host {host}
   }
   ```
3. Restart proxy and verify headers are forwarded

---

**Problem**: CORS preflight fails (browser error)

**Symptoms**:
- Browser console shows CORS error: "Access-Control-Allow-Origin header missing"
- OPTIONS request to `/api/v1/public/booking-requests` fails

**Root Cause**:
- Origin not in `CORS_ALLOWED_ORIGINS` (or `ALLOWED_ORIGINS` if not set)
- CORS middleware not configured or disabled

**Solution**:
1. Add origin to CORS_ALLOWED_ORIGINS:
   ```bash
   CORS_ALLOWED_ORIGINS="https://app.customer.com,https://www.customer.com"
   ```
2. Restart backend:
   ```bash
   docker restart pms-backend
   ```
3. Test CORS preflight:
   ```bash
   curl -X OPTIONS https://api.example.com/api/v1/public/booking-requests \
     -H "Origin: https://app.customer.com" \
     -H "Access-Control-Request-Method: POST" \
     -H "Access-Control-Request-Headers: content-type,idempotency-key" \
     -v
   # Expected: Access-Control-Allow-Origin: https://app.customer.com
   ```

---

**Related Documentation**:
- [P3b Domain/Host/CORS Smoke Script](../../scripts/pms_p3b_domain_host_cors_smoke.sh) - Smoke test for P3b features
- [Domain Resolution Implementation](../../app/core/tenant_domain.py) - Domain-based tenant resolution logic
- [Host Allowlist Implementation](../../app/core/public_host_allowlist.py) - Host enforcement for public endpoints
- [Public Booking Routes](../../app/api/routes/public_booking.py) - Integration point (updated for P3b)
- [Migration 20260106190000](../../../supabase/migrations/20260106190000_add_agency_domains.sql) - Agency domains table
- [Config](../../app/core/config.py) - ALLOWED_HOSTS, CORS_ALLOWED_ORIGINS, TRUST_PROXY_HEADERS settings

---


## P3c: Audit Review Actions + Request/Correlation ID + Idempotency (Review Endpoints)

### Overview

P3c completes the P3 hardening initiative by adding comprehensive audit logging, request tracing, and optional idempotency support for review workflow endpoints (approve/decline).

**What P3c Provides:**
1. **Audit Events for Review Actions**: Audit log entries are automatically created when booking requests are approved or declined via internal review endpoints
2. **Request/Correlation ID Capture**: Standardized request ID extraction from headers (`X-Request-ID`, `X-Correlation-ID`, `CF-Ray`, `X-Amzn-Trace-Id`) for end-to-end tracing
3. **Optional Idempotency for Review Transitions**: Prevents duplicate approve/decline transitions when the same `Idempotency-Key` is used (protects against retry/double-click)
4. **Ops Endpoint for Audit Log Reads**: Admin-only API endpoint for querying audit log entries (enables automated verification)

**Key Benefits:**
- **Accountability**: Track who approved/declined which booking requests and when
- **Traceability**: Correlate requests across systems using request IDs
- **Retry Safety**: Idempotency prevents accidental duplicate state transitions
- **Automated Verification**: Smoke tests can verify audit events are written correctly

### Request/Correlation ID Headers

P3c automatically extracts request IDs from incoming HTTP headers for correlation and tracing.

**Supported Headers (checked in order):**
1. `X-Request-ID` - Common custom request tracking header
2. `X-Correlation-ID` - Alternative correlation header
3. `CF-Ray` - Cloudflare trace ID (if using Cloudflare)
4. `X-Amzn-Trace-Id` - AWS ALB/API Gateway trace ID (if using AWS)

**Behavior:**
- First non-empty header value is used as the request ID
- If no headers present, a new UUID is generated automatically
- Request ID is stored in audit log entries for each action

**Example:**
```bash
curl -X POST https://api.example.com/api/v1/booking-requests/{id}/approve \
  -H "Authorization: Bearer $JWT" \
  -H "X-Request-ID: abc123-unique-id" \
  -d '{"internal_note": "Approved after guest verification"}'
```

### Audit Log Events

P3c emits audit events for the following review workflow actions:

**Actions Audited:**
- `booking_request_approved` - When a booking request is approved (status: requested/under_review → confirmed)
- `booking_request_declined` - When a booking request is declined (status: requested/under_review → cancelled)

**Audit Event Structure:**
```json
{
  "id": "uuid",
  "created_at": "2026-01-06T22:00:00Z",
  "agency_id": "uuid",
  "actor_type": "user",
  "actor_user_id": "uuid",
  "action": "booking_request_approved",
  "entity_type": "booking_request",
  "entity_id": "uuid",
  "request_id": "abc123-unique-id",
  "idempotency_key": "optional-idempotency-key",
  "ip": "192.168.1.1",
  "user_agent": "curl/7.68.0",
  "metadata": {
    "previous_status": "requested",
    "new_status": "confirmed",
    "internal_note": "Approved after verification",
    "booking_id": "uuid"
  }
}
```

**Audit Behavior:**
- **Best-effort**: Audit logging failures are logged but do NOT break the main request
- **Tenant-scoped**: All audit events include `agency_id` for multi-tenant isolation
- **Automatic**: No manual instrumentation needed - events are emitted automatically

### Idempotency for Review Endpoints

P3c extends the idempotency support from P3a to review endpoints (approve/decline).

**How to Use Idempotency:**
1. Include `Idempotency-Key` header in approve/decline requests
2. Use a unique key per logical operation (e.g., `approve-{uuid}`)
3. Reuse the same key for retries of the same operation

**Example - Approve with Idempotency:**
```bash
# First attempt
curl -X POST https://api.example.com/api/v1/booking-requests/{id}/approve \
  -H "Authorization: Bearer $JWT" \
  -H "Idempotency-Key: approve-20260106-abc123" \
  -d '{"internal_note": "Approved"}'
# Returns: 200 OK (approval executed)

# Retry with same key (e.g., after timeout/network error)
curl -X POST https://api.example.com/api/v1/booking-requests/{id}/approve \
  -H "Authorization: Bearer $JWT" \
  -H "Idempotency-Key: approve-20260106-abc123" \
  -d '{"internal_note": "Approved"}'
# Returns: 200 OK (cached response, no duplicate transition)
```

**Idempotency Behavior:**
- **Same key + same payload** → Returns cached response (status 200, no DB write)
- **Same key + different payload** → Returns 409 Conflict with `idempotency_conflict` error
- **No key provided** → Standard behavior (no idempotency check)

**Idempotency Key TTL:**
- 24 hours (same as P3a public booking requests)
- Stored in `public.idempotency_keys` table (shared with P3a)
- Agency-scoped with unique constraint: `(agency_id, endpoint, method, idempotency_key)`

### Ops Endpoint: Audit Log Reads

P3c adds a new admin-only API endpoint for querying audit log entries.

**Endpoint:** `GET /api/v1/ops/audit-log`

**Authentication:** JWT with admin role required

**Query Parameters:**
- `action` (optional): Filter by action (e.g., `booking_request_approved`)
- `entity_id` (optional): Filter by entity UUID
- `limit` (optional): Max records to return (1-500, default: 50)

**Example - Query Audit Log:**
```bash
# Get recent booking_request_approved events
curl -X GET "https://api.example.com/api/v1/ops/audit-log?action=booking_request_approved&limit=10" \
  -H "Authorization: Bearer $JWT"

# Get audit events for specific booking request
curl -X GET "https://api.example.com/api/v1/ops/audit-log?entity_id={booking_request_id}" \
  -H "Authorization: Bearer $JWT"
```

**Response:**
```json
{
  "items": [
    {
      "id": "uuid",
      "created_at": "2026-01-06T22:00:00Z",
      "action": "booking_request_approved",
      "actor_type": "user",
      "actor_user_id": "uuid",
      "entity_type": "booking_request",
      "entity_id": "uuid",
      "request_id": "abc123",
      "idempotency_key": "approve-20260106-xyz",
      "metadata": {...}
    }
  ],
  "total": 1,
  "limit": 50
}
```

**Usage:**
- Automated smoke tests: Verify audit events are written
- Manual auditing: Investigate who performed which actions
- Troubleshooting: Trace request flow via `request_id`

### Smoke Testing

**Script:** `backend/scripts/pms_p3c_audit_review_smoke.sh`

**Purpose:** Verify P3c audit logging, request ID capture, and idempotency for review endpoints.

**What It Tests:**
1. Create public booking requests
2. Approve booking request with `Idempotency-Key`
3. Test idempotent replay (same key → cached response)
4. Decline booking request with `Idempotency-Key`
5. Verify audit log entries via `/api/v1/ops/audit-log` endpoint

**Prerequisites:**
- `jq` (JSON parser)
- Admin JWT token
- Property UUID (`PID`)

**Usage:**
```bash
export API_BASE_URL="https://api.fewo.kolibri-visions.de"
export PID="<property-uuid>"
export JWT="<admin-jwt-token>"
./backend/scripts/pms_p3c_audit_review_smoke.sh
```

**Expected Output:**
```
✅ TEST PASSED: All tests passed
Smoke test: PASS
rc=0
```

**Exit Codes:**
- `0` - Success (all tests passed)
- `1` - Test failure (unexpected status code or missing audit events)
- `2` - Server error (500 response detected)

### Troubleshooting

#### Missing Audit Log Entries

**Symptom:** Audit log query returns 0 results after approve/decline action.

**Possible Causes:**
1. **Best-effort audit failed**: Audit emission is non-blocking; database errors don't break requests
2. **Wrong tenant scope**: Audit log is tenant-scoped; verify JWT agency_id matches entity agency
3. **Timing issue**: Allow 1-2 seconds after action before querying audit log (eventual consistency)
4. **Wrong filter**: Check `action` and `entity_id` query parameters match expected values

**How to Debug:**
```bash
# Check backend logs for audit emission errors
tail -f logs/backend.log | grep "Failed to emit audit event"

# Query all recent audit events (no filters)
curl -X GET "https://api.example.com/api/v1/ops/audit-log?limit=50" \
  -H "Authorization: Bearer $JWT"

# Verify entity belongs to correct agency
curl -X GET "https://api.example.com/api/v1/booking-requests/{id}" \
  -H "Authorization: Bearer $JWT"
```

**Solution:**
- If audit emission consistently fails, check database connectivity and `audit_log` table schema
- If timing issue, add 2-second delay before verification in smoke tests
- If wrong tenant, ensure JWT token agency_id matches booking request agency_id

#### Idempotency Conflict (409) Unexpectedly

**Symptom:** Getting 409 `idempotency_conflict` when retrying approve/decline with same key.

**Possible Causes:**
1. **Payload mismatch**: Request body differs between attempts (even whitespace/ordering matters)
2. **Cached from different request**: Same key used for different operation/entity
3. **Expired but re-added**: Key expired (24h TTL) and recreated with different payload

**How to Debug:**
```bash
# Check idempotency_keys table
SELECT idempotency_key, request_hash, response_status, created_at, expires_at
FROM idempotency_keys
WHERE idempotency_key = 'your-key-here'
AND agency_id = '{agency-uuid}'
AND expires_at > NOW()
LIMIT 1;

# Compare request hashes (should match for idempotent replay)
```

**Solution:**
- Ensure request payload is byte-for-byte identical (JSON key ordering, whitespace)
- Use unique idempotency keys per operation (don't reuse across different entities)
- Wait for TTL expiration (24h) or use new key if payload needs to change

#### Auth Scope Issue (403 Forbidden)

**Symptom:** `GET /api/v1/ops/audit-log` returns 403 Forbidden.

**Possible Causes:**
1. **Non-admin role**: Endpoint requires admin role
2. **Invalid JWT**: Token expired or malformed
3. **Cross-tenant access**: JWT agency_id doesn't match audit records

**How to Debug:**
```bash
# Verify JWT role claim
echo $JWT | cut -d'.' -f2 | base64 -d | jq '.role'
# Should return: "admin"

# Check JWT expiration
echo $JWT | cut -d'.' -f2 | base64 -d | jq '.exp'
# Convert to date: date -r $(echo $JWT | cut -d'.' -f2 | base64 -d | jq -r '.exp')
```

**Solution:**
- Use JWT with admin role (manager role is NOT sufficient for `/ops/audit-log`)
- Refresh expired JWT using authentication endpoint
- Ensure JWT agency_id matches the agency you're querying

#### Request ID Not Captured

**Symptom:** Audit log entries have `request_id` but it's a random UUID (not the header value).

**Possible Causes:**
1. **Header not sent**: Request didn't include `X-Request-ID` or similar headers
2. **Proxy stripped headers**: Load balancer/proxy removed custom headers
3. **Header name mismatch**: Used non-standard header name not in P3c header list

**How to Debug:**
```bash
# Test header pass-through
curl -X POST https://api.example.com/api/v1/booking-requests/{id}/approve \
  -H "Authorization: Bearer $JWT" \
  -H "X-Request-ID: test-header-123" \
  -d '{"internal_note": "Test"}' \
  -v 2>&1 | grep -i request-id

# Check audit log entry
curl -X GET "https://api.example.com/api/v1/ops/audit-log?action=booking_request_approved&limit=1" \
  -H "Authorization: Bearer $JWT" | jq '.items[0].request_id'
# Should return: "test-header-123" (if header was captured)
```

**Solution:**
- Verify header is sent by client (use `-v` flag in curl to see request headers)
- If behind proxy, configure proxy to preserve `X-Request-ID` header
- Use one of the supported headers: `X-Request-ID`, `X-Correlation-ID`, `CF-Ray`, `X-Amzn-Trace-Id`
- If no header provided, P3c generates a UUID automatically (this is expected behavior)


#### Smoke Script 422 Validation Error

**Symptom:** `pms_p3c_audit_review_smoke.sh` fails with HTTP 422 validation_error when creating public booking requests.

**Possible Causes:**
1. **Payload mismatch**: Script sending incorrect field names or missing required fields
2. **Schema drift**: API schema changed but smoke script not updated
3. **Env var issues**: Required environment variables (PID, DATE_FROM, DATE_TO) not set or invalid

**How to Debug:**
The script automatically prints the sent payload on 422 errors:
```bash
❌ FAIL: Returned 422 validation_error (payload mismatch)
Response: {"detail":[{"type":"missing","loc":["body","date_from"],...}]}

Payload sent:
{
  "property_id": "...",
  "check_in": "2037-06-01",  # ← WRONG: should be "date_from"
  ...
}
```

**Common Issues:**
- Using `check_in`/`check_out` instead of `date_from`/`date_to`
- Using `num_adults`/`num_children` instead of `adults`/`children`
- Using `guest_info` instead of `guest`
- Missing required fields: `property_id`, `date_from`, `date_to`, `adults`, `guest`

**Solution:**
Ensure the script uses the correct BookingRequestInput schema:
```json
{
  "property_id": "uuid",
  "date_from": "YYYY-MM-DD",
  "date_to": "YYYY-MM-DD",
  "adults": 2,
  "children": 0,
  "guest": {
    "first_name": "...",
    "last_name": "...",
    "email": "...",
    "phone": "..."
  },
  "currency": "EUR"
}
```

If schema changed, update the smoke script payload builder to match current API requirements.



---

## P3 Public Direct Booking Hardening (Consolidated)

**Overview:** This section covers the consolidated P3 Direct Booking Hardening smoke test that verifies all P3 components (P3a, P3b, P3c) in a single test workflow.

**Purpose:** Validate that public direct booking endpoints are properly hardened with idempotency, CORS/origin controls, and comprehensive audit logging.

**Components Included:**
- **P3a**: Idempotency-Key support for `/api/v1/public/booking-requests`
- **P3b**: CORS/Origin/Host allowlist for public endpoints
- **P3c**: Audit log for booking request lifecycle events

**Smoke Test Script:** `backend/scripts/pms_public_direct_booking_hardening_smoke.sh`

**What The Consolidated Test Verifies:**
1. CORS preflight with allowed origin returns proper headers
2. First booking request with Idempotency-Key succeeds
3. Retry with same Idempotency-Key + same payload returns same booking ID
4. Retry with same Idempotency-Key + different payload returns HTTP 409
5. Audit log contains `public.booking_request.created` event

**Required Environment:**
- `HOST`: PMS backend base URL
- `JWT_TOKEN`: JWT with manager/admin role
- Optional: `AGENCY_ID`, `PROPERTY_ID`, `ALLOWED_ORIGIN`

**Public Booking Requests Payload Format:**
- The `/api/v1/public/booking-requests` endpoint requires `date_from` and `date_to` fields (YYYY-MM-DD format)
- Do NOT use `check_in`/`check_out` - these will cause validation errors
- Example: `{"property_id": "...", "date_from": "2026-02-01", "date_to": "2026-02-05", "adults": 2, "children": 0, "guest": {...}}`

**Usage:**
```bash
HOST=https://pms-backend.production.example.com \
JWT_TOKEN="eyJ..." \
./backend/scripts/pms_public_direct_booking_hardening_smoke.sh
```

**Expected Success Output:**
```
✅ Test 1 PASSED: CORS preflight returned allow-origin header
✅ Test 2 PASSED: Created booking request with idempotency key: 770e8400-...
✅ Test 3 PASSED: Idempotency works - same booking ID returned: 770e8400-...
✅ Test 4 PASSED: Idempotency conflict returned 409 as expected
✅ Test 5 PASSED: Found audit log event for booking request created
✅ All P3 Public Direct Booking Hardening smoke tests passed! 🎉
```

**PROD Evidence (2026-01-10):**

Verified in production with commit `b651b6220a048df674e6ebec26ec6944e7d38cc8` (started_at: 2026-01-10T14:54:05.329699+00:00).

```bash
# Deploy verification
export API_BASE_URL="https://api.fewo.kolibri-visions.de"
./backend/scripts/pms_verify_deploy.sh
# Expected: rc=0 (commit match)

# P3 consolidated smoke test
export HOST="https://api.fewo.kolibri-visions.de"
export API_BASE_URL="$HOST"
export PROPERTY_ID="23dd8fda-59ae-4b2f-8489-7a90f5d46c66"
export PID="$PROPERTY_ID"
export AGENCY_ID="ffd0123a-10b6-40cd-8ad5-66eee9757ab7"
export TEST_ORIGIN="https://fewo.kolibri-visions.de"
export DOMAIN="api.fewo.kolibri-visions.de"
bash ./backend/scripts/pms_public_direct_booking_hardening_smoke.sh
# Expected: rc=0
```

**Tests Validated:**
- CORS preflight (P3b): PASS
- Idempotency first request (P3a): PASS
- Idempotency retry same key (P3a): PASS
- Idempotency conflict detection (P3a): PASS
- Audit log booking request created event (P3c): PASS (attempt 1)

### Common Issues

#### Test 1 Skipped (CORS)

**Symptom:** Test 1 reports "SKIPPED: CORS may not be configured".

**Cause:** CORS middleware not configured in test environment or allowed origin mismatch.

**Impact:** Non-critical for functionality testing. Production should have CORS configured.

**Solution:** Verify `ALLOWED_ORIGINS` environment variable in production matches `ALLOWED_ORIGIN` test value.

#### Test 2 Fails (Create Booking Request)

**Symptom:** Test 2 fails with 400/404/500 error.

**Possible Causes:**
1. Property doesn't exist or user lacks access
2. JWT token invalid/expired
3. Required booking fields missing/invalid
4. Database connectivity issue

**How to Debug:**
```bash
# Verify property exists and is accessible
curl -X GET "$HOST/api/v1/properties/?limit=1" \
  -H "Authorization: Bearer $JWT_TOKEN"

# Check JWT expiration
echo $JWT_TOKEN | cut -d'.' -f2 | base64 -d | jq '.exp'

# Test minimal booking request
curl -X POST "$HOST/api/v1/public/booking-requests" \
  -H "Content-Type: application/json" \
  -H "Idempotency-Key: test-$(date +%s)" \
  -d '{"property_id":"<uuid>","check_in":"2026-02-01","check_out":"2026-02-05","adults":2,"children":0,"guest":{"email":"test@example.com","first_name":"Test","last_name":"User"}}'
```

**Solution:** Fix property access, refresh JWT, or correct booking payload.

#### Test 3 Fails (Idempotency Retry)

**Symptom:** Retry returns different booking ID instead of same ID.

**Cause:** Idempotency middleware not enabled or migration not applied.

**How to Debug:**
```bash
# Check if idempotency middleware is active (should see Idempotency-Key in response headers)
curl -v -X POST "$HOST/api/v1/public/booking-requests" \
  -H "Idempotency-Key: test-123" \
  -d '...' 2>&1 | grep -i idempotency

# Verify migration applied
psql $DATABASE_URL -c "SELECT EXISTS (SELECT 1 FROM information_schema.tables WHERE table_name = 'idempotency_keys');"
```

**Solution:** Enable idempotency middleware in app startup, apply migration 20260106160000.

#### Test 4 Fails (Expected 409 Conflict)

**Symptom:** Retry with different payload returns 200/201 instead of 409.

**Cause:** Payload hash comparison not working or idempotency key TTL expired.

**How to Debug:**
```bash
# Check idempotency_keys table for stored payload hash
psql $DATABASE_URL -c "SELECT idempotency_key, payload_hash, created_at FROM idempotency_keys ORDER BY created_at DESC LIMIT 5;"

# Verify tests run within 24h TTL window
# If payload_hash is NULL, hash calculation is broken
```

**Solution:** Check `app/core/idempotency.py` hash calculation logic, ensure tests complete within TTL.

#### Test 5 Failed (Audit Log)

**Symptom:** Test 5 reports "FAILED: Could not find audit event for booking request" after polling retries.

**Possible Causes:**
1. JWT lacks admin role (audit-log endpoint requires admin)
2. Audit emission failed (database issue)
3. Multi-tenant setup requires `AGENCY_ID` to be set for x-agency-id header
4. Timing issue (audit not yet committed, though script polls up to ~10s)

**How to Debug:**
```bash
# Verify JWT role
echo $JWT_TOKEN | cut -d'.' -f2 | base64 -d | jq '.role'
# Should return: "admin" (manager is NOT sufficient)

# Check if AGENCY_ID is needed (multi-tenant setups)
# Export AGENCY_ID before running script:
export AGENCY_ID="ffd0123a-10b6-40cd-8ad5-66eee9757ab7"

# Check audit_log table directly
psql $DATABASE_URL -c "SELECT action, actor_type, entity_id, metadata FROM audit_log WHERE action LIKE '%booking%request%' ORDER BY created_at DESC LIMIT 5;"

# Check backend logs for audit emission errors
docker logs pms-backend --tail 100 | grep -i audit
```

**Solution:**
- Use admin JWT token
- Set `AGENCY_ID` environment variable if you have multiple agencies
- Verify audit_log migration applied (20260106170000)
- Check database connectivity
- Script automatically polls up to 10 times (1s intervals) for eventual consistency

### Testing Against Production

**Pre-flight Checklist:**
- [ ] JWT token has valid admin role
- [ ] `HOST` points to production backend URL
- [ ] `PROPERTY_ID` exists and belongs to agency (or omit for auto-pick)
- [ ] `ALLOWED_ORIGIN` matches production CORS config (default: `https://fewo.kolibri-visions.de`)
- [ ] Production database has migrations 20260106160000 (idempotency) and 20260106170000 (audit) applied

**Recommended Workflow:**
1. Run consolidated smoke script first for quick validation
2. If any test fails, run individual P3a/b/c scripts for detailed debugging
3. Check specific runbook sections (P3a, P3b, P3c) for component-specific issues

**Related Documentation:**
- [P3a Runbook Section](#p3a-idempotency--audit-log-public-booking-requests) - Idempotency details
- [P3b Runbook Section](#p3b-domain-tenant-resolution--host-allowlist--cors-public-endpoints) - CORS/domain details
- [P3c Runbook Section](#p3c-audit-review-actions--requestcorrelation-id--idempotency-review-endpoints) - Audit details
- [Scripts README](../../scripts/README.md#p3-public-direct-booking-hardening-smoke-test-consolidated) - Script usage guide


### P3 Configuration: CORS_ALLOWED_ORIGINS

**Overview:** P3b introduced explicit CORS configuration for public endpoints via the `CORS_ALLOWED_ORIGINS` environment variable. A bug in the initial implementation prevented this variable from being honored by the CORS middleware. This has been fixed.

**CORS Configuration Hierarchy:**
1. **`CORS_ALLOWED_ORIGINS`** (preferred, P3b): Explicit CORS origins for public endpoints (no wildcards by default)
2. **`ALLOWED_ORIGINS`** (fallback): Legacy/general allowed origins setting

**How It Works:**
- The `effective_cors_origins` property in `app/core/config.py` implements the fallback logic
- CORS middleware in `app/main.py` uses `settings.effective_cors_origins` (not `settings.cors_origins`)
- If `CORS_ALLOWED_ORIGINS` is set: Use it exclusively
- If `CORS_ALLOWED_ORIGINS` is NOT set: Fall back to `ALLOWED_ORIGINS`

**Environment Variable Format:**
```bash
# Comma-separated list of origins (no wildcards by default)
CORS_ALLOWED_ORIGINS="https://fewo.kolibri-visions.de,https://www.customer.com,https://admin.customer.com"

# Must include protocol (https://) and no trailing slash
# Wildcards (*) are NOT supported by default (FastAPI CORSMiddleware behavior)
```

**Verification Commands:**

```bash
# Test CORS preflight with specific origin
curl -X OPTIONS "$HOST/api/v1/public/booking-requests" \
  -H "Origin: https://fewo.kolibri-visions.de" \
  -H "Access-Control-Request-Method: POST" \
  -i

# Expected: HTTP 200 with headers:
# access-control-allow-origin: https://fewo.kolibri-visions.de
# access-control-allow-credentials: true
# access-control-allow-methods: GET,POST,PUT,DELETE,PATCH
```

**Troubleshooting: Origin Not Allowed**

**Symptom:** Browser console shows CORS error: "Access to fetch at '...' has been blocked by CORS policy: No 'Access-Control-Allow-Origin' header is present".

**Root Cause:** Frontend origin not in `CORS_ALLOWED_ORIGINS` (or `ALLOWED_ORIGINS` fallback).

**How to Debug:**
```bash
# Check backend logs on startup
docker logs pms-backend 2>&1 | grep -i "cors\|allowed"

# Verify CORS preflight manually
curl -X OPTIONS "$HOST/api/v1/public/ping" \
  -H "Origin: https://your-frontend-origin.com" \
  -i | grep -i "access-control"

# If no access-control headers: origin not allowed
```

**Solution:**
1. Add frontend origin to `CORS_ALLOWED_ORIGINS` in Coolify:
   ```
   CORS_ALLOWED_ORIGINS=https://fewo.kolibri-visions.de,https://your-frontend-origin.com
   ```
2. Restart backend container
3. Test CORS preflight again (should return Allow-Origin header)

### P3 Canonical Smoke Test

**Script Name:** `backend/scripts/pms_p3_direct_booking_hardening_smoke.sh`

**Purpose:** This is the canonical/primary smoke test script for P3 Direct Booking Hardening. It wraps the underlying consolidated script (`pms_public_direct_booking_hardening_smoke.sh`) and provides:
- Canonical naming convention for P3 verification
- Support for both `HOST` and `API_BASE_URL` environment variable naming
- Clear delegation message showing which underlying script is executed

**Why Two Scripts?**
- `pms_p3_direct_booking_hardening_smoke.sh`: Canonical name for P3 (use this for documentation/SOPs)
- `pms_public_direct_booking_hardening_smoke.sh`: Implementation script (can be used directly)

**Usage:**
```bash
# Canonical script (recommended for SOPs)
HOST=https://api.fewo.kolibri-visions.de \
JWT_TOKEN="eyJ..." \
./backend/scripts/pms_p3_direct_booking_hardening_smoke.sh

# Alternative with API_BASE_URL (automatically mapped to HOST)
API_BASE_URL=https://api.fewo.kolibri-visions.de \
JWT_TOKEN="eyJ..." \
./backend/scripts/pms_p3_direct_booking_hardening_smoke.sh
```

**Expected Output:**
```
ℹ ════════════════════════════════════════════════════════════════
ℹ P3 Direct Booking Hardening Smoke Test (Canonical Script)
ℹ Delegating to: pms_public_direct_booking_hardening_smoke.sh
ℹ ════════════════════════════════════════════════════════════════

ℹ Starting P3 Public Direct Booking Hardening smoke test...
[... test execution ...]
✅ All P3 Public Direct Booking Hardening smoke tests passed! 🎉
```

**Exit Code:** Same as underlying script (0 = success, 1+ = failures)

---

## Customer Domain Onboarding SOP

**Purpose**: Step-by-step procedure for onboarding new customer domains to the PMS multi-tenant system.

**Target Audience**: Junior admins, ops engineers, support staff

**Estimated Time**: 15-30 minutes (including DNS propagation wait if not using pre-DNS verification)

### Pre-requisites

**Required Access:**
- ✅ Domain registrar access (or customer cooperation for DNS changes)
- ✅ Plesk admin access (for DNS zone management)
- ✅ Supabase owner/admin access (for SQL Editor and agency_domains table)
- ✅ Coolify admin access (for environment variable updates)
- ✅ Backend server SSH/shell access (for verification script execution)

**Required Information:**
- Customer domain (e.g., `customer.example.com`)
- Agency UUID (from agencies table in Supabase)
- Server IP address (for pre-DNS testing, from Coolify or server config)
- Frontend origin (e.g., `https://app.customer.example.com`, if applicable for CORS)

### Step-by-Step Procedure

#### Step 1: DNS Configuration (Plesk or Registrar)

**Goal**: Point customer domain to PMS backend server IP.

**Option A: CNAME Record (Recommended)**
```dns
customer.example.com.  3600  IN  CNAME  pms.your-server.com.
```

**Option B: A/AAAA Record (Direct IP)**
```dns
customer.example.com.  3600  IN  A      1.2.3.4
customer.example.com.  3600  IN  AAAA   2001:db8::1
```

**Important Notes:**
- Use **lowercase** domain names (backend normalizes to lowercase)
- **NO trailing dot** in Supabase/ENV vars (Plesk DNS may require it, but app does not)
- TTL 3600 (1 hour) is safe default
- If using Plesk DNS: Add record in domain zone file editor
- If using external DNS: Provide DNS change instructions to customer

**Validation:**
```bash
# Wait for DNS propagation (or skip if using pre-DNS verification)
nslookup customer.example.com
# Should resolve to server IP

# Test with dig (more detailed)
dig customer.example.com A
dig customer.example.com AAAA
```

#### Step 2: Supabase Database Mapping

**Goal**: Map customer domain to agency_id in agency_domains table.

**SQL Editor Query:**
```sql
-- Replace with actual values (NO trailing dots, lowercase)
INSERT INTO agency_domains (agency_id, domain)
VALUES (
  '12345678-1234-1234-1234-123456789abc',  -- Agency UUID from agencies table
  'customer.example.com'                     -- Customer domain (lowercase, no trailing dot)
)
ON CONFLICT (domain) DO NOTHING;
```

**Important Notes:**
- Domain must be **lowercase** and **no trailing dot**
- Agency UUID must exist in agencies table (verify first)
- ON CONFLICT prevents duplicate errors if domain already mapped
- RLS policies enforce tenant isolation (domain cannot be shared across agencies)

**Validation:**
```sql
-- Verify mapping exists
SELECT * FROM agency_domains WHERE domain = 'customer.example.com';
-- Should return one row with correct agency_id
```

#### Step 3: Coolify Environment Variables

**Goal**: Update backend ENV vars to allow customer domain in host allowlist and CORS.

**ENV Var Updates in Coolify:**

1. **ALLOWED_HOSTS** (Host Allowlist):
```bash
# Add customer domain to comma-separated list (NO trailing dots, lowercase)
ALLOWED_HOSTS=localhost,pms.your-server.com,customer.example.com
```

2. **CORS_ALLOWED_ORIGINS** (CORS Allow List):
```bash
# Add frontend origin if applicable (e.g., for SPA/React)
CORS_ALLOWED_ORIGINS=http://localhost:3000,https://app.customer.example.com
```

3. **TRUST_PROXY_HEADERS** (Required for X-Forwarded-Host):
```bash
# Must be "true" for domain routing to work behind proxy
TRUST_PROXY_HEADERS=true
```

**Important Notes:**
- ENV var changes require **backend restart** (Coolify auto-restarts on ENV change)
- Domain in ALLOWED_HOSTS must **exactly match** request Host header (lowercase, no port)
- CORS origin must include **protocol** (https://) and **no trailing slash**
- Coolify may take 30-60 seconds to restart backend after ENV change

**Validation:**
```bash
# Check ENV vars are applied (SSH to backend container)
echo $ALLOWED_HOSTS
echo $CORS_ALLOWED_ORIGINS
echo $TRUST_PROXY_HEADERS
```

#### Step 4: TLS/Certificate Provisioning (Let's Encrypt)

**Goal**: Ensure HTTPS works for customer domain.

**Coolify Automatic Provisioning:**
1. Navigate to Coolify project → Domains tab
2. Add customer domain to domain list
3. Enable "Let's Encrypt" toggle
4. Coolify will automatically provision certificate via ACME challenge
5. Wait 1-2 minutes for certificate issuance

**Manual Validation (if needed):**
```bash
# Test TLS handshake
curl -v https://customer.example.com/api/v1/ops/version 2>&1 | grep -i "SSL certificate"
# Should show valid certificate (not self-signed)

# Check certificate expiry
echo | openssl s_client -connect customer.example.com:443 -servername customer.example.com 2>/dev/null | openssl x509 -noout -dates
```

**Common Issues:**
- **DNS not propagated**: Let's Encrypt ACME challenge fails → Wait for DNS TTL or use pre-DNS testing
- **Coolify domain not added**: Certificate not provisioned → Add domain in Coolify UI
- **Port 80 blocked**: ACME HTTP-01 challenge fails → Verify firewall allows port 80

#### Step 5: Verification (Pre-DNS or Post-DNS)

**Goal**: Verify domain routing, tenant isolation, and CORS before go-live.

**Option A: Pre-DNS Verification (Recommended)**
```bash
# Test before DNS propagation using curl --resolve
DOMAIN=customer.example.com \
SERVER_IP=1.2.3.4 \
./backend/scripts/pms_domain_onboarding_verify.sh
```

**Option B: Post-DNS Verification**
```bash
# Test after DNS propagation (omit SERVER_IP)
DOMAIN=customer.example.com \
./backend/scripts/pms_domain_onboarding_verify.sh
```

**Option C: With CORS Testing**
```bash
# Test CORS preflight for SPA/frontend
DOMAIN=customer.example.com \
TEST_ORIGIN=https://app.customer.example.com \
./backend/scripts/pms_domain_onboarding_verify.sh
```

**Expected Output:**
```
🔍 Verifying domain onboarding: customer.example.com
Using direct IP bypass (pre-DNS): 1.2.3.4
✅ Health check passed (HTTP 200)
✅ TLS certificate valid
✅ Agency ID confirmed: 12345678-1234-1234-1234-123456789abc
✅ CORS preflight passed (origin: https://app.customer.example.com)
✅ All checks passed - domain is ready for production traffic
```

**Troubleshooting:**
- See verification script output for actionable hints
- Common failures documented in script header comments
- Refer to "Customer Domain Onboarding Troubleshooting" section below

### Post-Onboarding Checklist

**Before Go-Live:**
- ✅ DNS resolves to correct IP (nslookup/dig)
- ✅ HTTPS works (valid certificate, not self-signed)
- ✅ Health endpoint returns 200 (not 403 host_not_allowed)
- ✅ Agency ID matches expected value
- ✅ CORS preflight passes (if applicable)
- ✅ Customer notified of go-live (if external domain)

**Documentation:**
- ✅ Record domain onboarding in ops log (domain, agency_id, date, operator)
- ✅ Update customer documentation with API base URL
- ✅ Provide customer with health endpoint for their monitoring: `https://customer.example.com/api/v1/ops/version`

**Monitoring:**
- ✅ Add domain to uptime monitoring (e.g., Pingdom, UptimeRobot)
- ✅ Set up alerts for certificate expiry (Let's Encrypt renews at 60 days)
- ✅ Monitor backend logs for 403 host_not_allowed errors

### Rollback Procedure

**If onboarding fails or needs to be reverted:**

1. **Remove Coolify ENV Vars:**
   - Remove customer domain from ALLOWED_HOSTS
   - Remove frontend origin from CORS_ALLOWED_ORIGINS
   - Coolify will auto-restart backend

2. **Delete Supabase Mapping:**
```sql
DELETE FROM agency_domains WHERE domain = 'customer.example.com';
```

3. **Revert DNS (if needed):**
   - Remove CNAME/A/AAAA record in Plesk or registrar
   - Wait for DNS TTL expiry (or flush caches)

4. **Remove Coolify Domain (if added):**
   - Remove domain from Coolify domains list
   - Let's Encrypt certificate will auto-expire (90 days)

**Rollback is safe and idempotent** - no data loss, only routing changes.

### Customer Domain Onboarding Troubleshooting

#### Problem: Verification Script Returns 403 host_not_allowed

**Symptom:**
```
❌ Health check failed: HTTP 403
Response: {"detail":"Host not allowed: customer.example.com"}
```

**Root Cause:**
- Customer domain not in ALLOWED_HOSTS environment variable
- ENV var change not applied (backend not restarted)
- Domain case mismatch (customer.example.com vs CUSTOMER.EXAMPLE.COM)

**Solution:**
1. Verify ALLOWED_HOSTS includes customer domain (lowercase, no port, no trailing dot)
2. Restart backend in Coolify (or wait for auto-restart)
3. Retry verification script

**Verification:**
```bash
# SSH to backend container
echo $ALLOWED_HOSTS | grep -i customer.example.com
# Should return the domain
```

#### Problem: Verification Script Returns 503 No Available Server

**Symptom:**
```
❌ Health check failed: HTTP 503
Response: <html>503 Service Temporarily Unavailable</html>
```

**Root Cause:**
- Backend is down (crashed, restart loop, deployment in progress)
- Proxy/load balancer cannot reach backend (network issue)
- TLS/certificate provisioning in progress (Coolify blocking traffic)

**Solution:**
1. Check backend health: `curl http://localhost:8000/api/v1/ops/version` (from server)
2. Check Coolify deployment logs for errors
3. Verify backend is running: `docker ps | grep backend`
4. Wait for deployment to complete (Coolify shows "Running" status)
5. Retry verification script

#### Problem: CORS Preflight Fails (403 or Missing Headers)

**Symptom:**
```
❌ CORS preflight failed: HTTP 403 (or missing Access-Control-Allow-Origin header)
```

**Root Cause:**
- Frontend origin not in CORS_ALLOWED_ORIGINS
- ENV var change not applied
- Origin format mismatch (http vs https, trailing slash, case)

**Solution:**
1. Verify CORS_ALLOWED_ORIGINS includes frontend origin (exact match, with protocol)
2. Restart backend in Coolify
3. Test with exact origin: `TEST_ORIGIN=https://app.customer.example.com ./script.sh`

**Verification:**
```bash
# Manual CORS preflight test
curl -X OPTIONS https://customer.example.com/api/v1/health \
  -H "Origin: https://app.customer.example.com" \
  -H "Access-Control-Request-Method: GET" \
  -v 2>&1 | grep -i "access-control"
# Should return Access-Control-Allow-Origin: https://app.customer.example.com
```

#### Problem: TLS Certificate Invalid or Self-Signed

**Symptom:**
```
❌ TLS error: self signed certificate
curl: (60) SSL certificate problem: self signed certificate
```

**Root Cause:**
- Let's Encrypt certificate not provisioned yet (Coolify in progress)
- DNS not propagated (ACME challenge fails)
- Domain not added to Coolify domains list

**Solution:**
1. Wait 2-5 minutes for Let's Encrypt provisioning
2. Verify DNS propagation: `nslookup customer.example.com`
3. Check Coolify logs for ACME errors
4. Ensure domain added to Coolify domains list with Let's Encrypt enabled
5. Retry verification script (or skip TLS check for pre-DNS testing)

**Workaround for Pre-DNS Testing:**
```bash
# Skip TLS verification (for testing only, not production)
curl -k https://customer.example.com/api/v1/ops/version
```

#### Problem: Agency ID Mismatch

**Symptom:**
```
⚠️  WARNING: Agency ID mismatch
Expected: 12345678-1234-1234-1234-123456789abc
Actual:   87654321-4321-4321-4321-cba987654321
```

**Root Cause:**
- Wrong agency_id in Supabase agency_domains mapping
- Database mapping not created (using default/fallback agency)
- Domain typo in Supabase (customer.example.com vs customer.exmaple.com)

**Solution:**
1. Verify agency_domains mapping: `SELECT * FROM agency_domains WHERE domain = 'customer.example.com';`
2. Update mapping if wrong: `UPDATE agency_domains SET agency_id = '...' WHERE domain = '...';`
3. Verify domain spelling (lowercase, no trailing dot)
4. Retry verification script

**Note**: Agency ID warning does not fail verification (exit code still 0), but should be investigated.

### Related Scripts

- **pms_domain_onboarding_verify.sh**: Automated verification script (this SOP)
- **pms_verify_deploy.sh**: Deployment verification (checks version/modules, not domain-specific)

### Related Documentation

- [Backend Configuration](../app/core/config.py) - ALLOWED_HOSTS, CORS_ALLOWED_ORIGINS, TRUST_PROXY_HEADERS
- [Domain Middleware](../app/middleware/domain.py) - Domain-to-tenant resolution logic
- [Database Schema](../../supabase/migrations/20250106000005_agency_domains.sql) - agency_domains table structure

### Maintenance

**Regular Tasks:**
- Monitor certificate expiry (Let's Encrypt auto-renews, but verify)
- Review agency_domains table for orphaned mappings (agency deleted but domain remains)
- Update ALLOWED_HOSTS when domains change or are removed

**Deprecation:**
- If domain is no longer needed, follow rollback procedure
- Archive ops log entry for domain removal (date, reason, operator)



---

## 1 VPS per Customer (Single-Tenant Installations Playbook)

**Purpose**: Complete step-by-step procedure for provisioning a dedicated VPS for a single customer (single-tenant deployment). This playbook enables junior admins to reliably set up isolated customer instances with their own domains, database, and infrastructure.

**Target Audience**: Junior admins, ops engineers, DevOps staff

**Estimated Time**: 2-4 hours (including DNS propagation and Let's Encrypt provisioning)

**Deployment Model**:
- **Single-Tenant**: One VPS per customer, one agency per VPS
- **Isolation**: Dedicated infrastructure (compute, database, domains)
- **White-Label**: Customer domains (www.kunde1.de, admin.kunde1.de, api.kunde1.de)
- **Architecture**: Full stack per VPS (Supabase/Postgres, Backend, Worker, Admin UI)

### Pre-requisites

**Required Access**:
- ✅ Hetzner Cloud account (or equivalent VPS provider)
- ✅ Coolify admin access (for app deployment and proxy configuration)
- ✅ Customer domain registrar access (or customer cooperation for DNS changes)
- ✅ Plesk admin access (for DNS zone management, if applicable)
- ✅ SSH access to customer VPS (root or sudo privileges)
- ✅ Git repository access (for source code deployment)

**Required Information**:
- Customer name/identifier (e.g., "kunde1")
- Customer domains (e.g., www.kunde1.de, admin.kunde1.de, api.kunde1.de)
- VPS size requirements (CPU, RAM, disk based on expected load)
- Database credentials strategy (auto-generated or customer-provided)
- SSL/TLS certificate requirements (Let's Encrypt recommended)

**Recommended Customer Domain Layout**:
```
www.kunde1.de        → Public-facing website/booking interface
admin.kunde1.de      → Backoffice/admin UI (staff access)
api.kunde1.de        → Backend API (used by admin UI and public site)
```

**Note**: API under customer domain is supported and recommended for white-label deployments. This differs from the internal/owner instance which uses `api.fewo.kolibri-visions.de`.

### Step-by-Step Procedure

#### Step 1: Provision VPS (HETZNER CLOUD UI)

**Goal**: Create a new dedicated VPS for the customer.

**Hetzner Cloud Console**:
1. Navigate to: https://console.hetzner.cloud/
2. Select project (or create new project for customer)
3. Click "Add Server"
4. Configuration:
   - **Location**: Select region closest to customer (e.g., Falkenstein, Nuremberg, Helsinki)
   - **Image**: Ubuntu 22.04 LTS (recommended) or Ubuntu 24.04 LTS
   - **Type**: 
     - **Starter**: CPX11 (2 vCPU, 2GB RAM) - for testing/small deployments
     - **Production**: CPX21 (3 vCPU, 4GB RAM) - recommended minimum
     - **High Load**: CPX31 (4 vCPU, 8GB RAM) - for high-traffic sites
   - **Networking**: 
     - Enable IPv4 and IPv6 (both recommended)
     - No additional networks required (unless customer has special requirements)
   - **Firewall**: 
     - Create firewall rule: Allow TCP 22 (SSH), 80 (HTTP), 443 (HTTPS)
     - Block all other inbound traffic
   - **SSH Keys**: Add your SSH public key for root access
   - **Name**: customer-vps-kunde1 (or similar descriptive name)
5. Click "Create & Buy Now"
6. Wait 1-2 minutes for provisioning
7. **Record VPS IP addresses** (IPv4 and IPv6) for DNS configuration

**Validation**:
```bash
# Test SSH access (from your local machine)
ssh root@<VPS_IPv4>
# Should connect successfully

# Check system info
uname -a
# Should show Ubuntu 22.04 or 24.04

# Check available resources
free -h
df -h
```

**Important**: Keep VPS IP addresses handy for Step 2 (DNS configuration).

#### Step 2: DNS Configuration (PLESK UI or DNS PROVIDER)

**Goal**: Point customer domains to the new VPS IP addresses.

**Recommended DNS Records** (using Plesk or customer DNS provider):

```dns
# Apex domain (A record for IPv4, AAAA for IPv6)
www.kunde1.de.       3600  IN  A      <VPS_IPv4>
www.kunde1.de.       3600  IN  AAAA   <VPS_IPv6>

# Admin subdomain
admin.kunde1.de.     3600  IN  A      <VPS_IPv4>
admin.kunde1.de.     3600  IN  AAAA   <VPS_IPv6>

# API subdomain
api.kunde1.de.       3600  IN  A      <VPS_IPv4>
api.kunde1.de.       3600  IN  AAAA   <VPS_IPv6>
```

**Alternative (CNAME for subdomains)**:
```dns
# If you prefer CNAME for subdomains (points to apex)
admin.kunde1.de.     3600  IN  CNAME  www.kunde1.de.
api.kunde1.de.       3600  IN  CNAME  www.kunde1.de.

# Note: Apex (www) must still use A/AAAA records
www.kunde1.de.       3600  IN  A      <VPS_IPv4>
www.kunde1.de.       3600  IN  AAAA   <VPS_IPv6>
```

**Plesk Configuration**:
1. Log in to Plesk: https://plesk.your-dns-server.com/
2. Navigate to: Domains → kunde1.de → DNS Settings
3. Add DNS records as shown above
4. Click "Apply" or "Update"
5. Wait for DNS propagation (typically 5-30 minutes, up to 24 hours for global propagation)

**Important Notes**:
- Use **lowercase** domain names (DNS is case-insensitive, but consistency helps)
- **NO trailing dot** in application configs (Plesk DNS editor may require it, but app does not)
- TTL 3600 (1 hour) is a safe default for production
- For testing, use TTL 300 (5 minutes) to allow faster changes

**Validation**:
```bash
# Check DNS propagation (wait 5-30 minutes after DNS changes)
dig www.kunde1.de A
dig www.kunde1.de AAAA
dig admin.kunde1.de A
dig api.kunde1.de A

# All should resolve to VPS IP addresses

# Quick test from multiple locations
nslookup www.kunde1.de 8.8.8.8  # Google DNS
nslookup www.kunde1.de 1.1.1.1  # Cloudflare DNS
```

**Pre-DNS Testing Option**: You can proceed with Steps 3-5 and use `pms_customer_vps_verify.sh` with `SERVER_IP` parameter to bypass DNS propagation (see Step 6).

#### Step 3: Install Coolify on Customer VPS (HOST-SERVER-TERMINAL)

**Goal**: Install Coolify as the deployment platform on the customer VPS.

**SSH to Customer VPS**:
```bash
ssh root@<VPS_IPv4>
```

**Install Coolify** (one-line installer):
```bash
# Run Coolify installer (official script)
curl -fsSL https://cdn.coollabs.io/coolify/install.sh | bash

# Installation takes 5-10 minutes
# Follow prompts (typically defaults are OK)
```

**Post-Installation**:
```bash
# Verify Coolify is running
docker ps | grep coolify
# Should show multiple Coolify containers (coolify, postgres, redis, proxy)

# Get Coolify web UI URL
echo "Coolify UI: http://<VPS_IPv4>:8000"

# Get initial admin password (shown during installation, or reset if needed)
# First-time setup: Navigate to http://<VPS_IPv4>:8000 and create admin account
```

**Coolify Initial Setup** (COOLIFY UI):
1. Open browser: http://<VPS_IPv4>:8000
2. Create admin account (email + password)
3. Configure server settings:
   - **Server Name**: customer-vps-kunde1
   - **Server IP**: <VPS_IPv4>
   - **Wildcard Domain**: (leave empty for now)
4. Configure proxy (Traefik):
   - Coolify automatically installs Traefik as reverse proxy
   - No additional configuration needed at this stage

**Validation**:
```bash
# Check Coolify proxy (Traefik) is running
docker ps | grep coolify-proxy
# Should show coolify-proxy container

# Check Traefik logs (should show no errors)
docker logs coolify-proxy --tail 50

# Verify Docker network exists
docker network ls | grep coolify
# Should show 'coolify' network
```

**Important**: Record Coolify admin credentials securely (password manager recommended).

#### Step 4: Deploy Database Stack (COOLIFY UI + HOST-SERVER-TERMINAL)

**Goal**: Deploy Supabase (or standalone Postgres) for customer data.

**Option A: Supabase Self-Hosted (Recommended for Full Stack)**

**Coolify UI**:
1. Navigate to: Projects → Create New Project
2. Project Name: `kunde1-supabase`
3. Add Service → Docker Compose
4. Paste Supabase docker-compose.yml (from Supabase self-hosting docs)
5. Configure environment variables:
   - `POSTGRES_PASSWORD`: Generate strong password (save securely)
   - `JWT_SECRET`: Generate 32+ character secret (save securely)
   - `ANON_KEY`: Generate JWT with anon role (use Supabase JWT generator)
   - `SERVICE_ROLE_KEY`: Generate JWT with service_role (save securely)
6. Deploy → Start

**Option B: Standalone Postgres (Simpler, Database Only)**

**Coolify UI**:
1. Navigate to: Projects → Create New Project
2. Project Name: `kunde1-database`
3. Add Service → Postgres (from Coolify templates)
4. Configure:
   - `POSTGRES_DB`: `pms_kunde1`
   - `POSTGRES_USER`: `pms_user`
   - `POSTGRES_PASSWORD`: Generate strong password (save securely)
   - **Persistent Volume**: `/var/lib/postgresql/data` (ensure data persistence)
5. Deploy → Start

**Validation**:
```bash
# SSH to customer VPS
ssh root@<VPS_IPv4>

# For Supabase: Check all services are running
docker ps | grep supabase
# Should show: postgres, kong, auth, rest, realtime, storage, etc.

# For standalone Postgres: Check container is running
docker ps | grep postgres

# Test database connection
docker exec -it <postgres_container_id> psql -U <user> -d <database>
# Should connect successfully

# Run \dt to list tables (empty initially)
\dt

# Exit psql
\q
```

**Important**: 
- **Save DATABASE_URL** for backend configuration: `postgresql://<user>:<password>@<host>:<port>/<database>`
- **Save JWT_SECRET** (must match between Supabase/GoTrue and backend)
- For Supabase: Expose Kong API Gateway port (default 8000) internally only (no public access)

#### Step 5: Deploy Backend Stack (COOLIFY UI)

**Goal**: Deploy PMS backend API, worker, and admin UI on customer VPS.

**5a. Create Backend API Service (COOLIFY UI)**

1. Navigate to: Projects → Create New Project
2. Project Name: `kunde1-pms-backend`
3. Add Service → Docker Image or Git Repository
   - **Source**: Git repository (your PMS backend repo)
   - **Branch**: main (or specific release tag)
   - **Dockerfile Path**: `backend/Dockerfile`
4. Configure Domains:
   - Click "Add Domain"
   - Enter: `api.kunde1.de`
   - Enable HTTPS: ✅ (Let's Encrypt automatic)
5. Configure Traefik Labels (IMPORTANT):
   - Click "Labels" tab
   - Add custom label:
     - **Key**: `traefik.docker.network`
     - **Value**: `coolify`
   - Verify Host rule (auto-generated by Coolify):
     - `traefik.http.routers.<service>.rule=Host(\`api.kunde1.de\`)`
   - **Note**: Backticks in Host() rule are critical (not single quotes)
6. Configure Environment Variables:
   ```bash
   # Database
   DATABASE_URL=postgresql://pms_user:<password>@<postgres_host>:5432/pms_kunde1
   
   # JWT/Auth (must match Supabase/GoTrue)
   SUPABASE_JWT_SECRET=<jwt_secret_from_supabase>
   JWT_SECRET=<jwt_secret_from_supabase>
   JWT_AUDIENCE=authenticated
   
   # Redis/Celery
   REDIS_URL=redis://<redis_host>:6379/0
   CELERY_BROKER_URL=redis://<redis_host>:6379/0
   CELERY_RESULT_BACKEND=redis://<redis_host>:6379/1
   
   # Proxy Configuration
   TRUST_PROXY_HEADERS=true
   
   # Host Allowlist (IMPORTANT)
   ALLOWED_HOSTS=api.kunde1.de,admin.kunde1.de,www.kunde1.de
   
   # CORS Configuration
   CORS_ALLOWED_ORIGINS=https://www.kunde1.de,https://admin.kunde1.de
   
   # Environment
   ENVIRONMENT=production
   SOURCE_COMMIT=<git_commit_sha>  # Optional but recommended for tracking
   
   # Feature Flags (if applicable)
   MODULES_ENABLED=true
   ```
7. Deploy → Start
8. Monitor deployment logs (Coolify UI → Logs tab)

**5b. Create Worker Service (COOLIFY UI)**

1. Projects → kunde1-pms-backend → Add Service
2. **Source**: Same Git repository as backend
   - **Branch**: main
   - **Dockerfile Path**: `backend/Dockerfile` (or `backend/worker.Dockerfile` if separate)
   - **Command Override**: `celery -A app.celery_app worker -l info`
3. Configure Environment Variables:
   - Use **same environment variables** as backend API
   - No domain configuration needed (worker is internal only)
4. **No Traefik labels** needed (worker doesn't serve HTTP traffic)
5. Deploy → Start

**5c. Create Admin UI Service (COOLIFY UI)**

1. Projects → kunde1-pms-backend → Add Service (or separate project)
2. **Source**: Git repository (frontend/admin)
   - **Branch**: main
   - **Dockerfile Path**: `frontend/Dockerfile` (or appropriate path)
3. Configure Domains:
   - Add Domain: `admin.kunde1.de`
   - Enable HTTPS: ✅
4. Configure Traefik Labels:
   - Add: `traefik.docker.network=coolify`
5. Configure Environment Variables:
   ```bash
   # API endpoint (points to backend)
   NEXT_PUBLIC_API_URL=https://api.kunde1.de
   NEXT_PUBLIC_SUPABASE_URL=https://api.kunde1.de  # or Supabase Kong URL
   NEXT_PUBLIC_SUPABASE_ANON_KEY=<anon_key>
   
   # Environment
   NODE_ENV=production
   ```
6. Deploy → Start

**Validation**:
```bash
# Check all services are running
docker ps | grep kunde1
# Should show: backend, worker, admin

# Test backend health
curl https://api.kunde1.de/health
# Should return: {"status": "healthy"} or similar

# Test backend version
curl https://api.kunde1.de/api/v1/ops/version
# Should return JSON with service, source_commit, environment, etc.

# Test admin UI (browser)
# Open: https://admin.kunde1.de
# Should load admin interface (may show login screen)
```

**Common Failure: 503 Service Unavailable**

**Symptoms**: Curl returns 503 error or "no available server" message

**Possible Causes**:
1. **Invalid Traefik Host rule**: Backticks missing or escaped incorrectly
   - ✅ Correct: `Host(\`api.kunde1.de\`)`
   - ❌ Wrong: `Host('api.kunde1.de')` or `Host(api.kunde1.de)`
2. **Wrong Docker network**: Service not on `coolify` network
   - Fix: Add label `traefik.docker.network=coolify`
3. **Service not running**: Container crashed or failed to start
   - Check: `docker ps -a | grep kunde1`
   - Check logs: `docker logs <container_id>`
4. **Wrong port exposed**: Traefik trying to reach wrong service port
   - Fix: Add label `traefik.http.services.<service>.loadbalancer.server.port=8000`
5. **Firewall blocking**: VPS firewall blocking traffic
   - Check: `ufw status` (if UFW enabled)
   - Allow: `ufw allow 80/tcp && ufw allow 443/tcp`

**Troubleshooting Checklist**:
```bash
# 1. Verify Traefik can see the service
docker logs coolify-proxy 2>&1 | grep -i "kunde1"
# Should show backend service registration

# 2. Check service is on coolify network
docker inspect <container_id> | grep -i network
# Should include "coolify" network

# 3. Test service directly (bypass Traefik)
docker exec -it <backend_container> curl localhost:8000/health
# Should return healthy response

# 4. Check Traefik configuration
docker exec coolify-proxy cat /etc/traefik/traefik.toml
# Verify providers.docker.network = "coolify"

# 5. Restart Traefik if needed
docker restart coolify-proxy
```

#### Step 6: Run Database Migrations (HOST-SERVER-TERMINAL)

**Goal**: Apply database schema migrations to create tables and seed data.

**SSH to Customer VPS**:
```bash
ssh root@<VPS_IPv4>

# Get backend container ID
docker ps | grep kunde1.*backend
BACKEND_CONTAINER=<container_id>
```

**Apply Migrations** (using Alembic or custom migration tool):

**Option A: Alembic Migrations**:
```bash
# Run migrations inside backend container
docker exec -it $BACKEND_CONTAINER alembic upgrade head

# Verify migrations applied
docker exec -it $BACKEND_CONTAINER alembic current
# Should show current revision
```

**Option B: SQL Migrations (Supabase SQL Editor)**:

1. Navigate to Supabase UI (if using Supabase):
   - URL: http://<VPS_IPv4>:8000 (Supabase Kong Gateway, internal only)
   - Or use Supabase Studio if deployed
2. SQL Editor → New Query
3. Copy/paste migration files from `supabase/migrations/` (in order)
4. Execute each migration file
5. Verify tables created:
   ```sql
   SELECT table_name FROM information_schema.tables 
   WHERE table_schema = 'public' 
   ORDER BY table_name;
   ```

**Option C: Manual Psql Migration**:
```bash
# Copy migration files to VPS
scp -r supabase/migrations/ root@<VPS_IPv4>:/tmp/

# SSH to VPS and run migrations
ssh root@<VPS_IPv4>

# Find postgres container
POSTGRES_CONTAINER=$(docker ps | grep postgres | awk '{print $1}')

# Apply migrations in order
for file in /tmp/migrations/*.sql; do
    echo "Applying $file..."
    docker exec -i $POSTGRES_CONTAINER psql -U <user> -d <database> < "$file"
done

# Verify schema
docker exec -it $POSTGRES_CONTAINER psql -U <user> -d <database> -c '\dt'
```

**Validation**:
```bash
# Check critical tables exist
docker exec -it $POSTGRES_CONTAINER psql -U <user> -d <database> -c "
SELECT table_name FROM information_schema.tables 
WHERE table_schema = 'public' AND table_name IN (
    'agencies', 'users', 'properties', 'bookings', 
    'guests', 'audit_log', 'agency_domains'
);"
# Should return all critical tables
```

#### Step 7: Bootstrap Single-Tenant Data (SUPABASE SQL EDITOR or HOST-TERMINAL)

**Goal**: Create initial agency, admin user, and optional seed data for customer.

**Important**: Single-tenant deployment means **one agency per VPS**. The multi-tenant code remains enabled, but operationally there's only one agency.

**Bootstrap SQL** (run in Supabase SQL Editor or psql):

```sql
-- 1. Create customer agency
INSERT INTO agencies (id, name, created_at, updated_at)
VALUES (
    gen_random_uuid(),  -- Or use specific UUID for tracking
    'Kunde1 GmbH',      -- Customer company name
    NOW(),
    NOW()
)
RETURNING id;  -- Save this agency_id for next steps

-- Record agency_id from above (e.g., 'a1b2c3d4-...')

-- 2. Create admin user for customer
INSERT INTO users (id, agency_id, email, role, first_name, last_name, created_at, updated_at)
VALUES (
    gen_random_uuid(),
    '<agency_id_from_step1>',
    'admin@kunde1.de',
    'admin',
    'Admin',
    'Kunde1',
    NOW(),
    NOW()
);

-- 3. (Optional) Map customer domain to agency
INSERT INTO agency_domains (agency_id, domain)
VALUES 
    ('<agency_id>', 'api.kunde1.de'),
    ('<agency_id>', 'admin.kunde1.de'),
    ('<agency_id>', 'www.kunde1.de')
ON CONFLICT (domain) DO NOTHING;

-- 4. (Optional) Seed a test property
INSERT INTO properties (id, agency_id, name, address, max_guests, created_at, updated_at)
VALUES (
    gen_random_uuid(),
    '<agency_id>',
    'Test Property - Kunde1',
    'Teststraße 1, 12345 Berlin',
    4,
    NOW(),
    NOW()
);

-- Verify bootstrap
SELECT id, name FROM agencies;
SELECT id, email, role FROM users WHERE agency_id = '<agency_id>';
SELECT id, name FROM properties WHERE agency_id = '<agency_id>';
```

**Alternative: Bootstrap Script** (if available):
```bash
# SSH to VPS
ssh root@<VPS_IPv4>

# Run bootstrap script (if exists in your repo)
docker exec -it $BACKEND_CONTAINER python scripts/bootstrap_tenant.py \
    --agency-name "Kunde1 GmbH" \
    --admin-email "admin@kunde1.de" \
    --admin-password "<secure_password>"

# Script should output agency_id and admin credentials
```

**Validation**:
```sql
-- Verify agency exists
SELECT * FROM agencies WHERE name = 'Kunde1 GmbH';

-- Verify admin user exists
SELECT * FROM users WHERE email = 'admin@kunde1.de';

-- Verify domain mapping (if used)
SELECT * FROM agency_domains WHERE domain LIKE '%kunde1.de%';
```

**Important**: 
- **Save agency_id** for verification step
- **Save admin credentials** securely (share with customer via secure channel)
- If using Supabase Auth, create user in GoTrue as well (via Supabase UI or API)

#### Step 8: Configure SSL/TLS Certificates (COOLIFY UI - Automatic)

**Goal**: Ensure all customer domains have valid HTTPS certificates.

**Coolify Automatic Certificate Provisioning**:

Coolify automatically provisions Let's Encrypt certificates for all domains configured in Step 5. No manual intervention required if:
1. DNS is properly configured (domains resolve to VPS IP)
2. Ports 80 and 443 are open on VPS firewall
3. Domains are added to Coolify service configuration with HTTPS enabled

**Validation**:
```bash
# Test HTTPS for all customer domains
curl -I https://api.kunde1.de
curl -I https://admin.kunde1.de
curl -I https://www.kunde1.de

# All should return HTTP 200 or 30x (not certificate errors)

# Check certificate details
echo | openssl s_client -connect api.kunde1.de:443 -servername api.kunde1.de 2>/dev/null | openssl x509 -noout -dates -issuer
# Should show Let's Encrypt issuer and valid dates
```

**Manual Certificate Check (Coolify UI)**:
1. Navigate to: Project → Service → Domains tab
2. Each domain should show: ✅ HTTPS Enabled, Certificate Valid
3. Expiry date should be ~90 days from now (Let's Encrypt default)

**Troubleshooting Certificate Issues**:

**Symptom**: Certificate provisioning fails or self-signed certificate error

**Possible Causes**:
1. **DNS not propagated**: Let's Encrypt ACME challenge fails
   - Fix: Wait for DNS TTL expiry (1-24 hours) or use lower TTL
   - Verify: `dig api.kunde1.de` should resolve to VPS IP
2. **Port 80 blocked**: ACME HTTP-01 challenge requires port 80
   - Fix: Ensure VPS firewall allows port 80 (ufw allow 80/tcp)
3. **Domain not added in Coolify**: Certificate not requested
   - Fix: Add domain in Coolify service config, enable HTTPS
4. **Rate limit hit**: Let's Encrypt has rate limits (50 certs per domain per week)
   - Fix: Wait 1 week or use staging environment for testing

**Certificate Renewal**:
- Let's Encrypt certificates expire after 90 days
- Coolify automatically renews certificates at 60 days (no manual intervention)
- Monitor certificate expiry via monitoring tools (UptimeRobot, Pingdom)

#### Step 9: Verification (HOST-SERVER-TERMINAL or LOCAL)

**Goal**: Verify customer VPS is ready for production traffic using automated verification script.

**Run Verification Script**:

**Option A: Post-DNS Verification** (after DNS propagation):
```bash
# From your local machine or CI/CD
API_BASE_URL=https://api.kunde1.de \
./backend/scripts/pms_customer_vps_verify.sh
```

**Option B: Pre-DNS Verification** (before DNS propagation, recommended during setup):
```bash
# Use direct IP to bypass DNS
API_BASE_URL=https://api.kunde1.de \
SERVER_IP=<VPS_IPv4> \
./backend/scripts/pms_customer_vps_verify.sh
```

**Option C: Full Verification** (with commit check and CORS):
```bash
API_BASE_URL=https://api.kunde1.de \
ADMIN_BASE_URL=https://admin.kunde1.de \
PUBLIC_BASE_URL=https://www.kunde1.de \
EXPECT_COMMIT=caabb0b \
TEST_ORIGIN=https://www.kunde1.de \
./backend/scripts/pms_customer_vps_verify.sh
```

**Expected Output** (Success):
```
ℹ Verifying customer VPS deployment: https://api.kunde1.de

ℹ Check 1/5: GET /health (liveness)
✅ Health check passed (HTTP 200)

ℹ Check 2/5: GET /health/ready (readiness)
✅ Readiness check passed (HTTP 200)

ℹ Check 3/5: GET /api/v1/ops/version (deployment metadata)
✅ Version endpoint accessible
ℹ   Environment: production
ℹ   API Version: 0.1.0
ℹ   Source Commit: caabb0b...
✅ Source commit matches expected prefix: caabb0b

ℹ Check 4/5: Public router preflight
✅ Public router mounted (endpoint returned HTTP 422, not 404)

ℹ Check 5/5: CORS preflight (Origin: https://www.kunde1.de)
✅ CORS preflight passed (Allow-Origin: https://www.kunde1.de)

✅ All checks passed - customer VPS is ready for production traffic
ℹ Admin UI: https://admin.kunde1.de
ℹ Public Site: https://www.kunde1.de
```

**If Verification Fails**:
- Review error messages in script output (actionable hints provided)
- Check troubleshooting sections above for specific error codes
- Common issues: DNS not propagated, CORS misconfiguration, missing env vars

**Optional: Run Smoke Tests** (additional validation):
```bash
# Set up environment for smoke tests
export HOST=https://api.kunde1.de
export JWT_TOKEN=<admin_jwt_token>  # Generate via auth endpoint
export AGENCY_ID=<agency_id_from_bootstrap>

# Run public booking smoke test
./backend/scripts/pms_direct_booking_public_smoke.sh

# Run pricing quote smoke test (if pricing module enabled)
./backend/scripts/pms_pricing_quote_smoke.sh
```

#### Step 10: Customer Handoff (DOCUMENTATION)

**Goal**: Provide customer with access credentials and documentation.

**Handoff Package** (secure delivery via encrypted email or password manager):

1. **Access URLs**:
   - Admin UI: https://admin.kunde1.de
   - API Docs: https://api.kunde1.de/docs (FastAPI auto-generated docs)
   - Health Check: https://api.kunde1.de/health (for customer monitoring)

2. **Admin Credentials**:
   - Email: admin@kunde1.de
   - Password: <secure_password_from_bootstrap>
   - Role: admin (full access)

3. **Database Access** (optional, only if customer needs direct access):
   - Host: <VPS_IP> (not publicly exposed, VPN or SSH tunnel required)
   - Database: pms_kunde1
   - User: pms_user
   - Password: <postgres_password>
   - **Security Note**: Database should NOT be publicly accessible

4. **VPS Access** (optional, only for technical customers):
   - SSH: root@<VPS_IP> (add customer SSH key if requested)
   - Coolify UI: http://<VPS_IP>:8000 (create separate Coolify admin if needed)

5. **Support Contacts**:
   - Technical Support: support@your-company.com
   - Emergency Hotline: +49 xxx xxx xxxx (if applicable)

6. **Documentation Links**:
   - API Documentation: https://api.kunde1.de/docs
   - Admin User Guide: [Link to customer-facing docs]
   - FAQ: [Link to FAQ]

**Post-Handoff Checklist**:
- ✅ Customer can log in to admin UI
- ✅ Customer can create test booking (if applicable)
- ✅ Customer understands how to add properties/users
- ✅ Customer has emergency contact info
- ✅ Monitoring set up for customer VPS (see Step 11)

#### Step 11: Monitoring and Maintenance (OPTIONAL)

**Goal**: Set up monitoring and establish maintenance schedule for customer VPS.

**Uptime Monitoring** (recommended):
- Use UptimeRobot, Pingdom, or similar service
- Monitor endpoints:
  - https://api.kunde1.de/health (should return 200)
  - https://admin.kunde1.de (should return 200)
  - https://www.kunde1.de (should return 200)
- Alert on: HTTP 500/503 errors, downtime >5 minutes, certificate expiry

**Resource Monitoring** (Coolify built-in):
- Coolify UI → Server → Metrics
- Monitor: CPU usage, RAM usage, disk space
- Set up alerts: >80% CPU for 10+ minutes, >90% RAM, <10% disk free

**Certificate Expiry Monitoring**:
- Let's Encrypt certificates auto-renew at 60 days
- Set up alert at 30 days (if auto-renewal fails)
- Check manually: `openssl s_client -connect api.kunde1.de:443 -servername api.kunde1.de | openssl x509 -noout -dates`

**Backup Strategy**:
- Database backups (automated):
  - Coolify can enable automatic Postgres backups
  - Configure: Project → Service → Backups tab
  - Retention: 7 daily, 4 weekly, 12 monthly (adjust based on customer SLA)
- VPS snapshots (Hetzner):
  - Hetzner Cloud → Server → Snapshots
  - Create weekly snapshot (manual or via Hetzner API)
  - Keep 4 snapshots (1 month history)

**Update Schedule**:
- **Security Patches**: Apply within 7 days of release (OS + Docker images)
- **Application Updates**: Monthly or quarterly (coordinate with customer)
- **Database Migrations**: Test in staging before applying to production

**Maintenance Window**:
- Recommended: Weekly maintenance window (e.g., Sunday 2-4 AM local time)
- Notify customer 48 hours in advance for major updates
- Use Coolify zero-downtime deployments where possible

### Rollback Procedure

**If deployment fails or needs to be reverted**:

**1. Revert Backend Deployment (COOLIFY UI)**:
- Navigate to: Project → Service → Deployments tab
- Click "Rollback" next to previous successful deployment
- Wait for rollback to complete (2-5 minutes)
- Verify: `curl https://api.kunde1.de/api/v1/ops/version` (should show old commit)

**2. Revert Database Migrations (HOST-SERVER-TERMINAL)**:
```bash
# SSH to VPS
ssh root@<VPS_IPv4>

# Downgrade migrations (Alembic)
docker exec -it $BACKEND_CONTAINER alembic downgrade -1

# Or restore from database backup
docker exec -i $POSTGRES_CONTAINER psql -U <user> -d <database> < /backups/backup_YYYYMMDD.sql
```

**3. Revert Environment Variables (COOLIFY UI)**:
- Project → Service → Environment tab
- Restore previous values (Coolify keeps history)
- Redeploy service

**4. Revert DNS (if needed)**:
- Plesk → Domains → DNS Settings
- Remove or update DNS records
- Wait for TTL expiry (or flush local DNS cache)

**5. Remove VPS (if catastrophic failure)**:
- Hetzner Cloud → Server → Delete
- Update DNS to remove customer domains
- Notify customer and reschedule deployment

**Rollback is safe and tested** - practice rollback procedure during staging deployments.

### Troubleshooting Guide

#### Problem: Backend Returns 403 "Host not allowed"

**Symptom**: API requests return HTTP 403 with `{"detail": "Host not allowed: api.kunde1.de"}`

**Root Cause**: Customer domain not in ALLOWED_HOSTS environment variable

**Solution**:
1. Coolify UI → Project → Backend Service → Environment tab
2. Update ALLOWED_HOSTS: `api.kunde1.de,admin.kunde1.de,www.kunde1.de`
3. Redeploy service (Coolify auto-restarts on ENV change)
4. Verify: `curl https://api.kunde1.de/health` (should return 200)

#### Problem: CORS Errors in Browser Console

**Symptom**: Browser shows "CORS policy blocked" errors when accessing API from admin UI

**Root Cause**: Admin UI origin not in CORS_ALLOWED_ORIGINS

**Solution**:
1. Coolify UI → Backend Service → Environment tab
2. Update CORS_ALLOWED_ORIGINS: `https://admin.kunde1.de,https://www.kunde1.de`
3. Ensure TRUST_PROXY_HEADERS=true (for correct Origin detection)
4. Redeploy service
5. Test CORS: `curl -X OPTIONS https://api.kunde1.de/api/v1/public/booking-requests -H "Origin: https://admin.kunde1.de" -i`

#### Problem: Database Connection Refused

**Symptom**: Backend logs show "Connection refused" or "Database unavailable"

**Root Cause**: DATABASE_URL incorrect or database container not running

**Solution**:
1. Verify database container is running: `docker ps | grep postgres`
2. If stopped, start via Coolify UI or `docker start <container_id>`
3. Check DATABASE_URL format: `postgresql://user:password@host:5432/database`
4. For internal Docker networking, use container name as host (e.g., `postgres-kunde1`)
5. Test connection: `docker exec -it $BACKEND_CONTAINER curl postgres-kunde1:5432` (should connect)

#### Problem: Worker Not Processing Jobs

**Symptom**: Celery tasks stuck in pending state, never processed

**Root Cause**: Worker container not running or Redis connection failed

**Solution**:
1. Check worker container: `docker ps | grep worker`
2. Check worker logs: `docker logs <worker_container_id>`
3. Verify REDIS_URL in worker environment matches backend
4. Verify Redis container running: `docker ps | grep redis`
5. Restart worker: Coolify UI → Worker Service → Restart

#### Problem: Let's Encrypt Certificate Provisioning Fails

**Symptom**: HTTPS returns self-signed certificate error or ERR_CERT_AUTHORITY_INVALID

**Root Cause**: ACME challenge failed (DNS not propagated, port 80 blocked, rate limit)

**Solution**:
1. Verify DNS propagation: `dig api.kunde1.de` (should resolve to VPS IP)
2. Verify port 80 open: `curl http://api.kunde1.de/.well-known/acme-challenge/test` (should not timeout)
3. Check Coolify proxy logs: `docker logs coolify-proxy | grep -i acme`
4. If rate limited, wait 1 week or use Let's Encrypt staging for testing
5. Manual retry: Coolify UI → Service → Domains → Re-provision Certificate

### Related Scripts

- **pms_customer_vps_verify.sh**: Automated verification script for customer VPS deployments
- **pms_verify_deploy.sh**: General deployment verification (commit matching, module checks)
- **pms_direct_booking_public_smoke.sh**: Public booking flow smoke test
- **pms_pricing_quote_smoke.sh**: Pricing quote smoke test

### Related Documentation

- [Customer Domain Onboarding SOP](runbook.md#customer-domain-onboarding-sop) - For multi-tenant domain mapping
- [Database Schema](../../supabase/migrations/) - Database structure and migrations
- [API Documentation](../../backend/app/api/) - API endpoints and business logic
- [Project Status](../project_status.md) - Implementation status and verification criteria

### Maintenance

**Regular Tasks**:
- Weekly: Check uptime monitoring alerts
- Monthly: Review resource usage (CPU, RAM, disk)
- Quarterly: Review and rotate database backups
- Annually: Review and update SSL/TLS certificates (auto-renew, but verify)

**Security Hardening** (recommended post-deployment):
- Enable UFW firewall: `ufw enable && ufw allow 22,80,443/tcp`
- Disable root SSH login (use key-based auth only)
- Set up fail2ban for SSH brute-force protection
- Regular security updates: `apt update && apt upgrade -y`
- Database access: Only via SSH tunnel (no public exposure)

**Cost Optimization**:
- Monitor VPS usage: Right-size VPS based on actual load
- Consider reserved instances for long-term customers (Hetzner discounts)
- Archive old backups to object storage (Hetzner S3-compatible storage)


---

## Owner Portal O1

**Overview:** Read-only owner portal MVP with staff tools for owner profile management and property assignment.

**Purpose:** Allow property owners to view their properties and bookings through authenticated web UI, while staff (manager/admin) can create owner profiles and assign properties to owners.

**Architecture:**
- **Database**: `owners` table maps Supabase auth.users to owner profiles with agency scoping
- **RBAC**: 
  - Staff endpoints require manager/admin role (via `require_roles("manager", "admin")`)
  - Owner endpoints require `get_current_owner()` dependency (verifies auth_user_id + agency_id + is_active)
- **Property Ownership**: `properties.owner_id` FK references `owners.id`
- **Tenant Isolation**: All queries scoped by agency_id from JWT claims

**UI Routes:**
- `/owner` - Owner portal page (owner-only, lists properties + bookings)

**API Endpoints:**

Staff (manager/admin):
- `GET /api/v1/owners?active=&limit=&offset=` - List owner profiles
- `POST /api/v1/owners` - Create owner profile (requires auth_user_id)
- `PATCH /api/v1/owners/{id}` - Update owner profile (email, names, is_active)
- `PATCH /api/v1/properties/{id}/owner` - Assign/unassign property owner

Owner-only:
- `GET /api/v1/owner/properties?limit=&offset=` - List owned properties
- `GET /api/v1/owner/bookings?property_id=&limit=&offset=` - List bookings for owned properties

**Database Tables:**
- `owners` - Owner profiles mapped to auth users with agency scoping
- `properties.owner_id` - FK to owners.id (NULL = agency-owned, non-NULL = owner-assigned)

**Migration:** `20260109000000_add_owners_table.sql`

**Verification Commands:**

```bash
# [HOST-SERVER-TERMINAL] Pull latest code
cd /data/repos/pms-webapp
git fetch origin main && git reset --hard origin/main

# [HOST-SERVER-TERMINAL] Optional: Verify deploy after Coolify redeploy
export API_BASE_URL="https://api.fewo.kolibri-visions.de"
./backend/scripts/pms_verify_deploy.sh

# [HOST-SERVER-TERMINAL] Run owner portal smoke test
export HOST="https://api.fewo.kolibri-visions.de"
export MANAGER_JWT_TOKEN="<<<manager/admin JWT>>>"
export OWNER_JWT_TOKEN="<<<owner user JWT>>>"
export OWNER_AUTH_USER_ID="<<<Supabase auth.users.id for owner>>>"
# Optional:
# export PROPERTY_ID="23dd8fda-59ae-4b2f-8489-7a90f5d46c66"
# export AGENCY_ID="ffd0123a-10b6-40cd-8ad5-66eee9757ab7"
./backend/scripts/pms_owner_portal_smoke.sh
echo "rc=$?"

# Expected output: All 5 tests pass, rc=0
```

**Common Issues:**

### Tenant Resolution and x-agency-id Header

**Symptom:** API returns errors like "Missing agency_id in token claims" or "Cannot resolve agency: user has N agency memberships".

**Root Cause:** Standard Supabase JWTs don't contain `agency_id` claim. API uses robust tenant resolution with fallback chain.

**Tenant Resolution Order:**
1. **JWT claim** `agency_id` (if present, used directly)
2. **x-agency-id header** (validated via team_members membership check)
3. **Auto-detect** (if user has exactly one agency membership)
4. **Error 400** (if user has 0 or multiple memberships without explicit header)

**How to Use x-agency-id Header:**

For users with multiple agency memberships or when JWT doesn't contain agency_id:

```bash
# Get user's agency memberships
psql $DATABASE_URL -c "SELECT agency_id, role FROM team_members WHERE user_id = '<auth_user_id>';"

# Include x-agency-id header in API requests
curl -X GET "$HOST/api/v1/owners" \
  -H "Authorization: Bearer $JWT_TOKEN" \
  -H "x-agency-id: <agency_uuid>"
```

**Smoke Script Usage:**

The `pms_owner_portal_smoke.sh` script automatically:
- Derives AGENCY_ID from PROPERTY_ID (fetches property details)
- Includes x-agency-id header in manager calls when AGENCY_ID is set
- Falls back to auto-detection if AGENCY_ID not set (works for single-agency users)

To manually set AGENCY_ID:
```bash
export AGENCY_ID="ffd0123a-10b6-40cd-8ad5-66eee9757ab7"
./backend/scripts/pms_owner_portal_smoke.sh
```

**Troubleshooting:**
- **"Cannot resolve agency: user has N agency memberships"**: Set x-agency-id header or export AGENCY_ID for smoke script
- **403 "not a member of agency"**: x-agency-id header provided but user is not in team_members for that agency
- **403 "not a member" on owner routes with x-agency-id**: Fixed in 2026-01-09. Owner routes now check both owners table AND team_members. Ensure user has active owner profile in that agency (owners.auth_user_id = JWT sub, owners.is_active = true).

- **Auto-detect works**: User has exactly one agency membership, no header needed

### Owner Endpoints Return 403 (Not Registered)

**Symptom:** Owner user gets 403 Forbidden when accessing `/api/v1/owner/properties` or `/api/v1/owner/bookings`. Error message: "Access denied: user is not registered as an owner".

**Root Cause:** User's auth_user_id (JWT sub) is not mapped to an owner profile in the `owners` table.

**How to Debug:**
```bash
# Check JWT sub claim (auth_user_id)
echo $OWNER_JWT_TOKEN | cut -d'.' -f2 | base64 -d | jq '.sub'
# Example output: "550e8400-e29b-41d4-a716-446655440000"

# Check if owner profile exists
psql $DATABASE_URL -c "SELECT id, auth_user_id, is_active FROM owners WHERE auth_user_id = '550e8400-e29b-41d4-a716-446655440000';"

# Check agency_id matches JWT claim
echo $OWNER_JWT_TOKEN | cut -d'.' -f2 | base64 -d | jq '.agency_id'
```

**Solution:**
- Have a manager/admin create owner profile: `POST /api/v1/owners` with `auth_user_id` matching JWT sub
- Ensure `is_active=true` in owners table: `UPDATE owners SET is_active = true WHERE auth_user_id = '...'`
- Verify JWT contains correct `agency_id` claim matching owner's agency

### Owner Sees No Properties (Empty List)

**Symptom:** Owner successfully accesses `/api/v1/owner/properties` but receives empty array `[]`.

**Root Cause:** No properties have `owner_id` set to this owner's ID.

**How to Debug:**
```bash
# Get owner ID from auth_user_id
psql $DATABASE_URL -c "SELECT id FROM owners WHERE auth_user_id = '550e8400-e29b-41d4-a716-446655440000';"
# Example output: "660e8400-e29b-41d4-a716-446655440001"

# Check if any properties assigned
psql $DATABASE_URL -c "SELECT id, name, owner_id FROM properties WHERE owner_id = '660e8400-e29b-41d4-a716-446655440001';"
```

**Solution:**
- Have manager/admin assign property: `PATCH /api/v1/properties/{property_id}/owner` with `{"owner_id": "660e8400-..."}`
- Verify property belongs to same agency as owner

### Staff Endpoint Accessible by Owners (RBAC Bypass)

**Symptom:** Owner can access `GET /api/v1/owners` (should return 403).

**Root Cause:** Missing `require_roles("manager", "admin")` dependency on staff endpoints.

**How to Debug:**
```bash
# Test staff endpoint with owner token
curl -X GET "$HOST/api/v1/owners?limit=10" \
  -H "Authorization: Bearer $OWNER_JWT_TOKEN"

# Should return 403, not 200
```

**Solution:**
- Ensure all staff endpoints use `_role_check=Depends(require_roles("manager", "admin"))`
- Verify JWT role claim: `echo $OWNER_JWT_TOKEN | cut -d'.' -f2 | base64 -d | jq '.role'` (should be "owner", not "manager"/"admin")

### Owner Can See Other Owners' Properties

**Symptom:** Owner sees properties not assigned to them.

**Root Cause:** Missing `WHERE properties.owner_id = $owner_id` filter in owner endpoints.

**How to Debug:**
```bash
# Check query in backend/app/api/routes/owners.py
# Line ~335: list_owner_properties query must filter by owner_id

# Verify properties returned match owner_id
curl -X GET "$HOST/api/v1/owner/properties" \
  -H "Authorization: Bearer $OWNER_JWT_TOKEN" | jq '.[].id'

psql $DATABASE_URL -c "SELECT owner_id FROM properties WHERE id IN (...);"
```

**Solution:**
- Verify query filters: `WHERE owner_id = $1 AND agency_id = $2`
- Ensure get_current_owner() correctly returns owner profile with ID

### Migration Fails (FK Constraint Violation)

**Symptom:** Migration `20260109000000_add_owners_table.sql` fails with FK constraint error on `properties.owner_id`.

**Root Cause:** Existing `properties.owner_id` values reference `auth.users.id` but migration tries to FK to `owners.id`.

**How to Debug:**
```bash
# Check existing owner_id values
psql $DATABASE_URL -c "SELECT id, owner_id FROM properties WHERE owner_id IS NOT NULL LIMIT 10;"

# Check if those owner_id values exist in auth.users
psql $DATABASE_URL -c "SELECT id FROM auth.users WHERE id IN (...);"
```

**Solution:**
- Before adding FK, clear invalid owner_id values: `UPDATE properties SET owner_id = NULL WHERE owner_id NOT IN (SELECT id FROM owners);`
- Or manually migrate existing auth.users to owners table first
- Then re-run migration to add FK constraint

### Owner Bookings Schema Drift (total_price_cents Missing)

**Symptom:** GET /api/v1/owner/bookings returns 503 with error "column b.total_price_cents does not exist" or "UndefinedColumn: column does not exist".

**Root Cause:** Schema drift - migration `20260109000002_add_bookings_total_price_cents.sql` not applied. Owner bookings endpoint queries `bookings.total_price_cents` which is missing from schema.

**How to Debug:**
```bash
# Check if column exists
psql $DATABASE_URL -c "\d bookings" | grep total_price_cents

# Or query information_schema
psql $DATABASE_URL -c "SELECT column_name FROM information_schema.columns WHERE table_name = 'bookings' AND column_name = 'total_price_cents';" 
```

**Solution:**
Apply the missing migration. **IMPORTANT:** Supabase tables are owned by `supabase_admin`, not `postgres`. Running psql as postgres may fail with "must be owner of table bookings".

```bash
# Using docker exec with supabase_admin user (recommended for Supabase)
docker exec -i <supabase-db-container> psql -U supabase_admin -d postgres -v ON_ERROR_STOP=1 < supabase/migrations/20260109000002_add_bookings_total_price_cents.sql

# Or use Supabase CLI (if available)
supabase db push

# Or manual ALTER (if column already exists, this is harmless)
psql $DATABASE_URL -c "ALTER TABLE bookings ADD COLUMN IF NOT EXISTS total_price_cents BIGINT;"
psql $DATABASE_URL -c "COMMENT ON COLUMN bookings.total_price_cents IS 'Total booking price in cents (snapshot for owner portal / reporting).';"
```

**Note:** Migration uses `IF NOT EXISTS` - safe to run even if column already exists (e.g., from manual hotfix).




### 503 Service Unavailable (Transient Database Disconnect)

**Symptom:** API returns 503 with message "Database connection temporarily lost. Please retry." or "Database temporarily unavailable. Please retry."

**Root Cause:** Transient database connection drops during request processing. Common causes:
- Network interruption between API and database
- Database server restart or failover
- Connection pool churn (old connections closed, new ones not yet ready)
- Database under heavy load (temporary connection timeout)

**Architecture:** As of Phase 3, transient DB errors (ConnectionDoesNotExistError, InterfaceError) are caught and mapped to HTTP 503 instead of 500. This prevents false positives in error monitoring and signals clients to retry.

**How to Debug:**

1. **Check request ID in logs**:
   ```bash
   # Smoke script automatically includes X-Request-Id header: smoke-test-{manager|owner}-{pid}-{random}
   # Search backend logs for request ID
   docker logs pms-backend-container 2>&1 | grep "smoke-test-manager-12345-6789"
   ```

2. **Check for transient DB errors in logs**:
   ```bash
   # Look for ConnectionDoesNotExistError or InterfaceError
   docker logs pms-backend-container 2>&1 | grep -E "ConnectionDoesNotExistError|InterfaceError" | tail -20
   ```

3. **Verify database connectivity**:
   ```bash
   # Test direct connection to database
   psql $DATABASE_URL -c "SELECT 1;"
   
   # Check database server status
   docker ps | grep postgres
   
   # Check connection pool state via health endpoint
   curl "$HOST/health/db"
   ```

4. **Check timing pattern**:
   - **Single 503**: Likely transient network blip → Retry should succeed
   - **Frequent 503s**: Database overload or connectivity issue → Check DB metrics
   - **503 during tenant resolution**: DB query in get_current_agency_id or get_current_owner failed

**Solution:**

For clients:
- **Retry the request** (503 is designed to be safe to retry)
- **Exponential backoff**: Wait 1s, then 2s, then 4s between retries
- Smoke script already includes request IDs for debugging

For operators:
- **Check database health**: CPU, memory, connection count, slow queries
- **Check network**: Packet loss, latency between API and DB
- **Check pool settings**: May need to increase min_size/max_size in database.py
- **Check logs**: If frequent 503s with "connection dropped during tenant resolution", investigate team_members query performance

**Expected Behavior:**
- 503 errors should be rare (< 0.1% of requests)
- Retries should succeed within 1-2 attempts
- Logs should show "Connection cleanup failed (already closed)" warnings (expected, safely ignored)
- No "Task exception was never retrieved" errors (fixed in Phase 3 with safe cleanup)

**Distinguishing 503 from 4xx:**
- **503**: Transient, safe to retry (DB connection lost, pool exhausted, DNS failure)
- **400**: User error, don't retry (missing x-agency-id, invalid UUID format)
- **403**: Authorization failure, don't retry (not a member, inactive owner)
- **422**: Validation error, don't retry (invalid request body)


### Owner Portal Smoke Test with Automatic Retry

**Overview:** The `pms_owner_portal_smoke.sh` script now automatically retries API calls on transient 503 errors.

**Retry Behavior:**
- **Enabled for**: POST /api/v1/owners (Test 1), PATCH /api/v1/properties/{id}/owner (Test 2), GET /api/v1/owner/properties (Test 3), GET /api/v1/owner/bookings (Test 4)
- **Default**: 10 retry attempts with 1-second sleep between attempts (~10 seconds total)
- **Configurable**: Set `MAX_RETRIES` and `SLEEP_SECONDS` environment variables
- **Detection**: Retries only when HTTP 503 AND response contains "Database connection temporarily lost" OR "Database temporarily unavailable" OR "service_unavailable"
- **Partial success**: Test 1 detects if owner was created in earlier attempt (before exhaustion) by fetching existing owners

**Why This Matters:**
- Production databases may experience transient connection drops due to network blips, pool churn, or brief maintenance
- Without retry, smoke tests would fail intermittently even though the system is healthy
- With retry, tests succeed once database connection is restored

**Expected Outcome:**
- First attempt fails with 503 → Script retries automatically
- After 1-10 seconds, database connection restored → Retry succeeds
- Test continues normally

**Troubleshooting:**

If smoke test fails with "Max retries exhausted":
1. Check /health/ready output printed by script (indicates database availability)
2. Verify database is actually up: `psql $DATABASE_URL -c "SELECT 1;"`
3. Check if database is under heavy load (slow queries, high CPU)
4. Consider increasing MAX_RETRIES: `MAX_RETRIES=20 ./backend/scripts/pms_owner_portal_smoke.sh`

If /health/ready shows green but smoke still fails:
- Database may be accessible but individual connections dropping
- Check connection pool metrics (size, idle connections)
- Review backend logs for connection pool exhaustion


### Tenant Resolution DB Drops (503 on POST /owners)

**Symptom:** POST /api/v1/owners (or other owner endpoints) returns 503 with "Database connection temporarily lost" during tenant resolution (x-agency-id validation or auto-detect).

**Backend Logs Show:**
```
Database connection dropped during tenant resolution (x-agency-id header validation): 
ConnectionDoesNotExistError: connection was closed in the middle of operation
```

**Root Cause:** Transient DB connection drops during membership validation query in `get_current_agency_id` dependency.

**Architecture (Phase 3+):**
- Tenant resolution validates x-agency-id header by querying `team_members` table
- On connection drop (ConnectionDoesNotExistError, InterfaceError, ConnectionResetError, OSError):
  1. Backend automatically invalidates pool and recreates it
  2. Retries membership query once (max 2 attempts total)
  3. If both attempts fail: returns 503 with retry message
- Client (smoke script or app) retries with exponential backoff (1s, 1s, 2s, 3s, 5s)

**Quick Checks:**

1. **Verify health endpoint is green**:
   ```bash
   curl https://api.fewo.kolibri-visions.de/health/ready
   # Should return: {"status":"healthy"}
   ```

2. **Check backend logs for tenant resolution errors**:
   ```bash
   docker logs --since 20m pms-backend 2>&1 | grep -E "tenant resolution|pool invalid"
   # Look for: "Database connection dropped during tenant resolution"
   # Look for: "Invalidating connection pool due to tenant resolution connection drop"
   ```

3. **Check for pool recreation**:
   ```bash
   docker logs --since 20m pms-backend 2>&1 | grep -E "Pool invalidated|Database connection pool created"
   # Should see: pool invalidation followed by successful recreation
   ```

**Expected Behavior:**
- First attempt: 503 (connection drop detected)
- Backend: invalidates pool, recreates pool
- Retry: succeeds with fresh connection
- Total time: 1-5 seconds depending on retry attempt

**When to Escalate:**
- Persistent 503 after 5+ retry attempts → Database connectivity instability
- /health/ready red → Database unavailable
- Frequent pool invalidations (>10 per minute) → Network flapping or DNS issues

**Troubleshooting:**

If POST /owners returns 503 consistently:
1. Check database is actually up: `psql $DATABASE_URL -c "SELECT 1;"`
2. Verify network latency: `ping <db-host>`
3. Check DNS resolution: `nslookup <db-host>`
4. Review database connection pool settings (min_size, max_size in database.py)
5. Check for DNS flaps or network interruptions in infrastructure logs

If backend logs show frequent pool invalidations but queries eventually succeed:
- This is expected behavior for transient drops
- Client retries should handle it gracefully
- Monitor frequency: >10/min indicates infrastructure issue


### Smoke Script False Failures (Test 3/4 HTTP_STATUS Parsing)

**Symptom:** Smoke script reports "Test 3 FAILED: Expected HTTP 200, got: " or "Test 4 FAILED: Expected HTTP 200, got: " with empty HTTP status code, even though actual API returned HTTP 200 with correct JSON response.

**Root Cause:** `call_api_with_retry()` helper already appends `-w "\nHTTP_STATUS:%{http_code}"` to curl and strips it from response body (lines 167, 171). Test 3 and Test 4 were passing `-w` flag again, causing double-wrapping where HTTP_STATUS line appeared in response body instead of footer, resulting in empty HTTP_STATUS variable during string parsing.

**Fix (2026-01-09):** Removed duplicate `-w` flag and HTTP_STATUS parsing from Test 3 and Test 4. Tests now validate response structure directly:
- **Test 3**: Checks response is JSON list (not error object with "detail" or "error" keys)
- **Test 4**: Checks response is JSON list or object with `items` key (not error object)

**Expected Behavior:** Test 3/4 now succeed whenever endpoints return HTTP 200 with valid JSON structure. False failures eliminated.

**Verification:** Run smoke script - Test 3/4 should pass consistently without "Expected HTTP 200, got: " errors.


---

**Note (2026-01-09 bugfix):** Prior to this date, the retry handler incorrectly referenced `asyncpg.ConnectionResetError` (which doesn't exist), causing `AttributeError` and HTTP 500 instead of graceful retry + 503. Fixed by using built-in `ConnectionResetError` (Python exception, not asyncpg.*). Expected behavior is now: transient drops → pool invalidation → retry → 503 if still failing (never 500 from AttributeError).


**Implementation Detail (2026-01-09):** Tenant resolution retry logic now acquires a FRESH database connection from the pool on each attempt (via `pool.acquire()`). Previously, retries reused the same injected connection (which could be dead), causing retry failures even after pool recreation. With fresh connection acquisition per attempt, retries genuinely recover from transient connection drops. Backend logs will show "succeeded on attempt N with fresh connection from pool" when retry succeeds.


**Authorization (Agency Roles vs JWT Roles):**

Owner Portal O1 endpoints enforce **application roles** from the `team_members` table, NOT Supabase JWT role claims.

- **JWT role claim**: Always `"authenticated"` for normal Supabase users (system-level, not application-specific)
- **Application roles**: Stored in `team_members.role` (manager, admin, staff, owner, accountant) per agency
- **Staff endpoints** (POST /owners, PATCH /owners, etc.): Check `team_members.role` via `require_agency_roles("manager", "admin")`
- **Owner endpoints** (GET /owner/properties, etc.): Check `owners` table via `get_current_owner()` dependency

**Why this matters:**
- A user with JWT `role="authenticated"` can have `team_members.role="manager"` in one agency and `"staff"` in another
- Owner Portal endpoints resolve the user's agency (tenant resolution), then check their role in that agency's team_members table
- 403 errors will show actionable messages: "Requires agency role manager/admin. Your agency role is staff."

**Troubleshooting 403 Forbidden:**
- Check user is a team_member in the target agency: `SELECT role FROM team_members WHERE agency_id = $1 AND user_id = $2`
- Verify user's role matches endpoint requirements (manager/admin for staff endpoints)
- Note: `get_current_role()` dependency still returns JWT role; use `team_members.role` from DB for authorization


**Owner Verification Retry Hardening (2026-01-09):**

Owner Portal endpoints (GET /api/v1/owner/properties, GET /api/v1/owner/bookings) now use fresh connection retry for owner verification queries. This eliminates 503 flakiness caused by transient DB drops during owner membership lookups.

- **Before:** Owner verification used injected connection that could be dead, causing immediate 503 on transient drops
- **After:** Owner verification acquires fresh connection from pool on each attempt (max 2 attempts with pool invalidation)
- **Backend logs on success:** "Owner verification succeeded on attempt N with fresh connection from pool"
- **Result:** GET /owner/properties returns 200 consistently, no 503 flakiness

If persistent 503 on owner endpoints after multiple retries → Check database connectivity and network stability (see "Tenant Resolution DB Drops" troubleshooting).


**Owner Bookings 503 (Missing date_from/date_to Columns):**

**Symptom:** GET /api/v1/owner/bookings returns 503 "Database schema mismatch detected"

**Backend Logs Show:**
```
Database schema mismatch detected: column b.date_from does not exist
```

**Root Cause:** Owner bookings query references `date_from`/`date_to` columns but bookings table only has `check_in`/`check_out` columns.

**Fix:**
- Apply migration `20260109000001_add_bookings_date_from_to.sql` which adds `date_from`/`date_to` columns
- Migration automatically backfills from `check_in`/`check_out` where available
- Adds performance indexes for owner bookings queries

**Quick Verification:**
```bash
# Check if columns exist
psql $DATABASE_URL -c "SELECT column_name FROM information_schema.columns WHERE table_name = 'bookings' AND column_name IN ('date_from', 'date_to');"
# Expected: Both columns should be listed

# Apply missing migrations
cd /data/repos/pms-webapp
supabase db push
```

**After Fix:**
- GET /api/v1/owner/bookings returns 200 with booking array
- pms_owner_portal_smoke.sh Test 4 passes with HTTP 200


## Owner Portal O2 — Abrechnungen/Statements (CSV MVP)

**Overview:** Owner statement generation and CSV export for property owner payouts.

**Purpose:** Enable staff to generate period-based statements for owners, and allow owners to list and download their statements as CSV files.

**Architecture:**
- **Database**: `owner_statements` table (header with totals), `owner_statement_items` table (line items per booking)
- **RBAC**:
  - Staff endpoints require manager/admin role (via `require_agency_roles("manager", "admin")`)
  - Owner endpoints require `get_current_owner()` dependency
- **CSV Export**: Streaming response with Content-Disposition attachment header
- **Idempotency**: Unique constraint on (agency_id, owner_id, period_start, period_end) prevents duplicates

**API Endpoints:**

Staff (manager/admin):
- `POST /api/v1/owners/{owner_id}/statements/generate` - Generate statement for period (upsert-like: returns existing if already generated)
- `GET /api/v1/owners/{owner_id}/statements` - List statements for owner

Owner-only:
- `GET /api/v1/owner/statements` - List own statements
- `GET /api/v1/owner/statements/{statement_id}` - Get statement detail with items
- `GET /api/v1/owner/statements/{statement_id}/download?format=csv` - Download as CSV

**Database Tables:**
- `owner_statements` - Statement headers (period, totals, status)
- `owner_statement_items` - Line items (booking references, dates, amounts)

**Migrations:**
- `20260109000003_add_owner_statements.sql`

**Verification Commands:**

```bash
# [HOST-SERVER-TERMINAL] Pull latest code
cd /data/repos/pms-webapp
git fetch origin main && git reset --hard origin/main

# [HOST-SERVER-TERMINAL] Verify deployment
export API_BASE_URL="https://api.fewo.kolibri-visions.de"
EXPECT_COMMIT=<commit_sha> ./backend/scripts/pms_verify_deploy.sh

# [HOST-SERVER-TERMINAL] Run O2 smoke test
export HOST="https://api.fewo.kolibri-visions.de"
export MANAGER_JWT_TOKEN="<<<manager/admin JWT>>>"
export OWNER_JWT_TOKEN="<<<owner user JWT>>>"
export OWNER_AUTH_USER_ID="<<<Supabase auth.users.id>>>"
# Optional:
# export PROPERTY_ID="<property_uuid>"
# export AGENCY_ID="<agency_uuid>"
# export PERIOD_START="2025-10-01"
# export PERIOD_END="2025-12-31"
./backend/scripts/pms_owner_statements_smoke.sh
echo "rc=$?"

# Expected output: All 6 tests pass, rc=0
```

**Common Issues:**

### Test 0 Fails (Owner Profile Creation)

**Symptom:** Smoke test fails at Test 0 with "Could not create owner profile" or similar error.

**Root Cause:** Owner profile creation failed or existing owner not found. Common causes:
- JWT token expired (401 Unauthorized)
- Manager token lacks RBAC permissions (403 Forbidden)
- Validation error: invalid email format or auth_user_id (422/400)
- Email/auth_user_id mismatch between existing profile and provided values

**How to Debug:**
Test 0 now prints HTTP code and response snippet on failure. Check the output:
```bash
# Expected diagnostic output on failure:
# ⚠️  Owner create returned HTTP 401
# Response snippet: {"detail":"Token has expired"}
# Hint: Token may be expired. Re-login and try again.
```

**Solution:**
- **401 Unauthorized**: JWT token expired. Re-login to obtain fresh tokens:
  ```bash
  # Use Supabase CLI or auth API to re-login
  # Update MANAGER_JWT_TOKEN and OWNER_JWT_TOKEN
  ```
- **403 Forbidden**: Ensure MANAGER_JWT_TOKEN has manager or admin role. Verify:
  ```bash
  curl -H "Authorization: Bearer $MANAGER_JWT_TOKEN" "$HOST/api/v1/owners" | jq
  # Should return 200 OK with owner list, not 403
  ```
- **422/400 Validation Error**: Check email format and auth_user_id validity. Test 0 derives email from OWNER_JWT_TOKEN claim `email` or OWNER_EMAIL env var.
- **Idempotency**: Test 0 checks for existing owner first (GET /api/v1/owners). If owner already exists, it skips creation and proceeds.


### Statement Generation Returns Empty Items

**Symptom:** Statement is created but contains no line items (gross_total_cents = 0).

**Root Cause:** No bookings match the period filter criteria. Common reasons:
- Period is outside booking date range
- All bookings for owner's properties are cancelled/declined
- Bookings missing required columns (date_from, date_to, total_price_cents)

**How to Debug:**
```bash
# Check bookings for owner's properties in period
psql $DATABASE_URL <<SQL
SELECT b.id, b.property_id, b.date_from, b.date_to, b.status, b.total_price_cents
FROM bookings b
JOIN properties p ON b.property_id = p.id
WHERE p.owner_id = '<owner_id>'
  AND p.agency_id = '<agency_id>'
  AND b.date_from >= '2025-10-01'
  AND b.date_from < '2025-12-31'
  AND b.status NOT IN ('cancelled', 'declined')
  AND b.deleted_at IS NULL
ORDER BY b.date_from;
SQL
```

**Solution:**
- Verify bookings exist for the period
- Check booking status (must not be cancelled/declined)
- Ensure bookings.date_from/date_to/total_price_cents columns exist (migrations 20260109000001, 20260109000002)

### CSV Download Returns Empty or Invalid Content

**Symptom:** CSV download succeeds (HTTP 200) but file is empty or contains only headers without data rows.

**Root Cause:** Statement has no line items (see above), or statement_id not found.

**How to Debug:**
```bash
# Check statement items count
psql $DATABASE_URL <<SQL
SELECT COUNT(*) FROM owner_statement_items WHERE statement_id = '<statement_id>';
SQL

# Verify statement ownership
psql $DATABASE_URL <<SQL
SELECT id, owner_id, agency_id, period_start, period_end, gross_total_cents
FROM owner_statements
WHERE id = '<statement_id>';
SQL
```

**Solution:**
- Verify statement has items (not zero bookings)
- Ensure owner_id matches the requesting owner
- Re-generate statement if bookings were added after generation

### Duplicate Statement Generation Fails (409 Conflict)

**Symptom:** Generating statement for same owner+period returns 409 Conflict or constraint violation error.

**Root Cause:** Unique constraint `uq_owner_statements_owner_period` prevents duplicates.

**Expected Behavior:** This is intentional. The endpoint uses upsert-like behavior:
- First call: Creates statement, returns 201 Created
- Subsequent calls: Returns existing statement, returns 200 OK (not 201)

**Solution:**
- This is not an error. Endpoint is idempotent and returns existing statement.
- If you need to regenerate, first delete the existing statement (future feature).

### Statement Generation Fails with 503 (Schema Missing)

**Symptom:** POST `/api/v1/owners/{owner_id}/statements/generate` returns 503 Service Unavailable with error message: "Database schema not installed or out of date: relation \"owner_statements\" does not exist".

**Root Cause:** The `owner_statements` and `owner_statement_items` tables have not been created in the PROD database. This occurs when the required migration has not been applied.

**How to Debug:**

[Supabase SQL Editor]
```sql
-- Check if tables exist
SELECT tablename FROM pg_tables
WHERE schemaname = 'public'
  AND tablename IN ('owner_statements', 'owner_statement_items');

-- Expected: 0 rows if migration not applied, 2 rows if applied
```

**Solution:**

**Step 1: Verify deployment commit contains migration**

[HOST-SERVER-TERMINAL]
```bash
cd /data/repos/pms-webapp
git fetch origin main && git reset --hard origin/main
EXPECT_COMMIT="$(git rev-parse HEAD)"
export API_BASE_URL="https://api.fewo.kolibri-visions.de"
./backend/scripts/pms_verify_deploy.sh "$EXPECT_COMMIT"

# Expected: rc=0 and /ops/version source_commit == EXPECT_COMMIT

# Verify migration file exists locally
ls -la supabase/migrations/*owner_statements*.sql
# Expected: File listed (the migration creating owner_statements)
```

**Step 2: Find and apply migration in Supabase SQL Editor**

[HOST-SERVER-TERMINAL]
```bash
# Find the migration file
grep -l "CREATE TABLE.*owner_statements" supabase/migrations/*.sql
# Output shows: supabase/migrations/<timestamp>_add_owner_statements.sql

# Display migration filename
ls -1 supabase/migrations/*owner_statements*.sql
```

[Supabase SQL Editor]
1. Open Supabase Dashboard → SQL Editor
2. Create new query as `supabase_admin` user
3. Copy/paste the exact contents of the migration file identified above (the one creating owner_statements)
4. Run query
5. Verify tables created:
   ```sql
   SELECT tablename FROM pg_tables
   WHERE schemaname = 'public'
     AND tablename IN ('owner_statements', 'owner_statement_items');
   ```
   Expected: 2 rows (both tables)

6. Verify RLS enabled:
   ```sql
   SELECT tablename, rowsecurity
   FROM pg_tables
   WHERE schemaname = 'public'
     AND tablename IN ('owner_statements', 'owner_statement_items');
   ```
   Expected: 2 rows with rowsecurity = true

**Step 3: Verify with smoke test**

[HOST-SERVER-TERMINAL]
```bash
export HOST="https://api.fewo.kolibri-visions.de"
export MANAGER_JWT_TOKEN="<<<fresh manager/admin JWT>>>"
export OWNER_JWT_TOKEN="<<<fresh owner JWT>>>"
export OWNER_AUTH_USER_ID="<<<Supabase auth.users.id>>>"

./backend/scripts/pms_owner_statements_smoke.sh
echo "rc=$?"

# Expected: All 6 tests pass, rc=0
# Test 3 should succeed with "Statement generated"
```

**Step 4: Verify idempotency (3x loop)**

[HOST-SERVER-TERMINAL]
```bash
for i in {1..3}; do
  echo ""
  echo "========== Run $i/3 =========="
  ./backend/scripts/pms_owner_statements_smoke.sh && echo "rc=0" || echo "rc=$?"
  sleep 2
done

# Expected: All 3 runs pass with rc=0
```

**Notes:**
- Migration includes RLS policies (admin/manager/accountant staff access, owner read-only access)
- Migration is idempotent (uses `CREATE TABLE IF NOT EXISTS`, `CREATE INDEX IF NOT EXISTS`)
- Safe to re-run if interrupted
- After migration: statement generation, listing, and CSV download should work for both staff and owners


### Statement Generation 503: extract(unknown, integer) does not exist

**Symptom:** POST `/api/v1/owners/{owner_id}/statements/generate` returns **503** with:
`function pg_catalog.extract(unknown, integer) does not exist` (hint: add explicit type casts).

**Meaning:** If the `owner_statements` tables exist (see section above), this is **NOT** a missing-table migration issue.
It indicates a **backend SQL type mismatch** in the statement generation query.

**Root Cause:** PostgreSQL behavior:
- `DATE - DATE` returns an **INTEGER** (number of days), not an interval.
- `EXTRACT(...)` works with interval/timestamp types, not integers.
If the SQL uses `EXTRACT(DAY FROM (b.date_to - b.date_from))`, it will fail.

**Fix (code):**
- Replace `EXTRACT(DAY FROM (b.date_to - b.date_from)) AS nights` with `(b.date_to - b.date_from) AS nights`.
- File location: `backend/app/api/routes/owners.py` (statement generation SQL).

**Verification:**
1. Redeploy backend (Coolify).
2. Verify commit: `curl -sS https://api.fewo.kolibri-visions.de/api/v1/ops/version`
3. Re-run `./backend/scripts/pms_owner_statements_smoke.sh` (Test 3 must pass, rc=0).

### Statement Generation Returns 500 (created_at duplicate / asyncpg NameError)

**Symptom:** POST `/api/v1/owners/{owner_id}/statements/generate` returns **500 Internal Server Error**. Backend logs show one or both:
- `TypeError: OwnerStatementResponse() got multiple values for keyword argument 'created_at'`
- `NameError: name 'asyncpg' is not defined`

**Context:** Statement row is created successfully in DB, but response building crashes. Subsequent calls hit "Statement already exists" and crash again with same error.

**Root Cause:**
1. **Duplicate created_at:** Response construction unpacked `**dict(row)` (which includes `created_at` as datetime), then passed `created_at=...isoformat()` again as separate kwarg.
2. **Missing import:** Code referenced `asyncpg.UndefinedFunctionError` in except clause without importing asyncpg module.

**Fix (code):**
- Added `import asyncpg` at top of `backend/app/api/routes/owners.py`
- Created helper function `_build_statement_response()` that converts `created_at` to ISO string before constructing response
- Updated both code paths (new statement + existing statement) to use helper

**Verification:**
1. Redeploy backend (Coolify auto-deploys on push to main)
2. Verify commit: `curl -sS https://api.fewo.kolibri-visions.de/api/v1/ops/version`
3. Re-run `./backend/scripts/pms_owner_statements_smoke.sh` (Test 3 must pass with 200/201, rc=0)
4. Check backend logs: No TypeError or NameError

**Troubleshooting:**
- If error persists after redeploy, verify deployed commit matches fix commit (check git log for "response mapping")
- If statement was created but failed response, delete it manually: `DELETE FROM owner_statements WHERE id = '...'` to retry


### Owner Cannot Access Statements (403 Not Registered)

**Symptom:** Owner gets 403 Forbidden when accessing `/api/v1/owner/statements`.

**Root Cause:** Same as O1 - user not mapped in `owners` table or `is_active = false`.

**Solution:** See Owner Portal O1 troubleshooting section for owner profile setup.

### Smoke Script Fails with "command not found" (RC=127)

**Symptom:** Running `pms_owner_statements_smoke.sh` fails immediately with "log_info: command not found" or similar, exit code 127.

**Root Cause:** Script bootstrap failed to define fallback log functions. This should never occur after defensive fallback implementation.

**How to Debug:**
```bash
# Verify script has defensive fallbacks
grep -A4 "command -v log_info" backend/scripts/pms_owner_statements_smoke.sh

# Expected output: Lines defining log_info, log_success, log_error, log_warn as fallbacks
```

**Solution:**
- Script defines defensive fallback log functions using `command -v` checks after sourcing common helpers
- If this error occurs, script file may be corrupted or outdated
- Pull latest code: `git fetch origin main && git reset --hard origin/main`
- Verify script has lines 19-23 with `command -v log_*` function definitions

---

---

## Backoffice Owners UI (O3) — Status & Verifikation (WIP)

**Status:** WIP / not yet verified in PROD.

**Goal:** Backoffice UI for staff/manager to manage owners (list/detail), assign properties, and trigger statement generation.

**Backend dependencies:**
- Owner Portal O1/O2 endpoints (owners + statements) must be deployed and verified.
- RBAC: manager/admin required.

**Verification (when implemented):**
- [BROWSER] Navigate in Backoffice to Owners area (menu entry may be added later).
- Validate list + detail rendering, property assignment, statement generation, CSV download.
- Add PROD evidence to `backend/docs/project_status.md` only after automated deploy verify + smoke rc=0.

**See also:**
- Owner Portal O1/O2 sections for auth/role troubleshooting and statement generation issues.

---

## Owner Portal O3 — PROD Verification (Assignable Properties Filter)

**Overview:** Automated PROD verification for Owner Portal O3 assignable properties filter API behavior.

**Purpose:** Verify that the `assignable_for_owner_id` query parameter on GET /api/v1/properties correctly filters properties and that mutual exclusion validation works as expected.

**Verification Commands:**

```bash
# HOST-SERVER-TERMINAL
cd /data/repos/pms-webapp
git fetch origin main && git reset --hard origin/main

# Verify deployment
export API_BASE_URL="https://api.fewo.kolibri-visions.de"
./backend/scripts/pms_verify_deploy.sh EXPECT_COMMIT=<commit_hash>

# Run O3 assignments smoke test
export HOST="https://api.fewo.kolibri-visions.de"
export MANAGER_JWT_TOKEN="<manager/admin JWT>"
export OWNER_A_AUTH_USER_ID="<Supabase auth.users.id for owner A>"
export OWNER_B_AUTH_USER_ID="<Supabase auth.users.id for owner B>"
# Optional:
# export AGENCY_ID="<agency UUID>"
./backend/scripts/pms_owner_o3_assignments_smoke.sh
echo "rc=$?"

# Expected output: All 2 tests pass, rc=0
```

**Common Issues:**

### 401 "Token has expired" Error

**Symptom:** Smoke test fails with `401 Unauthorized` and error message "Token has expired".

**Root Cause:** MANAGER_JWT_TOKEN is expired. JWTs typically have short TTL (1-24 hours).

**Solution:**
Regenerate a fresh JWT token using one of these methods:

```bash
# Option 1: Use get_fresh_token.sh (if configured)
export MANAGER_JWT_TOKEN=$(./backend/scripts/get_fresh_token.sh)

# Option 2: Manual token generation via Supabase API
# (requires SUPABASE_URL, SUPABASE_ANON_KEY, manager email/password)
```

**Verification:**
After regenerating token, re-run the smoke test:
```bash
./backend/scripts/pms_owner_o3_assignments_smoke.sh
# Should now pass with rc=0
```

### Token Validation with Supabase Kong

**Important:** If manually validating JWT tokens via Supabase Kong endpoint `/auth/v1/user`, you MUST include both headers:
- `Authorization: Bearer <token>` (the JWT)
- `apikey: <anon_key>` (Supabase anon key)

Without the `apikey` header, Kong will reject the request even if the JWT is valid.

**Example:**
```bash
curl -X GET "https://<project>.supabase.co/auth/v1/user" \
  -H "Authorization: Bearer $MANAGER_JWT_TOKEN" \
  -H "apikey: $SUPABASE_ANON_KEY"
```

---


## Frontend Build Failures (pms-admin)

### pms-admin build fails: apiClient.patch missing

**Symptom:** Coolify deployment for pms-admin fails during `npm run build` with TypeScript error:
```
./app/owners/[ownerId]/page.tsx:151:23
Type error: Property 'patch' does not exist on type 'apiClient'
```

**Root Cause:** The `apiClient` utility in `frontend/app/lib/api-client.ts` was missing the `patch` method. The Owners UI O3 detail page (`owners/[ownerId]/page.tsx`) uses `apiClient.patch()` to assign properties to owners via `PATCH /api/v1/properties/{id}/owner`.

**Fix:**
- Added `patch` method to apiClient with signature: `patch<T>(endpoint: string, body?: any, token?: string): Promise<T>`
- Method sends JSON body with Authorization header when token is provided
- Follows same pattern as existing `post` and `put` methods

**Verification:**
```bash
# Check apiClient has patch method
rg "patch:" frontend/app/lib/api-client.ts

# Check usage in owners detail page
rg "apiClient\.patch\(" frontend/app/owners
```

**Prevention:**
- Ensure all standard HTTP methods (GET, POST, PUT, PATCH, DELETE) are implemented in apiClient before using them in pages/components.
- In our workflow we rely on the Coolify build (`npm run build`) as the verification gate; use deployment logs as the source of truth when a build fails.

---


### Owners UI (O3): Property assignment dropdown empty

**Symptom:** In Backoffice Owners UI (`/owners/[ownerId]`), the "Objekt zuweisen" dropdown is empty, showing no properties to assign.

**Root Causes:**
1. **Response shape mismatch:** API returns `{ items: Property[] }` but frontend only checked for direct array
2. **Missing trailing slash:** GET `/api/v1/properties` may trigger redirect, frontend didn't handle `/api/v1/properties/`
3. **Field name mismatch:** Properties may have `ownerId` (camelCase) but code only checked `owner_id` (snake_case)
4. **Display label incomplete:** If `internal_name` is missing, dropdown shows blank or crashes

**Fix (applied):**
- Added trailing slash to endpoint: `/api/v1/properties/?limit=500&offset=0`
- Support multiple response shapes: `{ items: [] }`, `{ data: [] }`, or `[]` directly
- Normalize `owner_id` field: `p.owner_id ?? p.ownerId ?? null`
- Robust display labels with fallbacks: `internal_name || name || title || 'Objekt {id}'`
- Clear empty state message with link to properties page

**Verification (Browser):**
```bash
# 1. Open DevTools → Network tab
# 2. Navigate to /owners/[ownerId]
# 3. Check GET /api/v1/properties/ request:
#    - Response status: 200 OK
#    - Response body contains items/data/array with properties
#    - Properties have id, internal_name or name fields
# 4. Verify dropdown shows property options (at least unassigned ones)
# 5. Select property → click "Zuweisen" → success message appears
```

**Troubleshooting:**
- **Still empty after fix:** Check browser console for errors, verify API returns non-empty array
- **All properties assigned:** Expected if all properties have `owner_id` set to other owners
- **Dropdown shows "Objekt abc12345":** Property has no `internal_name`, `name`, or `title` fields (uses ID fallback)
- **Assignment fails:** Check PATCH request body contains `{"owner_id": "..."}` (snake_case required by backend)

---


### Owners UI (O3): Dropdown zeigt nur unassigned Properties (by design)

**Symptom:** Manager will Objekt von Owner A zu Owner B neu zuweisen, aber Dropdown auf /owners/B zeigt nur freie (unassigned) Objekte.

**Root Cause:** By design. Dropdown filtert standardmäßig nur Objekte ohne `owner_id` (oder bereits diesem Owner zugewiesen), um versehentliche Neuzuweisungen zu verhindern.

**Expected Behavior:**
- Dropdown zeigt: Objekte mit `owner_id = NULL` oder `owner_id = <current_owner_id>`
- Wenn alle Objekte bereits anderen Ownern zugewiesen sind: Hinweis "Alle Objekte sind bereits anderen Eigentümern zugewiesen..." ist **korrekt**

**Workaround (API):**

Reassign via API statt UI:

```bash
# 1. Check current owner assignment
curl -X GET "$HOST/api/v1/properties/{property_id}" \
  -H "Authorization: Bearer $MANAGER_JWT_TOKEN"

# 2. Reassign property to different owner
curl -X PATCH "$HOST/api/v1/properties/{property_id}/owner" \
  -H "Authorization: Bearer $MANAGER_JWT_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"owner_id": "new-owner-uuid-here"}'

# 3. Verify reassignment
curl -X GET "$HOST/api/v1/properties/{property_id}" \
  -H "Authorization: Bearer $MANAGER_JWT_TOKEN"
# Check response: "owner_id" should now be "new-owner-uuid-here"

# Optional: Verify in Backoffice UI
# Navigate to /owners/new-owner-uuid-here
# Check that property appears under "Zugewiesene Objekte"
```

Siehe auch: "Owner Portal O1" Abschnitt in diesem Runbook für Details zu PATCH /api/v1/properties/{id}/owner Endpoint.

**Planned Improvement (WIP):**

Geplante UI-Verbesserung (noch nicht deployed):
- Toggle-Checkbox "Auch bereits zugewiesene Objekte anzeigen (Neu zuweisen)"
- Wenn aktiv: Dropdown zeigt ALLE Objekte (auch assigned)
- Objekte mit anderem Owner werden markiert: "⚠️ <label> (bereits zugewiesen: <owner_prefix>)"
- Confirm-Dialog vor Neuzuweisung: "WARNUNG: Objekt ist bereits zugewiesen..."

Status: Geplant/WIP (noch nicht implementiert oder deployed).


### Owners UI (O3): Dropdown empty due to 422 (limit validation)

**Symptom:** Property assignment dropdown stays empty on /owners/[ownerId]. Browser console shows:
```
GET /api/v1/properties?limit=500&offset=0 → 422 Unprocessable Content
```

**Root Cause:** Frontend requested limit=500, but backend has lower max limit constraint (e.g., 200). Request validation fails with 422.

**Fix (applied):**
- Changed frontend request from `limit=500` to `limit=200` (safe limit)
- Removed trailing slash: `/api/v1/properties?limit=200&offset=0` (not `/properties/`)
- Added retry logic: On 422, retry with `limit=100` automatically
- Added error message in UI: "Objekte konnten nicht geladen werden (HTTP 422: ...)"

**Verification:**
```bash
# Browser DevTools → Network tab
# Verify GET /api/v1/properties?limit=200&offset=0 returns 200 (not 422)

# Optional: curl test
curl -X GET "$HOST/api/v1/properties?limit=200&offset=0" \
  -H "Authorization: Bearer $MANAGER_JWT_TOKEN"
# Should return 200 with properties array
```

**Troubleshooting:**
- If still 422: Backend max limit may be lower than 200 → adjust frontend limit further (e.g., 100 or 50)
- If limit=1 works but limit=200 fails: Check backend validation schema for max_limit parameter


### Owners UI (O3): HTTP 422 when properties limit > 100

**Symptom:** Property assignment dropdown empty. Browser DevTools shows:
```
GET /api/v1/properties?limit=200&offset=0 → 422 Unprocessable Content (validation_error)
```

**Root Cause:** Backend enforces strict validation: `query.limit <= 100`. Any request with limit > 100 returns 422 validation error.

**Fix (Frontend):**
- Frontend now uses pagination with `limit=100` from the start (no initial 422 + retry)
- Loop with `offset` increments: offset=0, offset=100, offset=200, etc.
- Accumulates all properties across pages (max 20 pages as safety cap)
- Checks `has_more` field or `items.length === limit` to determine if more pages exist
- No trailing slash: `/api/v1/properties?limit=100&offset=0` (backend redirects `/properties/` → `/properties`)

**Verification (Browser DevTools):**
1. Open https://admin.fewo.kolibri-visions.de/owners/[ownerId]
2. DevTools → Network tab → Filter "properties"
3. Verify: All requests show `limit=100` and return **200 OK** (no 422)
4. Check: Multiple requests with increasing offset if total properties > 100

**Optional curl test:**
```bash
# Test limit=100 works
curl -X GET "$API/api/v1/properties?limit=100&offset=0" \
  -H "Authorization: Bearer $MANAGER_JWT_TOKEN"
# Should return 200

# Test limit=101 fails
curl -X GET "$API/api/v1/properties?limit=101&offset=0" \
  -H "Authorization: Bearer $MANAGER_JWT_TOKEN"
# Should return 422 validation_error
```

**Troubleshooting:**
- If still 422 with limit=100: Backend max_limit changed → check API validation schema
- If dropdown empty after fix: Check browser console for other errors (401, 403, 503)
- If properties missing: Check pagination loop completed (not capped at maxPages=20)


### Owners UI (O3): Zuweisung aufheben (owner_id -> null) in UI

**Feature:** In Backoffice Owner Detail (/owners/[ownerId]), jedes zugewiesene Objekt hat Button "Zuweisung aufheben".

**Workflow:**
1. Navigate to /owners/[ownerId]
2. Section "Zugewiesene Objekte" shows list of properties owned by this owner
3. Each property row has "Zuweisung aufheben" button (red text)
4. Click button → Confirm dialog appears: "Möchten Sie die Zuweisung des Objekts ... wirklich aufheben?"
5. Click OK → Frontend calls: PATCH /api/v1/properties/{propertyId}/owner with {"owner_id": null}
6. On success: Property removed from "Zugewiesene Objekte" list and appears in dropdown (free properties)
7. On error: Alert shows error message

**Expected Behavior:**
- After unassign: property.owner_id = null (unassigned)
- Property visible in dropdown "Objekt zuweisen" for re-assignment
- Owner detail page refreshes to reflect change

**Verification (Browser):**
1. Login as manager/admin
2. Navigate to /owners/<ownerId> with at least one assigned property
3. Click "Zuweisung aufheben" on a property
4. Confirm dialog → Click OK
5. Verify: Property disappears from "Zugewiesene Objekte" list
6. Verify: Dropdown "Objekt zuweisen" now shows the property as available
7. Optional: Navigate to /properties/<propertyId> → verify no owner reference displayed

**Fallback Workaround (API):**
If UI button fails, unassign via curl:
```bash
# Unassign property (set owner_id to null)
curl -X PATCH "$API/api/v1/properties/{propertyId}/owner" \
  -H "Authorization: Bearer $MANAGER_JWT_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"owner_id": null}'
# Should return 200

# Verify unassignment
curl -X GET "$API/api/v1/properties/{propertyId}" \
  -H "Authorization: Bearer $MANAGER_JWT_TOKEN"
# Check response: "owner_id" should be null
```

**Troubleshooting:**
- **Button nicht sichtbar:** Check role is manager/admin (not owner-only), verify property is actually assigned to this owner
- **Confirm dialog not showing:** Browser blocks window.confirm (unlikely) → use API workaround
- **Error "Fehler beim Aufheben der Zuweisung":** Check PATCH /api/v1/properties/{id}/owner endpoint exists and accepts {"owner_id": null}
- **Property stays in list after unassign:** Check browser console for errors, verify fetchProperties() was called after PATCH success


### Owners UI (O3): Dropdown zeigt assigned Objekte / Browser-Popup Confirm

**Ist-Zustand (aktuelle PROD-Situation):**
1. **Dropdown-Filter Bug:** Dropdown "Objekt zuweisen" kann Objekte anzeigen, die bereits anderen Ownern zugewiesen sind (owner_id ≠ null), oder zeigt falsche Liste
2. **Browser-Popup Confirm:** Unassign verwendet browser-native `window.confirm()` Popup statt In-Page-Dialog
3. **Browser-alert Feedback:** Erfolg/Fehler-Meldungen erscheinen als `alert()` Popups statt In-Page-Banner
4. **Mobile Layout:** Actions ("Zuweisung aufheben", "Öffnen") können auf kleinen Bildschirmen awkward/unbrauchbar sein

**Workaround (API):**
Unassign direkt per API:
```bash
# Unassign property (set owner_id to null)
curl -X PATCH "$API/api/v1/properties/{propertyId}/owner" \
  -H "Authorization: Bearer $MANAGER_JWT_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"owner_id": null}'
# Should return 200

# Verify unassignment
curl -X GET "$API/api/v1/properties/{propertyId}" \
  -H "Authorization: Bearer $MANAGER_JWT_TOKEN"
# Check response: "owner_id" should be null

# Optional: Reload UI to see updated dropdown
# Navigate to https://admin.fewo.kolibri-visions.de/owners/<ownerId>
```

**Planned Improvement (WIP):**
Geplante Frontend-Verbesserungen (noch nicht implementiert oder deployed):
- Dropdown soll NUR Properties mit `owner_id = null` zeigen (strikt unassigned-only)
- Zusätzlicher Filter: Exclude properties bereits in "Zugewiesene Objekte" Liste
- In-Page Confirmation Modal statt `window.confirm()` Browser-Popup
- In-Page Banner/Toast für Erfolg/Fehler statt `alert()` Browser-Popup
- Responsive Actions Layout (mobile-friendly stack)

Status: Geplant/WIP (noch nicht implementiert oder deployed).

**Troubleshooting:**
- **Dropdown zeigt assigned properties:** Use API workaround to unassign, dann UI reload
- **Browser popup statt modal:** Expected behavior bis planned improvement deployed
- **Alert statt banner:** Expected behavior bis planned improvement deployed
- **Optional DevTools check:** Hard refresh (Cmd+Shift+R / Ctrl+F5) wenn unsicher ob latest frontend loaded
- **Unassign schlägt fehl:** Check PATCH endpoint accessibility, verify JWT token valid, check property exists

#### PROD Update (2026-01-10) — DEPLOYED (not VERIFIED)

**Deployed to Production:**
- Deployed commit: `ae9aa7a96f88fd4257e2a08fed48616b97009db4`
- Backend started_at: `2026-01-10T10:36:05.314675+00:00`
- Deploy verification: `backend/scripts/pms_verify_deploy.sh` rc=0 + commit match confirmed
- Manual Browser Check (Admin UI):
  - ✅ Check 1: Dropdown "Objekt zuweisen" zeigt nur unassigned Properties (owner_id=null), keine Duplikate, exclude bereits zugewiesene
  - ✅ Check 2: In-page Confirmation Modal (kein window.confirm Browser-Popup)
  - ✅ Check 3: In-page Success/Error Banner (kein alert() Browser-Popup)
  - ✅ Check 4: Mobile Layout responsive (Actions usable auf kleinen Bildschirmen)

**Hinweis:** Status bleibt ✅ IMPLEMENTED (nicht VERIFIED). Kein automatisierter UI-Smoke-Script für O3 vorhanden. Manuelle Browser-Verifikation in PROD durchgeführt.



---

## Owner Portal O3 — Owners UI Dropdown/Dialogs/Mobile Hardening

**Overview:** Enhanced owner portal UI with assignable properties filtering, dialog replacements, and mobile responsiveness.

**Purpose:** Improve owner property assignment UX by filtering dropdown to show only assignable properties (unassigned OR already owned by target owner), replace browser popups with in-page dialogs/toasts, and ensure mobile-friendly layout.

**Architecture:**
- **Backend API**: New `assignable_for_owner_id` query parameter on GET /api/v1/properties
- **SQL Filter**: `WHERE (owner_id IS NULL OR owner_id = $specified_owner_id)` excludes properties owned by other owners
- **Validation**: `assignable_for_owner_id` and `owner_id` filters are mutually exclusive (returns 400 if both used)
- **Frontend**: Dropdown uses assignable filter, replaces window.confirm/alert with Dialog/toast components

**API Endpoints:**

Staff (manager/admin):
- `GET /api/v1/properties?assignable_for_owner_id=<owner_id>&limit=&offset=` - List assignable properties for owner

**Frontend Changes:**
- `/owners/[ownerId]` page: Updated property assignment dropdown to use `assignable_for_owner_id` filter
- Replaced `window.confirm()` with in-page Dialog component for unassign confirmation
- Replaced `alert()` with toast messages for success/error feedback
- Mobile-responsive actions layout

**Verification Commands:**

```bash
# [HOST-SERVER-TERMINAL] Pull latest code
cd /data/repos/pms-webapp
git fetch origin main && git reset --hard origin/main

# [HOST-SERVER-TERMINAL] Optional: Verify deploy after Coolify redeploy
export API_BASE_URL="https://api.fewo.kolibri-visions.de"
./backend/scripts/pms_verify_deploy.sh

# [HOST-SERVER-TERMINAL] Run O3 smoke test
export HOST="https://api.fewo.kolibri-visions.de"
export MANAGER_JWT_TOKEN="<<<manager/admin JWT>>>"
export OWNER_A_AUTH_USER_ID="<<<Supabase auth.users.id for owner A>>>"
export OWNER_B_AUTH_USER_ID="<<<Supabase auth.users.id for owner B>>>"
# Optional:
# export AGENCY_ID="ffd0123a-10b6-40cd-8ad5-66eee9757ab7"
./backend/scripts/pms_owner_o3_assignments_smoke.sh
echo "rc=$?"

# Expected output: All 2 tests pass, rc=0
```

**Common Issues:**

### Assignable Filter Returns Wrong Properties

**Symptom:** GET /api/v1/properties?assignable_for_owner_id=<owner_id> returns properties owned by other owners.

**Root Cause:** SQL WHERE clause in property_service.py not correctly filtering by owner_id.

**How to Debug:**
```bash
# Test assignable filter
curl -X GET "$HOST/api/v1/properties?assignable_for_owner_id=<owner_id>&limit=100" \
  -H "Authorization: Bearer $MANAGER_JWT_TOKEN" | jq '.items[] | {id, name, owner_id}'

# Check if any returned properties have owner_id != null and owner_id != <owner_id>
# These should NOT be in the results
```

**Solution:**
- Verify property_service.py line ~132-136: WHERE clause should be `(p.owner_id IS NULL OR p.owner_id = $param_idx)`
- Check params.append(UUID(filters["assignable_for_owner_id"])) is correctly binding the parameter
- Verify param_idx is incremented after adding the parameter

### Mutual Exclusion Not Enforced

**Symptom:** GET /api/v1/properties?assignable_for_owner_id=<id1>&owner_id=<id2> returns 200 instead of 400.

**Root Cause:** Route validation in properties.py not checking for mutually exclusive filters.

**How to Debug:**
```bash
# Test mutual exclusion
curl -X GET "$HOST/api/v1/properties?assignable_for_owner_id=<owner_id>&owner_id=<other_id>" \
  -H "Authorization: Bearer $MANAGER_JWT_TOKEN" -w "\n%{http_code}\n"

# Should return 400 Bad Request with error detail
```

**Solution:**
- Verify properties.py route line ~86-91: should raise HTTPException 400 when both filters present
- Check if condition: `elif filters.assignable_for_owner_id and filters.owner_id:`
- Ensure error message: "Cannot use both assignable_for_owner_id and owner_id filters simultaneously"

### Dropdown Still Shows Properties Owned by Other Owners (UI)

**Symptom:** Frontend dropdown includes properties with owner_id != null and owner_id != target owner.

**Root Cause:** Frontend not using assignable_for_owner_id query parameter.

**How to Debug:**
```bash
# Check frontend network tab (DevTools)
# Should see: GET /api/v1/properties?assignable_for_owner_id=<ownerId>&limit=100
# NOT: GET /api/v1/properties?limit=100

# Verify fetchProperties call in frontend/app/owners/[ownerId]/page.tsx line ~123-129
```

**Solution:**
- Verify page.tsx uses: `assignable_for_owner_id=${ownerId}` query parameter
- Check availableProperties filter line ~355-358: should only exclude ownedIds, not filter by owner_id
- Hard refresh frontend (Cmd+Shift+R / Ctrl+F5) to clear cache

### Integration Tests Fail

**Symptom:** pytest tests/integration/test_properties.py::TestAssignablePropertiesFilter fails.

**Root Cause:** Test data setup incorrect or SQL query not working as expected.

**How to Debug:**
```bash
# Run specific test with verbose output
cd /Users/khaled/Documents/KI/Claude/Claude\ Code/Projekte/PMS-Webapp/backend
DATABASE_URL="postgresql://postgres:postgres@localhost:5432/test" \
JWT_SECRET="test-secret-key-1234567890123456" \
python -m pytest tests/integration/test_properties.py::TestAssignablePropertiesFilter::test_assignable_includes_unassigned_properties -v --tb=short

# Check SQL query executed (add logging to property_service.py)
# logger.debug(f"Assignable filter query: {query}, params: {params}")
```

**Solution:**
- Verify test creates properties with correct owner_id assignments
- Check test uses correct agency1_admin_token for auth
- Ensure test cleanup doesn't interfere with assertions (properties deleted before check)
- Verify test fixtures create owners in same agency as properties

---

## Epic A — Onboarding & RBAC (Team Management & Agency Settings)

**Overview:** Team member management, role-based access control (RBAC), and agency settings APIs with minimal admin UI.

**Purpose:** Enable multi-tenant team collaboration with role-based permissions (admin/agent/owner), team invitations, and agency configuration management.

**Architecture:**
- **Database**: `team_members` and `team_invites` tables for RBAC and invitation workflow
- **RBAC**:
  - Team-based roles stored in team_members table (agency_id + user_id → role)
  - Role resolution: prefer team_members.role, fallback to JWT claim if not found
  - Admin-only endpoints for team management (require_admin_for_team dependency)
- **Invitations**: Pending/accepted/revoked status, email-based invites for team onboarding
- **Tenant Isolation**: All queries scoped by agency_id from JWT claims
- **Identity**: /me endpoint provides user_id, agency_id, role, email for frontend state

**UI Routes:**
- `/organisation` - Agency settings page (admin-only, edit agency name)
- `/team` - Team members list, invite modal, pending invitations with revoke action

**API Endpoints:**

Identity:
- `GET /api/v1/me` - Get current user identity (user_id, agency_id, role, email)

Agencies:
- `GET /api/v1/agencies/current` - Get current agency details
- `PATCH /api/v1/agencies/current` - Update agency name (admin-only)

Team Management (admin-only):
- `GET /api/v1/team/members?limit=&offset=` - List team members
- `POST /api/v1/team/invites` - Create team invitation
- `GET /api/v1/team/invites?status=&limit=&offset=` - List team invitations
- `POST /api/v1/team/invites/{id}/revoke` - Revoke pending invitation
- `POST /api/v1/team/invites/{id}/resend` - Resend invitation email
- `DELETE /api/v1/team/members/{id}` - Remove team member

**Database Tables:**
- `team_members` - Team member profiles with role assignments (agency_id, user_id, role)
- `team_invites` - Team invitation workflow (email, role, status: pending/accepted/revoked)

**Migration:** `20260111000000_add_epic_a_team_rbac.sql`

**Verification Commands:**

```bash
# [HOST-SERVER-TERMINAL] Pull latest code
cd /data/repos/pms-webapp
git fetch origin main && git reset --hard origin/main

# [HOST-SERVER-TERMINAL] Run Epic A smoke test
export API_BASE_URL="https://api.fewo.kolibri-visions.de"
export JWT_TOKEN="<<<admin JWT token>>>"
# Optional: export SB_URL="..." SB_ANON_KEY="..." for invite-accept tests
./backend/scripts/pms_epic_a_onboarding_rbac_smoke.sh
echo "rc=$?"

# Expected output: All 6 tests pass, rc=0
```

**Common Issues:**

### /me Returns 401 Unauthorized

**Symptom:** GET /api/v1/me returns 401 even with valid JWT.

**Root Cause:** JWT token expired or invalid signature.

**How to Debug:**
```bash
# Check JWT expiration
echo $JWT_TOKEN | cut -d'.' -f2 | base64 -d | jq '.exp'
date +%s

# Compare exp (expiration timestamp) with current timestamp
# If exp < current timestamp, token is expired
```

**Solution:**
- Refresh JWT token via Supabase auth
- Check JWT_SECRET matches between app and token issuer
- Verify JWT contains required claims: user_id, agency_id

### Team Endpoints Return 403 (Not Admin)

**Symptom:** Admin user gets 403 Forbidden when accessing team management endpoints.

**Root Cause:** User role is not 'admin' in team_members table or JWT claim.

**How to Debug:**
```bash
# Check role from /me endpoint
curl -X GET "$API_BASE_URL/api/v1/me" \
  -H "Authorization: Bearer $JWT_TOKEN" | jq '.role'

# Check team_members table
psql $DATABASE_URL -c "SELECT user_id, role FROM team_members WHERE user_id = '<user_id from JWT>';"

# Check JWT role claim
echo $JWT_TOKEN | cut -d'.' -f2 | base64 -d | jq '.role'
```

**Solution:**
- Ensure user is in team_members table with role='admin'
- Or ensure JWT claim has role='admin' if user not yet in team_members
- Verify require_admin_for_team dependency checks both sources

### Invite Email Not Sent

**Symptom:** POST /api/v1/team/invites succeeds but no email received.

**Root Cause:** Email service not configured or disabled in current implementation.

**How to Debug:**
```bash
# Check invite was created
curl -X GET "$API_BASE_URL/api/v1/team/invites" \
  -H "Authorization: Bearer $JWT_TOKEN" | jq '.invites[] | select(.email=="test@example.com")'

# Invite should exist with status='pending'
```

**Solution:**
- Epic A MVP does not include email sending (Phase A1 scope)
- Manual invite acceptance: Admin shares invite link or creates user directly in Supabase
- Future: Integrate email service (SendGrid, AWS SES) for automated invite emails

### Role Not Resolving from team_members

**Symptom:** /me endpoint returns role from JWT claim instead of team_members table.

**Root Cause:** User not found in team_members table or query failed silently.

**How to Debug:**
```bash
# Check if user exists in team_members
psql $DATABASE_URL -c "SELECT * FROM team_members WHERE user_id = '<user_id>' AND agency_id = '<agency_id>';"

# Check get_current_role logic in auth.py
# Should query team_members first, fallback to JWT claim
```

**Solution:**
- Ensure user is added to team_members when accepting invite
- Verify get_current_role dependency in backend/app/core/auth.py line ~150-180
- Check agency_id matches between JWT and team_members row

### Team Invites/Members 500 Error (Permission Denied for Schema auth)

**Symptom:** POST /api/v1/team/invites or GET /api/v1/team/members returns HTTP 500 with error message containing "permission denied for schema auth".

**Root Cause:** Application queries `auth.users` table but database role lacks permissions to access `auth` schema (by design - app should only access `public` schema).

**How to Debug:**
```bash
# Check backend logs for InsufficientPrivilegeError
docker logs pms-backend | grep "permission denied for schema auth"

# Verify current queries don't reference auth schema
grep -r "auth\\.users\|auth\\.identities" backend/app/
```

**Solution:**
1. Deploy latest version that uses `profiles` table instead of `auth.users`
2. Verify deployment:
```bash
# [HOST-SERVER-TERMINAL] Verify deploy
export API_BASE_URL="https://api.fewo.kolibri-visions.de"
./backend/scripts/pms_verify_deploy.sh

# [HOST-SERVER-TERMINAL] Run Epic A smoke test
export HOST="https://api.fewo.kolibri-visions.de"
export JWT_TOKEN="<<<admin JWT token>>>"
./backend/scripts/pms_epic_a_onboarding_rbac_smoke.sh
# Expected: rc=0, all tests pass
```

**Note:** Backend DB role should NOT be granted access to `auth` schema. Application must use `public.profiles` for user email lookups.

### Team Invites/Members 503 Error (Column Missing - Schema Drift)

**Symptom:** POST /api/v1/team/invites or GET /api/v1/team/members returns HTTP 503 with error message containing "column 'user_id' does not exist" or similar database schema error.

**Root Cause:** Schema drift between expected columns and actual database schema. Epic A endpoints dynamically detect identity columns in `public.profiles` and `public.team_members` tables to support schema variants:
- profiles join key: `profiles.user_id` (preferred) OR `profiles.id` (fallback)
- team_members user ref: `team_members.user_id` (preferred) OR `team_members.profile_id` (fallback)

**How to Debug:**
```bash
# Check actual column names in profiles table
psql $DATABASE_URL -c "SELECT column_name FROM information_schema.columns WHERE table_schema='public' AND table_name='profiles' AND column_name IN ('user_id', 'id');"

# Check actual column names in team_members table
psql $DATABASE_URL -c "SELECT column_name FROM information_schema.columns WHERE table_schema='public' AND table_name='team_members' AND column_name IN ('user_id', 'profile_id');"

# Check backend logs for column resolution
docker logs pms-backend | grep "Epic A identity columns resolved"
# Expected: "Epic A identity columns resolved: profiles_key=user_id, team_members_key=user_id"
```

**Solution:**
1. If using Supabase default schema, ensure migrations create `profiles.user_id` and `team_members.user_id` columns:
```sql
-- Verify profiles table has user_id column
ALTER TABLE profiles ADD COLUMN IF NOT EXISTS user_id UUID REFERENCES auth.users(id);
CREATE INDEX IF NOT EXISTS idx_profiles_user_id ON profiles(user_id);

-- Verify team_members table has user_id column
-- (should already exist from Epic A migration 20260111000000)
```

2. Verify deployment after schema fix:
```bash
# [HOST-SERVER-TERMINAL] Verify deploy
export API_BASE_URL="https://api.fewo.kolibri-visions.de"
./backend/scripts/pms_verify_deploy.sh

# [HOST-SERVER-TERMINAL] Run Epic A smoke test
export HOST="https://api.fewo.kolibri-visions.de"
export JWT_TOKEN="<<<admin JWT token>>>"
./backend/scripts/pms_epic_a_onboarding_rbac_smoke.sh
# Expected: rc=0, all tests pass
```

**Note:** Epic A endpoints use dynamic column detection with module-level caching. If schema is fixed during runtime, restart backend workers to clear cache and re-detect columns.

### Team Invites/Members 503 Error (Missing profiles.email Column)

**Symptom:** GET /api/v1/team/members or POST /api/v1/team/invites returns HTTP 503 with error message containing "column p.email does not exist" or similar database error referencing profiles.email.

**Root Cause:** Profiles table is missing email column. Epic A endpoints require email for team member display and invitation verification, but PROD database profiles table may not have an email column.

**How to Debug:**
```bash
# Check if profiles table has email column
psql $DATABASE_URL -c "SELECT column_name FROM information_schema.columns WHERE table_schema='public' AND table_name='profiles' AND column_name IN ('email', 'primary_email', 'contact_email');"

# Check backend logs for email column resolution
docker logs pms-backend | grep "Epic A columns resolved"
# Expected: "Epic A columns resolved: profiles_key=..., team_members_key=..., profiles_email=email"
# Or if missing: "Epic A columns resolved: profiles_key=..., team_members_key=..., profiles_email=NONE"
```

**Solution:**

**Option A: Add email column to profiles table (recommended for full functionality):**
```sql
-- Add email column to profiles table
ALTER TABLE profiles ADD COLUMN IF NOT EXISTS email TEXT;

-- Optionally populate from auth.users if accessible
-- (Only run this if your DB role has access to auth schema)
UPDATE profiles p
SET email = (SELECT au.email FROM auth.users au WHERE au.id = p.user_id)
WHERE p.email IS NULL AND p.user_id IS NOT NULL;

-- Create index for email lookups
CREATE INDEX IF NOT EXISTS idx_profiles_email ON profiles(LOWER(email));
```

**Option B: Deploy code fix (graceful degradation - already implemented):**

The latest Epic A code tolerates missing email columns:
- Team member listings return `email: null` for all members
- Invitations can still be created (duplicate invite check works via team_invites table)
- "Already a member" check is skipped when email column is missing (logged as warning)
- Invitation acceptance relies on JWT email claim instead of profiles.email

To deploy:
```bash
# [HOST-SERVER-TERMINAL] Pull latest code and verify deploy
cd /data/repos/pms-webapp
git fetch origin main && git reset --hard origin/main

export API_BASE_URL="https://api.fewo.kolibri-visions.de"
./backend/scripts/pms_verify_deploy.sh EXPECT_COMMIT=<new_commit>

# [HOST-SERVER-TERMINAL] Run Epic A smoke test
export HOST="https://api.fewo.kolibri-visions.de"
export JWT_TOKEN="<<<admin JWT token>>>"
./backend/scripts/pms_epic_a_onboarding_rbac_smoke.sh
# Expected: rc=0, all tests pass (email fields may be null)
```

**Note:** With email column missing, team member listings show null emails and "already a member" checks are skipped. For full functionality, add the email column (Option A). Code gracefully degrades without 503 errors.

---

**Verification (PROD, 2026-01-12):**

All Epic A schema drift issues (auth schema, user_id columns, profiles.email) have been resolved and verified in production:

- **Source Commit**: 054bd62bff32ce6335185b71d1bdd3aca93a6b4f
- **Deploy Verification**: `backend/scripts/pms_verify_deploy.sh` (rc=0, EXPECT_COMMIT=054bd62)
- **Smoke Test**: `backend/scripts/pms_epic_a_onboarding_rbac_smoke.sh` (rc=0)
- **Verification Date**: 2026-01-12
- **Notes**:
  - Epic A no longer queries auth schema
  - Dynamic column detection handles schema variants (profiles.user_id OR profiles.id; team_members.user_id OR team_members.profile_id)
  - Email column detection tolerates missing profiles.email (returns email=null with graceful degradation)
  - All 6 smoke tests pass: /me, agencies/current, team/invites (create/list/revoke), team/members

---

### DELETE Team Member Fails (Foreign Key Constraint)

**Symptom:** DELETE /api/v1/team/members/{id} returns 409 Conflict.

**Root Cause:** Team member has associated records (e.g., created properties, bookings).

**How to Debug:**
```bash
# Check for dependent records
psql $DATABASE_URL -c "SELECT table_name, column_name FROM information_schema.columns WHERE column_name = 'created_by' OR column_name = 'user_id';"

# Example: Check if team member created any properties
psql $DATABASE_URL -c "SELECT COUNT(*) FROM properties WHERE created_by = '<team_member_user_id>';"
```

**Solution:**
- Current implementation: Cannot delete team members with dependent records (safe default)
- Alternative: Update team_member to inactive status instead of deleting
- Future: Implement cascading delete or reassignment workflow

### Admin UI Check

**Purpose:** Verify Epic A pages are accessible and functional in Admin UI.

**How to Test:**
1. Log in to Admin UI as admin user
2. Navigate to sidebar → Einstellungen (Settings) section
3. Click "Organisation" - should load agency details
4. Click "Team" - should show team members list and invitations

**Expected Behavior:**
- `/organisation` page loads agency name and allows editing (admin only)
- `/team` page shows team members, allows creating invitations, revoking pending invites
- No API 404 errors in browser console

**Common Failures:**

**Symptom:** Organisation/Team not visible in sidebar

**Root Cause:** User role is not "admin" or sidebar navigation config missing entries.

**How to Debug:**
```bash
# Check user role via /me endpoint
curl -X GET "$API_BASE_URL/api/v1/me" \
  -H "Authorization: Bearer $JWT_TOKEN" | jq '.role'

# Should return "admin"
```

**Solution:**
- Ensure user has admin role in team_members table or JWT claim
- Verify sidebar config includes Organisation and Team entries with `roles: ["admin"]`

**Symptom:** Page loads but shows "API Error 404: Not Found"

**Root Cause:** Epic A endpoints not mounted in backend (module not registered).

**How to Debug:**
```bash
# Check if Epic A endpoints exist in OpenAPI
curl -sS "$API_BASE_URL/openapi.json" | jq '.paths | keys[]' | grep -E "agencies/current|team/members"

# Should show:
#   "/api/v1/agencies/current"
#   "/api/v1/team/members"
```

**Solution:**
- Ensure `backend/app/modules/epic_a.py` exists and is imported in `bootstrap.py`
- Verify `MODULES_ENABLED=true` (default) in production
- Check backend logs for "Epic A module not available" warnings

**Symptom:** Page loads but API calls fail with CORS or wrong host

**Root Cause:** Frontend API client using wrong base URL.

**How to Debug:**
```bash
# Check browser DevTools Network tab
# API calls should go to api.* domain, not admin.* domain
# Example: https://api.fewo.kolibri-visions.de/api/v1/agencies/current

# Verify NEXT_PUBLIC_API_BASE env var in frontend container
docker exec pms-admin env | grep NEXT_PUBLIC_API_BASE
```

**Solution:**
- Set `NEXT_PUBLIC_API_BASE` to API service URL in Coolify frontend environment
- Ensure pages use `apiClient` from `lib/api-client.ts` (not direct fetch)
- Clear browser cache and hard refresh (Cmd+Shift+R / Ctrl+F5)

#### Organisation/Team Layout Fix (AdminShell)

**Symptom:** When navigating to `/organisation` or `/team`, pages render without sidebar/topbar (no AdminShell chrome).

**Root Cause:** Missing route-level layout.tsx files that wrap page content in AdminShell component.

**Fix Applied:**
- Created `frontend/app/organisation/layout.tsx`
- Created `frontend/app/team/layout.tsx`
- Both layouts follow established pattern from `frontend/app/profile/layout.tsx`:
  - Call `getAuthenticatedUser()` for auth guard
  - Wrap children in `<AdminShell>` with userRole, userName, agencyName props
  - Set `export const dynamic = "force-dynamic"` for SSR

**Verification Checklist (Browser):**
1. Open Admin UI: https://admin.fewo.kolibri-visions.de
2. Log in as admin user
3. Navigate to sidebar → Einstellungen → Organisation
   - ✅ Sidebar visible with navigation
   - ✅ Topbar shows page title "Organisation"
   - ✅ Agency details card displays: Name (editable), Organisations-ID, Erstellt am, E-Mail, Abonnement
4. Navigate to sidebar → Einstellungen → Team
   - ✅ Sidebar visible with navigation
   - ✅ Topbar shows page title "Team"
   - ✅ Team members table shows "E-Mail / User-ID" column header
   - ✅ UUID entries display as "User-ID: ..." in muted smaller text
   - ✅ Email entries display normally
5. No console errors or missing styles

**Common Failures:**
- **Pages still missing chrome**: Hard refresh browser (Cmd+Shift+R / Ctrl+F5) to clear Next.js cache
- **401/403 errors**: Verify user is logged in and has admin role
- **Layout props undefined**: Check `getAuthenticatedUser()` returns all required fields (role, name, agencyName)


### UI Polish (Production-Grade Organisation & Team Pages)

**Overview:** Enhanced Organisation and Team pages with production-grade UI components, replacing basic MVP implementation.

**Improvements Implemented:**

**Organisation Page (`/organisation`):**
- ✅ Dialog-based editing (replaces inline editing)
- ✅ Copy-to-clipboard button for Organisation ID (with visual feedback)
- ✅ Loading skeletons (animated placeholders during data fetch)
- ✅ Toast notifications (success/error banners with auto-dismiss)
- ✅ Error banners (styled error states)
- ✅ Improved visual hierarchy (icons, typography, spacing)
- ✅ All fields displayed: Name, Organisations-ID, Erstellt am, E-Mail, Abonnement
- ✅ "Nicht festgelegt" placeholders for null values

**Team Page (`/team`):**
- ✅ Sectioned layout: Mitglieder (Members) + Einladungen (Invitations) as separate cards
- ✅ Copy-to-clipboard buttons for emails and user IDs (with visual feedback)
- ✅ Role badges (color-coded: Admin = purple, Agent = blue, Owner = green)
- ✅ Status badges for invites (Ausstehend = yellow, Akzeptiert = green, Widerrufen = gray)
- ✅ Loading skeletons (animated placeholders)
- ✅ Empty states with icons ("Keine Teammitglieder", "Keine Einladungen")
- ✅ Replaced `window.confirm()` with dialog component for invite revocation
- ✅ Replaced `alert()` with toast notifications (success/error banners)
- ✅ Enhanced invite dialog (improved styling and validation)
- ✅ Note: "Rollenänderungen folgen in einer späteren Phase" shown in actions column

**UI Components Used:**
- Custom Dialog: Fixed overlay (`fixed inset-0 z-50`) + modal card pattern
- Toast Banners: Conditional styling (green for success, red for error) with auto-dismiss (5s timeout)
- Badges: Inline `<span>` with Tailwind utility classes (`inline-flex items-center px-2.5 py-0.5 rounded-full`)
- Skeleton Loaders: `bg-gray-200 animate-pulse` divs with varying widths/heights
- Copy Buttons: `lucide-react` icons (Copy/Check) with state-based rendering

**No External UI Libraries:**
- ❌ No shadcn/ui (not installed)
- ❌ No @radix-ui components
- ❌ No sonner/react-hot-toast
- ✅ All components custom-built with Tailwind CSS + lucide-react icons

**Browser Verification Checklist:**

**Organisation Page:**
1. Navigate to `/organisation` (admin-only)
2. ✅ Loading skeleton appears briefly during data fetch
3. ✅ Page shows agency details with Building2 icon header
4. ✅ Copy button next to Organisation ID works (shows "Kopiert" feedback)
5. ✅ Click "Bearbeiten" → opens dialog (not inline editing)
6. ✅ Submit edit → toast appears with success message, auto-dismisses after 5s
7. ✅ Error state: disconnect internet → reload → shows red error banner

**Team Page:**
1. Navigate to `/team` (admin-only)
2. ✅ Loading skeleton appears briefly during data fetch
3. ✅ Page shows two sections: "Mitglieder (N)" and "Einladungen (N)"
4. ✅ Members table shows email or User-ID with copy buttons
5. ✅ Role badges show correct colors (Admin = purple, Agent = blue, Owner = green)
6. ✅ Copy button works (icon changes to Check, reverts after 2s)
7. ✅ Click "Mitglied einladen" → opens dialog (not browser modal)
8. ✅ Submit invite → dialog closes, toast appears, invites list updates
9. ✅ Click "Widerrufen" on pending invite → confirmation dialog appears
10. ✅ Confirm revoke → toast appears, invite removed from list
11. ✅ Empty state: remove all members/invites → shows icon + "Keine Teammitglieder/Einladungen"
12. ✅ Actions column shows: "Rollenänderungen folgen in einer späteren Phase"

**Troubleshooting:**

**Symptom:** Copy button doesn't work (no feedback, clipboard empty)

**Root Cause:** `navigator.clipboard.writeText()` requires HTTPS or localhost (not supported on HTTP).

**How to Debug:**
```bash
# Check browser console for clipboard errors
# Open DevTools → Console → look for "Failed to copy" or "clipboard-write permission denied"
```

**Solution:**
- Ensure site is served over HTTPS (production: `https://admin.fewo.kolibri-visions.de`)
- Localhost development: works on `http://localhost:3000`
- HTTP deployment: clipboard API will fail silently, fallback not implemented (would require manual `<textarea>` copy workaround)

**Symptom:** Toast messages don't auto-dismiss (stuck on screen)

**Root Cause:** JavaScript timers (`setTimeout`) not firing or component unmounting before timeout.

**How to Debug:**
```bash
# Check browser console for React strict mode warnings
# Check if user navigates away from page before 5s timeout
```

**Solution:**
- Toast timeout set to 5000ms (5 seconds) for success, same for errors
- If navigating away quickly, toast may not dismiss (expected behavior)
- No cleanup on unmount currently implemented (toast state tied to page component)

**Symptom:** Dialogs not centered on mobile (off-screen)

**Root Cause:** Fixed positioning with `flex items-center justify-center` requires viewport height awareness on mobile browsers with URL bar behavior.

**How to Debug:**
```bash
# Test on mobile device or Chrome DevTools mobile emulation
# Check if dialog appears above/below viewport center when URL bar hides/shows
```

**Solution:**
- Current implementation uses `fixed inset-0` which should handle viewport correctly
- If issue persists: add `mx-4` (horizontal margin) to dialog card for mobile padding (already implemented)
- No additional mobile fixes needed for MVP

**Symptom:** Loading skeletons flicker (appear and disappear too fast)

**Root Cause:** API response is very fast (<100ms), skeleton briefly visible causing flicker.

**How to Debug:**
```bash
# Check Network tab in DevTools → see API response time
# If < 200ms, skeleton may flicker
```

**Solution:**
- Acceptable behavior for fast API responses (shows system is performant)
- To prevent flicker: add minimum loading delay (e.g., `await new Promise(r => setTimeout(r, 300))`)
- Not implemented in MVP (prefer fast loading over artificial delays)

**Symptom:** Role badges show wrong colors or labels

**Root Cause:** Backend returns role string not matching frontend badge mapping.

**How to Debug:**
```bash
# Check API response: GET /api/v1/team/members
# Verify role values are exactly "admin", "agent", or "owner" (lowercase)
```

**Solution:**
- Badge mapping defined in `getRoleBadge()` function (frontend/app/team/page.tsx:105-123)
- Supported roles: admin (purple), agent (blue), owner (green)
- Unknown roles fall back to gray badge with raw role string
- Ensure backend returns lowercase role strings

**Mobile Responsiveness:**

**Organisation Page:**
- ✅ Copy button: shows icon + "Kopieren" text on desktop, icon only on mobile would be better but text included for clarity
- ✅ Dialog: max-width constraint + `mx-4` padding prevents overflow on small screens
- ✅ Fields: single column layout (no grid on mobile) for better readability

**Team Page:**
- ✅ Header: flex-col on mobile (button stacks below title), flex-row on desktop
- ✅ Tables: `overflow-x-auto` wrapper allows horizontal scroll on narrow screens
- ✅ Copy buttons: icon-only (compact) for space efficiency
- ✅ Dialogs: max-width constraint + `mx-4` padding prevents overflow

**Performance Notes:**
- Skeleton loaders: pure CSS animations (no JavaScript), negligible performance impact
- Copy-to-clipboard: native browser API (no external library), instant execution
- Toast auto-dismiss: single `setTimeout` per toast, cleaned up with state update
- Dialog rendering: conditional (`{dialog && ...}`), not rendered when closed (no hidden DOM overhead)

### PROD Schema Drift Fix (Epic A)

**Overview:** During Epic A deployment to production, schema drift was discovered where the migration file had syntax errors and missing columns. This section documents the symptoms, resolution, and verification.

**Symptoms (API Errors in PROD):**
- GET /api/v1/agencies/current → 503 Service Unavailable
  - Error detail: `column agencies.email does not exist`
  - Error detail: `column agencies.subscription_tier does not exist`
- POST /api/v1/team/invites → 503 Service Unavailable
  - Error detail: `relation "team_invites" does not exist`
- Migration file `20260111000000_add_epic_a_team_rbac.sql` fails with syntax error:
  - `syntax error at or near "WHERE"` (line 72: invalid WHERE clause on UNIQUE constraint)

**Root Cause:**
1. Migration file had invalid SQL syntax:
   - Table-level UNIQUE constraint with WHERE clause (not supported in PostgreSQL)
   - Should be a partial unique index created separately
2. Migration did not create agencies.email and agencies.subscription_tier columns
3. PROD database was missing team_invites and team_members tables entirely

**Fix Applied:**

**[Supabase SQL Editor]** Applied schema patch:
```sql
-- Add missing agencies columns
ALTER TABLE agencies ADD COLUMN IF NOT EXISTS email TEXT;
ALTER TABLE agencies ADD COLUMN IF NOT EXISTS subscription_tier TEXT;

-- Create team_members table (if missing)
CREATE TABLE IF NOT EXISTS team_members (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  agency_id UUID NOT NULL REFERENCES agencies(id) ON DELETE CASCADE,
  user_id UUID NOT NULL,
  role TEXT NOT NULL CHECK (role IN ('admin', 'agent', 'owner')),
  created_at TIMESTAMPTZ NOT NULL DEFAULT now(),
  updated_at TIMESTAMPTZ NOT NULL DEFAULT now(),
  CONSTRAINT team_members_agency_user_unique UNIQUE (agency_id, user_id)
);

-- Create team_invites table (if missing)
CREATE TABLE IF NOT EXISTS team_invites (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  agency_id UUID NOT NULL REFERENCES agencies(id) ON DELETE CASCADE,
  invited_by_user_id UUID NOT NULL,
  email TEXT NOT NULL,
  role TEXT NOT NULL CHECK (role IN ('admin', 'agent', 'owner')),
  status TEXT NOT NULL DEFAULT 'pending' CHECK (status IN ('pending', 'accepted', 'revoked')),
  created_at TIMESTAMPTZ NOT NULL DEFAULT now(),
  accepted_at TIMESTAMPTZ,
  revoked_at TIMESTAMPTZ
);

-- Create partial unique index for pending invites (idempotent)
DO $$
BEGIN
  IF NOT EXISTS (
    SELECT 1 FROM pg_indexes
    WHERE schemaname = 'public'
      AND tablename = 'team_invites'
      AND indexname = 'team_invites_agency_email_pending_unique'
  ) THEN
    CREATE UNIQUE INDEX team_invites_agency_email_pending_unique
      ON team_invites(agency_id, LOWER(email))
      WHERE status = 'pending';
  END IF;
END $$;

-- Create indexes
CREATE INDEX IF NOT EXISTS team_members_agency_id_idx ON team_members(agency_id);
CREATE INDEX IF NOT EXISTS team_invites_agency_id_idx ON team_invites(agency_id);
CREATE INDEX IF NOT EXISTS team_invites_agency_status_idx ON team_invites(agency_id, status);
```

**Verification Steps:**

**[HOST-SERVER-TERMINAL]** Recheck Epic A endpoints:
```bash
# 1. Check agencies endpoint
curl -i -X GET "https://api.fewo.kolibri-visions.de/api/v1/agencies/current" \
  -H "Authorization: Bearer $JWT_TOKEN"
# Expected: 200 OK with {"id": "...", "name": "...", "email": null, "subscription_tier": null}

# 2. Check team invites endpoint
curl -i -X GET "https://api.fewo.kolibri-visions.de/api/v1/team/invites" \
  -H "Authorization: Bearer $JWT_TOKEN"
# Expected: 200 OK with {"invites": [], "total": 0}

# 3. Run full Epic A smoke test
export API_BASE_URL="https://api.fewo.kolibri-visions.de"
export JWT_TOKEN="<<<admin JWT token>>>"
cd /data/repos/pms-webapp
./backend/scripts/pms_epic_a_onboarding_rbac_smoke.sh
echo "rc=$?"
# Expected: All 6 tests pass, rc=0
```

**PROD Verification (2026-01-11):**

**Backend Deployment:**
- API Base URL: https://api.fewo.kolibri-visions.de
- Deployed Commit: `3d0e1ca9390b209a5bbdaeea69e02bb284a8159e`
- Started At: 2026-01-10T22:20:05.387167+00:00
- Source: GET /api/v1/ops/version

**Deploy Verification:**
```bash
# pms_verify_deploy.sh rc=0 (commit match)
export API_BASE_URL="https://api.fewo.kolibri-visions.de"
EXPECT_COMMIT=3d0e1ca9390b209a5bbdaeea69e02bb284a8159e ./backend/scripts/pms_verify_deploy.sh
# Output: ✅ Deployment verified (commit match)
```

**Epic A Smoke Test:**
```bash
# pms_epic_a_onboarding_rbac_smoke.sh rc=0
./backend/scripts/pms_epic_a_onboarding_rbac_smoke.sh
# Output:
#   ✅ Test 1 PASSED: /me returned user_id=..., role=admin
#   ✅ Test 2 PASSED: Agency name=Kolibri Visions Agency
#   ✅ Test 3 PASSED: Invite created with id=...
#   ✅ Test 4 PASSED: Found 1 invite(s)
#   ✅ Test 5 PASSED: Found 1 member(s)
#   ✅ Test 6 PASSED: Invite revoked
#   ✅ All Epic A smoke tests passed! 🎉
```

**Endpoints Verified:**
- ✅ GET /api/v1/me → 200 (user identity)
- ✅ GET /api/v1/agencies/current → 200 (agency details with email/subscription_tier)
- ✅ POST /api/v1/team/invites → 201 (create invitation)
- ✅ GET /api/v1/team/invites → 200 (list invitations)
- ✅ GET /api/v1/team/members → 200 (list team members)
- ✅ POST /api/v1/team/invites/{id}/revoke → 200 (revoke invitation)

**Migration File Fix:**
- Updated `supabase/migrations/20260111000000_add_epic_a_team_rbac.sql`
- Fixed syntax error: removed invalid WHERE clause from UNIQUE constraint
- Added partial unique index creation with idempotent DO $$ block
- Added agencies.email and agencies.subscription_tier columns (IF NOT EXISTS)
- Migration now runnable from scratch for new installations

---

## Epic B — Direct Booking Funnel (Public → Quote → Request → Review → Confirm)

**Overview:** End-to-end direct booking workflow enabling public guests to check availability, request pricing quotes, submit booking requests, and staff to review/approve/decline requests for confirmation.

**Purpose:** Allow public guests to initiate bookings through a self-service funnel, while staff (manager/admin) review and approve booking requests to create confirmed bookings.

**Architecture:**
- **Public Flow**: Guest checks availability → gets pricing quote (optional) → submits booking request (creates guest if needed)
- **Staff Flow**: Manager/admin reviews pending requests → approves (creates confirmed booking) or declines
- **Idempotency**: All POST endpoints support `Idempotency-Key` header to prevent duplicate submissions
- **Tenant Resolution**: Public endpoints resolve agency via domain or property lookup
- **Status Mapping**: Database "inquiry" status maps to API "under_review" for booking requests
- **Audit Trail**: All state transitions logged via `emit_audit_event` (user_id, action, resource_type, resource_id, details)

**Frontend Routes:**
- `/buchung` - Public booking page (guest funnel: availability check → pricing quote → booking request submission)
- `/booking-requests` - Admin page for managing booking requests (review/approve/decline)

**API Endpoints:**

Public (no authentication):
- `GET /api/v1/public/availability?property_id=&date_from=&date_to=` - Check property availability for date range
- `POST /api/v1/public/booking-requests` - Create booking request (idempotent with `Idempotency-Key` header)

Authenticated (manager/admin):
- `GET /api/v1/properties?limit=&offset=` - List properties for availability check
- `POST /api/v1/pricing/quote` - Get pricing quote for property/dates/guests (best-effort, may 503 if pricing not configured)
- `GET /api/v1/booking-requests?status=&property_id=&limit=&offset=` - List booking requests
- `GET /api/v1/booking-requests/{id}` - Get booking request details
- `POST /api/v1/booking-requests/{id}/review` - Transition booking request to under_review (status: pending → under_review)
- `POST /api/v1/booking-requests/{id}/approve` - Approve booking request (status: under_review/pending → confirmed, creates booking)
- `POST /api/v1/booking-requests/{id}/decline` - Decline booking request (status: * → declined)
- `GET /api/v1/bookings/{id}` - Verify confirmed booking exists

**Database Tables:**
- `booking_requests` - Pending booking requests from guests (status: pending, under_review, confirmed, declined, expired, canceled)
- `bookings` - Confirmed bookings created from approved booking requests
- `guests` - Guest profiles (auto-created from booking request guest data if email not exists)
- `properties` - Properties available for booking
- `audit_events` - Audit log for all booking request state transitions

**Smoke Script:** `backend/scripts/pms_epic_b_direct_booking_funnel_smoke.sh`

**Important Notes:**
- **HOST env var required**: Epic B smoke script uses `HOST` (not `API_BASE_URL`) for the base URL
- **Quote endpoint 422 is WARNING**: When pricing not configured, quote endpoint returns 422; smoke script treats this as optional (WARNING) and continues
- **Auto-shift behavior**: Availability check may auto-shift date window due to existing bookings (double_booking) until free window found (expected behavior, controlled via `SHIFT_DAYS` and `MAX_WINDOW_TRIES`)

**Test Flow:**
1. GET /api/v1/properties (authenticated) - Pick property for test
2. GET /api/v1/public/availability (public) - Check availability (with date shifting on conflict)
3. POST /api/v1/pricing/quote (authenticated) - Get pricing quote (best-effort, continues on 503)
4. POST /api/v1/public/booking-requests (public) - Create booking request with idempotency
5. POST /api/v1/booking-requests/{id}/review (authenticated) - Mark under_review
6. POST /api/v1/booking-requests/{id}/approve (authenticated) - Approve and create booking
7. GET /api/v1/bookings/{id} (authenticated) - Verify booking status=confirmed

**Verification Commands:**

```bash
# [HOST-SERVER-TERMINAL] Pull latest code
cd /data/repos/pms-webapp
git fetch origin main && git reset --hard origin/main

# [HOST-SERVER-TERMINAL] Optional: Verify deploy after Coolify redeploy
export API_BASE_URL="https://api.fewo.kolibri-visions.de"
./backend/scripts/pms_verify_deploy.sh

# [HOST-SERVER-TERMINAL] Run Epic B smoke test
export HOST="https://api.fewo.kolibri-visions.de"
export JWT_TOKEN="<<<manager/admin JWT token>>>"
# Optional:
# export PROPERTY_ID="23dd8fda-59ae-4b2f-8489-7a90f5d46c66"
# export DATE_FROM="2026-03-01"
# export DATE_TO="2026-03-05"
# export SHIFT_DAYS=7
# export MAX_WINDOW_TRIES=10
./backend/scripts/pms_epic_b_direct_booking_funnel_smoke.sh
echo "rc=$?"

# Expected output: All 7 tests pass, rc=0
```

**Common Issues:**

### Availability Check Always Returns Unavailable

**Symptom:** GET /api/v1/public/availability always returns `{"available": false, "reason": "..."}` even for dates with no bookings.

**Root Cause:** Property has blocking availability records, calendar not initialized, or date range conflicts with existing bookings.

**How to Debug:**
```bash
# Check property availability records
psql $DATABASE_URL -c "SELECT id, property_id, date, available, reason FROM property_availability WHERE property_id = '<property_id>' AND date BETWEEN '<date_from>' AND '<date_to>' ORDER BY date;"

# Check existing bookings for property
psql $DATABASE_URL -c "SELECT id, date_from, date_to, status FROM bookings WHERE property_id = '<property_id>' AND status NOT IN ('canceled', 'declined') ORDER BY date_from;"

# Test availability endpoint
curl -X GET "$HOST/api/v1/public/availability?property_id=<property_id>&date_from=2026-03-01&date_to=2026-03-05"
```

**Solution:**
- If property_availability has `available=false` records: Update or delete blocking records
- If overlapping bookings exist: Smoke script will auto-shift dates (use `SHIFT_DAYS=7`, `MAX_WINDOW_TRIES=10`)
- If property not found: Verify PROPERTY_ID exists in properties table and is active

**Smoke Script Auto-Retry Behavior:**
- When availability returns `{"available": false, "reason": "double_booking"}` or any other blocking reason (e.g., "blocked", "maintenance"), the smoke script automatically shifts the date window forward by `SHIFT_DAYS` (default: 7 days) and retries
- Maximum retry attempts: `MAX_WINDOW_TRIES` (default: 10)
- Only fails if no available window found after all retry attempts
- Override retry behavior: `SHIFT_DAYS=3 MAX_WINDOW_TRIES=20 ./backend/scripts/pms_epic_b_direct_booking_funnel_smoke.sh`
- **Important:** `available=false` is treated as normal (not an error) - the script validates field presence and type, then auto-shifts and retries when unavailable

### Pricing Quote Returns 503 (Service Unavailable)

**Symptom:** POST /api/v1/pricing/quote returns 503 with "Pricing service unavailable" error.

**Root Cause:** Pricing rules not configured for property, or pricing service down.

**How to Debug:**
```bash
# Test pricing quote endpoint
curl -X POST "$HOST/api/v1/pricing/quote" \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $JWT_TOKEN" \
  -d '{
    "property_id": "<property_id>",
    "date_from": "2026-03-01",
    "date_to": "2026-03-05",
    "num_adults": 2,
    "num_children": 0
  }' -w "\n%{http_code}\n"

# Check if property has pricing rules
psql $DATABASE_URL -c "SELECT id, property_id, base_price_cents, currency FROM pricing_rules WHERE property_id = '<property_id>';"
```

**Solution:**
- Pricing quote is **best-effort** in Epic B smoke script (continues on 503)
- Configure pricing rules for property via admin UI or SQL insert
- If pricing not needed: Smoke script warning is expected behavior (Test 3 shows "WARNING: Quote endpoint returned 503")

### Quote Returns 422 "No Default Rate Plan Set"

**Symptom:** Quote endpoint returns HTTP 422 with message "No default rate plan set. Found N active rate plan(s). Set one as default or provide explicit rate_plan_id."

**Root Cause:** Multiple active rate plans exist for property/agency, but none are marked as default (is_default=false for all). Quote endpoint cannot determine which plan to use.

**How to Debug:**
```bash
# Check rate plans for property
curl -X GET "$API_BASE_URL/api/v1/pricing/rate-plans?property_id=<property_id>&limit=100" \
  -H "Authorization: Bearer $JWT_TOKEN" | python3 -c "
import sys, json
plans = json.load(sys.stdin).get('items', [])
print(f'Total plans: {len(plans)}')
for p in plans:
    print(f\"  {p['id'][:8]}... name={p['name']} is_default={p.get('is_default')} archived={p.get('archived_at') is not None} active={p.get('active')}\")
"

# Expected: If multiple plans exist and all have is_default=false, 422 is correct behavior

# Check if any plan is set as default (Supabase SQL Editor)
SELECT id, name, property_id, is_default, archived_at, active
FROM rate_plans
WHERE agency_id = '<agency_id>'
  AND (property_id = '<property_id>' OR property_id IS NULL)
  AND archived_at IS NULL
  AND active = true
ORDER BY property_id NULLS LAST, is_default DESC;
# Should show at least one plan with is_default=true
```

**Solution:**
- Set one rate plan as default via PATCH /api/v1/pricing/rate-plans/{id} with {"is_default": true}
- Or use Admin UI: Navigate to rate plans list, click "Make Default" on preferred plan
- Or provide explicit rate_plan_id in quote request body
- Priority: Property-specific default > Agency-level default > Single plan fallback

### Quote Selects Wrong Rate Plan (Default Resolution)

**Symptom:** Quote endpoint selects unexpected rate plan when rate_plan_id not provided (e.g., agency default instead of property default).

**Root Cause:** Default resolution priority not working as expected, or property-specific default is archived/inactive.
- **Priority Order Clarification:** Property-specific defaults (where property_id matches the target property) ALWAYS override agency-level defaults (where property_id IS NULL). This is enforced by the ORDER BY property_id NULLS LAST clause in the default resolution query.

**How to Debug:**
```bash
# Test quote without rate_plan_id
curl -X POST "$API_BASE_URL/api/v1/pricing/quote" \
  -H "Authorization: Bearer $JWT_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "property_id": "<property_id>",
    "check_in": "2026-07-15",
    "check_out": "2026-07-17",
    "adults": 2
  }' | python3 -c "import sys, json; q=json.load(sys.stdin); print(f\"rate_plan_id={q.get('rate_plan_id')} rate_plan_name={q.get('rate_plan_name')}\")"

# Check default plans (Supabase SQL Editor)
-- Property-specific default (priority 1)
SELECT id, name, 'property-default' as type
FROM rate_plans
WHERE agency_id = '<agency_id>'
  AND property_id = '<property_id>'
  AND is_default = true
  AND archived_at IS NULL
  AND active = true;

-- Agency-level default (priority 2)
SELECT id, name, 'agency-default' as type
FROM rate_plans
WHERE agency_id = '<agency_id>'
  AND property_id IS NULL
  AND is_default = true
  AND archived_at IS NULL
  AND active = true;
```

**Solution:**
- Verify property-specific default exists and is not archived: archived_at IS NULL, active=true, is_default=true
- If property default archived, system correctly falls back to agency default
- If neither default exists and only one plan available, system auto-selects it (fallback)
- To force property default: Ensure property_id matches, is_default=true, archived_at IS NULL, active=true
- Priority: Property default > Agency default > Single plan > Error 422

**Resolution Precedence (Detailed)**:

When quote endpoint is called without rate_plan_id, the system resolves in this order:

1. **Property-specific default** (property_id matches, is_default=true, active, not archived)
2. **Single property-specific plan fallback** (exactly 1 active property plan, no default needed)
3. **Agency-level default** (property_id IS NULL, is_default=true, active, not archived)
4. **Single agency-level plan fallback** (exactly 1 active agency plan, no default needed)
5. **Error 422** with counts if multiple plans exist in any checked scope

**Example Error**: "No default rate plan set for property <UUID>. Property-specific active: 2, agency-level active: 5. Set one as default or provide rate_plan_id."

**How to Fix 422**:
- If you want one property plan to auto-select: Archive or delete the others, OR set one as default
- If you want an agency plan to be used: Remove all property plans for this property, ensure only one agency plan exists OR set one as default
- For explicit control: Always provide rate_plan_id in quote requests

### Default Resolution Smoke Fails on Dirty Property

**Symptom:** `pms_pricing_default_resolution_smoke.sh` fails with "Property-specific active: 8, agency-level active: N/A" or similar.

**Root Cause:** The target property has existing rate plans from real usage, causing ambiguity in resolution tests.

**Solution (Automatic):** The smoke script now auto-detects this and creates an isolated SMOKE property. You should see:
```
⚠️  Property has 8 existing non-smoke rate plans. Creating isolated SMOKE property...
✅ Created isolated property: <UUID>
```

If you want to force using a specific property that you know is clean, ensure it has 0 active property-specific rate plans before running the script.

## Pricing Model Decision: Property-Scoped Rate Plans

**Product Decision**: Rate plans are primarily property-scoped (Ferienwohnung/Ferienhaus pricing).

- **Property-scoped rate plans** (property_id = UUID): Active pricing for a specific property
- **Agency-level rate plans** (property_id IS NULL): Templates for copying/reference only

**Smoke Test Implications**:
- Default resolution smoke test only creates/tests property-scoped plans
- Does NOT create or modify agency-level defaults (PROD-safe)
- Requires a clean property (0 active property-scoped plans)
- Fails fast if no clean property available (no auto-mutation)

**Auto-Select Clean Property**:
- When PROPERTY_ID is not set, script fetches all properties for agency
- Validates JSON responses (exits with clear error if non-JSON)
- Prints response preview (first 200 chars) on parse errors for debugging
- Requires valid JWT token with correct agency_id claim

**Admin UI Requirement**:
- All new Admin UI pages must use AdminShell layout (top bar + left nav)
- Prefer route-group layouts: `(admin)/layout.tsx` or per-section `layout.tsx`
- Ensures consistent navigation and user experience

**Cleanup:** Script archives only SMOKE-prefixed rate plans. If isolated property cleanup fails, manually delete property with ID shown in logs.

**Admin UI Behavior (Property-Aware /pricing/rate-plans)**:
- **Property Selector**: Dropdown at top, persisted to localStorage (key: "selectedPropertyId")
- **Two Tabs**:
  - "Tarifpläne" (default): Shows only property-scoped rate plans (property_id != null). API call includes `property_id` parameter.
  - "Vorlagen (Agentur)": Shows agency-level templates (property_id IS NULL). Filtered client-side.
- **Templates Restrictions**: is_default checkbox hidden for templates tab (cannot be set as default)
- **Copy Template Workflow**: "Als Tarifplan kopieren" button copies template to selected property, switches to "Tarifpläne" tab to show new plan
- **Route**: /pricing/rate-plans (no breaking changes)

**Bugfix 2026-01-16** (commit bee4b7b):
- **Success Feedback Visibility**: Toast/banner notifications now properly visible above topbar (fixed z-index stacking, no longer hidden behind navigation)
- **Tab Counts Accuracy**: Tab badge counts ("Tarifpläne (N)", "Vorlagen (M)") remain stable and accurate when switching between tabs and after copy operation (fixed stale state issue)


### Booking Request Creation Returns 400 (Bad Request)

**Symptom:** POST /api/v1/public/booking-requests returns 400 with validation error.

**Root Cause:** Missing required fields, invalid date format, adults/children validation failure, or property_id not found.

**How to Debug:**
```bash
# Check request payload structure
# Required fields: property_id, date_from, date_to, adults, children, currency, guest (first_name, last_name, email, phone)

# Test with minimal valid payload
curl -X POST "$HOST/api/v1/public/booking-requests" \
  -H "Content-Type: application/json" \
  -H "Idempotency-Key: test-$(date +%s)" \
  -d '{
    "property_id": "<property_id>",
    "date_from": "2026-03-01",
    "date_to": "2026-03-05",
    "adults": 2,
    "children": 0,
    "currency": "EUR",
    "guest": {
      "first_name": "Test",
      "last_name": "User",
      "email": "test@example.com",
      "phone": "+49123456789"
    },
    "notes": "",
    "website": ""
  }' -w "\n%{http_code}\n"
```

**Solution:**
- Ensure date_from < date_to (check-out must be after check-in)
- Ensure adults >= 1 (at least one adult required)
- Ensure guest.email is valid email format
- Ensure property_id exists and is active
- Dates must be YYYY-MM-DD format (ISO 8601)

### Review/Approve Endpoints Return 403 (Forbidden)

**Symptom:** POST /api/v1/booking-requests/{id}/review or /approve returns 403 Forbidden.

**Root Cause:** JWT token missing, invalid, or user role is not manager/admin.

**How to Debug:**
```bash
# Check JWT token role claim
echo $JWT_TOKEN | cut -d'.' -f2 | base64 -d | jq '.role'
# Should return "manager" or "admin"

# Test /me endpoint to verify authentication
curl -X GET "$HOST/api/v1/me" \
  -H "Authorization: Bearer $JWT_TOKEN"
# Should return 200 with user_id and role
```

**Solution:**
- Ensure JWT_TOKEN is valid and not expired (use Supabase dashboard to generate new token)
- Verify user role is "manager" or "admin" (not "agent" or "owner")
- Review/approve/decline endpoints require `require_roles("manager", "admin")` dependency

### Approve Endpoint Returns 409 (Conflict)

**Symptom:** POST /api/v1/booking-requests/{id}/approve returns 409 Conflict with "Property not available" error.

**Root Cause:** Dates became unavailable between booking request creation and approval (another booking was confirmed for overlapping dates).

**How to Debug:**
```bash
# Check booking request dates
psql $DATABASE_URL -c "SELECT id, property_id, date_from, date_to, status FROM booking_requests WHERE id = '<booking_request_id>';"

# Check for overlapping confirmed bookings
psql $DATABASE_URL -c "
SELECT id, date_from, date_to, status 
FROM bookings 
WHERE property_id = '<property_id>' 
  AND status NOT IN ('canceled', 'declined')
  AND (
    (date_from, date_to) OVERLAPS ('<req_date_from>'::date, '<req_date_to>'::date)
  )
ORDER BY date_from;
"
```

**Solution:**
- Decline the conflicting booking request (dates no longer available)
- Or cancel/modify the existing overlapping booking first, then approve request
- Epic B smoke script creates fresh booking requests to avoid this (uses unique guest email per run)

### Idempotency Not Working (Duplicate Booking Requests)

**Symptom:** Multiple POST /api/v1/public/booking-requests with same `Idempotency-Key` create duplicate booking requests.

**Root Cause:** Idempotency middleware not enabled, or key format invalid.

**How to Debug:**
```bash
# Test idempotency with same key twice
IDEMPOTENCY_KEY="test-$(date +%s)-unique"

# First request (should create booking request)
curl -X POST "$HOST/api/v1/public/booking-requests" \
  -H "Content-Type: application/json" \
  -H "Idempotency-Key: $IDEMPOTENCY_KEY" \
  -d '{...}' -w "\n%{http_code}\n"
# Expected: 201 Created

# Second request with same key (should return cached response)
curl -X POST "$HOST/api/v1/public/booking-requests" \
  -H "Content-Type: application/json" \
  -H "Idempotency-Key: $IDEMPOTENCY_KEY" \
  -d '{...}' -w "\n%{http_code}\n"
# Expected: 201 Created (same response, no duplicate created)
```

**Solution:**
- Verify idempotency middleware is enabled in `backend/app/core/middleware.py`
- Check `idempotency_cache` table exists (migration `20250106000000_add_idempotency_table.sql`)
- Ensure `Idempotency-Key` header is present and non-empty (format: `<source>-<timestamp>-<random>`)
- Idempotency cache TTL is 24 hours (cached responses expire after 24h)

### Booking Not Created After Approval

**Symptom:** POST /api/v1/booking-requests/{id}/approve returns 200 OK with `status=confirmed` and `booking_id`, but GET /api/v1/bookings/{booking_id} returns 404 Not Found.

**Root Cause:** Database transaction rolled back, or booking creation failed silently.

**How to Debug:**
```bash
# Check booking request status
psql $DATABASE_URL -c "SELECT id, status, booking_id FROM booking_requests WHERE id = '<booking_request_id>';"
# Should show status=confirmed and booking_id not null

# Check if booking exists
psql $DATABASE_URL -c "SELECT id, property_id, guest_id, date_from, date_to, status FROM bookings WHERE id = '<booking_id>';"
# Should return 1 row with status=confirmed

# Check application logs for errors
docker logs <backend-container> | grep -i "error.*booking"
```

**Solution:**
- If booking_request.booking_id is null: Approval transaction failed, retry approval
- If booking exists but GET endpoint returns 404: Agency mismatch (booking belongs to different agency), verify JWT agency_id claim
- Check backend logs for database constraint violations or errors during booking creation

### Date Shifting Fails in Smoke Script

**Symptom:** Smoke script fails with "Could not find available dates after N attempts" even though calendar has availability.

**Root Cause:** All tested date windows are unavailable, or date shifting logic broken (BSD vs GNU date command incompatibility).

**How to Debug:**
```bash
# Test date shifting manually
CHECK_IN="2026-03-01"
SHIFT_DAYS=7

# macOS (BSD date)
date -u -j -v+${SHIFT_DAYS}d -f "%Y-%m-%d" "$CHECK_IN" +%Y-%m-%d
# Expected: 2026-03-08

# Linux (GNU date)
date -u -d "$CHECK_IN + $SHIFT_DAYS days" +%Y-%m-%d
# Expected: 2026-03-08

# Check if gdate (coreutils) is available on macOS
command -v gdate
# If available: brew install coreutils
```

**Solution:**
- Smoke script supports both BSD date (macOS) and GNU date (Linux)
- On macOS: Install GNU coreutils if date shifting fails: `brew install coreutils` (provides `gdate`)
- Override date window manually: `DATE_FROM=2026-04-01 DATE_TO=2026-04-05 ./backend/scripts/pms_epic_b_direct_booking_funnel_smoke.sh`
- Increase max attempts: `MAX_WINDOW_TRIES=20 SHIFT_DAYS=3 ./backend/scripts/pms_epic_b_direct_booking_funnel_smoke.sh`

### PROD Verification (2026-01-11)

**Deployment Verified:**
- **API Base URL**: https://api.fewo.kolibri-visions.de
- **Deployed Commit**: 9016fad5fb6980b122697bc855e7b1c708ea9d67
- **Started At**: 2026-01-11T10:45:05.266237+00:00

**Verification Commands:**

```bash
# [HOST-SERVER-TERMINAL] Check deployed version
curl -sS https://api.fewo.kolibri-visions.de/api/v1/ops/version | jq '.'
# Expected: source_commit matches deployed commit above

# [HOST-SERVER-TERMINAL] Verify deploy (commit match)
cd /data/repos/pms-webapp
git fetch origin main && git reset --hard origin/main
export API_BASE_URL="https://api.fewo.kolibri-visions.de"
./backend/scripts/pms_verify_deploy.sh
# Expected: rc=0, ✅ Deployment verified (commit match)

# [HOST-SERVER-TERMINAL] Run Epic B smoke test
export HOST="https://api.fewo.kolibri-visions.de"
export JWT_TOKEN="<<<manager/admin JWT token>>>"
./backend/scripts/pms_epic_b_direct_booking_funnel_smoke.sh
echo "rc=$?"
# Expected: rc=0, all 7 tests pass
```

**Verified Test Results (2026-01-11):**
- ✅ Test 1 PASSED: Auto-picked property 23dd8fda-59ae-4b2f-8489-7a90f5d46c66
- ✅ Test 2 PASSED: Property available for 2026-02-08 to 2026-02-11 (after auto-shifting from 2026-02-01 due to `available=false` reason: double_booking)
- ⚠️ Test 3 WARNING: Quote endpoint returned HTTP 422 (best-effort endpoint, smoke continues)
- ✅ Test 4 PASSED: Created booking request e9645066-5776-4fdc-a0b5-689e23507ace
- ✅ Test 5 PASSED: Booking request marked as under_review
- ✅ Test 6 PASSED: Booking request approved (booking_id: e9645066-5776-4fdc-a0b5-689e23507ace)
- ✅ Test 7 PASSED: Booking verified (status: confirmed)
- **Result**: rc=0 (all critical tests passed)

**Troubleshooting Notes:**
- **Availability `available=false` responses**: Script automatically shifts date window forward by SHIFT_DAYS (default: 7) and retries up to MAX_WINDOW_TRIES (default: 10). This is expected behavior when dates are unavailable due to double_booking, blocked, or maintenance reasons.
- **Quote step returns HTTP 422**: Pricing quote endpoint is best-effort in current setup. Smoke script treats non-200 responses as WARNING and continues (until Pricing v1 is finalized). This does not block booking request flow.

---

## Epic C — Public Website v1 (White-Label, Block-Based CMS + SEO)

**Overview:** Public-facing website with block-based CMS, white-label customization, and full SEO stack.

**Purpose:** Provide agencies with a customizable public website for showcasing properties, enabling direct booking funnel integration, and SEO optimization.

**Architecture:**
- **Database**: `public_site_settings` (white-label config), `public_site_pages` (block-based CMS pages)
- **Backend API**: Public endpoints for site settings, pages, and properties (tenant-scoped via domain)
- **Frontend**: Next.js public layout with block renderer, SEO components (robots.txt, sitemap.xml, metadata, JSON-LD)
- **Block System**: Flexible content blocks (hero_search, usp_grid, rich_text, faq, etc.) stored as JSONB

**Frontend Routes:**
- `/` - Home page (renders blocks from slug=home)
- `/p/[slug]` - Generic CMS pages (kontakt, faq, angebote, impressum, datenschutz)
- `/unterkuenfte` - Properties listing page
- `/unterkuenfte/[id]` - Property detail page with booking CTA
- `/robots.txt` - Dynamic robots.txt (per tenant)
- `/sitemap.xml` - Dynamic sitemap.xml (pages + properties per tenant)

**API Endpoints (Public, No Auth):**
- `GET /api/v1/public/site/settings` - Get site settings (theme, contact, nav, SEO defaults)
- `GET /api/v1/public/site/pages` - List published pages
- `GET /api/v1/public/site/pages/{slug}` - Get page by slug (title, meta, blocks)
- `GET /api/v1/public/properties?limit=&offset=&q=` - List public properties
- `GET /api/v1/public/properties/{id}` - Get property details

**Database Tables:**
- `public_site_settings` - White-label configuration per agency (site_name, colors, contact, nav, footer_links, seo_defaults)
- `public_site_pages` - Block-based CMS pages per agency (slug, title, meta, blocks JSONB, is_published)
- `properties.is_public` - Boolean flag for public website visibility (default true)

**Migration:** `20260111000001_add_epic_c_public_website.sql`

**Smoke Script:** `backend/scripts/pms_epic_c_public_website_smoke.sh`

**What the smoke test validates:** Site settings, published pages (including "home"), properties list, property detail, robots.txt (with sitemap reference), sitemap.xml (valid XML structure). Uses PUBLIC_HOST env var for tenant resolution headers.

**Customization (Supabase SQL Editor):**

To customize site content, edit the JSONB blocks in `public_site_pages`:

```sql
-- Example: Update home page headline
UPDATE public_site_pages
SET blocks = jsonb_set(
    blocks,
    '{0,props,headline}',
    '"Willkommen in unserem Ferienparadies"'
)
WHERE slug = 'home' AND agency_id = '<your-agency-id>';

-- Example: Add new FAQ item
UPDATE public_site_pages
SET blocks = jsonb_set(
    blocks,
    '{1,props,items}',
    blocks->'1'->'props'->'items' || '[{"question": "Neue Frage?", "answer": "Antwort hier."}]'::jsonb
)
WHERE slug = 'faq' AND agency_id = '<your-agency-id>';

-- Example: Update site settings
UPDATE public_site_settings
SET 
    site_name = 'Meine Ferienvermietung',
    phone = '+49 123 456789',
    email = 'info@beispiel.de',
    primary_color = '#1e40af',
    accent_color = '#3b82f6'
WHERE agency_id = '<your-agency-id>';
```

**Supported Block Types:**
- `hero_search` - Hero section with headline, subheadline, CTA
- `usp_grid` - Grid of USPs/features (icon, title, text)
- `rich_text` - HTML content block
- `contact_cta` - Contact CTA with phone/email
- `faq` - FAQ accordion (question, answer pairs)
- `featured_properties` - Property grid (auto-fetches from API)

**Verification Commands:**

```bash
# [HOST-SERVER-TERMINAL] Pull latest code
cd /data/repos/pms-webapp
git fetch origin main && git reset --hard origin/main

# [HOST-SERVER-TERMINAL] Optional: Verify deploy after Coolify redeploy
export API_BASE_URL="https://api.fewo.kolibri-visions.de"
./backend/scripts/pms_verify_deploy.sh

# [HOST-SERVER-TERMINAL] Run Epic C smoke test
export API_BASE_URL="https://api.fewo.kolibri-visions.de"
# Optional: export PUBLIC_HOST="fewo.kolibri-visions.de"
./backend/scripts/pms_epic_c_public_website_smoke.sh
echo "rc=$?"

# Expected output: All 6 tests pass (or 5 if no properties), rc=0
```

**Manual Browser Verification:**

1. **Home Page** (`https://<domain>/`):
   - ✅ Hero section renders with headline and CTA
   - ✅ USP grid displays features
   - ✅ Navigation shows Unterkünfte, FAQ, Kontakt
   - ✅ Footer displays contact info and legal links

2. **Properties Listing** (`https://<domain>/unterkuenfte`):
   - ✅ Page blocks render (if configured)
   - ✅ Properties grid displays available properties
   - ✅ Click property → navigates to detail page

3. **Property Detail** (`https://<domain>/unterkuenfte/<id>`):
   - ✅ Property name, city, facts display
   - ✅ Description renders
   - ✅ "Jetzt anfragen" CTA links to /buchung?property_id=<id>

4. **Generic Pages** (`https://<domain>/p/kontakt`, `/p/faq`):
   - ✅ Content blocks render correctly
   - ✅ FAQ accordion works
   - ✅ Contact CTA displays phone/email

5. **SEO** (`https://<domain>/robots.txt`, `/sitemap.xml`):
   - ✅ robots.txt returns with sitemap reference
   - ✅ sitemap.xml includes all published pages and properties

**Common Issues:**

### Site Settings Return Default Values

**Symptom:** GET /api/v1/public/site/settings returns generic defaults instead of customized settings.

**Root Cause:** No settings row exists for agency, or tenant resolution failed.

**How to Debug:**
```bash
# Check if settings exist
psql $DATABASE_URL -c "SELECT agency_id, site_name FROM public_site_settings WHERE agency_id = '<agency-id>';"

# Test tenant resolution with explicit headers
curl -H "X-Forwarded-Host: <customer-domain>" \
     -H "Origin: https://<customer-domain>" \
     https://api.fewo.kolibri-visions.de/api/v1/public/site/settings | jq '.agency_id'
```

**Solution:**
- Ensure `public_site_settings` row exists for agency (migration seeds defaults)
- Verify tenant resolution headers: `X-Forwarded-Host` and `Origin` must match customer domain
- Check `agency_domains` table has mapping for custom domain

### Home Page Returns 404

**Symptom:** GET /api/v1/public/site/pages/home returns 404.

**Root Cause:** Migration did not seed default pages, or page was unpublished/deleted.

**How to Debug:**
```bash
# Check if home page exists
psql $DATABASE_URL -c "SELECT slug, title, is_published FROM public_site_pages WHERE slug = 'home' AND agency_id = '<agency-id>';"
```

**Solution:**
- Re-run migration seeding (idempotent): `psql $DATABASE_URL < supabase/migrations/20260111000001_add_epic_c_public_website.sql`
- Or manually insert home page:
  ```sql
  INSERT INTO public_site_pages (agency_id, slug, title, blocks, is_published)
  VALUES ('<agency-id>', 'home', 'Willkommen', '[]'::jsonb, true);
  ```

### Properties Not Showing on Public Website

**Symptom:** GET /api/v1/public/properties returns empty array.

**Root Cause:** All properties have `is_public = false`, or no properties exist for agency.

**How to Debug:**
```bash
# Check properties count and is_public status
psql $DATABASE_URL -c "SELECT id, name, is_public FROM properties WHERE agency_id = '<agency-id>' LIMIT 10;"
```

**Solution:**
- Set `is_public = true` for properties: `UPDATE properties SET is_public = true WHERE agency_id = '<agency-id>';`
- Verify properties table has rows for the agency

### Sitemap.xml Empty or Errors

**Symptom:** /sitemap.xml returns empty or malformed XML.

**Root Cause:** API fetch failed, or no published pages/properties available.

**How to Debug:**
```bash
# Test sitemap endpoint
curl https://<domain>/sitemap.xml

# Check if pages exist
curl -H "X-Forwarded-Host: <domain>" https://api.fewo.kolibri-visions.de/api/v1/public/site/pages | jq '.'
```

**Solution:**
- Ensure API is accessible from frontend (NEXT_PUBLIC_API_BASE or same-domain deployment)
- Verify published pages exist: `SELECT slug FROM public_site_pages WHERE is_published = true;`
- Check network logs for API fetch errors

### Block Renderer Shows "Unknown block type"

**Symptom:** Frontend displays yellow "Unknown block type: X" placeholders.

**Root Cause:** Block type in database not implemented in BlockRenderer component.

**How to Debug:**
```bash
# Check which block types are used
psql $DATABASE_URL -c "
  SELECT DISTINCT jsonb_array_elements(blocks)->>'type' AS block_type
  FROM public_site_pages
  WHERE agency_id = '<agency-id>';
"
```

**Solution:**
- Implement missing block type in `frontend/app/(public)/components/BlockRenderer.tsx`
- Or update database to use supported block types (hero_search, usp_grid, rich_text, contact_cta, faq, featured_properties)

### API Endpoints Return 404 Not Found

**Symptom:** GET /api/v1/public/site/settings returns 404 with `{"detail":"Not Found"}`, even though deployment commit is correct and /api/v1/public/ping works.

**Root Cause:** Public site router not mounted, wrong prefix used, or router gated behind environment flag/module condition.

**How to Debug:**
```bash
# Check if routes appear in OpenAPI
curl https://api.fewo.kolibri-visions.de/openapi.json | grep -c "/api/v1/public/site/settings"
# Should return 1 or more (if 0, router not mounted)

curl https://api.fewo.kolibri-visions.de/openapi.json | grep -c "/api/v1/public/properties"
# Should return 1 or more (if 0, router not mounted)

# Check backend startup logs for router mounting messages
# [Coolify container logs or docker logs <backend-container>]
# Look for: "Failsafe: Public site router mounted" or "Public site router already mounted"
```

**Fix Checklist:**
1. **Verify router registration in `backend/app/main.py`**:
   - Check failsafe path (MODULES_ENABLED=true): Both `public_booking_routes_exist` AND `public_site_routes_exist` checks must be present
   - Check fallback path (MODULES_ENABLED=false): Both `public_booking.router` AND `public_site.router` must be included
   - Ensure correct prefix: `prefix="/api/v1/public"`
   - Ensure correct tags: `tags=["Public Website"]`

2. **Check environment flags**:
   - If MODULES_ENABLED=true, ensure module system includes public_site router
   - If MODULES_ENABLED=false, ensure fallback path mounts router explicitly

3. **Verify route paths in router file** (`backend/app/api/routes/public_site.py`):
   - Routes should be defined as `/site/settings`, `/site/pages`, `/properties` (WITHOUT /api/v1/public prefix)
   - Prefix is added when mounting with `app.include_router(..., prefix="/api/v1/public")`

4. **Re-run smoke script with OpenAPI preflight**:
   ```bash
   API_BASE_URL=https://api.fewo.kolibri-visions.de \
   ./backend/scripts/pms_epic_c_public_website_smoke.sh
   # Preflight will fail with actionable error if routes not in OpenAPI
   ```

**Solution:**
- If failsafe check was too broad (only checked for any `/api/v1/public` routes), update to check specifically for `/api/v1/public/site/settings` route
- Ensure both failsafe and fallback paths mount `public_site.router`
- Redeploy and verify OpenAPI contains routes before running full smoke test

### API Endpoints Return 500 Internal Server Error

**Symptom:** GET /api/v1/public/site/settings returns 500 with `{"detail":"Internal server error"}`. Backend logs show: `'Connection' object has no attribute 'headers'` or `AttributeError: 'Connection' object has no attribute 'headers'`.

**Root Cause:** Tenant resolution function `resolve_agency_id_for_public_endpoint()` called with swapped arguments. The function expects `(request: Request, conn: Connection, ...)` but was called with `(conn, request, ...)`, causing the Connection object to be passed where Request is expected.

**How to Debug:**
```bash
# Check backend logs for the full traceback
# [Coolify container logs or docker logs <backend-container>]
# Look for:
#   - "AttributeError: 'Connection' object has no attribute 'headers'"
#   - Call stack showing: resolve_agency_id_for_public_endpoint → extract_request_host → request.headers.get

# Verify function signature in backend/app/core/tenant_domain.py
rg -A 5 "async def resolve_agency_id_for_public_endpoint" backend/app/core/tenant_domain.py
# Should show: (request: Request, conn, *, property_id=..., trust_proxy=...)

# Verify call sites match signature
rg -B 2 -A 2 "resolve_agency_id_for_public_endpoint\(" backend/app/api/routes/
# All calls should use: (request, conn) or (request, db) positionally
```

**Fix Checklist:**
1. **Verify function signature** in `backend/app/core/tenant_domain.py`:
   - Must be: `async def resolve_agency_id_for_public_endpoint(request: Request, conn, *, property_id=None, trust_proxy=True)`
   - Request MUST be first parameter, connection MUST be second parameter

2. **Verify all call sites** in `backend/app/api/routes/`:
   - `public_site.py`: Should call with `(request, conn)` positionally
   - `public_booking.py`: Should call with `(http_request, db, property_id=..., trust_proxy=...)`
   - NO calls should use `db=db, request=request` with old parameter order

3. **Check defensive guard is present**:
   - Function should have `if not hasattr(request, 'headers'):` guard at the top
   - Guard should raise RuntimeError with clear message about swapped args

4. **Verify tenant resolution works**:
   ```bash
   # Test with X-Forwarded-Host header
   curl -sS -i \
     -H "X-Forwarded-Host: fewo.kolibri-visions.de" \
     -H "Origin: https://fewo.kolibri-visions.de" \
     "https://api.fewo.kolibri-visions.de/api/v1/public/site/settings"
   # Should return HTTP 200 with JSON containing agency_id
   ```

**Solution:**
- Update function signature to `(request: Request, conn, *, ...)` with request FIRST
- Update all call sites to match the signature (positional or named args consistent with new order)
- Add defensive runtime guard to catch swapped args early
- Redeploy and verify tenant resolution works with X-Forwarded-Host/Host headers

**Related:** Tenant resolution uses X-Forwarded-Host (preferred) or Host header to resolve agency_id via `agency_domains` table. Ensure TRUST_PROXY_HEADERS is set correctly and agency has a domain mapping in the database.

### Smoke Test 6 Fails with jq Parsing Error

**Symptom:** Epic C smoke script fails at Test 6 with error: `jq: Cannot index object with number`. Tests 1-5 pass successfully (settings, pages, home, properties list).

**Root Cause:** Smoke script assumed properties list endpoint returns a plain array `[{...}]`, but may encounter paginated object response `{items: [...], total: N}` or error object. The jq expression `.[0].id` fails when response is an object structure instead of array.

**How to Debug:**
```bash
# Test properties list endpoint directly
curl -sS \
  -H "X-Forwarded-Host: fewo.kolibri-visions.de" \
  -H "Origin: https://fewo.kolibri-visions.de" \
  "https://api.fewo.kolibri-visions.de/api/v1/public/properties?limit=10" | jq 'type'

# Should return: "array" or "object"

# If object, check structure
curl -sS \
  -H "X-Forwarded-Host: fewo.kolibri-visions.de" \
  -H "Origin: https://fewo.kolibri-visions.de" \
  "https://api.fewo.kolibri-visions.de/api/v1/public/properties?limit=10" | jq 'keys'

# If array, check first element
curl -sS \
  -H "X-Forwarded-Host: fewo.kolibri-visions.de" \
  -H "Origin: https://fewo.kolibri-visions.de" \
  "https://api.fewo.kolibri-visions.de/api/v1/public/properties?limit=10" | jq '.[0] | keys'
```

**Fix Applied:**
- Smoke script now handles both response shapes:
  - Plain array: `[{id:...}, {id:...}]`
  - Paginated object: `{items: [{id:...}], total: N}`
- Added HTTP status code checks for Test 5 (properties list)
- Added robust validation for Test 6 (property detail):
  - Validates response is object with `.id` field
  - Verifies returned ID matches requested ID
  - Shows truncated response body (first 200 chars) on errors for debugging
- No API contract changes needed (endpoint returns array as designed)

**Verification:**
```bash
# Run smoke script with enhanced diagnostics
API_BASE_URL=https://api.fewo.kolibri-visions.de \
PUBLIC_HOST=fewo.kolibri-visions.de \
./backend/scripts/pms_epic_c_public_website_smoke.sh

# Expected: All 6 tests pass, rc=0
# If Test 6 fails, script now shows:
# - HTTP status code
# - Response shape (object vs array)
# - First 200 chars of response body
```

### Public Properties Endpoint Returns 500 (Duplicate Currency Kwarg)

**Symptom:** GET /api/v1/public/properties returns 500 Internal Server Error. Backend logs show: `TypeError: PublicPropertyListItem() got multiple values for keyword argument 'currency'` in `backend/app/api/routes/public_site.py` line ~265.

**Root Cause:** Property list endpoint passes `**dict(row)` which already contains `currency` field from SQL SELECT, then also passes `currency=row.get("currency", "EUR")` as a keyword argument, causing duplicate kwarg error. Same issue exists in property detail endpoint.

**How to Debug:**
```bash
# Test properties list endpoint (use X-Forwarded-Host, NOT Host header override)
curl -k -sS -i \
  -H "X-Forwarded-Host: fewo.kolibri-visions.de" \
  -H "Origin: https://fewo.kolibri-visions.de" \
  "https://api.fewo.kolibri-visions.de/api/v1/public/properties?limit=1"

# Expected: HTTP 200 with JSON array
# If 500, check backend logs for TypeError about duplicate currency kwarg

# GOTCHA: Do NOT override Host header directly (e.g., -H "Host: fewo.kolibri-visions.de")
# This can cause Traefik 503 "no available server" because Traefik routing uses
# the Host header to select backend. Always use X-Forwarded-Host for tenant resolution
# while keeping Host as api.fewo.kolibri-visions.de for routing.
```

**Fix Applied:**
- List endpoint (`/api/v1/public/properties`): Refactored to pop `currency` from dict, apply default if null/empty, then set explicitly before passing to model
- Detail endpoint (`/api/v1/public/properties/{id}`): Same fix applied
- Pattern:
  ```python
  data = dict(row)
  currency = data.pop("currency", None) or "EUR"
  data["currency"] = currency
  return PublicPropertyListItem(**data, images=[])
  ```
- Ensures currency defaults to "EUR" if DB value is null/empty, without duplicate kwarg

**Verification:**
```bash
# After fix deployed, test properties list
curl -k -sS -i \
  -H "X-Forwarded-Host: fewo.kolibri-visions.de" \
  -H "Origin: https://fewo.kolibri-visions.de" \
  "https://api.fewo.kolibri-visions.de/api/v1/public/properties?limit=1"

# Expected: HTTP 200 with JSON array containing properties with currency field

# Run Epic C smoke test
API_BASE_URL=https://api.fewo.kolibri-visions.de \
PUBLIC_HOST=fewo.kolibri-visions.de \
./backend/scripts/pms_epic_c_public_website_smoke.sh
# Expected: rc=0, all 6 tests pass
```

### Public Properties Endpoint Returns 500 (Bathrooms Decimal/Fractional)

**Symptom:** GET /api/v1/public/properties returns 500 Internal Server Error. Backend logs show Pydantic `ValidationError`: `PublicPropertyListItem.bathrooms` expects `int` but got `Decimal('1.5')` in `backend/app/api/routes/public_site.py` line ~270.

**Root Cause:** Database stores `bathrooms` as `numeric` (can be fractional like 1.5 for "1 and a half bathrooms"). PostgreSQL returns these as `Decimal` objects. Original Pydantic schema defined `bathrooms: Optional[int]`, rejecting fractional values. Properties with integer bathrooms (1, 2) work fine, but properties with fractional bathrooms (1.5, 2.5) trigger ValidationError.

**Why Limit Matters:**
- `limit=1` might return 200 if first property has integer bathrooms
- `limit=10` fails if ANY property in results has fractional bathrooms
- This explains intermittent failures depending on query results

**How to Debug:**
```bash
# Test with small limit (may pass if first property has integer bathrooms)
curl -k -sS -i \
  -H "X-Forwarded-Host: fewo.kolibri-visions.de" \
  -H "Origin: https://fewo.kolibri-visions.de" \
  "https://api.fewo.kolibri-visions.de/api/v1/public/properties?limit=1"

# Test with larger limit (more likely to hit fractional bathrooms)
curl -k -sS -i \
  -H "X-Forwarded-Host: fewo.kolibri-visions.de" \
  -H "Origin: https://fewo.kolibri-visions.de" \
  "https://api.fewo.kolibri-visions.de/api/v1/public/properties?limit=10"

# Expected after fix: Both return HTTP 200 with valid JSON array
# bathrooms field shows as number (e.g., 1, 1.5, 2.0) in response

# Check backend logs for ValidationError if still failing
# Look for: "value is not a valid integer" or "got Decimal('1.5')"
```

**Fix Applied:**
- Schema updated: `bathrooms: Optional[int]` → `Optional[float]` in both `PublicPropertyListItem` and `PublicPropertyDetail`
- Defensive conversion added to both endpoints:
  ```python
  if "bathrooms" in data and data["bathrooms"] is not None:
      data["bathrooms"] = float(data["bathrooms"])
  ```
- Ensures Decimal values from DB are converted to float before Pydantic validation
- OpenAPI now shows bathrooms as `number` (accepts integers and fractional values)

**Verification:**
```bash
# After fix deployed, test properties list with large limit
curl -k -sS -i \
  -H "X-Forwarded-Host: fewo.kolibri-visions.de" \
  -H "Origin: https://fewo.kolibri-visions.de" \
  "https://api.fewo.kolibri-visions.de/api/v1/public/properties?limit=10"

# Expected: HTTP 200 with JSON array, bathrooms field shows as 1.5, 2.0, etc.

# Run Epic C smoke test
API_BASE_URL=https://api.fewo.kolibri-visions.de \
PUBLIC_HOST=fewo.kolibri-visions.de \
./backend/scripts/pms_epic_c_public_website_smoke.sh
# Expected: rc=0, all 6 tests pass (Test 5 no longer fails with 500)
```

### PROD Verification Evidence (2026-01-11)

**Status:** ✅ VERIFIED in production

**Evidence:**
- **Source Commit:** d5e92fa8dd4fca874e61a6ccafb76933a351b6b2 (verified via /api/v1/ops/version)
- **Deploy Verification:** `backend/scripts/pms_verify_deploy.sh` rc=0 (commit match exact)
- **Epic C Smoke Test:** `backend/scripts/pms_epic_c_public_website_smoke.sh` rc=0 (all 6 tests passed)
- **Agency:** ffd0123a-10b6-40cd-8ad5-66eee9757ab7
- **Stats:** 7 pages, 10 properties, fractional bathrooms supported
- **Conclusion:** Epic C fully operational in production at https://api.fewo.kolibri-visions.de with tenant resolution via fewo.kolibri-visions.de


### Epic C Post-Verification Hardening

**Overview:** Post-verification improvements to Epic C public website APIs for production reliability and performance.

**Purpose:** Harden public website endpoints with caching headers, pagination support, and SEO file edge case handling.

**Changes Made:**
1. **Caching Headers**: All public GET endpoints return Cache-Control headers with stale-while-revalidate support
2. **Pagination**: `/api/v1/public/properties` supports optional `?paginated=true` query param (backward compatible)
3. **robots.txt**: Always returns 200 with cache headers, uses X-Forwarded-Host for tenant resolution
4. **sitemap.xml**: Always returns 200 with valid XML, improved error handling, backward compatible API response parsing
5. **Smoke Test Extension**: Tests 7-8 added for robots.txt and sitemap.xml validation

**Architecture:**

- **Cache-Control Headers** (backend/app/api/routes/public_site.py):
  - Site settings/pages: `public, max-age=60, stale-while-revalidate=300` (1 min fresh, 5 min stale)
  - Properties list/detail: `public, max-age=30, stale-while-revalidate=120` (30s fresh, 2 min stale)
  - Helper: `add_cache_headers(response, max_age, stale_while_revalidate)`

- **Pagination** (backward compatible):
  - Default: Returns array `[{id, name, ...}, ...]` (existing clients unaffected)
  - With `?paginated=true`: Returns `{items: [...], total: N, limit: L, offset: O}`
  - Uses COUNT query for total when paginated=true
  - Pydantic model: `PaginatedPropertiesResponse`

- **robots.txt** (frontend/app/robots.txt/route.ts):
  - Uses X-Forwarded-Host header (proxy support)
  - Always returns 200 text/plain
  - Cache-Control: `public, max-age=300` (5 min)
  - Includes Sitemap reference

- **sitemap.xml** (frontend/app/sitemap.xml/route.ts):
  - Uses X-Forwarded-Host header (proxy support)
  - Separate try/catch for pages and properties fetches (graceful degradation)
  - Backward compatible: handles both array and `{items: [...]}` API responses
  - Always returns 200 application/xml (even if empty)
  - Cache-Control: `public, max-age=300` (5 min)
  - Includes /unterkuenfte listing page when properties exist

**Smoke Test Extension** (backend/scripts/pms_epic_c_public_website_smoke.sh):
- Test 7: Validates robots.txt contains "Sitemap:" line
- Test 8: Validates sitemap.xml contains "<urlset" (valid XML structure)
- Both tests check HTTP 200 response

**Verification Commands:**

```bash
# [HOST-SERVER-TERMINAL] Pull latest code
cd /data/repos/pms-webapp
git fetch origin main && git reset --hard origin/main

# [HOST-SERVER-TERMINAL] Run Epic C smoke test (now includes Tests 7-8)
API_BASE_URL=https://api.fewo.kolibri-visions.de \
PUBLIC_HOST=fewo.kolibri-visions.de \
./backend/scripts/pms_epic_c_public_website_smoke.sh
# Expected: rc=0, all 8 tests pass (Tests 7-8 new)

# [CLIENT-TERMINAL] Test cache headers
curl -I https://api.fewo.kolibri-visions.de/api/v1/public/site/settings \
  -H "X-Forwarded-Host: fewo.kolibri-visions.de"
# Expected: Cache-Control: public, max-age=60, stale-while-revalidate=300

curl -I https://api.fewo.kolibri-visions.de/api/v1/public/properties?limit=10 \
  -H "X-Forwarded-Host: fewo.kolibri-visions.de"
# Expected: Cache-Control: public, max-age=30, stale-while-revalidate=120

# [CLIENT-TERMINAL] Test pagination (backward compatible)
# Default: array response
curl -sS https://api.fewo.kolibri-visions.de/api/v1/public/properties?limit=3 \
  -H "X-Forwarded-Host: fewo.kolibri-visions.de" | jq 'type'
# Expected: "array"

# With paginated=true: wrapper response
curl -sS "https://api.fewo.kolibri-visions.de/api/v1/public/properties?limit=3&paginated=true" \
  -H "X-Forwarded-Host: fewo.kolibri-visions.de" | jq '{type: type, keys: keys}'
# Expected: {"type": "object", "keys": ["items", "total", "limit", "offset"]}

# [CLIENT-TERMINAL] Test robots.txt and sitemap.xml
curl -I https://fewo.kolibri-visions.de/robots.txt
# Expected: HTTP 200, Cache-Control: public, max-age=300

curl -sS https://fewo.kolibri-visions.de/robots.txt | grep "Sitemap:"
# Expected: Sitemap: https://fewo.kolibri-visions.de/sitemap.xml

curl -I https://fewo.kolibri-visions.de/sitemap.xml
# Expected: HTTP 200, Content-Type: application/xml, Cache-Control: public, max-age=300

curl -sS https://fewo.kolibri-visions.de/sitemap.xml | head -5
# Expected: <?xml version="1.0"...?> <urlset xmlns="...">
```

**Common Issues:**

### Cache Headers Not Present

**Symptom:** curl -I shows no Cache-Control header on public endpoints.

**Root Cause:** Response parameter not added to endpoint signature or add_cache_headers not called.

**How to Debug:**
```bash
# Check if Response is imported
grep "from fastapi import.*Response" backend/app/api/routes/public_site.py

# Check if response parameter in endpoint signature
grep "response: Response" backend/app/api/routes/public_site.py

# Check if add_cache_headers called
grep "add_cache_headers" backend/app/api/routes/public_site.py
```

**Solution:**
- Verify all public GET endpoints have `response: Response` parameter
- Verify all endpoints call `add_cache_headers(response, max_age, stale_while_revalidate)` before async operations
- Redeploy backend

### Pagination Breaks Existing Clients

**Symptom:** Frontend or external clients receive unexpected response shape.

**Root Cause:** Default response changed from array to object (backward compatibility broken).

**How to Debug:**
```bash
# Test default response (should be array)
curl -sS "https://api.fewo.kolibri-visions.de/api/v1/public/properties?limit=3" \
  -H "X-Forwarded-Host: fewo.kolibri-visions.de" | jq 'type'
# Expected: "array" (NOT "object")

# Test paginated response (should be object)
curl -sS "https://api.fewo.kolibri-visions.de/api/v1/public/properties?limit=3&paginated=true" \
  -H "X-Forwarded-Host: fewo.kolibri-visions.de" | jq 'type'
# Expected: "object"
```

**Solution:**
- Verify `paginated` query param defaults to False: `paginated: bool = Query(False, ...)`
- Verify response logic: `if paginated: return PaginatedPropertiesResponse(...) else: return items`
- Existing clients without `?paginated=true` must receive array (backward compatible)

### sitemap.xml Returns Empty or Errors

**Symptom:** sitemap.xml returns empty <urlset> or 500 error.

**Root Cause:** API fetch errors not handled gracefully, or response parsing fails.

**How to Debug:**
```bash
# Check backend logs for API fetch errors during sitemap generation
# Look for "Error fetching pages for sitemap" or "Error fetching properties for sitemap"

# Test API endpoints directly
curl -sS "https://api.fewo.kolibri-visions.de/api/v1/public/site/pages" \
  -H "X-Forwarded-Host: fewo.kolibri-visions.de" | jq 'type, length'

curl -sS "https://api.fewo.kolibri-visions.de/api/v1/public/properties?limit=100" \
  -H "X-Forwarded-Host: fewo.kolibri-visions.de" | jq 'type'
```

**Solution:**
- sitemap.xml should have separate try/catch for pages and properties fetches
- Handle both array and `{items: [...]}` response shapes: `properties = Array.isArray(data) ? data : (data.items || [])`
- Always return valid XML (even if empty): `<urlset>` must close even if no pages/properties
- Verify X-Forwarded-Host and Origin headers passed to API fetch


### /robots.txt or /sitemap.xml Returns 404 (Backend)

**Symptom:** GET /robots.txt or /sitemap.xml on backend API returns 404 {"detail":"Not Found"}.

**Root Cause:** Backend root meta router not mounted or mounted with incorrect prefix.

**How to Debug:**
```bash
# Test backend root routes with tenant headers
curl -I https://api.fewo.kolibri-visions.de/robots.txt \
  -H "X-Forwarded-Host: fewo.kolibri-visions.de"
# Expected: HTTP 200, Content-Type: text/plain

curl -I https://api.fewo.kolibri-visions.de/sitemap.xml \
  -H "X-Forwarded-Host: fewo.kolibri-visions.de"
# Expected: HTTP 200, Content-Type: application/xml

# Check backend logs for router mounting
# Look for: "Root meta routes mounted at / (robots.txt, sitemap.xml)"
```

**Solution:**
- Verify `backend/app/api/routes/public_root_meta.py` exists
- Verify `backend/app/main.py` imports and mounts the router WITHOUT prefix:
  ```python
  from .api.routes import public_root_meta
  app.include_router(public_root_meta.router)  # NO prefix parameter
  ```
- Router must be mounted in BOTH failsafe section (MODULES_ENABLED=true) and fallback section (MODULES_ENABLED=false)
- Redeploy backend

### Domain Verify Fails with httpx TLSError

**Symptom:** Admin UI "Jetzt prüfen" button fails with "module 'httpx' has no attribute 'TLSError'" or similar AttributeError.

**Root Cause:** httpx.TLSError does not exist in httpx API. Correct exceptions are `httpx.TransportError`, `httpx.TimeoutException`, `ssl.SSLError`.

**How to Debug:**
```bash
# Check backend logs for AttributeError traceback
# Look for: AttributeError: module 'httpx' has no attribute 'TLSError'

# Verify exception handling in domain verify endpoint
grep "httpx.TLSError" backend/app/api/routes/public_domain_admin.py
# Should return nothing (TLSError should not be used)

grep "ssl.SSLError" backend/app/api/routes/public_domain_admin.py
# Should return matches (correct exception)
```

**Solution:**
- Import `ssl` module at top of `public_domain_admin.py`
- Replace exception handling:
  ```python
  # WRONG (httpx.TLSError does not exist):
  except (httpx.TLSError, httpx.ConnectError) as e:

  # CORRECT:
  except (httpx.TimeoutException, httpx.TransportError, ssl.SSLError) as e:
  ```
- Use `timeout=5.0, follow_redirects=True` in AsyncClient for domain verification
- Only accept HTTP 200 as verified (404 means robots.txt missing, return 409 with actionable message)
- Redeploy backend

---

### Public Domain Self-Service (Admin UI)

**Overview:** Agency admins can configure custom domain for public website via Admin UI (/organisation page).

**Purpose:** Allow agencies to use their own domain (e.g., ferienwohnungen-beispiel.de) instead of default subdomain (e.g., fewo.kolibri-visions.de) for their public website.

**Architecture:**
- **Database Table**: `agency_domains` (id, agency_id, domain, is_primary, validated_at)
- **Admin API Endpoints** (auth required): GET/PUT/POST for domain status, save, verify
- **SSRF Protection**: Reject localhost, *.local, private IPs; no redirect following; short timeout
- **Domain Validation**: Normalize domain (remove protocol/path/port), check uniqueness per agency
- **Verification**: HTTP/HTTPS GET to https://<domain>/robots.txt with 3s timeout (best-effort)

**Admin UI Location:**
- **Route**: `/organisation` page (authenticated, admin/manager only)
- **Section**: "Öffentliche Website" card with domain input, status pill, action buttons

**API Endpoints (Auth Required):**
- `GET /api/v1/public-site/domain` - Get current domain status (missing/pending/verified)
- `PUT /api/v1/public-site/domain` - Save/update domain (sets is_primary=true, validated_at=NULL)
- `POST /api/v1/public-site/domain/verify` - Verify domain reachability (updates validated_at on success)

**Domain Statuses:**
- **missing**: No domain configured (domain=NULL)
- **pending**: Domain saved but not yet validated (validated_at=NULL)
- **verified**: Domain validated (validated_at IS NOT NULL)

**Verification Process:**
1. User enters domain in Admin UI (e.g., "beispiel.de")
2. Backend normalizes domain (remove https://, path, port)
3. Backend validates domain (not private IP, not in use by other agency)
4. Backend saves domain with is_primary=true, validated_at=NULL (status=pending)
5. User clicks "Jetzt prüfen" button
6. Backend attempts HTTP GET to https://<domain>/robots.txt (3s timeout, no redirects)
7. On success (200/301/302/404): updates validated_at=NOW() (status=verified)
8. On failure: returns 409 with guidance message

**DNS Setup Requirements:**
- CNAME record: `<custom-domain>` → `fewo.kolibri-visions.de`
- OR A/AAAA records pointing to server IP
- Proxy (Coolify/Traefik): Add custom domain as Host for Public Website app
- SSL certificate: Automatic via Let's Encrypt (Coolify/Traefik)

**SSRF Protection:**
- No redirect following (`follow_redirects=False` in httpx)
- Reject private/local domains: localhost, *.local, 127.*, 10.*, 172.16-31.*, 192.168.*, ::1
- Short timeout: 3 seconds
- Domain normalization: strip protocol, path, query, fragment, port

**Verification Commands:**

```bash
# [HOST-SERVER-TERMINAL] Pull latest code
cd /data/repos/pms-webapp
git fetch origin main && git reset --hard origin/main

# [HOST-SERVER-TERMINAL] Optional: Verify deploy after Coolify redeploy
export API_BASE_URL="https://api.fewo.kolibri-visions.de"
./backend/scripts/pms_verify_deploy.sh

# [LOCAL-TERMINAL] Manual API test (requires manager/admin JWT)
export HOST="https://api.fewo.kolibri-visions.de"
export JWT_TOKEN="<your-manager-or-admin-jwt>"

# Get domain status
curl -X GET "$HOST/api/v1/public-site/domain" \
  -H "Authorization: Bearer $JWT_TOKEN"
# Expected: {"domain": null, "status": "missing", ...} OR {"domain": "...", "status": "pending|verified", ...}

# Save domain
curl -X PUT "$HOST/api/v1/public-site/domain" \
  -H "Authorization: Bearer $JWT_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"domain": "beispiel.de"}'
# Expected: 200 OK with {"domain": "beispiel.de", "status": "pending", "validated": false, ...}

# Verify domain
curl -X POST "$HOST/api/v1/public-site/domain/verify" \
  -H "Authorization: Bearer $JWT_TOKEN"
# Expected (success): 200 OK with {"success": true, "status": "verified", ...}
# Expected (failure): 409 Conflict with guidance message

# UI verification
# 1. Login to Admin UI: https://admin.fewo.kolibri-visions.de
# 2. Navigate to /organisation
# 3. Scroll to "Öffentliche Website" section
# 4. Enter domain (e.g., "beispiel.de")
# 5. Click "Speichern" → Should show status "Nicht verifiziert" (yellow pill)
# 6. Click "Jetzt prüfen" → Should show "Verifiziert" (green pill) if DNS/proxy configured
# 7. Click "Website öffnen" → Should open https://beispiel.de in new tab
```

**Common Issues:**

### Domain Verification Fails (409)

**Symptom:** POST /api/v1/public-site/domain/verify returns 409 with "Domain ist nicht erreichbar".

**Root Cause:** DNS not configured, Proxy/Traefik not configured, or SSL certificate missing.

**How to Debug:**
```bash
# Check DNS resolution
dig beispiel.de
# Expected: A/AAAA records pointing to server IP OR CNAME to fewo.kolibri-visions.de

# Check if domain reachable from outside
curl -I https://beispiel.de/robots.txt
# Expected: HTTP 200/301/302/404 (not timeout/connection refused)

# Check Traefik/Coolify configuration (HOST-SERVER-TERMINAL)
# Ensure domain is added as Host for Public Website app

# Check SSL certificate
openssl s_client -connect beispiel.de:443 -servername beispiel.de </dev/null 2>/dev/null | openssl x509 -noout -subject -issuer -dates
# Expected: Valid Let's Encrypt certificate (not expired, not self-signed)
```

**Solution:**
1. Configure DNS: Add CNAME record `beispiel.de` → `fewo.kolibri-visions.de`
2. Configure Proxy: In Coolify, add `beispiel.de` to Public Website app's domains
3. Wait for DNS propagation (5-60 minutes)
4. Wait for SSL certificate issuance (automatic via Let's Encrypt, 1-5 minutes)
5. Retry verification in Admin UI

**Note (Smoke Test Behavior):**
- 409 response from POST /api/v1/public-site/domain/verify is EXPECTED when DNS is not configured or domain is not yet reachable
- Smoke script (`backend/scripts/pms_epic_c_public_domain_smoke.sh`) treats 409 as a warning, not a failure (exit code remains 0)
- This confirms backend validation logic works correctly; actual domain verification depends on external DNS/network configuration
- Feature is fully functional even when verify returns 409; it indicates domain not yet publicly reachable (expected for new/unconfigured domains)

### Domain Already Used by Another Agency (409)

**Symptom:** PUT /api/v1/public-site/domain returns 409 with "Domain wird bereits von einer anderen Organisation verwendet".

**Root Cause:** Domain already exists in `agency_domains` table for a different agency_id.

**How to Debug:**
```bash
# Check if domain exists (DATABASE_URL required)
psql $DATABASE_URL -c "SELECT agency_id, domain, is_primary, validated_at FROM agency_domains WHERE domain = 'beispiel.de';"
```

**Solution:**
- Contact support to reassign domain to correct agency
- OR use a different domain

### Domain Contains Path/Query/Port (400)

**Symptom:** PUT /api/v1/public-site/domain returns 400 with "Domain darf keinen Pfad/Query/Port enthalten".

**Root Cause:** User entered full URL instead of domain (e.g., "https://beispiel.de/home" instead of "beispiel.de").

**Solution:**
- Enter only the domain name without protocol, path, query, or port
- Examples:
  - ✅ Correct: `beispiel.de`, `www.beispiel.de`
  - ❌ Wrong: `https://beispiel.de`, `beispiel.de:8080`, `beispiel.de/home`

### Private/Local Domain Rejected (400)

**Symptom:** PUT /api/v1/public-site/domain returns 400 with "Private oder lokale Domains sind nicht erlaubt".

**Root Cause:** SSRF protection rejects localhost, *.local, and private IP addresses.

**Solution:**
- Use a public domain name (not localhost, not *.local, not 192.168.*, etc.)
- For local development, use ngrok or similar tunneling service

---

### Public Domain Endpoints Return 404

**Symptom:** GET/PUT/POST /api/v1/public-site/domain* returns 404. OpenAPI schema does not include /public-site/domain paths.

**Root Cause:** Router not mounted. The public_domain_admin router must be included in the FastAPI app via failsafe mechanism (when MODULES_ENABLED=true) or fallback (when MODULES_ENABLED=false).

**How to Debug:**
```bash
# Check if endpoints exist in OpenAPI schema
curl -s https://api.fewo.kolibri-visions.de/openapi.json | jq '.paths | keys[]' | grep public-site/domain
# Expected: /api/v1/public-site/domain, /api/v1/public-site/domain/verify
# If empty: Router not mounted

# Check backend logs for router mounting
docker logs pms-backend --tail 100 | grep "Public domain admin"
# Expected: "✅ Failsafe: Public domain admin router mounted at /api/v1/public-site"
# OR: "✅ Public domain admin router already mounted via module system"
# If missing: Router mounting logic not executed

# Test endpoint with curl (should return 401/403 without token, not 404)
curl -I https://api.fewo.kolibri-visions.de/api/v1/public-site/domain
# Expected: HTTP 401 Unauthorized OR HTTP 403 Forbidden
# Got 404: Router not mounted
```

**Solution:**
1. Verify main.py includes public_domain_admin in failsafe section:
   ```bash
   rg -A 3 "public_domain_admin_routes_exist" backend/app/main.py
   # Should show: any(route.path == "/api/v1/public-site/domain" for route in app.routes)
   ```

2. Verify import statement in failsafe section:
   ```bash
   rg "from .api.routes import.*public_domain_admin" backend/app/main.py
   # Should show: from .api.routes import public_booking, public_site, public_domain_admin
   ```

3. Verify mounting logic in failsafe section:
   ```bash
   rg -A 2 "if not public_domain_admin_routes_exist" backend/app/main.py
   # Should show: app.include_router(public_domain_admin.router, prefix="/api/v1/public-site", ...)
   ```

4. Redeploy backend (trigger rebuild via Coolify or git push)

5. Verify router mounted:
   ```bash
   # Check OpenAPI again after redeploy
   curl -s https://api.fewo.kolibri-visions.de/openapi.json | jq '.paths | keys[]' | grep public-site/domain
   # Expected: /api/v1/public-site/domain, /api/v1/public-site/domain/verify
   ```

---

### GET /api/v1/public-site/domain Returns 500 (Agency ID Resolution)

**Symptom:** GET/PUT/POST /api/v1/public-site/domain* returns 500 with traceback:
```
TypeError: one of the hex, bytes, bytes_le, fields, or int arguments must be given
File "/app/app/api/routes/public_domain_admin.py", line 92:
  agency_id = UUID(current_user["agency_id"])
```

**Root Cause:** JWT does not include `agency_id` claim. The endpoint tried to extract `current_user["agency_id"]` directly, which raised TypeError when the key was missing or None.

**How It's Fixed:**
Endpoints now use `get_current_agency_id()` dependency which implements robust agency resolution:
1. JWT claim "agency_id" (if present)
2. x-agency-id header (validated via team_members membership)
3. Auto-detect if user has exactly one agency membership
4. Otherwise raise 400/403 with actionable error message

**Resolution Flow:**
- **JWT has agency_id claim**: Use it directly (fastest path)
- **JWT + x-agency-id header**: Validate user is member of specified agency (multi-tenant)
- **JWT + no header + 1 membership**: Auto-detect agency (convenience)
- **JWT + no header + 0 memberships**: 400 "user has no agency memberships, set x-agency-id header"
- **JWT + no header + 2+ memberships**: 400 "user has N memberships, set x-agency-id header"

**How to Debug:**
```bash
# Decode JWT to check for agency_id claim
echo $JWT_TOKEN | cut -d'.' -f2 | base64 -d | jq '.agency_id'
# Output: "ffd0123a-10b6-40cd-8ad5-66eee9757ab7" OR null

# If null, check user's agency memberships
export USER_ID=$(echo $JWT_TOKEN | cut -d'.' -f2 | base64 -d | jq -r '.sub')
psql $DATABASE_URL -c "SELECT agency_id FROM team_members WHERE user_id = '$USER_ID';"

# If 0 rows: User not assigned to any agency (403)
# If 1 row: Endpoint auto-detects this agency
# If 2+ rows: User must send x-agency-id header
```

**Solution (Single Membership):**
User is auto-detected:
```bash
# No x-agency-id header needed
curl -X GET https://api.fewo.kolibri-visions.de/api/v1/public-site/domain \
  -H "Authorization: Bearer $JWT_TOKEN"
# Expected: 200 OK with domain status
```

**Solution (Multiple Memberships):**
User must specify agency:
```bash
# Get user's agency memberships
psql $DATABASE_URL -c "SELECT agency_id FROM team_members WHERE user_id = '$USER_ID';"

# Pick one agency and use x-agency-id header
curl -X GET https://api.fewo.kolibri-visions.de/api/v1/public-site/domain \
  -H "Authorization: Bearer $JWT_TOKEN" \
  -H "x-agency-id: ffd0123a-10b6-40cd-8ad5-66eee9757ab7"
# Expected: 200 OK with domain status for specified agency
```

**Solution (No Memberships):**
Contact admin to add user to agency:
```sql
-- Admin assigns user to agency via team_members
INSERT INTO team_members (agency_id, user_id, role) 
VALUES ('ffd0123a-10b6-40cd-8ad5-66eee9757ab7', '<user_id>', 'staff');
```

**Expected Errors (After Fix):**
- **400 Bad Request**: "Cannot resolve agency: user has N agency memberships. Please set x-agency-id header."
- **403 Forbidden**: "Access denied: user is not a member of agency <uuid>" (invalid x-agency-id)
- **Not 500**: Agency resolution never raises unhandled TypeError

**Verification:**
```bash
# Test with JWT that has no agency_id claim
curl -X GET https://api.fewo.kolibri-visions.de/api/v1/public-site/domain \
  -H "Authorization: Bearer $JWT_TOKEN"

# Expected (single membership): 200 OK
# Expected (multiple memberships): 400 with actionable message
# Expected (no memberships): 400 with actionable message
# NOT expected: 500 with TypeError
```

---

### Public Domain Smoke Test Returns rc=1 Despite PASS

**Symptom:** Running `pms_epic_c_public_domain_smoke.sh` prints "Test 1 PASSED" but exits with rc=1.

**Root Cause (Fixed):** Earlier versions had exit code tracking issues:
1. Early `exit 1` statements bypassed final exit logic when tests failed
2. Counter increments using `((PASS++))` could fail with `set -euo pipefail` on arithmetic errors
3. Inconsistent exit code handling in different code paths

**How It's Fixed (Commit 0a07ed0+):**
- Replaced `((PASS++))` with `PASS=$((PASS + 1))` for safer arithmetic
- Removed early exits after test failures - tests now continue and set FAIL counter
- Consistent exit logic at end of script:
  - If `PUBLIC_DOMAIN` not set: exit based on FAIL counter after Test 1
  - If conflict (409) on Test 2: exit based on FAIL counter
  - End of all tests: exit 0 if FAIL=0, else exit 1
- Added `print_response()` helper to show first 200 chars of response body on failures
- Added 500 error handling to all test cases with diagnostic output

**Additional Improvements:**
- Script now accepts `API_BASE_URL` (preferred) OR `HOST` (backward compatible)
- Better error messages when env vars missing (exit 2 for config errors)
- Always prints "PUBLIC_DOMAIN not set, skipping mutation tests" when applicable
- Improved jq error handling with `2>/dev/null || echo "error"` fallbacks

**How to Update:**
```bash
# Pull latest version
cd /data/repos/pms-webapp
git fetch origin main && git reset --hard origin/main

# Verify you have the fixed version
head -30 backend/scripts/pms_epic_c_public_domain_smoke.sh | grep "API_BASE_URL OR HOST"
# Expected: Should find the updated env var documentation
```

**Verification (After Update):**
```bash
# Test with API_BASE_URL (preferred)
API_BASE_URL=https://api.fewo.kolibri-visions.de \
JWT_TOKEN="<your-jwt>" \
./backend/scripts/pms_epic_c_public_domain_smoke.sh
echo "rc=$?"

# Expected output:
# ℹ Test 1: GET domain status (auth required)
# ✅ Test 1 PASSED: GET domain status returned 200 (...)
# ℹ PUBLIC_DOMAIN not set, skipping mutation tests (save/verify)
# ℹ Summary: 1 passed, 0 failed
# rc=0

# Test with HOST (backward compatible)
HOST=https://api.fewo.kolibri-visions.de \
JWT_TOKEN="<your-jwt>" \
./backend/scripts/pms_epic_c_public_domain_smoke.sh
echo "rc=$?"

# Expected: Same output, rc=0
```

**Expected Behavior (After Fix):**
- All tests pass → rc=0
- Any test fails → rc=1 (but script continues and shows summary)
- Config error (missing env vars) → rc=2
- Script always prints summary before exiting
- Failures show HTTP code + first 200 chars of response body

---

## Frontend Host Routing (admin.* vs Public)

**Overview:** Next.js frontend routes requests differently based on hostname to separate Admin UI from Public Website.

**Purpose:** Prevent public website visitors from being redirected to /login. Admin UI requires authentication, public website is anonymous.

**Routing Rules:**
- **admin.*** prefix (e.g., `admin.fewo.kolibri-visions.de`) → Admin UI mode
  - Root `/` redirects to `/login`
  - `/login` renders login form
  - All admin routes require authentication
- **public** host (e.g., `fewo.kolibri-visions.de`, no `admin.` prefix) → Public Website mode
  - Root `/` redirects to `/unterkuenfte` (properties listing)
  - `/login` redirects to `https://admin.<public-host>/login` (except localhost)
  - Public routes (pages, properties) do not require authentication
- **localhost** → Both modes work (dev mode safety)

**Implementation:**
- `frontend/lib/host_mode.ts`: Helper functions for host detection
  - `getRequestHost()`: Reads x-forwarded-host or host header (server-side)
  - `isAdminHost(host)`: Returns true if host starts with "admin."
  - `getAdminHost(host)`: Converts public host to admin host (prepends "admin.")
- `frontend/app/page.tsx`: Dynamic root page with host-based redirect
- `frontend/app/login/page.tsx`: Redirects to admin host if accessed on public host (except localhost)

**Next.js Redirect Behavior (NEXT_REDIRECT Digest):**
When Next.js uses `redirect()`, it returns HTTP 200 with HTML body containing:
```
NEXT_REDIRECT;action;/target/path
```
This is **intentional** Next.js behavior for client-side navigation. The browser JavaScript consumes the digest and performs the actual redirect.

**When using curl**, you'll see HTTP 200 + NEXT_REDIRECT body instead of HTTP 302. This is normal.

**Verification Commands:**

```bash
# Test public host root (should NOT redirect to /login)
curl -k -sS -H 'Cache-Control: no-cache' "https://fewo.kolibri-visions.de/?cb=$(date +%s)" | head -20
# Expected: NEXT_REDIRECT;replace;/unterkuenfte (or public page HTML)
# NOT expected: NEXT_REDIRECT;replace;/login

# Test admin host root (should redirect to /login)
curl -k -sS -H 'Cache-Control: no-cache' "https://admin.fewo.kolibri-visions.de/?cb=$(date +%s)" | head -20
# Expected: NEXT_REDIRECT;replace;/login

# Test public host /login (should redirect to admin host)
curl -k -sS -H 'Cache-Control: no-cache' "https://fewo.kolibri-visions.de/login?cb=$(date +%s)" | head -20
# Expected: NEXT_REDIRECT;replace;https://admin.fewo.kolibri-visions.de/login

# Test admin host /login (should render login form or redirect to login)
curl -k -sS -H 'Cache-Control: no-cache' "https://admin.fewo.kolibri-visions.de/login?cb=$(date +%s)" | head -20
# Expected: NEXT_REDIRECT;replace;/login OR login form HTML
```

**Common Issues:**

### Public Host Shows Login Page

**Symptom:** Accessing `https://fewo.kolibri-visions.de/` redirects to `/login` instead of `/unterkuenfte`.

**Root Cause:** Frontend not detecting public host correctly, treating all hosts as admin.

**How to Debug:**
```bash
# Check x-forwarded-host header is set by proxy/CDN
curl -k -I "https://fewo.kolibri-visions.de/" | grep -i "x-forwarded-host"

# Check Next.js is reading headers (dynamic route, not statically cached)
# frontend/app/page.tsx should use headers() to be dynamic
grep "headers()" frontend/app/page.tsx
# Expected: import { headers } from "next/headers"; and await headers() call

# Check isAdminHost logic
grep "isAdminHost" frontend/lib/host_mode.ts
# Expected: baseHost.startsWith("admin.")
```

**Solution:**
- Verify x-forwarded-host header is passed by proxy (Traefik/Coolify)
- Verify root page is dynamic (uses `await headers()` or `await getRequestHost()`)
- Clear Next.js cache: `rm -rf frontend/.next` and rebuild
- Redeploy frontend

### Admin Host Does Not Require Login

**Symptom:** Accessing `https://admin.fewo.kolibri-visions.de/` does not redirect to `/login`.

**Root Cause:** Frontend treating admin host as public host (isAdminHost returns false).

**How to Debug:**
```bash
# Test host detection
curl -k -sS "https://admin.fewo.kolibri-visions.de/?cb=$(date +%s)" | grep "NEXT_REDIRECT"
# Expected: NEXT_REDIRECT;replace;/login
# If missing: host detection broken

# Check x-forwarded-host header
curl -k -I "https://admin.fewo.kolibri-visions.de/" | grep -i "x-forwarded-host"
# Expected: x-forwarded-host: admin.fewo.kolibri-visions.de
```

**Solution:**
- Verify proxy passes x-forwarded-host with "admin." prefix
- Verify DNS CNAME for admin subdomain points to correct target
- Check Traefik/Coolify host routing config includes admin subdomain
- Redeploy frontend

---

### Middleware Implementation (2026-01-11 Fix)

**Problem:** Original implementation in commit 0a6dd27 caused HTTP 500 on both public and admin root routes.

**Root Cause:** Route handlers (app/page.tsx) tried to use headers() which can fail in certain edge cases. Direct routing in page components is fragile.

**Solution:** Moved routing logic to Next.js middleware (frontend/middleware.ts).

**How Middleware Works:**
1. Reads hostname from `x-forwarded-host` or `host` header
2. Strips port, lowercases, determines if admin host (starts with "admin.")
3. Routes BEFORE page handlers execute:
   - **PUBLIC /** → rewrites to `/_public` (URL stays `/`, content from `/_public`)
   - **PUBLIC /login** → redirects 307 to `https://admin.<basedomain>/login`
   - **ADMIN /** → redirects 307 to `/login`
   - **ADMIN /unterkuenfte or /p/** → redirects 307 to public host (safety)
4. Returns proper HTTP 307 redirects (not NEXT_REDIRECT digest)

**Safe Routes:**
- `frontend/app/_public/page.tsx`: Minimal public homepage (no auth, no admin deps, never throws)
- `frontend/app/page.tsx`: Simplified fallback (rarely hit, middleware handles routing)

**Automated Smoke Test:**
Script: `backend/scripts/pms_frontend_host_routing_smoke.sh`

Tests:
1. PUBLIC / → expect 200 (not 500, no error indicators)
2. PUBLIC /login → expect 307 to admin host
3. ADMIN / → expect 307 to /login
4. ADMIN /login → expect 200

Usage:
```bash
# Default hosts (fewo.kolibri-visions.de, admin.fewo.kolibri-visions.de)
./backend/scripts/pms_frontend_host_routing_smoke.sh

# Custom hosts
PUBLIC_HOST=example.com ADMIN_HOST=admin.example.com \
./backend/scripts/pms_frontend_host_routing_smoke.sh

# Expected output: All 4 tests pass, rc=0
```

**Verification After Fix:**
```bash
# PUBLIC / must return 200 (not 500)
curl -k -sS -w "\n%{http_code}" "https://fewo.kolibri-visions.de/" | tail -5
# Expected: HTTP 200, public homepage HTML (no "__next_error__" or "500:")

# PUBLIC /login must redirect to admin host
curl -k -sS -I "https://fewo.kolibri-visions.de/login" | grep -i "location"
# Expected: Location: https://admin.fewo.kolibri-visions.de/login

# ADMIN / must redirect to /login
curl -k -sS -I "https://admin.fewo.kolibri-visions.de/" | grep -i "location"
# Expected: Location: /login (or absolute URL ending in /login)

# ADMIN /login must return 200
curl -k -sS -w "\n%{http_code}" "https://admin.fewo.kolibri-visions.de/login" | tail -5
# Expected: HTTP 200, login form HTML
```



**Update (2026-01-12): Fixed PUBLIC / returning 404**

**Problem:** After commit a2370c6, PUBLIC "/" returned HTTP 404 with header `x-middleware-rewrite: /_public`.

**Root Cause:** Middleware rewrote "/" to "/_public" but that route doesn't exist (app/_public/page.tsx was never created).

**Solution:**
1. Removed rewrite to "/_public" in middleware.ts - let PUBLIC "/" pass through with NextResponse.next()
2. The existing frontend/app/page.tsx serves as public root (simple static landing page)
3. Fixed smoke script parsing bugs (empty status codes due to curl output capture issues)
4. Added env var fallback: NEXT_PUBLIC_API_BASE_URL (Coolify compatibility) in api-client.ts

**Middleware Behavior After Fix:**
- **PUBLIC /** → passes through normally (no rewrite), renders frontend/app/page.tsx
- **PUBLIC /login** → redirects 307 to `https://admin.<basedomain>/login` (unchanged)
- **ADMIN /** → redirects 307 to `/login` (unchanged)
- **ADMIN /unterkuenfte or /p/** → redirects 307 to public host (unchanged)

**Verification:**
```bash
# PUBLIC / must return 200 (not 404)
curl -k -sS -o /dev/null -w "%{http_code}" "https://fewo.kolibri-visions.de/"
# Expected: 200

# Verify no rewrite header
curl -k -sS -I "https://fewo.kolibri-visions.de/" | grep -i "x-middleware-rewrite"
# Expected: No output (header should not exist)

# Run smoke script (should pass all 4 tests)
./backend/scripts/pms_frontend_host_routing_smoke.sh
# Expected: rc=0, all tests pass
```

**PROD Evidence (2026-01-12):**

Verified in production with commit 08ed721b93cc040bc7bd01cc868b06143cc906ec deployed to both frontend containers.

```bash
# [HOST-SERVER-TERMINAL] Verify deployed commit matches
docker ps --format 'table {{.Names}}\t{{.Image}}' | egrep -i 'public-website|pms-admin'
# Expected output:
# public-website    so4cw4c04c84s4ww8cccs0gg:08ed721b93cc040bc7bd01cc868b06143cc906ec
# pms-admin         nwwsswgoswgosck4kcgcooss:08ed721b93cc040bc7bd01cc868b06143cc906ec

# [HOST-SERVER-TERMINAL] Run smoke test
cd /data/repos/pms-webapp
./backend/scripts/pms_frontend_host_routing_smoke.sh
echo "rc=$?"
# Expected: All 4 tests pass, rc=0
# ✅ Test 1 PASSED: PUBLIC / returns 200
# ✅ Test 2 PASSED: PUBLIC /login redirects to admin host
# ✅ Test 3 PASSED: ADMIN / redirects to /login
# ✅ Test 4 PASSED: ADMIN /login returns 200

# [HOST-SERVER-TERMINAL] Verify no rewrite header
curl -k -sS -I "https://fewo.kolibri-visions.de/" | egrep -i 'HTTP/|x-middleware-rewrite|location'
# Expected output:
# HTTP/2 200
# (no x-middleware-rewrite header - confirms no rewrite happening)
```

---

### Admin UI Returns 404 for API Calls (Relative fetch Paths)

**Symptom:** Admin pages (e.g., `/pricing`) return 404 errors when trying to fetch API data. Browser DevTools shows failed requests like `https://admin.fewo.kolibri-visions.de/api/v1/pricing/fees` (404 Not Found).

**Root Cause:** Frontend code uses relative `fetch("/api/v1/...")` calls instead of `apiClient`. Relative paths resolve to the current hostname (admin.*) which does not host the API. The API is hosted on api.* subdomain.

**Impact:**
- Admin pages fail to load data
- Users see empty states or error messages
- Admin functionality is broken on admin.* subdomain

**Architecture:**
- API backend runs on `api.fewo.kolibri-visions.de` (port 8000)
- Admin UI runs on `admin.fewo.kolibri-visions.de` (Next.js frontend)
- Public website runs on `fewo.kolibri-visions.de` (same Next.js app, different routing)

**How to Debug:**
```bash
# Check if page uses relative fetch calls (INCORRECT)
rg -n 'fetch\("/api/v1' frontend/app/pricing/page.tsx
# Expected: No matches (all should use apiClient)
# If matches found: Page needs fixing

# Check browser DevTools network tab
# Look for failed requests to https://admin.fewo.kolibri-visions.de/api/v1/*
# Should be: https://api.fewo.kolibri-visions.de/api/v1/*
```

**Solution:**

1. **Replace relative fetch calls with apiClient:**
   ```typescript
   // WRONG: Relative fetch (hits admin.* host)
   const res = await fetch("/api/v1/properties?limit=50", {
     credentials: "include",
   });

   // CORRECT: Use apiClient (routes to api.* host)
   import { useAuth } from "../lib/auth-context";
   import { apiClient, ApiError } from "../lib/api-client";
   
   const { accessToken } = useAuth();
   const data = await apiClient.get("/api/v1/properties?limit=50", accessToken);
   ```

2. **Add accessToken guard checks:**
   ```typescript
   const fetchData = async () => {
     if (!accessToken) return; // Don't fetch without token
     try {
       const data = await apiClient.get("/api/v1/...", accessToken);
       // ...
     } catch (error) {
       if (error instanceof ApiError) {
         // Handle API errors with status codes
       }
     }
   };
   ```

3. **Verify no relative fetches remain:**
   ```bash
   # Guard check (should return no matches)
   rg -n 'fetch\("/api/v1' frontend
   ```

**Prevention:**

Add guard check to CI/pre-commit hooks:
```bash
# Reject commits with relative /api/v1 fetch calls
if rg -q 'fetch\("/api/v1' frontend; then
  echo "ERROR: Found relative fetch('/api/v1') calls. Use apiClient instead."
  exit 1
fi
```

**Reference Commits:**
- Fix for frontend/app/pricing/page.tsx: `frontend: route pricing API calls via apiClient (fix admin-domain 404)`

---

## Public docs mirror (pms-webapp-docs) — source of truth + publish workflow

**Overview**: The project maintains a public mirror repository for documentation because the main `PMS-Webapp` repository is private.

**Source of Truth**: `PMS-Webapp/backend/docs` (private repository)

**Public Mirror**: `pms-webapp-docs` repository mirrors the markdown files 1:1, especially `project_status.md` and other operational documentation.

**Important**: Do NOT attempt manual pushes to `pms-webapp-docs` from the host server if the SSH key is read-only. This is expected behavior for security reasons. The read-only key error message is:

```
ERROR: The key you are authenticating with has been marked as read only.
```

**Publish Mechanism**:

The mirror is updated through an automated publish workflow that:
1. Reads the latest documentation from `PMS-Webapp/backend/docs`
2. Copies files to the `pms-webapp-docs` repository
3. Commits and pushes changes using appropriate credentials

The exact mechanism may involve GitHub Actions, automated sync scripts, or other CI/CD tooling. Check the repository settings and workflows for details.

**Verification Commands** (from HOST-SERVER-TERMINAL):

To verify the mirror is 1:1 with the private repo, download the raw files and compare:

```bash
# Example: Verify project_status.md is in sync
cd /tmp

# Download raw file from public mirror (replace URL with actual mirror URL)
curl -sS -o project_status_mirror.md \
  https://raw.githubusercontent.com/your-org/pms-webapp-docs/main/project_status.md

# Compare with private repo file
cd /data/repos/pms-webapp
sha256sum backend/docs/project_status.md
sha256sum /tmp/project_status_mirror.md

# Quick diff check (silent if identical)
diff -q backend/docs/project_status.md /tmp/project_status_mirror.md
echo "rc=$?"
# Expected: rc=0 (files are identical)
```

**Note on curl error (23)**:

When downloading large files and piping to `head` for preview, you may see:

```
curl: (23) Failure writing output to destination
```

This occurs because `head` closes the pipe early after reading its requested lines, causing curl to fail when trying to write remaining data. This is harmless for preview purposes but can be avoided by:

```bash
# Safe alternative 1: Download to file first
curl -sS -o /tmp/file.md https://example.com/file.md
head -20 /tmp/file.md

# Safe alternative 2: Use sed for line extraction
curl -sS https://example.com/file.md | sed -n '1,20p'
```

**Common Issues**:

### Mirror is out of sync

**Symptom**: `sha256sum` or `diff -q` shows differences between private repo and public mirror.

**Cause**: Publish workflow has not run yet, or failed to complete.

**Solution**:
1. Check if there are pending commits in `PMS-Webapp/backend/docs` that haven't been published
2. Trigger the publish workflow manually (if available)
3. Check workflow logs for errors
4. If workflow is broken, fix and re-run

### Cannot push to mirror manually

**Symptom**: `git push` to `pms-webapp-docs` fails with "read only" key error.

**Cause**: SSH key configured on the host is intentionally read-only for security.

**Solution**: This is expected. Use the automated publish workflow instead. Do not attempt to work around this with different credentials.

---

## Epic A UI Polish — Automated UI Smoke (Playwright)

**Overview:** Automated UI testing for Epic A "Admin UI: Organisation & Team Production-Grade Polish" using Playwright in Docker. Verifies in-page dialogs, success feedback, and UI interactions work correctly.

**Purpose:** Enable automated verification of Epic A UI polish features:
- In-page dialogs (not browser popups) on /organisation and /team
- Success/error feedback with toasts/banners (not alert popups)
- Interactive elements (copy buttons, invite creation/revocation)
- Mobile-responsive layout

**Verification Commands:**

```bash
# HOST-SERVER-TERMINAL
cd /data/repos/pms-webapp
git fetch origin main && git reset --hard origin/main

# Verify both backend and admin UI deployment with matching commits
./backend/scripts/pms_verify_deploy.sh \
  API_BASE_URL=https://api.fewo.kolibri-visions.de \
  ADMIN_BASE_URL=https://admin.fewo.kolibri-visions.de \
  EXPECT_COMMIT=<commit_hash>

# Run automated UI smoke test with admin credentials
E2E_ADMIN_EMAIL="admin@example.com" \
E2E_ADMIN_PASSWORD="password" \
ADMIN_BASE_URL=https://admin.fewo.kolibri-visions.de \
./backend/scripts/pms_epic_a_ui_polish_smoke.sh
echo "rc=$?"

# Expected: All tests pass, rc=0
```

**Prerequisites:**
- Docker installed and running on host machine
- Admin credentials with manager/admin role
- Admin UI deployed with `/api/ops/version` endpoint

**Required Environment Variables:**
- `E2E_ADMIN_EMAIL`: Admin email for UI login (must have manager/admin role)
- `E2E_ADMIN_PASSWORD`: Admin password for UI login
- These credentials are used to perform actual UI login via the auth form (no JWT injection)

**Known Non-Fatal Warnings:**
- Admin `/api/ops/version` may return `SOURCE_COMMIT: (not set)` - This is a non-fatal warning until Coolify deployment config is updated to inject the commit SHA into the admin UI build
- UI smoke may log "Invite not visible in list (may require page refresh)" - This is non-fatal if the POST `/api/v1/team/invites` returned status 201 and the test passed

**Common Issues:**

### JWT Token Expired (Redirect to Login)

**Symptom:** Test fails with "Redirected to login page - JWT token may be expired or invalid"

**Root Cause:** MANAGER_JWT_TOKEN is expired. JWTs typically have short TTL (1-24 hours).

**Solution:**
```bash
# Regenerate a fresh JWT token
export MANAGER_JWT_TOKEN=$(./backend/scripts/get_fresh_token.sh)

# Or manually via Supabase API
# (requires SUPABASE_URL, SUPABASE_ANON_KEY, manager email/password)

# Retry smoke test
ADMIN_BASE_URL=https://admin.fewo.kolibri-visions.de \
MANAGER_JWT_TOKEN="$MANAGER_JWT_TOKEN" \
./backend/scripts/pms_epic_a_ui_polish_smoke.sh
```

### Docker Not Running

**Symptom:** Error "Docker is not running or not accessible"

**Solution:**
```bash
# Check Docker status
docker ps

# On Linux: ensure Docker service is running
sudo systemctl status docker
sudo systemctl start docker

# On macOS: ensure Docker Desktop is running
open -a Docker

# Check permissions
docker run hello-world
```

### Admin ops/version Endpoint Missing

**Symptom:** Admin verification in pms_verify_deploy.sh returns 404

**Root Cause:** Admin UI not deployed with `/api/ops/version` endpoint yet.

**Solution:**
- Ensure `frontend/app/api/ops/version/route.ts` is deployed
- Endpoint exposes `source_commit` from `process.env.SOURCE_COMMIT` or `process.env.NEXT_PUBLIC_SOURCE_COMMIT`
- Verify endpoint manually:
```bash
curl -sS https://admin.fewo.kolibri-visions.de/api/ops/version | jq
# Expected: {"service":"pms-admin","source_commit":"abc123",...}
```

### Admin /api/ops/version returns source_commit: null

**Symptom:** Admin endpoint returns `{"service":"pms-admin","source_commit":null,"started_at":null,"environment":"production"}` even though `SOURCE_COMMIT` env var is set in container. Response headers show `x-nextjs-cache: HIT` and response stays null even with cachebust query parameter.

**Root Cause:** Next.js statically optimizes route responses at build time, preventing runtime environment variables from being read.

**Solution:**
Force dynamic runtime rendering in `frontend/app/api/ops/version/route.ts`:
- Add `export const runtime = "nodejs"`
- Add `export const dynamic = "force-dynamic"`
- Add `export const revalidate = 0`
- Add `export const fetchCache = "force-no-store"`
- Import and call `unstable_noStore()` from `next/cache` at top of GET() handler: `import { unstable_noStore as noStore } from "next/cache"` then `noStore()` in function body
- Use bracket notation for ALL env reads: `process.env["SOURCE_COMMIT"]`, `process.env["NEXT_PUBLIC_SOURCE_COMMIT"]`, `process.env["STARTED_AT"]`, `process.env["NODE_ENV"]`
- Keep `Cache-Control: no-store, must-revalidate` in response headers
- Update `pms_verify_deploy.sh` to add cachebust query with timestamp and Cache-Control header

**Verification:**
```bash
# In Coolify container terminal, check SOURCE_COMMIT env var
docker exec -it pms-admin-container env | grep SOURCE_COMMIT
# Example output: SOURCE_COMMIT=2dedc18acc2251752b5b8e4f73328e7aa35f6283

# Direct curl with cachebust (from host or workstation)
curl -sS "https://admin.fewo.kolibri-visions.de/api/ops/version?cb=$(date +%s)" -H "Cache-Control: no-cache" | jq .source_commit
# Expected: equals $SOURCE_COMMIT from container (not null)

# Via deploy verification script (includes admin check with cachebust)
export API_BASE_URL="https://api.fewo.kolibri-visions.de"
export ADMIN_BASE_URL="https://admin.fewo.kolibri-visions.de"
export EXPECT_COMMIT="2dedc18"
./backend/scripts/pms_verify_deploy.sh
# Expected: "Admin commit verification passed: 2dedc18acc22..."
```

### localStorage Key Mismatch

**Symptom:** Tests redirect to login even with valid JWT, or UI shows "not authenticated"

**Root Cause:** Playwright script sets JWT in localStorage with key `pms_jwt`, but frontend expects a different key.

**Solution:**
1. Check frontend code for localStorage key:
```bash
cd /data/repos/pms-webapp/frontend
rg "localStorage.getItem|localStorage.setItem" --type ts | grep -i "jwt\|token\|auth"
```

2. Update Playwright test script (`pms_epic_a_ui_polish_smoke.sh`) to use correct key in `addInitScript` section

3. Script already tries multiple common keys: `pms_jwt`, `token`, `jwt`, `authToken`. If none work, add your key to the list.

### Test Fails But Manual UI Works

**Symptom:** Automated test fails (e.g., element not found), but manually clicking through UI works fine.

**Root Cause:** UI element selectors in Playwright test don't match actual rendered HTML, or timing issues.

**Debugging:**
1. Check screenshots in `/tmp/<tempdir>/screenshots/` for failure point
2. Adjust selectors in test script to match your UI components
3. Increase wait timeouts if elements load slowly
4. Verify German text matches: "Bearbeiten", "Mitglied einladen", "Widerrufen", etc.

**Solution:** Update test.spec.ts in `pms_epic_a_ui_polish_smoke.sh` with correct selectors and timing.

### Error: No named projects are specified in the configuration file

**Symptom:** Playwright fails with `Error: No named projects are specified in the configuration file` when running `pms_epic_a_ui_polish_smoke.sh`.

**Root Cause:** Playwright requires a playwright.config.ts with named projects when using the `--project` flag. Earlier versions of the script (before commit 991e373) didn't generate this config file.

**Solution:**
The issue is fixed in the current version of the script. It now generates both `playwright.config.ts` (with named projects: chromium, firefox, webkit) and `test.spec.ts` in the temp directory before running tests.

**Verification:**
```bash
# Re-run the smoke test with the fixed script
MANAGER_JWT_TOKEN="<jwt>" \
ADMIN_BASE_URL=https://admin.fewo.kolibri-visions.de \
./backend/scripts/pms_epic_a_ui_polish_smoke.sh

# Use a different browser if needed
PW_PROJECT=firefox MANAGER_JWT_TOKEN="<jwt>" \
./backend/scripts/pms_epic_a_ui_polish_smoke.sh
```

**Prerequisites:**
- Docker installed and running
- Fresh manager/admin JWT token (tokens expire after 1-24 hours)

**If Still Failing:**
1. Check Docker is running: `docker ps`
2. Verify Docker can pull images: `docker run hello-world`
3. Check temp directory has both files: Look for "Generated Playwright config" in output
4. Review diagnostics output showing Playwright version and temp dir contents

### Auth via UI Login (New Method)

**Overview:** As of commit 906dde2+, the smoke script authenticates via UI login form instead of JWT injection to localStorage. This method is more reliable and matches real user workflows.

**Symptom:** Test fails with "Failed to authenticate: still on login page after login attempt"

**Root Cause:** Login credentials incorrect, or login form selectors don't match the actual admin UI.

**Solution:**

1. **Verify Credentials:**
```bash
# Test credentials manually
open https://admin.fewo.kolibri-visions.de/login
# Enter E2E_ADMIN_EMAIL and E2E_ADMIN_PASSWORD manually
# Ensure login succeeds and user has manager/admin role
```

2. **Check Environment Variables:**
```bash
echo "Email: $E2E_ADMIN_EMAIL"
echo "Password length: ${#E2E_ADMIN_PASSWORD}"
# Verify both are set and email has valid format
```

3. **Debug Login Form Selectors:**
If credentials are correct but login fails, the login form might use custom selectors. Check the admin UI HTML:
- Email input should match: `input[type="email"]` or `input[name*="email" i]`
- Password input should match: `input[type="password"]`
- Submit button should match: `button:has-text("Anmelden")`, `button:has-text("Login")`, or `button[type="submit"]`

4. **Check User Role:**
```bash
# Verify user has manager/admin role (not just staff)
# In Supabase Dashboard → Authentication → Users → Check "role" metadata
# Or query team_members table:
psql $DATABASE_URL -c "SELECT role FROM team_members WHERE user_id = (SELECT id FROM auth.users WHERE email = 'admin@example.com');"
# Expected: 'manager' or 'admin'
```

5. **Review Screenshots for Debugging:**
After a failed run, check screenshots:
```bash
# Screenshots are saved before script cleans up temp dir
# Copy them quickly after failure:
ls -la /tmp/tmp.*/screenshots/
# Look for login page screenshot to see what went wrong
```

**Expected Behavior:**
- First test (/organisation): Detects login redirect, fills form, submits, waits for redirect away from /login
- Second test (/team): Reuses existing session (no login redirect)
- Login flow logs: "ℹ️  Detected login redirect", "✅ Filled email field", "✅ Filled password field", "✅ Clicked login button", "✅ Login successful"

### Dialog Detection Fails (Heuristic Approach)

**Overview:** As of commit d4e7d45+, the smoke script uses **heuristic dialog detection** instead of relying on single selectors like `[role="dialog"]`. Custom Tailwind modals may not have standard ARIA attributes, so the script tries multiple detection patterns.

**Symptom:** Test fails with "Organisation edit dialog did not become visible within 15000ms" or "Team invite dialog did not become visible within 15000ms"

**Root Cause:** None of the dialog detection candidates matched a visible element. This can happen if:
1. Dialog uses non-standard classes/attributes not in our candidate list
2. Dialog animation takes longer than 15 seconds
3. Dialog is rendered but has `display: none` or `visibility: hidden`
4. z-index stacking issue (dialog rendered behind other elements)

**How Dialog Detection Works:**

The script uses `expectAnyVisible()` which tries multiple selector candidates:

**Organisation Edit Dialog:**
- `[role="dialog"]:visible`, `[role="alertdialog"]:visible`, `[data-state="open"]:visible`
- `div.fixed.inset-0.z-50:visible` (Tailwind overlay pattern)
- Buttons: "Abbrechen", "Cancel", "Speichern", "Save"

**Team Invite Dialog:**
- Same standard selectors as above
- Buttons: "Einladen", "Senden", "Send", "Abbrechen", "Cancel"
- Form inputs: email input via placeholder/label/type selectors

The helper polls all candidates every 250ms for up to 15 seconds. As soon as ANY candidate becomes visible, it succeeds.

**Debugging Steps:**

1. **Check failure screenshots:**
```bash
# Look for special failure screenshot
ls -la /tmp/tmp.*/screenshots/*-detection-failed.png

# This screenshot is taken when all candidates fail
# Shows the page state at failure time
```

2. **Verify dialog opens manually:**
```bash
# Open admin UI in browser
open https://admin.fewo.kolibri-visions.de/organisation
# Click "Bearbeiten" button
# Inspect the dialog element (F12 DevTools)
# Check for:
#   - What classes/attributes it has
#   - Is it visible (display, visibility, opacity)?
#   - What z-index does it have?
#   - Are there multiple dialog elements (hidden ones)?
```

3. **Check console logs for candidate results:**
The script logs which candidate succeeded:
```
✅ Organisation edit dialog detected via candidate 4/8
```
If all fail, error message includes which candidates were tried.

4. **Add missing selectors:**
If your dialog uses different patterns, update the script:
```typescript
// In pms_epic_a_ui_polish_smoke.sh, add to dialogCandidates array:
page.locator('your-custom-dialog-class:visible'),
page.getByTestId('your-dialog-test-id'),
```

**Solution (Common Cases):**

**Case 1: Dialog uses shadcn/ui or Radix UI primitives**
These often use `data-state="open"` which is already in our candidates. If still failing, check for `data-radix-*` attributes:
```typescript
page.locator('[data-radix-dialog-content]:visible'),
```

**Case 2: Dialog has long animation (>1s)**
Increase the initial wait timeout before detection:
```typescript
await page.waitForTimeout(2000); // Increase from 1000ms
```

**Case 3: Multiple hidden dialogs exist**
The script avoids `.first()` on dialog selectors for this reason. But if you have persistent hidden dialogs, ensure candidates check `:visible` pseudo-selector.

**Case 4: z-index stacking issue**
Dialog rendered but not clickable/visible due to stacking:
```bash
# Check z-index in DevTools
# Ensure dialog has z-50 or higher
# Check for overlapping fixed elements
```

**Verification:**
```bash
# Re-run smoke test after fixes
E2E_ADMIN_EMAIL="admin@example.com" \
E2E_ADMIN_PASSWORD="password" \
./backend/scripts/pms_epic_a_ui_polish_smoke.sh

# Expected:
# ✅ Organisation edit dialog detected via candidate X/Y
# ✅ Invite dialog is in-page (not browser prompt)
```

### Pointer Events Intercepted (Submit Button Click)

**Overview:** As of commit f6090d1+, the smoke script uses **dialog-scoped locators** to avoid clicking elements behind modal overlays.

**Symptom:** Test fails with error like:
- "Element is outside of the viewport"
- "pointer events are intercepted by <div class='fixed inset-0 z-50 ...'>"
- Submit button click times out or has no effect

**Root Cause:** The submit button selector (`page.locator()`) is finding a button behind the modal overlay instead of the button inside the dialog. This happens when:
1. Multiple buttons with same text exist (one visible in dialog, others hidden in page)
2. Playwright's click actionability checks detect the overlay blocking the click
3. `.first()` on page-level selector picks wrong element

**How It's Fixed:**

The script now:
1. Calls `findVisibleDialog()` to get a reference to the visible dialog element
2. Scopes all subsequent searches to that dialog: `inviteDialog.locator(...)`
3. Uses dialog-scoped selectors for:
   - Email input: `inviteDialog.locator('input[type="email"]').first()`
   - Role select: `inviteDialog.locator('select[name*="role"]').first()`
   - Submit button: `inviteDialog.locator('button[type="submit"]').first()`
4. Tries normal click first, falls back to `click({ force: true })` if intercepted

**Debugging Steps:**

1. **Check console logs:**
```
✅ Submit button clicked (normal)    # Success with normal click
⚠️  Normal click failed (...), trying force click...  # Fallback triggered
✅ Submit button clicked (force)     # Success with force click
```

2. **Review screenshots:**
```bash
# Screenshots are in temp directory (auto-cleaned on exit)
# Copy them immediately after failure:
ls -la /tmp/tmp.*/screenshots/
cp -r /tmp/tmp.*/screenshots /tmp/epic-a-failure-debug

# Check these files:
# - team-invite-dialog.png: Dialog state before submit
# - team-invite-created.png: State after submit attempt
```

3. **Verify dialog z-index:**
```bash
# Open admin UI manually and inspect dialog
open https://admin.fewo.kolibri-visions.de/team
# Click "Mitglied einladen"
# F12 DevTools → Inspect submit button
# Check:
#   - Button is inside dialog container (not sibling)
#   - Dialog has z-50 or higher
#   - No other fixed overlays with higher z-index
```

4. **Test manual click:**
```bash
# If manual click works but test fails:
# - Check button selector matches actual rendered HTML
# - Verify button is not disabled during form validation
# - Check for JavaScript intercepting submit events
```

**Solution (If Still Failing):**

If both normal and force click fail:

1. **Update button selector in script:**
```typescript
// In pms_epic_a_ui_polish_smoke.sh, adjust submitButton selector:
const submitButton = inviteDialog.locator('button[type="submit"]').first();
// Or use more specific selector if your UI has custom attributes:
const submitButton = inviteDialog.getByRole('button', { name: /einladen|senden|invite/i });
```

2. **Increase timeouts:**
```typescript
await expect(submitButton).toBeVisible({ timeout: 20000 }); // Increase from 15s
await submitButton.click({ timeout: 15000 }); // Increase from 10s
```

3. **Check for form validation blocking submit:**
Some forms disable submit button until validation passes. Ensure email input is properly filled and valid.

**Verification:**
```bash
# Re-run smoke test
E2E_ADMIN_EMAIL="admin@example.com" \
E2E_ADMIN_PASSWORD="password" \
./backend/scripts/pms_epic_a_ui_polish_smoke.sh

# Expected output:
# ✅ Team invite dialog found via selector: [role="dialog"]
# ✅ Filled email: smoke-epic-a+1705420800000@example.com
# ✅ Submit button clicked (normal)
# ✅ Invite submitted
```

## Periodic Regression Verification (Sprint Checkpoint)

**Overview:** Orchestrated regression pack that runs multiple smoke tests in sequence for comprehensive post-sprint verification.

**Purpose:** Verify core functionality remains operational after deployments or sprint milestones. Combines deploy verification, API smoke tests, and UI smoke tests into a single automated run.

**Script:** `backend/scripts/pms_regression_verify.sh`

**Prerequisites:**
- All smoke scripts available in `backend/scripts/`
- JWT token with manager/admin role
- Admin credentials for UI tests
- Docker installed and running (for UI tests; if unavailable, UI tests are skipped with warnings)

**Note:** Script automatically exports `API` and `TOKEN` environment variable aliases (API=$API_BASE_URL, TOKEN=$JWT_TOKEN) for compatibility with channel scripts that use legacy naming.

**Usage:**

```bash
# EXECUTION LOCATION: HOST-SERVER-TERMINAL

# Required environment variables
export API_BASE_URL="https://api.fewo.kolibri-visions.de"
export ADMIN_BASE_URL="https://admin.fewo.kolibri-visions.de"
export EXPECT_COMMIT="<commit_hash>"
export JWT_TOKEN="<manager/admin JWT>"
export AGENCY_ID="<agency UUID>"
export E2E_ADMIN_EMAIL="admin@example.com"
export E2E_ADMIN_PASSWORD="password"

# Optional: Enable PUT test in branding smoke (default: false)
export BRANDING_PUT_TEST="false"

# Run regression pack
./backend/scripts/pms_regression_verify.sh

# Expected output:
# ╔════════════════════════════════════════════════════════════╗
# ║ PMS Periodic Regression Verification                       ║
# ╚════════════════════════════════════════════════════════════╝
#
# ℹ  Validating configuration...
# ✅ Configuration valid
# ✅ Docker available (UI smoke tests will run)
#
# ╔════════════════════════════════════════════════════════════╗
# ║ Test 1: Deploy Verification
# ╚════════════════════════════════════════════════════════════╝
# ...
# ✅ Test 1 PASSED: Deploy Verification
#
# [... additional tests ...]
#
# ╔════════════════════════════════════════════════════════════╗
# ║ Regression Verification - Summary                         ║
# ╚════════════════════════════════════════════════════════════╝
#
# Tests Run:     7
# Tests Passed:  7
# Tests Failed:  0
# Tests Skipped: 0
#
# ╔════════════════════════════════════════════════════════════╗
# ║ ✅ All tests passed! 🎉                                    ║
# ╚════════════════════════════════════════════════════════════╝
```

**Tests Executed (in order):**

1. **Deploy Verification** (`pms_verify_deploy.sh`)
   - Verifies backend and admin UI commit match `EXPECT_COMMIT`
   - Checks `/health`, `/health/ready`, `/api/v1/ops/version`, `/api/ops/version`

2. **P3 Direct Booking Hardening** (`pms_public_direct_booking_hardening_smoke.sh`)
   - Idempotency-Key support
   - CORS/Origin/Host allowlist
   - Audit log for booking lifecycle

3. **Epic A RBAC APIs** (`pms_epic_a_onboarding_rbac_smoke.sh`)
   - Team invitations (create/list/revoke)
   - Agency settings API
   - GET /api/v1/me (role verification)

4. **Epic A UI Polish** (`pms_epic_a_ui_polish_smoke.sh`)
   - Organisation page dialogs
   - Team page invite creation
   - **Requires Docker** (skipped if unavailable)

5. **Branding API Phase A** (`pms_branding_smoke.sh`)
   - GET /api/v1/branding (token defaults)
   - Optional: PUT /api/v1/branding (if `BRANDING_PUT_TEST=true`)

6. **Branding UI Phase B** (`pms_branding_ui_smoke.sh`)
   - CSS variables applied (`--t-primary`, `--t-accent`, etc.)
   - ThemeProvider functionality
   - **Requires Docker** (skipped if unavailable)

7. **Channel Sync Batch Details** (`pms_sync_batch_details_smoke.sh`)
   - GET /api/v1/channel-connections/{cid}/sync-batches
   - GET /api/v1/channel-connections/{cid}/sync-batches/{batch_id}

**Exit Codes:**
- `0` - All executed tests passed
- `1` - Configuration error (missing required env vars)
- `2+` - One or more tests failed (exit code = 1 + number of failures)

**Docker Not Available:**

If Docker is not installed or not running, UI tests (Epic A UI Polish, Branding UI) are **skipped** with warnings but the script continues. The summary will show:

```
Tests Run:     5
Tests Passed:  5
Tests Failed:  0
Tests Skipped: 2

Skipped tests:
  ⏭️  Test 4: Epic A UI Polish (Docker not available)
  ⏭️  Test 6: Branding UI Phase B (Docker not available)
```

This is **not a failure**. To enable UI tests, install Docker and ensure it's running.

**Common Issues:**

| Issue | Cause | Solution |
|-------|-------|----------|
| "Missing required environment variables" | One or more env vars not set | Export all required vars before running script |
| UI tests skipped | Docker not available | Install Docker Desktop or start Docker daemon |
| JWT_TOKEN expired | Token expired (24h TTL typical) | Refresh token using `./backend/scripts/get_fresh_token.sh` |
| AGENCY_ID mismatch | Wrong agency for user | Verify JWT claims match agency_id |
| Test 1 fails (deploy verify) | Commit mismatch or endpoint down | Check `EXPECT_COMMIT` matches deployed version |

**Troubleshooting:**

Individual test failures can be debugged by running the specific smoke script directly:

```bash
# Example: Debug Epic A RBAC test failure
API_BASE_URL=https://api.fewo.kolibri-visions.de \
JWT_TOKEN="$JWT_TOKEN" \
AGENCY_ID="$AGENCY_ID" \
./backend/scripts/pms_epic_a_onboarding_rbac_smoke.sh
```

Check the runbook section for each specific smoke script for detailed troubleshooting steps.

---

## Public Docs Mirror (pms-webapp-docs) Publishing

**Overview:** Documentation publishing to public GitHub repository.

**Purpose:** Automatically sync backend/docs to public pms-webapp-docs repository for external visibility.

**Architecture:**
- **Source of Truth**: PMS-Webapp/backend/docs (private repo)
- **Publishing**: Automatic via GitHub Actions `.github/workflows/publish-docs.yml`
- **Trigger**: Any push to main that modifies backend/docs/** files
- **Sync Strategy**: rsync --delete (full mirror, preserves .git/)
- **Commit Format**: "docs: publish from PMS-Webapp {short-sha} [skip ci]"

**Critical Rules:**
- ❌ NEVER commit directly to pms-webapp-docs public repository
- ✅ ALWAYS edit files in PMS-Webapp/backend/docs (private repo)
- ✅ GitHub Action automatically publishes within ~1 minute

**Verification Commands:**

```bash
# Check latest Action run
# Navigate to: https://github.com/Kolibri-Visions/PMS-Webapp/actions/workflows/publish-docs.yml

# Verify public repo reflects latest private commit
cd /path/to/pms-webapp-docs
git pull origin main
git log -1 --oneline
# Should show: "docs: publish from PMS-Webapp {sha} [skip ci]"

# Compare private source SHA with public commit message
cd /path/to/PMS-Webapp
git log -1 --oneline -- backend/docs/
# SHA should match the SHA in public repo commit message
```

**Common Issues:**

### Public Repo Out of Sync

**Symptom:** Changes made to backend/docs not appearing in pms-webapp-docs after 5+ minutes.

**Root Cause:** GitHub Action may have failed or not triggered.

**How to Debug:**
- Check Actions tab: https://github.com/Kolibri-Visions/PMS-Webapp/actions/workflows/publish-docs.yml
- Look for failed runs (red X) or no recent runs
- Check workflow logs for SSH key issues, rsync errors, or git push failures

**Solution:**
- If Action failed: Fix the error (usually SSH key or permissions), then manually trigger via workflow_dispatch
- If Action didn't trigger: Verify .github/workflows/publish-docs.yml paths filter includes your changed files
- Manual trigger: GitHub UI → Actions → "Publish Docs to Public Repo" → Run workflow

### Accidental Manual Commit to Public Repo

**Symptom:** Someone committed directly to pms-webapp-docs, causing conflicts with Action publishing.

**Root Cause:** Manual edit violated the "never edit public repo" rule.

**How to Debug:**
```bash
cd /path/to/pms-webapp-docs
git log --oneline -5
# Look for commits NOT from docs-bot
```

**Solution:**
- Revert the manual commit in public repo
- Make the equivalent change in PMS-Webapp/backend/docs instead
- Let Action re-publish cleanly
- Educate team: "Only edit private repo backend/docs, never public repo"

---

---

## P2.4 Pricing: Apply Season Template (Preview + Atomic Apply)

**Overview:** Enhanced season template application with dry-run preview and atomic transactional apply.

**Purpose:** Allow staff (manager/admin) to preview changes before applying a season template to a rate plan, with conflict detection and atomic database operations to prevent partial updates.

**Architecture:**
- **Dry-Run Preview**: POST with `dry_run: true` returns summary of changes without committing to database
- **Atomic Apply**: POST with `dry_run: false` uses database transaction (`async with db.transaction()`) to ensure all-or-nothing semantics
- **Conflict Detection**: In merge mode, detects date range overlaps between template periods and existing seasons, returns 422 with detailed conflicts
- **Soft Delete**: Replace mode archives existing seasons (sets archived_at) instead of hard delete
- **Response Schema**: Comprehensive response with status, summary counts, detailed changes, and conflict list

**API Endpoints:**

Staff (manager/admin):
- `POST /api/v1/pricing/rate-plans/{rate_plan_id}/apply-season-template`
  - Request body: `ApplySeasonTemplateRequest`
    - `template_id`: UUID of season template to apply
    - `mode`: "replace" or "merge" (default: "replace")
    - `dry_run`: boolean (default: false) - if true, returns preview without applying
  - Response: `ApplySeasonTemplateResponse`
    - `status`: "ok" or "error"
    - `dry_run`: boolean (echoes request parameter)
    - `rate_plan_id`: UUID
    - `template_id`: UUID
    - `mode`: string ("replace" or "merge")
    - `summary`: counts of existing_active, would_archive, would_create, would_update, conflicts
    - `changes`: detailed lists of archive_season_ids, create, update
    - `conflicts`: list of conflict details (only in merge mode when overlaps detected)
  - Status codes:
    - 200 OK: Dry-run preview successful OR apply successful
    - 422 Unprocessable Entity: Conflicts detected in merge mode (no changes applied)
    - 404 Not Found: Rate plan or template not found
    - 403 Forbidden: Insufficient permissions

**Apply Modes:**

1. **Replace Mode** (default):
   - Archives ALL existing active seasons for the rate plan (sets archived_at)
   - Creates new seasons from template periods (copies label, date_from, date_to)
   - No conflict detection (always replaces everything)
   - Use case: Full reset of seasonal pricing

2. **Merge Mode**:
   - Keeps existing active seasons
   - Adds new seasons from template periods
   - Conflict detection: Returns 422 if any template period overlaps with existing seasons
   - Overlap rule: Two date ranges overlap if `date_from < other.date_to AND date_to > other.date_from`
   - Use case: Add new seasons without losing existing configuration

**Dry-Run Preview:**
- Returns same response structure as actual apply, but with `dry_run: true`
- No database changes committed
- Shows exactly what WOULD happen if applied
- Use case: UI preview dialog before confirmation

**Database Transaction:**
- All database writes wrapped in `async with db.transaction()`
- If ANY operation fails (archive, create, update), entire transaction rolls back
- Prevents partial updates (e.g., archived old seasons but failed to create new ones)
- Ensures atomicity: either all changes succeed or none do

**Response Structure Example (Dry-Run Replace Mode):**
```json
{
  "status": "ok",
  "dry_run": true,
  "rate_plan_id": "123e4567-e89b-12d3-a456-426614174000",
  "template_id": "123e4567-e89b-12d3-a456-426614174001",
  "mode": "replace",
  "summary": {
    "existing_active": 3,
    "would_archive": 3,
    "would_create": 2,
    "would_update": 0,
    "conflicts": 0
  },
  "changes": {
    "archive_season_ids": ["...", "...", "..."],
    "create": [
      {
        "label": "Hauptsaison",
        "date_from": "2026-06-01",
        "date_to": "2026-08-31",
        "nightly_cents": null,
        "min_stay_nights": null
      },
      {
        "label": "Nebensaison",
        "date_from": "2026-09-01",
        "date_to": "2026-10-31",
        "nightly_cents": null,
        "min_stay_nights": null
      }
    ],
    "update": []
  },
  "conflicts": []
}
```

**Response Structure Example (Merge Mode with Conflicts):**
```json
{
  "status": "error",
  "dry_run": false,
  "rate_plan_id": "123e4567-e89b-12d3-a456-426614174000",
  "template_id": "123e4567-e89b-12d3-a456-426614174001",
  "mode": "merge",
  "summary": {
    "existing_active": 2,
    "would_archive": 0,
    "would_create": 0,
    "would_update": 0,
    "conflicts": 1
  },
  "changes": {
    "archive_season_ids": [],
    "create": [],
    "update": []
  },
  "conflicts": [
    {
      "period_label": "Hauptsaison",
      "period_date_from": "2026-06-01",
      "period_date_to": "2026-08-31",
      "conflicts_with_season_id": "123e4567-e89b-12d3-a456-426614174002",
      "conflicts_with_date_from": "2026-07-01",
      "conflicts_with_date_to": "2026-07-31",
      "message": "Template period 'Hauptsaison' (2026-06-01 to 2026-08-31) overlaps with existing season (2026-07-01 to 2026-07-31)"
    }
  ]
}
```

**Verification Commands:**

```bash
# [HOST-SERVER-TERMINAL] Pull latest code
cd /data/repos/pms-webapp
git fetch origin main && git reset --hard origin/main

# [HOST-SERVER-TERMINAL] Optional: Verify deploy after Coolify redeploy
export API_BASE_URL="https://api.fewo.kolibri-visions.de"
./backend/scripts/pms_verify_deploy.sh

# [HOST-SERVER-TERMINAL] Run P2.4 smoke test
export HOST="https://api.fewo.kolibri-visions.de"
export JWT_TOKEN="<<<manager/admin JWT>>>"
# Optional:
# export AGENCY_ID="ffd0123a-10b6-40cd-8ad5-66eee9757ab7"
./backend/scripts/pms_season_template_apply_smoke.sh
echo "rc=$?"

# Expected output: All 7 tests pass, rc=0
```

**Common Issues:**

### Dry-Run Returns 200 But Apply Returns 422 (Conflicts)

**Symptom:** Dry-run preview shows `conflicts: 0`, but actual apply (without dry_run) returns 422 with conflicts.

**Root Cause:** Race condition - another user added a conflicting season between dry-run and apply.

**How to Debug:**
```bash
# Check seasons list for rate plan
curl -X GET "$HOST/api/v1/pricing/rate-plans/$RATE_PLAN_ID/seasons?include_archived=false" \
  -H "Authorization: Bearer $JWT_TOKEN" | jq '.[] | {id, label, date_from, date_to}'

# Compare with template periods
curl -X GET "$HOST/api/v1/pricing/season-templates/$TEMPLATE_ID" \
  -H "Authorization: Bearer $JWT_TOKEN" | jq '.periods[] | {label, date_from, date_to}'
```

**Solution:**
- Re-run dry-run preview to see current state
- Use replace mode instead of merge mode to override conflicts
- Or adjust template periods to avoid overlap

### Apply Returns 200 But Seasons Not Updated

**Symptom:** POST /api/v1/pricing/rate-plans/{id}/apply-season-template returns 200, but GET /seasons shows old seasons.

**Root Cause:** Response was from dry-run preview (`dry_run: true` in request body).

**How to Debug:**
```bash
# Check response dry_run field
# Should be "dry_run": false for actual apply

# Re-run apply without dry_run parameter or with dry_run: false
curl -X POST "$HOST/api/v1/pricing/rate-plans/$RATE_PLAN_ID/apply-season-template" \
  -H "Authorization: Bearer $JWT_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"template_id": "'$TEMPLATE_ID'", "mode": "replace", "dry_run": false}' | jq '.dry_run'
```

**Solution:**
- Ensure request body has `"dry_run": false` or omits dry_run parameter (defaults to false)
- Check UI code sends correct request payload

### Replace Mode Archives Seasons But Fails to Create New Ones (Partial Update)

**Symptom:** Existing seasons archived, but no new seasons created. Rate plan left with no active seasons.

**Root Cause:** This should NEVER happen due to transaction wrapping. If it does, transaction was not properly used or database connection issue.

**How to Debug:**
```bash
# Check backend logs for transaction errors
# Look for: "Transaction rolled back" or "asyncpg.exceptions"

# Verify all seasons for rate plan
psql $DATABASE_URL -c "SELECT id, label, date_from, date_to, archived_at FROM rate_plan_seasons WHERE rate_plan_id = '$RATE_PLAN_ID' ORDER BY archived_at NULLS FIRST, date_from;"

# Check if any seasons have archived_at set but were not supposed to be archived
```

**Solution:**
- This indicates a bug in transaction handling or database connection issue
- Manual recovery: Un-archive old seasons if needed:
  ```sql
  UPDATE rate_plan_seasons SET archived_at = NULL WHERE rate_plan_id = '$RATE_PLAN_ID' AND archived_at IS NOT NULL;
  ```
- Report bug with backend logs and database state

### Merge Mode Does Not Detect Overlaps (Accepts Conflicting Periods)

**Symptom:** Merge mode applies template with overlapping periods without returning 422.

**Root Cause:** Overlap detection logic bug (date range comparison incorrect).

**How to Debug:**
```bash
# Manual overlap check
# Two ranges overlap if: date_from < other.date_to AND date_to > other.date_from

# Example:
# Existing: 2026-07-01 to 2026-07-31
# Template: 2026-06-01 to 2026-08-31
# Overlap: 2026-06-01 < 2026-07-31 AND 2026-08-31 > 2026-07-01 = TRUE

# Check backend logs for overlap detection
# Look for: "Overlap detected" or conflict details
```

**Solution:**
- Verify backend/app/api/routes/pricing.py line ~2220-2240 (overlap detection logic)
- Ensure comparison uses: `period.date_from < existing.date_to and period.date_to > existing.date_from`
- Report bug if logic is incorrect

### UI Preview Dialog Shows Wrong Counts

**Symptom:** Preview dialog shows `would_create: 0` but template has 2 periods.

**Root Cause:** UI not parsing response correctly or backend not returning correct summary.

**How to Debug:**
```bash
# Test dry-run endpoint directly
curl -X POST "$HOST/api/v1/pricing/rate-plans/$RATE_PLAN_ID/apply-season-template" \
  -H "Authorization: Bearer $JWT_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"template_id": "'$TEMPLATE_ID'", "mode": "replace", "dry_run": true}' | jq '.summary'

# Expected:
# {
#   "existing_active": N,
#   "would_archive": N (replace mode) or 0 (merge mode),
#   "would_create": 2 (if template has 2 periods),
#   "would_update": 0,
#   "conflicts": 0 or N
# }
```

**Solution:**
- If backend response is correct: Fix UI parsing (check frontend/app/pricing/rate-plans/page.tsx preview data handling)
- If backend response is wrong: Fix backend logic (check pricing.py summary calculation)

---

---

## P2.5 Pricing: Quote v2 (Seasonal Breakdown + Fees + Taxes)

**Overview:** Enhanced quote calculation with per-night seasonal pricing breakdown.

**Purpose:** Provide accurate pricing quotes for booking requests with detailed breakdown showing seasonal rates applied to each night, plus fees and taxes.

**Architecture:**
- **Per-Night Calculation**: Loops through each night from check_in to check_out-1, queries rate_plan_seasons for applicable season
- **Seasonal Pricing**: Each night gets rate from matching season (if any) or falls back to base_nightly_cents
- **Breakdown Response**: Returns `nights_breakdown` array with date, nightly_cents, season_label, season_id for each night
  - **Important**: The breakdown is per-night (one entry per night), not aggregated by season. A 10-night stay spanning two seasons will have 10 entries in nights_breakdown.
- **Fees & Taxes**: Applies pricing_fees and pricing_taxes from database (property-specific or agency-wide)
- **Backward Compatibility**: Maintains existing `nightly_cents` field (set to first night's rate)

**API Endpoints:**

Staff (manager/admin/owner):
- `POST /api/v1/pricing/quote`
  - Request body: `QuoteRequest`
    - `property_id`: UUID (required)
    - `rate_plan_id`: UUID (required)
    - `check_in`: date (required)
    - `check_out`: date (required)
    - `adults`: int (default: 1, range: 1-20)
    - `children`: int (default: 0, range: 0-20)
  - Response: `QuoteResponse`
    - `property_id`, `rate_plan_id`, `check_in`, `check_out`, `nights`
    - `nightly_cents`: int (first night's rate, for compatibility)
    - `nights_breakdown`: list[NightBreakdown]
      - `date`: date (the night date)
      - `nightly_cents`: int (rate for this specific night)
      - `season_label`: string | null (season name if applied)
      - `season_id`: UUID | null (season ID if applied)
    - `subtotal_cents`: int (sum of all night prices)
    - `fees`: list[FeeLineItem] (name, type, amount_cents, taxable)
    - `fees_total_cents`: int
    - `taxable_amount_cents`: int (subtotal + taxable fees)
    - `taxes`: list[TaxLineItem] (name, percent, amount_cents)
    - `taxes_total_cents`: int
    - `total_cents`: int (subtotal + fees + taxes)
    - `currency`: string (e.g., "EUR")
    - `message`: string | null (info message if no pricing configured)
  - Status codes:
    - 200 OK: Quote calculated successfully
    - 400 Bad Request: Invalid input (check_in >= check_out, invalid dates)
    - 404 Not Found: Property or rate plan not found
    - 422 Unprocessable Entity: Rate plan resolver error (multiple plans, no default)

**Per-Night Calculation Logic:**
1. Loop: For each date from check_in to check_out-1
2. Query rate_plan_seasons:
   - WHERE rate_plan_id = $1 AND $2 >= date_from AND $2 < date_to AND active = true AND archived_at IS NULL
   - ORDER BY date_from DESC LIMIT 1 (most recent season wins on overlaps)
3. Use season.nightly_cents if found and not null, else use rate_plan.base_nightly_cents
4. Build nights_breakdown array
5. Calculate subtotal_cents as sum of all night prices

**Example Quote Response (Spanning Two Seasons):**
```json
{
  "property_id": "...",
  "rate_plan_id": "...",
  "check_in": "2026-02-10",
  "check_out": "2026-02-20",
  "nights": 10,
  "adults": 2,
  "children": 0,
  "nightly_cents": 10000,
  "nights_breakdown": [
    {"date": "2026-02-10", "nightly_cents": 10000, "season_label": "Nebensaison", "season_id": "..."},
    {"date": "2026-02-11", "nightly_cents": 10000, "season_label": "Nebensaison", "season_id": "..."},
    {"date": "2026-02-12", "nightly_cents": 10000, "season_label": "Nebensaison", "season_id": "..."},
    {"date": "2026-02-13", "nightly_cents": 10000, "season_label": "Nebensaison", "season_id": "..."},
    {"date": "2026-02-14", "nightly_cents": 10000, "season_label": "Nebensaison", "season_id": "..."},
    {"date": "2026-02-15", "nightly_cents": 15000, "season_label": "Hauptsaison", "season_id": "..."},
    {"date": "2026-02-16", "nightly_cents": 15000, "season_label": "Hauptsaison", "season_id": "..."},
    {"date": "2026-02-17", "nightly_cents": 15000, "season_label": "Hauptsaison", "season_id": "..."},
    {"date": "2026-02-18", "nightly_cents": 15000, "season_label": "Hauptsaison", "season_id": "..."},
    {"date": "2026-02-19", "nightly_cents": 15000, "season_label": "Hauptsaison", "season_id": "..."}
  ],
  "subtotal_cents": 125000,
  "fees": [
    {"name": "Cleaning Fee", "type": "per_stay", "amount_cents": 5000, "taxable": true}
  ],
  "fees_total_cents": 5000,
  "taxable_amount_cents": 130000,
  "taxes": [
    {"name": "VAT", "percent": 10.0, "amount_cents": 13000}
  ],
  "taxes_total_cents": 13000,
  "total_cents": 143000,
  "currency": "EUR"
}
```

**Admin UI:**
- Location: `/pricing/quote`
- Features:
  - Property dropdown (auto-selects first)
  - Rate plan dropdown (fetches plans for selected property)
  - Date range inputs (check-in, check-out)
  - Guests inputs (adults, children)
  - "Berechnen" button triggers quote calculation
  - Breakdown display: nights table (date, season, price), subtotal, fees, taxes, grand total
  - German labels throughout
  - Toast notifications for errors

**Verification Commands:**

```bash
# [HOST-SERVER-TERMINAL] Pull latest code
cd /data/repos/pms-webapp
git fetch origin main && git reset --hard origin/main

# [HOST-SERVER-TERMINAL] Optional: Verify deploy after Coolify redeploy
export API_BASE_URL="https://api.fewo.kolibri-visions.de"
./backend/scripts/pms_verify_deploy.sh

# [HOST-SERVER-TERMINAL] Run P2.5 smoke test
export HOST="https://api.fewo.kolibri-visions.de"
export MANAGER_JWT_TOKEN="<<<manager/admin JWT>>>"
# Optional:
# export PROPERTY_ID="23dd8fda-59ae-4b2f-8489-7a90f5d46c66"
# export AGENCY_ID="ffd0123a-10b6-40cd-8ad5-66eee9757ab7"
./backend/scripts/pms_pricing_quote_v2_smoke.sh
echo "rc=$?"

# Expected output: All 7 tests pass, rc=0
```

**Common Issues:**

### Subtotal Mismatch (Seasonal Pricing Not Applied)

**Symptom:** Quote subtotal_cents doesn't match expected seasonal rates (e.g., expected 125000 for 5×10000 + 5×15000, got 100000).

**Root Cause:** Per-night loop not querying seasons correctly, or falling back to base rate for all nights.

**How to Debug:**
```bash
# Test quote with seasonal override
curl -X POST "$HOST/api/v1/pricing/quote" \
  -H "Authorization: Bearer $JWT_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "property_id": "<property_id>",
    "rate_plan_id": "<rate_plan_id>",
    "check_in": "2026-02-10",
    "check_out": "2026-02-20",
    "adults": 2
  }' | jq '{nights, subtotal_cents, nights_breakdown}'

# Check if nights_breakdown shows correct season labels and rates
# Each entry should show which season (if any) was applied to that night
```

**Solution:**
- Verify season query WHERE clause: `date >= date_from AND date < date_to`
- Check season date ranges don't have gaps (nights between seasons use base rate)
- Ensure seasons are active and not archived

### nights_breakdown Field Missing

**Symptom:** Quote response doesn't include `nights_breakdown` field.

**Root Cause:** Old version of backend before P2.5 deployed.

**How to Debug:**
```bash
# Check deployed version
curl -sS "$HOST/api/v1/ops/version" | jq '.source_commit'

# Compare with expected commit (P2.5 implementation)
# P2.5 should be in commit with message "p2.5: pricing quote v2 breakdown + ui + smoke"
```

**Solution:**
- Deploy latest main branch with P2.5 changes
- Verify backend/app/schemas/pricing.py includes `NightBreakdown` class
- Verify backend/app/api/routes/pricing.py enhanced calculate_quote endpoint

### Quote Returns 422 (Rate Plan Resolver Error)

**Symptom:** POST /api/v1/pricing/quote returns 422 with message about multiple rate plans or no default.

**Root Cause:** Property has multiple active rate plans and no default set, or rate_plan_id not provided.

**How to Debug:**
```bash
# Check rate plans for property
curl -X GET "$HOST/api/v1/pricing/rate-plans?property_id=<property_id>" \
  -H "Authorization: Bearer $JWT_TOKEN" | jq '.[] | {id, name, is_default, property_id}'

# Look for is_default=true entry, or count how many plans exist
```

**Solution:**
- Option 1: Provide `rate_plan_id` explicitly in quote request
- Option 2: Set one rate plan as default: PATCH /api/v1/pricing/rate-plans/{id} {"is_default": true}
- Option 3: If only one rate plan exists, it should auto-select (no default needed)

### Fees or Taxes Not Applied

**Symptom:** Quote response shows fees_total_cents=0 or taxes_total_cents=0 when fees/taxes exist.

**Root Cause:** Fees/taxes inactive, or property_id filter not matching.

**How to Debug:**
```bash
# Check fees for property
curl -X GET "$HOST/api/v1/pricing/fees?property_id=<property_id>" \
  -H "Authorization: Bearer $JWT_TOKEN" | jq '.[] | {id, name, type, value_cents, active, property_id}'

# Check taxes for property
curl -X GET "$HOST/api/v1/pricing/taxes?property_id=<property_id>" \
  -H "Authorization: Bearer $JWT_TOKEN" | jq '.[] | {id, name, percent, active, property_id}'

# Verify active=true and property_id matches or is null (agency-wide)
```

**Solution:**
- Ensure fees/taxes have active=true
- Property-specific fees/taxes (property_id != null) override agency-wide (property_id IS NULL)
- Check fee type is valid: per_stay, per_night, per_person, percent


### Backend Startup Fails with Pydantic RecursionError (Import Loop)

**Symptom:** Backend container restarts continuously. Coolify logs show Python RecursionError or TypeError during import:
```
RecursionError: maximum recursion depth exceeded
File "/app/app/schemas/pricing.py", line 149, in <module>
    class NightBreakdown(BaseModel):
```
OR
```
TypeError: Forward references must evaluate to types. Got FieldInfo(annotation=NoneType, required=True...)
```

**Root Cause:** Pydantic v2 schema with field name shadowing imported type (e.g., `date: date = Field(...)` where `date` is both a field name and a `datetime.date` type). This caused namespace collision and Pydantic repr recursion during module import.

**How to Debug:**
```bash
# Reproduce crash locally with minimal import
cd /path/to/backend
python3 -c "import app.schemas.pricing as s; print('ok')"

# If it crashes with RecursionError or TypeError, check for field names that shadow types
# Common culprits: date, time, datetime, uuid, list, dict, etc.
```

**Solution (Applied in Hotfix):**
1. Added `from __future__ import annotations` at top of pricing.py (enables PEP 563 postponed annotation evaluation)
2. Renamed shadowing field from `date` to `night_date` in NightBreakdown schema
3. Updated all references in API routes and frontend to use new field name

**Verification:**
```bash
# After fix, minimal import must succeed
python3 -c "import app.schemas.pricing as s; print('ok')"
# Expected: "ok"

# Backend should start successfully
docker logs pms-backend --tail 50
# Expected: "Uvicorn running on..." NOT RecursionError
```

**Prevention:**
- Avoid field names that shadow imported types (date, uuid, list, etc.)
- Use `from __future__ import annotations` in all Pydantic schema files
- Test schema imports locally before deploying: `python3 -c "import app.schemas.MODULE"`

---
---

## Objekt-Preispläne Location (P2.6 UI IA)

**Change (2026-01-17)**: Objekt-Preispläne wurden aus der linken Navigation entfernt und in die Objekt-Detailseite verschoben.

**Location**:
- Navigate to: Objekte → Click on property → Tabs: "Überblick | Objekt-Preispläne"
- The "Objekt-Preispläne" tab shows property-scoped rate plans (filtered to current property)
- Property context is from route (no dropdown needed)
- Route pattern: `/properties/{property_id}/rate-plans`

**Navigation**:
- Left nav: "Tarifpläne" entry removed (no longer in navigation)
- Left nav: "Saisons (Agentur)" renamed to "Saisonzeiten" (same functionality, agency-wide seasons)
- Old route `/pricing/rate-plans` still accessible but not linked in nav (backward compatibility)

**Mobile**: Tab navigation responsive at 360px width with horizontal scroll.

**Status**: ✅ VERIFIED in PROD (2026-01-17, commit 8b2535b)

---

### pms-admin build fails: NightBreakdown field mismatch (night_date)

**Symptom:** Coolify deployment for pms-admin fails during `npm run build` with TypeScript error:
```
Type error: Property 'night_date' does not exist on type 'NightBreakdown'
  at frontend/app/pricing/quote/page.tsx:418
```

**Root Cause:** TypeScript type definition for `NightBreakdown` was not updated after P2.5 backend hotfix that renamed `date` → `night_date` and `price_cents` → `nightly_cents` in the Pydantic schema (`backend/app/schemas/pricing.py`). Frontend code used the new field names but the TS type still defined the old names.

**Context:** During P2.5 HOTFIX (2026-01-17), a Pydantic import recursion error was fixed by renaming the `date` field to `night_date` in `NightBreakdown` schema (field name was shadowing `datetime.date` type). The backend API and frontend display code were updated, but the TypeScript type definition was missed.

**Fix:**
- Updated `NightBreakdown` type in `frontend/app/pricing/quote/page.tsx` to use:
  - `night_date: string` (primary field, matches backend)
  - `date?: string` (optional fallback for backward compatibility)
  - `nightly_cents: number` (primary field, matches backend)
  - `price_cents?: number` (optional fallback for backward compatibility)
  - `season_id?: string | null` (added field from backend)
- Updated map loop in quote page to use defensive fallbacks:
  ```typescript
  const nightDate = night.night_date ?? night.date ?? "";
  const nightlyCents = night.nightly_cents ?? night.price_cents ?? 0;
  ```

**Verification:**
```bash
# Check TypeScript type includes night_date
rg -n "night_date" frontend/app/pricing/quote/page.tsx | head -5

# Check defensive fallback usage in map loop
sed -n '418,434p' frontend/app/pricing/quote/page.tsx

# After deploy: verify commit matches
curl -sS -H 'Cache-Control: no-cache' "https://admin.fewo.kolibri-visions.de/api/ops/version?cb=$(date +%s)" | jq .source_commit
```

**Prevention:**
- When renaming backend schema fields that affect API responses, update TypeScript types in frontend immediately (before committing)
- Run `npx tsc --noEmit` locally to catch type errors before pushing
- Consider generating TypeScript types from OpenAPI schema for single source of truth

---


## P2.7 Objekt-Preispläne: Saison-Editor + Template Apply (UI)

**Overview:** Full seasons management UI within property-scoped rate plans, including CRUD operations and template application with preview.

**Purpose:** Allow staff (manager/admin) to manage seasons directly from the property detail page, eliminating need to navigate away. Provides visual editor with template application preview for faster workflow.

**Location:**
- Navigate to: Objekte → Click property → Tab "Objekt-Preispläne" → Click "Saisons bearbeiten" on any rate plan
- Route pattern: `/properties/{property_id}/rate-plans` (modal overlay for seasons editor)

**Architecture:**
- **Frontend-Only**: No new backend endpoints required; uses existing P2.2 and P2.4 endpoints
- **Modal UI**: Seasons editor opens as full-screen modal with three sections:
  1. Current Seasons List (with archived toggle)
  2. Add/Edit Season Form (inline)
  3. Apply Template (with dry-run preview)
- **State Management**: React state handles seasons list, form data, template selection, preview data
- **Mobile-First**: Responsive layout with overflow-x-auto tables, stacked buttons on mobile

**Features:**

1. **List Seasons** (Section A):
   - Table showing: Label, Von (date_from), Bis (date_to), Preis/Nacht, Aktiv, Aktionen
   - Default: Active seasons only (archived_at IS NULL)
   - Optional: "Archivierte anzeigen" toggle to include archived seasons
   - Empty state: "Keine Seasons vorhanden. Erstellen Sie eine neue Season unten."
   - Actions per row: "Bearbeiten" | "Löschen"

2. **Create/Edit Season Form** (Section B):
   - Fields:
     - Label (text, optional, e.g., "Hauptsaison")
     - Von (date, required, YYYY-MM-DD)
     - Bis (date, required, YYYY-MM-DD)
     - Preis pro Nacht (number, cents, optional with live EUR preview)
     - Aktiv (checkbox, default true)
   - Validation:
     - date_from < date_to (inline error if invalid)
     - nightly_cents >= 0 (if provided)
   - Submit: "Erstellen" or "Aktualisieren" (depending on edit mode)
   - Cancel: "Abbrechen" (clears form and exits edit mode)
   - Success: Toast message + refreshes seasons list
   - Error: Toast with API error detail (e.g., "Overlap detected: ...")

3. **Delete Season**:
   - Click "Löschen" → Confirmation dialog: "Möchten Sie diese Season wirklich löschen?"
   - Shows: Season label (if any) and date range
   - Confirm → DELETE /api/v1/pricing/rate-plans/{id}/seasons/{season_id} (soft delete)
   - Success: Season disappears from list (archived_at set), toast "Season gelöscht"
   - Error: Toast with error message

4. **Apply Season Template** (Section C):
   - Dropdown: "Season-Vorlage" (lists all active templates with period counts)
   - Radio buttons: "Ersetzen (bestehende Seasons löschen)" | "Zusammenführen (bestehende Seasons behalten)"
   - Button: "Vorschau" (disabled if no template selected)
   - Click "Vorschau" → POST with dry_run=true → Opens preview dialog

5. **Template Preview Dialog**:
   - Shows summary counts:
     - Bestehende aktive Seasons: N
     - Werden archiviert: N (replace mode only)
     - Werden erstellt: N
     - Werden aktualisiert: N (usually 0)
   - Shows detailed changes:
     - Archive list: "Diese Seasons werden archiviert: [labels/dates]"
     - Create list: "Diese Seasons werden erstellt: [labels/dates/prices]"
   - Conflict warning (merge mode only):
     - Red alert box with conflict messages
     - Example: "Template period 'Hauptsaison' (2026-06-01 to 2026-08-31) overlaps with existing season (2026-07-01 to 2026-07-31)"
     - Suggests: "Bitte wählen Sie 'Ersetzen' Modus oder entfernen Sie die konfliktierenden Seasons manuell."
   - Actions:
     - "Abbrechen" (closes preview, no changes)
     - "Übernehmen" (disabled if conflicts exist)
   - Click "Übernehmen" → POST with dry_run=false → Applies template → Closes preview → Refreshes seasons list → Toast "Vorlage angewendet"

**API Endpoints Used:**

Staff (manager/admin):
- GET /api/v1/pricing/rate-plans/{id}/seasons?include_archived=false (list seasons)
- POST /api/v1/pricing/rate-plans/{id}/seasons (create season)
- PATCH /api/v1/pricing/rate-plans/{id}/seasons/{season_id} (update season)
- DELETE /api/v1/pricing/rate-plans/{id}/seasons/{season_id} (soft delete season)
- GET /api/v1/pricing/season-templates (list templates for dropdown)
- POST /api/v1/pricing/rate-plans/{id}/apply-season-template (preview + apply)

**Status**: ✅ VERIFIED in PROD (2026-01-17, commit 8b37450)

**PROD Verification Evidence:**
- Backend commit: 8b374502c7cb1e136109b9578b5dc663cd54cf63 (started 2026-01-17T13:53:05Z)
- Admin commit: 8b374502c7cb1e136109b9578b5dc663cd54cf63 (started 2026-01-17T13:50:51Z)
- Deploy verification: pms_verify_deploy.sh rc=0
- Smoke tests:
  - pms_rate_plan_seasons_smoke.sh rc=0 (P2.2 CRUD)
  - pms_season_templates_smoke.sh rc=0 (P2.4 templates)
  - pms_season_template_apply_smoke.sh rc=0 (P2.4 apply + merge conflict 422 after hotfix)

**Verification Commands (Manual UI Testing):**

```bash
# PROD Admin UI: https://admin.fewo.kolibri-visions.de

# Manual QA Checklist (Mobile-First: test at 360px width):
# 1. Navigate to Objekte → Click any property → Tab "Objekt-Preispläne"
# 2. Click "Saisons bearbeiten" on any rate plan → Seasons editor modal opens
# 3. Verify Section A: Current Seasons List
#    - Table shows seasons with all columns (Label, Von, Bis, Preis, Aktiv, Aktionen)
#    - Empty state shows helpful message if no seasons
#    - Horizontal scroll works on mobile (overflow-x-auto)
# 4. Verify Section B: Create Season
#    - Fill form: Label="Test Hauptsaison", Von=2026-06-01, Bis=2026-08-31, Preis=15000, Aktiv=checked
#    - Click "Erstellen" → Success toast appears
#    - Season appears in list with correct data
# 5. Verify Section B: Edit Season
#    - Click "Bearbeiten" on newly created season
#    - Form populates with existing data
#    - Change Preis to 18000
#    - Click "Aktualisieren" → Success toast appears
#    - Season updates in list
# 6. Verify Section B: Validation
#    - Try Von=2026-08-31, Bis=2026-06-01 → Inline error appears
#    - Fix dates → Error clears
# 7. Verify Delete Season
#    - Click "Löschen" → Confirmation dialog appears with season details
#    - Click "Löschen" → Success toast appears
#    - Season disappears from list
# 8. Verify Section C: Apply Template (Preview)
#    - Select a template from dropdown
#    - Choose "Ersetzen" mode
#    - Click "Vorschau" → Preview dialog opens
#    - Verify summary counts are correct
#    - Verify "Werden erstellt" list shows template periods
# 9. Verify Section C: Apply Template (Execute)
#    - In preview dialog, click "Übernehmen"
#    - Success toast appears
#    - Preview closes
#    - Seasons list refreshes with new seasons from template
# 10. Verify Mobile Responsiveness (360px width)
#    - Tables scroll horizontally without breaking layout
#    - Buttons stack vertically and remain tappable
#    - Modal fits screen with vertical scroll
# 11. Close editor → Click "Schließen" → Modal closes, returns to rate plans list
```

**Common Issues:**

### Seasons List Empty After Create (400/422 Error)

**Symptom:** Click "Erstellen" → Toast shows error, season not created.

**Root Cause:** Validation error from backend (e.g., date_from > date_to, overlap with existing season, invalid date format).

**How to Debug:**
```bash
# Check browser DevTools Console for API error response
# Look for: 422 Unprocessable Entity with detail field

# Example error: "date_from must be before date_to"
# Example error: "Season overlaps with existing season: Hauptsaison (2026-06-01 to 2026-08-31)"
```

**Solution:**
- Fix date range: Ensure Von < Bis
- Check for overlaps: List existing seasons, adjust date range to avoid conflicts
- If error unclear: Check backend logs for validation details

### Delete Season Succeeds But Season Still Visible

**Symptom:** Click "Löschen" → Success toast appears, but season remains in list.

**Root Cause:** UI not filtering archived seasons correctly (archived_at IS NOT NULL).

**How to Debug:**
```bash
# Check API response after delete
curl -X GET "$HOST/api/v1/pricing/rate-plans/$RATE_PLAN_ID/seasons?include_archived=false" \
  -H "Authorization: Bearer $JWT_TOKEN" | jq '.[] | {id, label, archived_at}'

# Season should have archived_at set (not null)
# If include_archived=false, season should NOT be in response
```

**Solution:**
- If season has archived_at but still visible: Frontend not filtering correctly (check seasons.filter(s => !s.archived_at))
- If season has no archived_at: Backend soft delete failed (check DELETE endpoint response)
- Hard refresh UI (Cmd+Shift+R / Ctrl+F5) to clear cache

### Template Preview Shows 0 Conflicts But Apply Returns 422

**Symptom:** Dry-run preview shows `conflicts: 0`, but clicking "Übernehmen" returns 422 with conflict error.

**Root Cause:** Race condition - another user added a conflicting season between preview and apply.

**How to Debug:**
```bash
# Re-run dry-run to see current state
curl -X POST "$HOST/api/v1/pricing/rate-plans/$RATE_PLAN_ID/apply-season-template" \
  -H "Authorization: Bearer $JWT_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"template_id": "'$TEMPLATE_ID'", "mode": "merge", "dry_run": true}' | jq '.summary.conflicts'

# If conflicts > 0: Preview data is stale
```

**Solution:**
- Close preview dialog
- Click "Vorschau" again to re-fetch latest preview
- Or switch to "Ersetzen" mode to override conflicts

### Template Apply Button Disabled (Grayed Out)

**Symptom:** "Übernehmen" button in preview dialog is disabled despite no conflicts shown.

**Root Cause:** UI logic disables button if `previewData.summary.conflicts > 0` (merge mode conflicts detected).

**How to Debug:**
- Check preview summary: If conflicts count > 0, button is correctly disabled
- Check conflict list: Red alert box should show conflict details

**Solution:**
- Read conflict messages (e.g., "Template period overlaps with existing season")
- Option 1: Switch to "Ersetzen" mode (replaces all existing seasons, no conflict check)
- Option 2: Manually delete conflicting seasons first, then re-run preview

### Stale Token Error (401 Unauthorized)

**Symptom:** Any action (create, edit, delete, preview) returns 401 error.

**Root Cause:** JWT token expired (typically after 1-24 hours depending on auth config).

**How to Debug:**
```bash
# Check browser DevTools → Network tab → Failed request → Response
# Look for: 401 Unauthorized with message "Token expired" or "Invalid token"

# Decode JWT to check expiration
echo $JWT_TOKEN | cut -d'.' -f2 | base64 -d | jq '.exp'
# Compare with current timestamp: date +%s
```

**Solution:**
- Refresh page to get new token (if using cookie-based auth)
- Or log out and log back in
- For API testing: Generate new JWT token with longer expiration

### "Archivierte anzeigen" Toggle Not Working

**Symptom:** Toggle "Archivierte anzeigen" checkbox, but list doesn't update.

**Root Cause:** UI not re-fetching seasons with updated `include_archived` parameter.

**How to Debug:**
- Check DevTools Network tab: Should see GET request with `include_archived=true` when toggle is on
- If no new request: Frontend state not triggering re-fetch

**Solution:**
- Hard refresh UI (Cmd+Shift+R / Ctrl+F5)
- If still broken: Check frontend code (rate-plans/page.tsx) for useEffect dependency on showArchived state

---


### Apply Season Template Returns 500 on Merge Conflicts (Fixed)

**Symptom:** POST /api/v1/pricing/rate-plans/{id}/apply-season-template with mode=merge returns HTTP 500 instead of 422 when conflicts are detected.

**Error Example:**
```
HTTP 500 Internal Server Error
{
  "detail": "Internal server error"
}
```

Or in backend logs:
```
TypeError: Object of type date is not JSON serializable
TypeError: Object of type UUID is not JSON serializable
```

**Root Cause (Fixed in P2.7 Hotfix):** HTTPException detail dict contained non-JSON-serializable objects (Python date and UUID) from Pydantic `.dict()` method. When FastAPI tried to serialize the exception to JSON response, it raised TypeError, resulting in 500 instead of intended 422.

**Fix Applied:** Changed conflict serialization from `.dict()` to `.model_dump(mode='json')` (Pydantic v2), which automatically converts:
- `date` objects → ISO string (e.g., "2026-06-01")
- `UUID` objects → string (e.g., "123e4567-...")
- All other types → JSON-compatible primitives

**How to Debug (if issue recurs):**
```bash
# Test merge mode with known conflicts
curl -X POST "$HOST/api/v1/pricing/rate-plans/$RATE_PLAN_ID/apply-season-template" \
  -H "Authorization: Bearer $JWT_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "template_id": "'$TEMPLATE_ID'",
    "mode": "merge",
    "dry_run": true
  }' -w "\n%{http_code}\n"

# Expected: 422 with JSON response containing conflicts array
# NOT expected: 500 with internal server error
```

**Expected 422 Response:**
```json
{
  "detail": {
    "message": "Cannot apply template in merge mode: 1 conflict(s) detected",
    "conflicts": [
      {
        "period_label": "Hauptsaison",
        "period_date_from": "2026-06-01",
        "period_date_to": "2026-08-31",
        "conflicts_with_season_id": "123e4567-e89b-12d3-a456-426614174000",
        "conflicts_with_date_from": "2026-07-01",
        "conflicts_with_date_to": "2026-07-31",
        "message": "Period 'Hauptsaison' (2026-06-01 to 2026-08-31) overlaps with existing season (2026-07-01 to 2026-07-31)"
      }
    ]
  }
}
```

**Verification:**
```bash
# Check backend logs for TypeError if 500 occurs
# Should NOT see: "Object of type date is not JSON serializable"

# Verify conflict response structure
curl -X POST "$HOST/api/v1/pricing/rate-plans/$RATE_PLAN_ID/apply-season-template" \
  -H "Authorization: Bearer $JWT_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"template_id": "'$TEMPLATE_ID'", "mode": "merge", "dry_run": true}' \
  | jq '.detail.conflicts[0] | keys'

# Expected: ["conflicts_with_date_from", "conflicts_with_date_to", "conflicts_with_season_id", "message", "period_date_from", "period_date_to", "period_label"]
# All values should be strings (not objects)
```

**Prevention:**
- Always use `.model_dump(mode='json')` instead of `.dict()` when serializing Pydantic models for HTTPException detail
- Test API error responses with curl to verify JSON serializability
- Unit tests should verify serialization with `json.dumps()` on exception detail

**Related:**
- backend/app/api/routes/pricing.py line ~2366 (conflict handling)
- backend/app/schemas/pricing.py line ~389 (SeasonConflict model)
- backend/tests/unit/test_season_template_apply_conflict.py (serialization tests)

---


## P2.8 UI: Objekt-Preispläne Mobile-first Polish

**Overview:** Mobile-first polish and UX hardening for the new Objekt-Preispläne flow with legacy route migration banner, enhanced helper text, and property overview summary.

**Purpose:** Improve discoverability and usability of object-specific rate plans after P2.6 UI IA changes that moved rate plans from global navigation into property detail tabs.

**Features:**

1. **Migration Banner on Legacy Route** (`/pricing/rate-plans`):
   - Informs users that property rate plans are now under "Objekte → Objekt → Objekt-Preispläne"
   - Provides direct link to properties list
   - Maintains legacy route for backward compatibility
   - Mobile-responsive with stacked layout on narrow screens

2. **Enhanced Helper Text** (`/properties/[id]/rate-plans`):
   - Clearer explanation of property-specific rate plans
   - Mentions relationship to seasonal pricing overrides
   - Improves first-time user onboarding

3. **Property Overview Summary** (`/properties/[id]`):
   - "Preiseinstellungen" section with rate plans count
   - Quick action link to open Objekt-Preispläne tab
   - Mobile-first grid layout (1 column on mobile, 2 on desktop)
   - Fetches count via GET /api/v1/pricing/rate-plans?property_id={id}

**UI Components:**

- **Migration Banner** (`frontend/app/pricing/rate-plans/page.tsx:730-749`)
  - Blue info banner with heading and description
  - CTA button "Zu Objekte →" linking to /properties
  - Responsive flex layout (column on mobile, row on desktop)

- **Helper Text** (`frontend/app/properties/[id]/rate-plans/page.tsx:577-580`)
  - 2-line description under "Objekt-Preispläne" heading
  - Explains purpose and seasonal override relationship

- **Summary Section** (`frontend/app/properties/[id]/page.tsx:262-281`)
  - Card with gradient background (primary/purple)
  - Left side: Active rate plans count (fetched on mount)
  - Right side: CTA button to open rate-plans tab
  - Grid layout: 1 col mobile, 2 col desktop

**Mobile-first Design:**

- All components tested at 360px viewport width
- Horizontal overflow handled with `overflow-x-auto`
- Touch-friendly button sizes (px-4 py-2 minimum)
- Readable text sizes (text-sm minimum, text-base preferred)
- Stacked layouts on narrow screens with `flex-col sm:flex-row`

**Verification:**

```bash
# Frontend build check (no TypeScript errors)
cd /Users/khaled/Documents/KI/Claude/Claude\ Code/Projekte/PMS-Webapp/frontend
npm run build

# QA proofs (verify text content)
rg "Diese Übersicht wurde verschoben" frontend/app/pricing/rate-plans/
rg "objektspezifische Preispläne" frontend/app/properties/
rg "Aktive Tarifpläne" frontend/app/properties/
```

**Common Issues:**

### Migration Banner Not Visible

**Symptom:** Users don't see the migration banner on `/pricing/rate-plans` page.

**Root Cause:** Component order issue or CSS display problem.

**How to Debug:**
```bash
# Check if banner is rendered in DOM
# DevTools Elements tab: Search for "Diese Übersicht wurde verschoben"

# Verify banner is not hidden by CSS
# Check computed styles: display should not be 'none'
```

**Solution:**
- Verify banner component is rendered before main content (line 730 in page.tsx)
- Check for conflicting CSS classes hiding the banner
- Hard refresh browser (Cmd+Shift+R / Ctrl+F5) to clear cache

### Rate Plans Count Shows "—" Instead of Number

**Symptom:** Property overview summary shows "—" for rate plans count instead of actual number.

**Root Cause:** API request failed or returned unexpected format.

**How to Debug:**
```bash
# Check browser DevTools Network tab for failed requests
# Should see: GET /api/v1/pricing/rate-plans?property_id={id}&limit=100

# Check response format (expecting array or {items: []})
curl -X GET "$HOST/api/v1/pricing/rate-plans?property_id={id}&limit=100" \
  -H "Authorization: Bearer $JWT_TOKEN" | jq 'type'

# Expected: "array" or "object" with .items field
```

**Solution:**
- Verify API endpoint returns valid response
- Check JWT token has permissions to read rate plans
- Ensure property_id is valid UUID
- Check browser console for errors in fetch handler

### Helper Text Not Updated

**Symptom:** Property rate-plans page still shows old helper text.

**Root Cause:** Cached page content or deployment not updated.

**How to Debug:**
```bash
# Check source file contains new text
grep "objektspezifische Preispläne" frontend/app/properties/[id]/rate-plans/page.tsx

# Verify deployment timestamp
curl -I $FRONTEND_URL | grep -i "last-modified"
```

**Solution:**
- Hard refresh browser (Cmd+Shift+R / Ctrl+F5)
- Verify latest deployment includes changes
- Check Coolify deployment logs for successful build

**Related:**
- P2.6 UI IA: Moved rate plans to property detail tabs
- frontend/app/pricing/rate-plans/page.tsx (legacy route with migration banner)
- frontend/app/properties/[id]/rate-plans/page.tsx (property rate-plans page)
- frontend/app/properties/[id]/page.tsx (property overview with summary)

---

## Rate Plans UI: Default Hide Archived + Status Badges

**Overview:** Admin UI improvements for Objekt-Preispläne rate plans list with default-hide archived behavior, toggle to show archived, and clear status badges.

**Purpose:** Improve UX after archive/delete operations by making archived plans disappear from default view, providing explicit toggle to show them, and displaying clear status badges to distinguish Aktiv/Entwurf/Archiviert states.

**UI Behavior:**

1. **Default Filter** (`/properties/[id]/rate-plans`):
   - Default view shows **only non-archived plans** (archived_at IS NULL)
   - API already filters deleted plans (deleted_at IS NULL), so they never appear
   - Empty state: "Keine Tarifpläne vorhanden"

2. **Toggle Control**:
   - Checkbox labeled "Archivierte anzeigen" (off by default)
   - Located in header area, top-right corner
   - When enabled: API call includes `include_archived=true` parameter
   - Immediately re-fetches list when toggled

3. **Status Badges** (Desktop Table + Mobile Cards):
   - **"Aktiv"** (green bg): `active=true AND archived_at IS NULL`
   - **"Entwurf / Inaktiv"** (gray bg): `active=false AND archived_at IS NULL`
   - **"Archiviert"** (gray bg): `archived_at IS NOT NULL`
   - Defensive check: If `archived_at != null`, **never** show "Aktiv" badge (enforces DB constraint invariant)

4. **After Actions**:
   - After Archive/Delete/Activate operations, list automatically re-fetches via `fetchRatePlans()`
   - No manual reload needed; changes appear immediately
   - Archived plans disappear from default view; appear when toggle is enabled

**Code Locations:**

- State: `frontend/app/properties/[id]/rate-plans/page.tsx:78` → `const [showArchived, setShowArchived] = useState(false);`
- Toggle UI: Lines 636-647 → Checkbox "Archivierte anzeigen"
- API call: Line 123 → `include_archived=${showArchived}`
- Desktop badges: Lines 702-708 → Status badge logic with defensive check
- Mobile badges: Lines 776-782 → Same status badge logic
- Re-fetch: Lines 163, 190, 224 → After archive/delete/activate

**DB Constraint Invariant:**
- Migration `20260118110000_rate_plans_delete_semantics_archive_invariants.sql` enforces:
  - `rate_plans_archived_not_active`: Archived plans cannot have active=true
  - `rate_plans_archived_not_default`: Archived plans cannot have is_default=true
- UI defensive check ensures badges reflect these invariants even if data inconsistency exists

**Status Badge Colors:**
```typescript
// Desktop & Mobile (same logic)
const isArchivedButActive = plan.archived_at && plan.active;
const statusBadge = plan.archived_at
  ? { text: "Archiviert", color: "bg-gray-200 text-gray-700" }
  : (plan.active && !isArchivedButActive)
  ? { text: "Aktiv", color: "bg-green-100 text-green-800" }
  : { text: "Entwurf / Inaktiv", color: "bg-gray-100 text-gray-600" };
```

**Verification:**

```bash
# UI build check
cd /Users/khaled/Documents/KI/Claude/Claude\ Code/Projekte/PMS-Webapp/frontend
npm run build

# QA proofs
rg "Archivierte anzeigen" frontend/app/properties/
rg "isArchivedButActive" frontend/app/properties/
rg "include_archived" frontend/app/properties/
```

**Common Issues:**

### Archived Plans Still Visible After Archive

**Symptom:** After clicking "Archivieren", plan still appears in list (default view).

**Root Cause:** Toggle "Archivierte anzeigen" is enabled (checked). Default view should have toggle OFF.

**How to Debug:**
- Check toggle state in UI (should be unchecked by default)
- Verify API call URL includes `include_archived=false` (or omits parameter)
- Check browser DevTools Network tab for GET request

**Solution:**
- Uncheck "Archivierte anzeigen" toggle to hide archived plans
- Verify component state: `showArchived` should be `false` by default (line 78)
- If toggle is stuck, hard refresh browser (Cmd+Shift+R / Ctrl+F5)

### Status Badge Shows "Aktiv" for Archived Plan

**Symptom:** Plan with archived_at != null displays "Aktiv" badge (should show "Archiviert").

**Root Cause:** Data inconsistency (archived_at set but active=true) or defensive check not working.

**How to Debug:**
```bash
# Check plan data in database
psql $DATABASE_URL -c "SELECT id, name, archived_at, active FROM rate_plans WHERE id = '{plan_id}';"

# Expected for archived plan: archived_at NOT NULL, active = false
# If active = true: DB constraint violation (should be blocked by constraint)
```

**Solution:**
- If DB shows `active=true` for archived plan: Migration constraint not applied
- Run migration: `20260118110000_rate_plans_delete_semantics_archive_invariants.sql`
- Backfill bad data: `UPDATE rate_plans SET active = false WHERE archived_at IS NOT NULL;`
- Verify constraint exists: `\d+ rate_plans` → should show `rate_plans_archived_not_active`

### Toggle Not Working (No Re-fetch)

**Symptom:** Clicking "Archivierte anzeigen" checkbox does not update list.

**Root Cause:** useEffect dependency not triggering on `showArchived` change.

**How to Debug:**
```bash
# Verify useEffect dependencies include showArchived
grep -A 2 "useEffect.*propertyId.*showArchived" frontend/app/properties/[id]/rate-plans/page.tsx
# Line 109-113: Should see }, [authLoading, propertyId, showArchived]);
```

**Solution:**
- Verify useEffect on line 109 has `showArchived` in dependency array
- Check browser console for errors during state update
- Hard refresh browser to clear cached component state

### Archivieren Button Sends DELETE Instead of PATCH (422)

**Symptom:** Clicking "Archivieren" button triggers DELETE request and returns HTTP 422 with error "Plan muss zuerst archiviert werden, bevor er gelöscht werden kann."

**Root Cause:** UI wiring bug - archive button handler incorrectly calls `apiClient.delete()` instead of `apiClient.patch()` to `/archive` endpoint.

**How to Debug:**
```bash
# Check browser DevTools Network tab when clicking Archivieren
# Wrong: DELETE /api/v1/pricing/rate-plans/{id} → 422
# Correct: PATCH /api/v1/pricing/rate-plans/{id}/archive → 204

# Verify frontend code
grep -A 3 "handleArchive.*async" frontend/app/properties/[id]/rate-plans/page.tsx
# Should call: apiClient.patch(`.../${plan.id}/archive`, {}, accessToken)
# Not: apiClient.delete(`.../${plan.id}`, accessToken)
```

**Expected Behavior:**
- Archive button sends: `PATCH /api/v1/pricing/rate-plans/{id}/archive` with empty body `{}`
- Expected response: HTTP 204 No Content
- Plan archived_at set, active=false, is_default=false
- List refreshes; plan disappears from default view (unless toggle ON)

**Solution:**
- Fix `handleArchive` function in `frontend/app/properties/[id]/rate-plans/page.tsx`
- Change line 160 from `apiClient.delete()` to `apiClient.patch()`
- Use endpoint: `/api/v1/pricing/rate-plans/${plan.id}/archive`
- Pass empty body: `{}`
- Redeploy frontend

---

## P2.9 Smoke: Objekt-Preisplaene & Saisonzeiten anwenden - Smoke

**Purpose**: Combined end-to-end smoke test for the complete pricing chain including property-scoped rate plans, season templates, preview/apply workflows (merge/replace modes), conflict detection, and quote calculation with seasonal breakdown.

**When to Run**:
- After deploying P2.x pricing features to PROD
- As part of PROD release verification
- Before marking P2.9 as VERIFIED in project_status.md

**Smoke Script**: `backend/scripts/pms_objekt_preisplaene_saisonzeiten_apply_smoke.sh`

### How to Run in PROD

**Prerequisites**:
- Valid JWT token with manager/admin role
- Backend accessible via HTTPS
- Database migrations applied (rate_plans, season_templates, season_template_periods tables exist)

**Environment Variables**:
```bash
# Required
export HOST=https://api.production.example.com              # or API_BASE_URL
export JWT_TOKEN="eyJhbGc..."                                # or MANAGER_JWT_TOKEN

# Optional
export PROPERTY_ID="23dd8fda-59ae-4b2f-8489-7a90f5d46c66"   # default: auto-pick first property
export AGENCY_ID="ffd0123a-10b6-40cd-8ad5-66eee9757ab7"     # for multi-tenant setups
```

**Run**:
```bash
cd /app  # or wherever backend is deployed

./backend/scripts/pms_objekt_preisplaene_saisonzeiten_apply_smoke.sh
```

**Expected Output**:
```
ℹ Starting P2.9 Combined Pricing Chain smoke tests...
ℹ Auto-selected property: 23dd8fda-59ae-4b2f-8489-7a90f5d46c66
✅ Test 1 PASSED: Property validated (Property Name)
✅ Test 2 PASSED: Created rate plan abc123...
✅ Test 3 PASSED: Created season template def456...
✅ Test 4 PASSED: Dry-run preview returned would_create=2
✅ Test 5 PASSED: Applied template (merge mode) - 2 seasons created
✅ Test 6 PASSED: Conflict detection works - 422 returned
✅ Test 7 PASSED: Applied template (replace mode) - archived 2, created 2
✅ Test 8 PASSED: Quote calculation verified ✓
✅ All P2.9 Combined Pricing Chain smoke tests passed!
```

**Exit Code**: `rc=0` on success, `rc=1` on any test failure

### Troubleshooting

#### Auth Errors (401/403)

**Symptom**: Test 1 or 2 fails with 401 Unauthorized or 403 Forbidden.

**Root Cause**: Invalid JWT token or insufficient permissions.

**How to Debug**:
```bash
# Check token expiry (decode JWT)
echo "$JWT_TOKEN" | cut -d. -f2 | base64 -d | jq '.exp, .role'

# Test token against /api/v1/properties endpoint
curl -v -H "Authorization: Bearer $JWT_TOKEN" "$HOST/api/v1/properties?limit=1"
```

**Solution**:
- Refresh JWT token if expired
- Verify token has manager or admin role claim
- Check user exists and has correct role in auth.users table

#### Conflict Detection Fails (Test 6)

**Symptom**: Test 6 expects 422 conflict but gets 200 OK instead.

**Root Cause**: Backend conflict detection logic not working (season overlap check broken).

**How to Debug**:
```bash
# Check backend logs for apply-season-template endpoint
# Look for overlap detection logic execution

# Verify existing seasons have correct date ranges
curl -H "Authorization: Bearer $JWT_TOKEN" \
  "$HOST/api/v1/pricing/rate-plans/$RATE_PLAN_ID/seasons" | jq '.[] | {label, date_from, date_to}'
```

**Solution**:
- Check backend/app/api/routes/pricing.py line ~2300-2400 (apply_season_template_to_rate_plan)
- Verify overlap detection logic is active (not commented out)
- Check season_template_periods have valid date ranges
- Review backend error logs for exceptions during conflict detection

#### Cleanup Issues

**Symptom**: Script creates resources but fails to clean up on exit (orphaned test data).

**Root Cause**: Script terminated before cleanup trap executes, or DELETE endpoints failing.

**How to Debug**:
```bash
# List orphaned smoke test resources
curl -H "Authorization: Bearer $JWT_TOKEN" \
  "$HOST/api/v1/pricing/rate-plans?limit=100" | \
  jq '.items[] | select(.name | startswith("SMOKE_TEST"))'

# List orphaned templates
curl -H "Authorization: Bearer $JWT_TOKEN" \
  "$HOST/api/v1/pricing/season-templates?limit=100" | \
  jq '.[] | select(.name | startswith("SMOKE_TEMPLATE"))'
```

**Solution**:
- Manually archive orphaned resources via DELETE endpoints
- Check DELETE endpoints return 200/204 (not 404/500)
- Verify cleanup trap is executed (test with `set -x` debug mode)
- If script killed abruptly (SIGKILL), run manual cleanup

#### Database 503 Errors

**Symptom**: Tests fail with 503 Service Unavailable errors.

**Root Cause**: Database connection pool exhausted or DB server unreachable.

**How to Debug**:
```bash
# Check backend health endpoint
curl "$HOST/health" | jq '.'

# Check database connectivity (from backend container)
psql -h $DB_HOST -U $DB_USER -d $DB_NAME -c "SELECT 1;"

# Review backend logs for connection errors
docker logs pms-backend --tail 50 | grep -i "database\|connection\|pool"
```

**Solution**:
- Restart backend to reset connection pool
- Check database server status and resources (CPU, memory, connections)
- Verify DATABASE_URL env var is correct
- Scale up DB connection pool size if consistently exhausted

**Related**:
- backend/scripts/README.md - P2.9 smoke script documentation
- backend/scripts/pms_objekt_preisplaene_saisonzeiten_apply_smoke.sh - Smoke script implementation
- P2.2 Rate Plan Seasons Editor (seasons CRUD backend)
- P2.4 Pricing: Apply Season Template (template apply backend)
- P2.5 Pricing: Quote v2 (seasonal breakdown backend)

---

## P2.10 Pricing UX: Saisonpreise pro Objekt (Vorlagen + Preise je Saison)

**Purpose**: Simplified UX for season-based pricing configuration per property, enabling property managers to select a season template and set per-season prices with preview/apply workflow.

**Overview**: P2.10 introduces a streamlined pricing workflow that consolidates base price configuration and season template application into a single, intuitive interface. Instead of navigating between multiple screens, users can now configure all pricing aspects for a property in one place.

### Features

**1. Base Price Renamed to "Fallbackpreis"**:
- Base nightly price field moved to "Erweitert" (Advanced) accordion section
- Renamed from "Basispreis" to "Fallbackpreis" to clarify it's used when no season matches
- Used when booking dates fall outside defined seasonal periods (gap filling)

**2. Saisonpreise Section**:
- New dedicated section for season-based pricing configuration
- Template selector dropdown: Choose from active season templates
- Season groups display: Shows grouped seasons with labels and date ranges
- Per-season price inputs: EUR input fields (automatically converted to cents for API)

**3. Preview/Apply Workflow**:
- Preview button: Shows what would happen (dry-run mode, no DB changes)
- Gap warning banner: Alerts users if seasonal coverage has gaps (fallback will be used)
- Apply button: Commits changes atomically to database
- Conflict detection: Warns if applying template would create overlapping seasons

**4. Price Overrides Support**:
- Each season displays EUR input for price override
- Overrides are stored per-season and take precedence over template defaults
- API endpoint supports optional price_overrides parameter

### How to Verify in PROD

**Prerequisites**:
```bash
# 1. Ensure user is logged in with admin/manager role
# 2. Navigate to: https://admin.fewo.kolibri-visions.de
# 3. Go to: Objekte → [Select Property] → "Preise" tab
```

**Step-by-Step Verification**:

1. **Base Price Configuration (Erweitert)**:
   ```
   - Expand "Erweitert" accordion
   - Verify "Fallbackpreis" field is present (renamed from Basispreis)
   - Set value: 8000 (EUR 80.00) and save
   - Expected: Rate plan base_nightly_cents updated to 8000
   ```

2. **Template Selection**:
   ```
   - Scroll to "Saisonpreise" section
   - Verify template selector dropdown is populated
   - If empty: Create season template first at /pricing/seasons
   - Select template: "Standardsaison 2026"
   - Expected: Season groups appear below dropdown (grouped by label)
   ```

3. **Season Groups Display**:
   ```
   - Verify each season group shows:
     - Label (e.g., "Hauptsaison")
     - Date range (e.g., "01.06.2026 - 31.08.2026")
     - EUR input field for price override
   - Expected: All template periods displayed correctly
   ```

4. **Per-Season Price Inputs**:
   ```
   - Enter prices for each season:
     - Hauptsaison: 120.00 EUR
     - Nebensaison: 80.00 EUR
   - Expected: Input accepts decimal format (00.00)
   ```

5. **Preview Workflow**:
   ```
   - Click "Vorschau" button
   - Expected: API call to POST /api/v1/pricing/rate-plans/{id}/apply-season-template
     with dry_run=true
   - Expected: Response shows summary.would_create count matching template periods
   - Expected: No database changes (verify seasons count remains same)
   ```

6. **Gap Warning**:
   ```
   - If template has gaps (e.g., missing winter months):
   - Expected: Warning banner appears: "Lücken in Saisonabdeckung gefunden.
     Fallbackpreis wird verwendet für Daten außerhalb der Saisonen."
   ```

7. **Apply Workflow**:
   ```
   - Click "Anwenden" button
   - Expected: API call to POST /api/v1/pricing/rate-plans/{id}/apply-season-template
     with dry_run=false, mode=merge or replace
   - Expected: Success message appears
   - Expected: GET /api/v1/pricing/rate-plans/{id}/seasons returns created seasons
   ```

8. **Price Overrides Applied**:
   ```
   - Verify created seasons have correct nightly_rate_cents:
   - Expected: Hauptsaison season has 12000 cents (120.00 EUR)
   - Expected: Nebensaison season has 8000 cents (80.00 EUR)
   ```

### Troubleshooting

#### Fallback Price Not Used (Gaps Exist but No Warning)

**Symptom**: Template has coverage gaps but UI doesn't show warning banner.

**Diagnosis**:
1. Check API response from preview endpoint
2. Verify `gaps` field in response body
3. Check frontend gap detection logic in component

**Fix**:
```bash
# Manual verification of gaps
curl -X POST "https://api.fewo.kolibri-visions.de/api/v1/pricing/rate-plans/{id}/apply-season-template" \
  -H "Authorization: Bearer $JWT_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "template_id": "...",
    "mode": "merge",
    "dry_run": true
  }'

# Expected: Response includes "gaps": [{"start": "2026-01-01", "end": "2026-05-31"}]
# If gaps missing: Backend gap detection logic broken
```

#### Gap Warning Banner Not Showing

**Symptom**: API returns gaps in preview response but UI doesn't display warning.

**Diagnosis**:
1. Open browser DevTools → Console
2. Check for JavaScript errors during preview
3. Verify gap warning component is rendering

**Fix**:
```bash
# Check frontend console for errors like:
# - "Cannot read property 'length' of undefined" → gaps array not destructured correctly
# - Component not rendering → conditional logic broken (gaps.length > 0 check)

# Workaround: Check API response directly in Network tab
# If gaps exist in API but not shown: Frontend regression, redeploy frontend
```

#### Price Overrides Not Applying

**Symptom**: Entered per-season prices but created seasons use template default prices.

**Diagnosis**:
1. Check API request payload in browser Network tab
2. Verify `price_overrides` field is present in POST body
3. Check backend logs for validation errors

**Fix**:
```bash
# Expected request payload:
{
  "template_id": "uuid-here",
  "mode": "merge",
  "dry_run": false,
  "price_overrides": {
    "season-period-uuid-1": 12000,  # 120.00 EUR in cents
    "season-period-uuid-2": 8000    # 80.00 EUR in cents
  }
}

# If price_overrides missing: Frontend not collecting input values correctly
# If price_overrides present but ignored: Backend apply logic not using overrides

# Verification query (after apply):
curl "https://api.fewo.kolibri-visions.de/api/v1/pricing/rate-plans/{id}/seasons" \
  -H "Authorization: Bearer $JWT_TOKEN"

# Expected: Seasons have nightly_rate_cents matching overrides (12000, 8000)
# If all seasons have same price: Overrides not applied, backend bug
```

#### Template Selector Empty (No Active Templates)

**Symptom**: "Saisonpreise" section shows empty dropdown, no templates available.

**Diagnosis**:
1. Verify season templates exist: GET /api/v1/pricing/season-templates
2. Check if templates are archived (is_active=false)
3. Verify user has permission to list templates

**Fix**:
```bash
# Check templates in database
curl "https://api.fewo.kolibri-visions.de/api/v1/pricing/season-templates" \
  -H "Authorization: Bearer $JWT_TOKEN"

# Expected: Response includes templates array with is_active=true
# If empty array: No templates created yet
# If templates exist but is_active=false: Templates archived, restore or create new

# Create template via UI:
# Navigate to: /pricing/seasons → Click "Neue Vorlage" → Add periods → Save
# Return to property pricing tab → Dropdown should now show template
```

#### Preview/Apply Endpoints Failing (500 Internal Server Error)

**Symptom**: Preview or Apply button triggers 500 error, seasons not created.

**Diagnosis**:
1. Check backend logs: docker logs pms-backend-container
2. Verify database schema is up to date (season_templates, season_template_periods tables exist)
3. Check for foreign key constraint violations

**Common Errors**:

```bash
# Error: "relation 'season_templates' does not exist"
# Fix: Run missing migrations
cd /path/to/backend
supabase db push  # or equivalent migration command

# Error: "foreign key constraint violation (rate_plan_id not found)"
# Fix: Rate plan doesn't exist or was deleted, create new rate plan first

# Error: "overlapping seasons detected (conflict)"
# Fix: Expected for merge mode with conflicts, use replace mode or remove conflicting seasons

# Verification:
# 1. Check /api/v1/ops/health → should return 200 (DB accessible)
# 2. Check /api/v1/pricing/rate-plans → should list rate plans (schema OK)
# 3. Check /api/v1/pricing/season-templates → should list templates (schema OK)
```

**Related**:
- P2.2 Rate Plan Seasons Editor (seasons CRUD backend)
- P2.4 Pricing: Apply Season Template (preview/apply workflow backend)
- P2.9 Combined Pricing Chain Smoke (end-to-end verification)
- frontend/app/properties/[id]/rate-plans/page.tsx (UI implementation)

**PROD Verification (2026-01-17):**
- Backend commit ab69ad1 verified via /api/v1/ops/version
- Admin commit ab69ad1 verified via /api/ops/version
- Smoke test: pms_objekt_preisplaene_saisonzeiten_apply_smoke.sh → p2_10_api_smoke_rc=0
- All preview/apply workflows, price overrides, and conflict detection validated in PROD

---

## P2.11 Objekt-Preispläne: Saisonpflicht + Vorlagen/Custom Editor

**Purpose**: Enforce season-based pricing as the standard for all properties, removing flat pricing options and providing a unified edit workflow with template or custom season editor modes.

**Overview**: P2.11 removes the "Als Standard" action from the rate plans table and enforces that each property must have exactly one active rate plan with season-based pricing. The UI displays pricing logic in a "Preislogik" column showing seasonal coverage, and provides a unified edit modal with two modes: template selection or custom season editor. Gap checking prevents saving plans with date gaps unless a fallback price is explicitly enabled.

### Features

**1. One Active Plan Per Property Rule**:
- Each property can have exactly one active (non-archived) rate plan
- UI enforces this with confirmation dialogs when activating/creating plans
- "Als Standard" action removed from table (no longer needed)
- Status implicitly determined by is_archived flag

**2. Preislogik Column Replaces Flat Price**:
- Table column shows "Saisonal (N Saisonzeiten)" instead of flat price amount
- Displays season count for active plans
- Shows "+ Fallback" suffix if fallback price is configured
- Examples:
  - "Saisonal (3 Saisonzeiten)"
  - "Saisonal (2 Saisonzeiten) + Fallback"

**3. Unified Edit Modal with Two Modes**:
- Template Mode:
  - Select from active season templates dropdown
  - Set prices per season group (EUR inputs)
  - Preview shows season count and date coverage
  - Apply creates seasons based on template periods
- Custom Mode:
  - Add/edit/remove individual seasons manually
  - Date pickers with overlap validation
  - Per-season price and label inputs
  - Visual feedback for overlaps and gaps

**4. Gap Checking with Save Blocking**:
- Backend calculates date coverage across all seasons
- Returns gaps array in validation response (e.g., Jan 1 - May 31)
- Frontend shows gap warning with specific date ranges
- Save button disabled by default if gaps exist
- Override available via Erweitert fallback toggle

**5. Fallback Price in Erweitert (Emergency Only)**:
- Collapsed accordion labeled "Erweitert"
- Toggle: "Fallbackpreis aktivieren"
- EUR input field (disabled unless toggle enabled)
- Warning text: "Nur für Notfälle verwenden. Saisonen sollten das ganze Jahr abdecken."
- Enabling fallback allows saving with gaps

**6. Delete Support**:
- Delete action in table row actions menu
- Confirmation dialog warns about dependent quotes/bookings
- Backend DELETE endpoint checks for dependencies
- Returns 422 if rate plan is referenced by active bookings/quotes
- Archives instead of hard deletes to preserve audit trail

### How to Verify in PROD

**Prerequisites**:
```bash
# 1. Ensure user is logged in with admin/manager role
# 2. Navigate to: https://admin.fewo.kolibri-visions.de
# 3. Go to: Objekte → [Select Property] → "Preispläne" tab
```

**Step-by-Step Verification**:

1. **Preislogik Column Display**:
   ```
   - Open rate plans table for any property
   - Verify "Preislogik" column header (no "Preis" or "Als Standard" columns)
   - For active plan with 3 seasons: Shows "Saisonal (3 Saisonzeiten)"
   - If fallback enabled: Shows "+ Fallback" suffix
   - Expected: No flat price amounts displayed
   ```

2. **One Active Plan Rule**:
   ```
   - Create new rate plan via "Neuer Preisplan" button
   - If property already has active plan:
     - Expected: Confirmation dialog appears
     - Message: "Diese Eigenschaft hat bereits einen aktiven Preisplan. Fortfahren wird den alten Plan archivieren."
   - Click "Fortfahren"
   - Expected: Old plan archived (is_archived=true), new plan active
   ```

3. **Edit Modal - Template Mode**:
   ```
   - Click "Bearbeiten" action on active rate plan row
   - Verify modal title: "Preisplan bearbeiten"
   - Verify tab/toggle for "Vorlage verwenden"
   - Select template from dropdown
   - Expected: Season groups appear with date ranges
   - Enter prices: Hauptsaison 120.00, Nebensaison 80.00
   - Click "Vorschau"
   - Expected: Preview shows "3 Saisonzeiten würden erstellt"
   - Click "Anwenden"
   - Expected: Seasons created, modal closes, table refreshes
   ```

4. **Edit Modal - Custom Mode**:
   ```
   - Click "Bearbeiten" on active plan
   - Toggle to "Eigene Saisonen"
   - Click "Saison hinzufügen"
   - Set: Label="Testsaison", Start=2026-06-01, End=2026-08-31, Price=100.00
   - Expected: Season added to list
   - Try adding overlapping season (same dates)
   - Expected: Validation error "Überschneidung mit bestehender Saison"
   - Remove overlap, add valid season
   - Click "Speichern"
   - Expected: Seasons saved, modal closes
   ```

5. **Gap Checking and Save Blocking**:
   ```
   - Edit plan with only summer season (Jun-Aug)
   - Custom mode: Remove all other seasons
   - Expected: Gap warning appears
   - Message: "Lücken gefunden: 01.01.2026 - 31.05.2026, 01.09.2026 - 31.12.2026"
   - Expected: "Speichern" button disabled
   - Try clicking: No action (button disabled)
   ```

6. **Fallback Price Override**:
   ```
   - With gaps present and save blocked:
   - Expand "Erweitert" accordion
   - Toggle "Fallbackpreis aktivieren"
   - Enter fallback price: 60.00 EUR
   - Expected: "Speichern" button now enabled
   - Warning text visible: "Nur für Notfälle verwenden..."
   - Click "Speichern"
   - Expected: Plan saved with fallback price, gaps allowed
   - Verify: GET /api/v1/pricing/rate-plans/{id} shows base_nightly_cents=6000
   ```

7. **Delete Action**:
   ```
   - Click "Löschen" action on archived plan row
   - Expected: Confirmation dialog appears
   - Message: "Dieser Preisplan wird gelöscht. Fortfahren?"
   - Click "Löschen bestätigen"
   - Expected: DELETE /api/v1/pricing/rate-plans/{id} called
   - If no dependencies: Plan removed from table
   - If dependencies exist:
     - Expected: Error dialog "Kann nicht löschen: Preisplan wird von aktiven Buchungen verwendet"
   ```

8. **Preislogik Column Updates**:
   ```
   - Create plan with 2 seasons (no fallback)
   - Expected: "Preislogik" shows "Saisonal (2 Saisonzeiten)"
   - Edit plan, enable fallback
   - Save and refresh table
   - Expected: "Preislogik" shows "Saisonal (2 Saisonzeiten) + Fallback"
   ```

### Troubleshooting

#### Gaps Blocking Save (Expected Behavior)

**Symptom**: Cannot save rate plan because gaps exist in seasonal coverage.

**Diagnosis**:
1. This is expected behavior to enforce complete coverage
2. Check gap warning message for specific date ranges
3. Verify season dates don't cover full year

**Fix**:
```bash
# Option 1: Add more seasons to fill gaps
# - Add winter season: Jan 1 - May 31
# - Add fall season: Sep 1 - Dec 31
# - Ensure no overlaps between seasons

# Option 2: Enable fallback price (emergency only)
# - Expand "Erweitert" accordion
# - Toggle "Fallbackpreis aktivieren"
# - Enter fallback amount (e.g., 60.00 EUR)
# - Save button will become enabled

# Verification:
# - Gap warning should still show (informational)
# - Save button enabled when fallback active
# - After save: Preislogik column shows "+ Fallback" suffix
```

#### Multiple Active Plans (UI Should Prevent)

**Symptom**: Property has multiple non-archived rate plans (violates one active plan rule).

**Diagnosis**:
1. Check database directly: SELECT * FROM rate_plans WHERE property_id='...' AND is_archived=false
2. Verify count > 1
3. This indicates UI validation bypass or backend race condition

**Fix**:
```bash
# Manual fix via API (archive extras)
curl -X PATCH "https://api.fewo.kolibri-visions.de/api/v1/pricing/rate-plans/{extra-plan-id}" \
  -H "Authorization: Bearer $JWT_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"is_archived": true}'

# Expected: Only one plan remains with is_archived=false

# Root cause investigation:
# 1. Check UI confirmation dialog logic (should prompt before creating if active exists)
# 2. Check backend validation (should prevent duplicate active plans per property)
# 3. If race condition: Add unique constraint or transaction locking
```

#### Fallback Toggle Not Showing

**Symptom**: "Erweitert" accordion exists but no fallback toggle inside.

**Diagnosis**:
1. Verify accordion is expanded (click to open)
2. Check browser console for React render errors
3. Verify component conditional rendering logic

**Fix**:
```bash
# Check if accordion is collapsed:
# - Click "Erweitert" header to expand
# - Toggle should appear inside

# If still not visible:
# - Check frontend logs: browser DevTools → Console
# - Look for errors like "Cannot read property 'base_nightly_cents' of undefined"
# - If data loading issue: Verify GET /api/v1/pricing/rate-plans/{id} returns base_nightly_cents field

# Workaround:
# - Set fallback via API directly:
curl -X PATCH "https://api.fewo.kolibri-visions.de/api/v1/pricing/rate-plans/{id}" \
  -H "Authorization: Bearer $JWT_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"base_nightly_cents": 6000}'

# Return to UI, refresh page, verify toggle shows enabled with 60.00 EUR
```

#### Delete Fails (Dependent Quotes/Bookings)

**Symptom**: Delete action returns 422 error, cannot delete rate plan.

**Diagnosis**:
1. Rate plan is referenced by active bookings or quotes
2. Backend prevents deletion to preserve data integrity
3. Check error response message for details

**Fix**:
```bash
# Expected error response:
# HTTP 422: {"detail": "Cannot delete rate plan: 3 active bookings depend on this plan"}

# Solution: Archive instead of delete
curl -X PATCH "https://api.fewo.kolibri-visions.de/api/v1/pricing/rate-plans/{id}" \
  -H "Authorization: Bearer $JWT_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"is_archived": true}'

# Expected: Plan archived, no longer shown as active in UI
# Bookings/quotes still reference plan (data integrity preserved)

# To force delete (NOT RECOMMENDED unless absolutely necessary):
# 1. Manually migrate all dependent bookings to new rate plan
# 2. Verify no references remain:
#    SELECT COUNT(*) FROM bookings WHERE rate_plan_id='{id}'
#    SELECT COUNT(*) FROM quotes WHERE rate_plan_id='{id}'
# 3. If count=0: Safe to delete
# 4. If count>0: Must migrate or wait for bookings to complete/cancel
```

#### Template Selector Empty (No Active Templates)

**Symptom**: Template mode dropdown shows no options.

**Diagnosis**:
1. No season templates exist in database
2. All templates are archived (is_active=false)
3. User lacks permission to view templates

**Fix**:
```bash
# Check templates via API
curl "https://api.fewo.kolibri-visions.de/api/v1/pricing/season-templates" \
  -H "Authorization: Bearer $JWT_TOKEN"

# Expected: Response includes templates array with is_active=true
# If empty array: No templates created yet
# If templates exist but is_active=false: Templates archived

# Solution: Create new template
# Navigate to: /pricing/seasons → Click "Neue Vorlage"
# Add periods:
#   - Hauptsaison: Jun 1 - Aug 31
#   - Nebensaison: Sep 1 - May 31
# Save template

# Return to rate plan edit modal
# Expected: Template now appears in dropdown
```

**Related**:
- P2.2 Rate Plan Seasons Editor (seasons CRUD backend)
- P2.4 Pricing: Apply Season Template (template apply workflow)
- P2.10 Pricing UX (foundation for edit workflow)
- frontend/app/properties/[id]/rate-plans/page.tsx (UI implementation)

**PROD Verification (Pending):**
- Status: NOT VERIFIED (awaiting PROD deployment)
- Implementation date: 2026-01-17
- Verification pending: Manual UI testing in PROD environment

---

## P2.12 Delete Rate Plans: Archive-First Validation

**Purpose**: Enforce archive-first deletion rule for rate plans at both UI and API levels. Prevent accidental deletion of active plans and ensure consistent soft delete behavior.

**Overview**: P2.12 implements delete parity between frontend and backend. The UI delete button is only enabled for archived rate plans (is_archived=true). The backend DELETE endpoint enforces the same rule, returning HTTP 422 if a non-archived plan is submitted. Both rate plans and seasons use soft delete (deleted_at timestamp) to preserve audit trails.

### Features

**1. Backend DELETE Endpoint**:
- Endpoint: DELETE /api/v1/pricing/rate-plans/{id}
- Archive-first validation: Returns 422 if is_archived=false
- Error message: "Bitte erst archivieren"
- Soft delete: Sets deleted_at timestamp (not hard delete)
- Cascade: Also soft-deletes all associated seasons
- Success response: HTTP 204 No Content

**2. Frontend UI Delete Guard**:
- Delete button disabled for non-archived plans (is_archived=false)
- Visual state: Grayed out with tooltip "Bitte erst archivieren"
- Archive action must be performed first
- Confirmation dialog includes archive-first reminder
- Graceful error handling for 422 responses
- Auto-refresh table after successful delete

**3. Soft Delete Semantics**:
- Rate plans: deleted_at column set to current timestamp
- Seasons: All associated seasons get deleted_at timestamp
- Queries exclude deleted records: WHERE deleted_at IS NULL
- Audit trail preserved for compliance
- Future undelete feature possible

### How to Verify in PROD

**Prerequisites**:
```bash
# 1. Ensure user logged in with manager/admin role
# 2. Navigate to: https://admin.fewo.kolibri-visions.de
# 3. Go to: Objekte → [Select Property] → "Preispläne" tab
```

**Step-by-Step Verification**:

1. **Delete Button Disabled for Active Plans**:
   ```
   - Open rate plans table for any property
   - Locate active (non-archived) rate plan
   - Expected: "Löschen" button disabled (grayed out)
   - Hover over button
   - Expected: Tooltip shows "Bitte erst archivieren"
   - Click button (should do nothing)
   ```

2. **Archive Enables Delete**:
   ```
   - Click "Archivieren" action on active plan
   - Confirm archival in dialog
   - Expected: Plan status changes to archived (is_archived=true)
   - Expected: "Löschen" button now enabled (not grayed)
   - Visual confirmation: Button color/state changes
   ```

3. **Delete Workflow Success**:
   ```
   - Click "Löschen" on archived plan
   - Expected: Confirmation dialog appears
   - Message: "Dieser Preisplan wird gelöscht. Fortfahren?"
   - Click "Löschen bestätigen"
   - Expected: DELETE /api/v1/pricing/rate-plans/{id} called
   - Expected: HTTP 204 response
   - Expected: Plan removed from table
   - Expected: Table auto-refreshes
   ```

4. **Backend Archive-First Enforcement (curl)**:
   ```bash
   # Create test plan (non-archived)
   PLAN_ID=$(curl -X POST "https://api.fewo.kolibri-visions.de/api/v1/pricing/rate-plans" \
     -H "Authorization: Bearer $JWT_TOKEN" \
     -H "Content-Type: application/json" \
     -d '{
       "property_id": "'"$PROPERTY_ID"'",
       "name": "Test Delete Plan",
       "base_nightly_cents": 10000,
       "is_archived": false
     }' | jq -r .id)

   # Attempt delete without archiving (should fail)
   curl -i -X DELETE "https://api.fewo.kolibri-visions.de/api/v1/pricing/rate-plans/$PLAN_ID" \
     -H "Authorization: Bearer $JWT_TOKEN"

   # Expected: HTTP 422 Unprocessable Entity
   # Expected body: {"detail": "Bitte erst archivieren"}

   # Archive the plan
   curl -X PATCH "https://api.fewo.kolibri-visions.de/api/v1/pricing/rate-plans/$PLAN_ID" \
     -H "Authorization: Bearer $JWT_TOKEN" \
     -H "Content-Type: application/json" \
     -d '{"is_archived": true}'

   # Now delete should succeed
   curl -i -X DELETE "https://api.fewo.kolibri-visions.de/api/v1/pricing/rate-plans/$PLAN_ID" \
     -H "Authorization: Bearer $JWT_TOKEN"

   # Expected: HTTP 204 No Content
   ```

5. **Soft Delete Verification**:
   ```sql
   -- In Supabase SQL Editor
   SELECT id, name, is_archived, deleted_at, updated_at
   FROM rate_plans
   WHERE id = 'plan-uuid-here';

   -- Expected: deleted_at IS NOT NULL (timestamp set)
   -- Record still exists in database (soft delete)

   -- Verify cascade to seasons
   SELECT id, rate_plan_id, deleted_at
   FROM rate_plan_seasons
   WHERE rate_plan_id = 'plan-uuid-here';

   -- Expected: All seasons have deleted_at IS NOT NULL
   ```

6. **Smoke Test Script**:
   ```bash
   # Run automated delete workflow test
   cd /app  # or local backend directory

   HOST=https://api.fewo.kolibri-visions.de \
   JWT_TOKEN=$YOUR_JWT_TOKEN \
   AGENCY_ID=$YOUR_AGENCY_ID \
   ./backend/scripts/pms_objekt_preisplan_loeschen_smoke.sh

   # Expected output:
   # ✓ Created test property
   # ✓ Created rate plan (non-archived)
   # ✓ DELETE on non-archived plan returns 422
   # ✓ Archived rate plan
   # ✓ DELETE on archived plan returns 204
   # ✓ Verified soft delete (deleted_at set)
   # ✓ Verified cascade delete to seasons
   # p2_12_smoke_rc=0
   ```

### Troubleshooting

#### Delete Button Disabled (Expected Behavior)

**Symptom**: Delete button is grayed out and not clickable.

**Diagnosis**:
1. This is expected for non-archived plans
2. Check plan's is_archived status in table
3. Tooltip should explain "Bitte erst archivieren"

**Fix**:
```bash
# Archive the plan first
# - Click "Archivieren" action in table row
# - Confirm archival in dialog
# - Delete button will become enabled after archive completes
```

#### DELETE Returns 422 (Expected for Non-Archived)

**Symptom**: Backend DELETE endpoint returns HTTP 422 with message "Bitte erst archivieren".

**Diagnosis**:
1. This is expected and correct behavior
2. The plan has is_archived=false
3. Archive-first rule is being enforced

**Fix**:
```bash
# Archive the plan via API
curl -X PATCH "https://api.fewo.kolibri-visions.de/api/v1/pricing/rate-plans/$PLAN_ID" \
  -H "Authorization: Bearer $JWT_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"is_archived": true}'

# Then retry delete
curl -X DELETE "https://api.fewo.kolibri-visions.de/api/v1/pricing/rate-plans/$PLAN_ID" \
  -H "Authorization: Bearer $JWT_TOKEN"

# Expected: HTTP 204 No Content
```

#### Smoke Script Step D Returns 422 "muss zuerst archiviert"

**Symptom**: Smoke test Step D (archive operation) returns HTTP 422 with error message "Rate plan muss zuerst archiviert werden" (expected only for DELETE endpoint).

**Root Cause**:
- Script is calling DELETE endpoint instead of PATCH /archive endpoint
- Step D should archive the plan, not attempt deletion
- Archive endpoint: `PATCH /api/v1/pricing/rate-plans/{id}/archive`
- Delete endpoint: `DELETE /api/v1/pricing/rate-plans/{id}` (only for Step E)

**Diagnosis**:
```bash
# Check which endpoint Step D is calling
grep -A5 "Step D" backend/scripts/pms_objekt_preisplan_loeschen_smoke.sh

# Expected: Should use PATCH with /archive suffix
# Wrong: DELETE /api/v1/pricing/rate-plans/$PLAN_ID
# Correct: PATCH /api/v1/pricing/rate-plans/$PLAN_ID/archive
```

**Fix**:
```bash
# Verify script uses correct archive endpoint for Step D
# Step D should call: PATCH /api/v1/pricing/rate-plans/{id}/archive
# Expected response: HTTP 204 No Content

# Step E should call: DELETE /api/v1/pricing/rate-plans/{id}
# Expected response: HTTP 204 No Content (only if archived in Step D)

# If script is incorrect, update to use:
# PATCH $HOST/api/v1/pricing/rate-plans/$PLAN_ID/archive (Step D)
# DELETE $HOST/api/v1/pricing/rate-plans/$PLAN_ID (Step E)
```

**Expected Behavior**:
- Step D: Archive endpoint returns 204, sets `is_archived=true`
- Step E: Delete endpoint returns 204, sets `deleted_at` timestamp
- If Step D calls DELETE by mistake, it will fail with 422 because plan is not yet archived

#### Smoke Script Step F: GET by ID Returns 307 Redirect

**Symptom**: Step F verification gets HTTP 307 when calling GET /api/v1/pricing/rate-plans/{id}.

**Root Cause**:
- Trailing-slash redirect handling in API routing
- FastAPI redirects requests without trailing slash to version with trailing slash
- This is a routing edge case, not a test failure

**Diagnosis**:
```bash
# GET by ID may return 307 redirect
curl -i "https://api.fewo.kolibri-visions.de/api/v1/pricing/rate-plans/$PLAN_ID" \
  -H "Authorization: Bearer $JWT_TOKEN"

# Expected: HTTP 307 Temporary Redirect
# Location: /api/v1/pricing/rate-plans/{id}/

# Use listings endpoint to verify deletion instead
curl "https://api.fewo.kolibri-visions.de/api/v1/pricing/rate-plans/" \
  -H "Authorization: Bearer $JWT_TOKEN" \
  -G -d "property_id=$PROPERTY_ID"

# Expected: Deleted plan not in response (deleted_at excludes it)
```

**Expected Behavior**:
- This is a routing edge case, not a test failure
- Script correctly uses listings endpoint to confirm deletion
- 307 is treated as warning, deletion confirmed via listings
- No action needed: Smoke script handles this correctly

**Resolution**:
- No fix required
- Smoke script already verifies deletion via listings endpoint
- 307 redirect is expected behavior and does not indicate test failure

#### Seasons Not Deleted

**Symptom**: Rate plan deleted but seasons remain in database without deleted_at timestamp.

**Diagnosis**:
```sql
-- Check if cascade delete worked
SELECT id, rate_plan_id, deleted_at
FROM rate_plan_seasons
WHERE rate_plan_id = 'plan-uuid-here';

-- Expected: All seasons have deleted_at IS NOT NULL
-- If deleted_at IS NULL: Cascade delete failed
```

**Fix**:
```bash
# Check backend logs for errors during delete
docker logs pms-backend --tail 100 | grep "DELETE.*rate-plans"

# Common issues:
# 1. Database constraint prevents cascade
# 2. Backend code missing cascade logic
# 3. Transaction rolled back due to error

# Manual cascade fix (if needed):
# SQL to soft-delete orphaned seasons
UPDATE rate_plan_seasons
SET deleted_at = NOW()
WHERE rate_plan_id = 'plan-uuid-here'
  AND deleted_at IS NULL;
```

#### Soft Delete Not Working

**Symptom**: Rate plan hard-deleted (removed from database) instead of soft-deleted.

**Diagnosis**:
```sql
-- Check if record still exists
SELECT id, name, deleted_at
FROM rate_plans
WHERE id = 'plan-uuid-here';

-- If returns 0 rows: Hard delete occurred (wrong behavior)
-- If returns 1 row with deleted_at: Soft delete worked correctly
```

**Fix**:
```bash
# This indicates backend bug (should not happen)
# 1. Check backend code: pricing.py DELETE endpoint
# 2. Ensure it uses UPDATE SET deleted_at instead of DELETE
# 3. Check database triggers (none should hard-delete)

# If data lost:
# - Check database backup/WAL logs
# - Restore from backup if critical
# - File bug report for hard delete behavior
```

#### UI Shows Deleted Plan

**Symptom**: After successful delete, plan still appears in table.

**Diagnosis**:
1. Table didn't auto-refresh after delete
2. Frontend cache issue
3. Backend query not excluding deleted_at IS NOT NULL

**Fix**:
```bash
# Frontend issue:
# - Hard refresh browser: Cmd+Shift+R / Ctrl+F5
# - Clear browser cache
# - Check browser console for JS errors

# Backend issue (if refresh doesn't help):
curl "https://api.fewo.kolibri-visions.de/api/v1/pricing/rate-plans?property_id=$PROPERTY_ID" \
  -H "Authorization: Bearer $JWT_TOKEN" | jq .

# If deleted plan appears in API response:
# - Backend query missing WHERE deleted_at IS NULL filter
# - Check pricing.py list endpoint
# - Ensure soft delete filter applied
```

### Related Documentation

- [P2.12 Project Status](../docs/project_status.md#p212-admin-ui--api--delete-rate-plans-parity) - Implementation details
- [P2.11 Runbook](../docs/ops/runbook.md#p211-objekt-preispläne-saisonpflicht--vorlagencustom-editor) - Parent feature
- [Smoke Test Script](../scripts/README.md#pms_objekt_preisplan_loeschen_smokesh) - Automated testing guide
- Rate Plans API: DELETE /api/v1/pricing/rate-plans/{id}

### Dependencies

**Required Features**:
- P2.11 Admin UI — Objekt-Preispläne: Saisonpflicht + Edit Workflow
  - Provides archive functionality (is_archived flag)
  - Rate plans table UI foundation

**Database Schema**:
- rate_plans.deleted_at column (timestamp, nullable)
- rate_plan_seasons.deleted_at column (timestamp, nullable)
- Indexes on deleted_at for query performance

**Files Modified**:
- backend/app/api/routes/pricing.py (DELETE endpoint)
- frontend/app/properties/[id]/rate-plans/page.tsx (UI delete guard)
- backend/scripts/pms_objekt_preisplan_loeschen_smoke.sh (new smoke test)

**PROD Verification (Pending):**
- Status: NOT VERIFIED (awaiting PROD deployment)
- Implementation date: 2026-01-17
- Smoke test: pms_objekt_preisplan_loeschen_smoke.sh
- Verification pending: Manual UI + API testing in PROD

---

## P2.13 Pricing Invariants Hardening

**Overview:** Enforce pricing system invariants at DB and API level to prevent human errors.

**Purpose:** Prevent multiple active rate plans per property, season overlaps, missing pricing scenarios through database constraints and API validations.

**Architecture:**
- **Database Constraints**:
  - Partial unique index on rate_plans: ONE active plan per property
  - Exclusion constraint on rate_plan_seasons: NO overlapping seasons per plan
  - Uses PostgreSQL btree_gist extension for daterange overlap detection
- **API Validations**:
  - 409 Conflict: Duplicate active rate plan creation
  - 422 Unprocessable Entity: Season overlaps, missing pricing, merge conflicts
  - German error messages with specific details (dates, labels, conflicting resources)
- **Fallback Pricing**:
  - rate_plans.fallback_price_cents (nullable): Default price for nights without season coverage
  - NULL = no fallback (strict coverage required)
  - Non-NULL = gaps allowed (fallback used)

**Invariants Documentation:** See backend/docs/architecture/pricing_invariants.md for complete specification

**API Endpoints Affected:**

- POST /api/v1/pricing/rate-plans (409 on duplicate active)
- PATCH /api/v1/pricing/rate-plans/{id} (fallback_price_cents support)
- POST /api/v1/pricing/rate-plans/{id}/seasons (422 on overlap)
- PATCH /api/v1/pricing/rate-plans/{id}/seasons/{season_id} (422 on overlap)
- POST /api/v1/pricing/rate-plans/{id}/apply-season-template (422 on merge conflicts)
- POST /api/v1/pricing/quote (422 on missing pricing)

**Smoke Scripts:**

```bash
# 1. Test duplicate active rate plan (409)
./backend/scripts/pms_preisplan_doppel_aktiv_smoke.sh

# 2. Test season overlap (422)
./backend/scripts/pms_saison_overlap_smoke.sh

# 3. Test quote without pricing/fallback (422)
./backend/scripts/pms_quote_keine_saison_smoke.sh

# 4. Test template merge conflicts (422)
./backend/scripts/pms_template_merge_konflikt_smoke.sh
```

**Verification Commands:**

```bash
# HOST-SERVER-TERMINAL
cd /data/repos/pms-webapp
git fetch origin main && git reset --hard origin/main

# Verify deploy (optional)
export API_BASE_URL="https://api.fewo.kolibri-visions.de"
./backend/scripts/pms_verify_deploy.sh

# Run all P2.13 smoke tests
export HOST="https://api.fewo.kolibri-visions.de"
export MANAGER_JWT_TOKEN="<<<manager/admin JWT>>>"
export AGENCY_ID="<<<agency UUID>>>" # optional

# Test 1: Duplicate active plan
./backend/scripts/pms_preisplan_doppel_aktiv_smoke.sh
echo "rc=$?"

# Test 2: Season overlap
./backend/scripts/pms_saison_overlap_smoke.sh
echo "rc=$?"

# Test 3: Quote without pricing
./backend/scripts/pms_quote_keine_saison_smoke.sh
echo "rc=$?"

# Test 4: Template merge conflict
./backend/scripts/pms_template_merge_konflikt_smoke.sh
echo "rc=$?"
```

**Common Issues:**

### 409 Creating Rate Plan (Duplicate Active Plan)

**Symptom:** POST /api/v1/pricing/rate-plans returns 409 Conflict with message "Es existiert bereits ein aktiver Preisplan für dieses Objekt".

**Root Cause:** Property already has an active (non-archived) rate plan. DB constraint allows only ONE active plan per property.

**How to Debug:**
```bash
# Check for existing active plans
curl -X GET "$HOST/api/v1/pricing/rate-plans?property_id=$PROPERTY_ID" \
  -H "Authorization: Bearer $JWT_TOKEN" | python3 -c "
import sys, json
plans = json.load(sys.stdin)
active = [p for p in plans if p.get('archived_at') is None]
print(f'Active plans: {len(active)}')
for p in active:
    print(f\"  - {p['id']}: {p['name']}\")
"

# Expected: 1 active plan (the conflicting one)
```

**Solution:**
- Archive existing active plan first: PATCH /api/v1/pricing/rate-plans/{id}/archive
- Then create new plan
- Or update existing plan instead of creating new one

**Expected Behavior:**
- This is correct enforcement of "one active plan per property" rule
- Prevents confusion from multiple competing pricing strategies
- Forces explicit archival before replacement

### 422 Creating Season (Overlap Detected)

**Symptom:** POST /api/v1/pricing/rate-plans/{id}/seasons returns 422 with message "Überschneidung mit bestehender Saison: {label} ({date_from} bis {date_to})".

**Root Cause:** New season's date range overlaps with existing active season in same rate plan. DB exclusion constraint prevents overlaps.

**How to Debug:**
```bash
# List existing seasons for rate plan
curl -X GET "$HOST/api/v1/pricing/rate-plans/$RATE_PLAN_ID" \
  -H "Authorization: Bearer $JWT_TOKEN" | python3 -c "
import sys, json
plan = json.load(sys.stdin)
seasons = plan.get('seasons', [])
print(f'Seasons ({len(seasons)}):')
for s in seasons:
    if s.get('archived_at') is None:
        print(f\"  - {s['label']}: {s['date_from']} to {s['date_to']}\")
"

# Check if new season [NEW_FROM, NEW_TO) overlaps with any existing [FROM, TO)
# Overlap if: NEW_FROM < TO AND NEW_TO > FROM
```

**Solution:**
- Adjust new season dates to avoid overlap
- Or archive conflicting existing season first
- Or update existing season instead of creating new one

**Date Semantics:**
- Seasons use half-open intervals: [date_from, date_to) (exclusive end)
- Example: [2026-06-01, 2026-08-31) covers nights June 1 through August 30 (not August 31)
- Overlap check: daterange(A) && daterange(B) in PostgreSQL

**Expected Behavior:**
- This is correct enforcement of "no overlaps per plan" rule
- Prevents ambiguous pricing (which season wins?)
- Forces explicit non-overlapping season definitions

### 422 Quoting Without Coverage (Missing Pricing)

**Symptom:** POST /api/v1/pricing/quote returns 422 with message "Keine Saison greift für Nacht {date}, kein Fallbackpreis definiert".

**Root Cause:** Quote requested for dates where no season matches AND rate plan has no fallback_price_cents configured.

**How to Debug:**
```bash
# Get rate plan details
curl -X GET "$HOST/api/v1/pricing/rate-plans/$RATE_PLAN_ID" \
  -H "Authorization: Bearer $JWT_TOKEN" | python3 -c "
import sys, json
plan = json.load(sys.stdin)
print(f\"Fallback price: {plan.get('fallback_price_cents', 'NULL')}\")
print(f\"Seasons:\")
for s in plan.get('seasons', []):
    if s.get('archived_at') is None:
        print(f\"  - {s['label']}: {s['date_from']} to {s['date_to']}\")
"

# Check if quote dates fall outside all seasons
# If so, fallback_price_cents must be set
```

**Solution (Option 1 - Add Fallback Price):**
```bash
# Update rate plan to add fallback price (e.g., 100 EUR = 10000 cents)
curl -X PATCH "$HOST/api/v1/pricing/rate-plans/$RATE_PLAN_ID" \
  -H "Authorization: Bearer $JWT_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"fallback_price_cents": 10000}'

# Now quote will use fallback for gap nights
```

**Solution (Option 2 - Add Covering Season):**
```bash
# Create season covering gap dates
curl -X POST "$HOST/api/v1/pricing/rate-plans/$RATE_PLAN_ID/seasons" \
  -H "Authorization: Bearer $JWT_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "label": "Nebensaison",
    "date_from": "2026-05-01",
    "date_to": "2026-06-01",
    "nightly_cents": 8000
  }'
```

**Expected Behavior:**
- This is strict pricing validation (P2.13 hardening)
- Prevents quotes with undefined pricing
- Forces explicit coverage or fallback configuration

### 422 Template Merge Conflict

**Symptom:** POST /api/v1/pricing/rate-plans/{id}/apply-season-template with mode=merge returns 422 with message "Template kann nicht im Merge-Modus angewendet werden: {count} Überschneidung(en) erkannt".

**Root Cause:** Template periods overlap with existing seasons, and merge mode requires no conflicts.

**How to Debug:**
```bash
# Check error response for conflict details
curl -X POST "$HOST/api/v1/pricing/rate-plans/$RATE_PLAN_ID/apply-season-template" \
  -H "Authorization: Bearer $JWT_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "template_id": "...",
    "mode": "merge",
    "year": 2026,
    "preview": true
  }' | python3 -c "
import sys, json
resp = json.load(sys.stdin)
if 'detail' in resp and 'conflicts' in resp['detail']:
    conflicts = resp['detail']['conflicts']
    print(f'Conflicts ({len(conflicts)}):')
    for c in conflicts:
        print(f\"  - {c.get('message', 'No message')}\")
"
```

**Solution (Option 1 - Use Replace Mode):**
```bash
# Replace mode removes all existing seasons before adding template
curl -X POST "$HOST/api/v1/pricing/rate-plans/$RATE_PLAN_ID/apply-season-template" \
  -H "Authorization: Bearer $JWT_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "template_id": "...",
    "mode": "replace",
    "year": 2026,
    "preview": false
  }'
```

**Solution (Option 2 - Archive Conflicting Seasons):**
```bash
# Archive existing seasons that conflict
curl -X PATCH "$HOST/api/v1/pricing/rate-plans/$RATE_PLAN_ID/seasons/$SEASON_ID/archive" \
  -H "Authorization: Bearer $JWT_TOKEN"

# Then retry merge
```

**Expected Behavior:**
- Merge mode is additive (keeps existing, adds new)
- Replace mode is destructive (removes existing, adds new)
- Conflicts in merge mode → 422 with details
- No conflicts in replace mode → always succeeds

### Migration Fails (Dirty Data)

**Symptom:** Migration 20260117214810_harden_rate_plans_invariants.sql fails with constraint violation errors.

**Root Cause:** Existing data violates new constraints (multiple active plans, overlapping seasons).

**How to Debug:**
```sql
-- Find properties with multiple active plans
SELECT property_id, COUNT(*) as active_count
FROM rate_plans
WHERE archived_at IS NULL AND property_id IS NOT NULL
GROUP BY property_id
HAVING COUNT(*) > 1;

-- Find overlapping seasons within same plan
SELECT s1.rate_plan_id, s1.id as season1_id, s2.id as season2_id,
       s1.label as season1_label, s2.label as season2_label,
       s1.date_from as s1_from, s1.date_to as s1_to,
       s2.date_from as s2_from, s2.date_to as s2_to
FROM rate_plan_seasons s1
JOIN rate_plan_seasons s2
  ON s1.rate_plan_id = s2.rate_plan_id
  AND s1.id < s2.id
  AND s1.archived_at IS NULL
  AND s2.archived_at IS NULL
WHERE daterange(s1.date_from, s1.date_to, '[)') &&
      daterange(s2.date_from, s2.date_to, '[)');
```

**Solution (Clean Existing Data):**
```sql
-- Archive duplicate active plans (keep newest)
WITH ranked_plans AS (
  SELECT id, property_id, created_at,
         ROW_NUMBER() OVER (PARTITION BY property_id ORDER BY created_at DESC) as rn
  FROM rate_plans
  WHERE archived_at IS NULL AND property_id IS NOT NULL
)
UPDATE rate_plans
SET archived_at = NOW()
WHERE id IN (SELECT id FROM ranked_plans WHERE rn > 1);

-- Archive overlapping seasons (keep first created)
WITH overlaps AS (
  SELECT s2.id as season_to_archive
  FROM rate_plan_seasons s1
  JOIN rate_plan_seasons s2
    ON s1.rate_plan_id = s2.rate_plan_id
    AND s1.id < s2.id
    AND s1.archived_at IS NULL
    AND s2.archived_at IS NULL
  WHERE daterange(s1.date_from, s1.date_to, '[)') &&
        daterange(s2.date_from, s2.date_to, '[)')
)
UPDATE rate_plan_seasons
SET archived_at = NOW()
WHERE id IN (SELECT season_to_archive FROM overlaps);

-- Re-run migration
```

**Expected Behavior:**
- Migration continues even if constraints fail on existing data
- Logs warnings with instructions for manual cleanup
- New data respects constraints immediately

### Smoke Tests Fail with 409 (Leftover Active Plan)

**Symptom:** P2.13 smoke tests fail immediately with HTTP 409: "Es existiert bereits ein aktiver Preisplan für dieses Objekt."

**Root Cause:** Previous smoke test run failed to clean up, leaving an active rate plan behind on the test property.

**How to Debug:**
```bash
# List active plans for property
curl -X GET "$HOST/api/v1/pricing/rate-plans?property_id=$PROPERTY_ID" \
  -H "Authorization: Bearer $JWT_TOKEN" | python3 -c "
import sys, json
plans = json.load(sys.stdin)
active = [p for p in plans if p.get('archived_at') is None]
print(f'Active plans: {len(active)}')
for p in active:
    print(f\"  - {p['id']}: {p['name']}\")
"

# Expected: 1 active plan with "- Smoke" suffix (orphaned from previous run)
```

**Solution (Option 1 - API Archive):**
```bash
# Archive orphaned plan via API (preferred)
ORPHAN_ID="<plan-uuid-from-debug-output>"
curl -X PATCH "$HOST/api/v1/pricing/rate-plans/$ORPHAN_ID/archive" \
  -H "Authorization: Bearer $JWT_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{}'

# Then delete (optional)
curl -X DELETE "$HOST/api/v1/pricing/rate-plans/$ORPHAN_ID" \
  -H "Authorization: Bearer $JWT_TOKEN"
```

**Solution (Option 2 - Use Different Property):**
```bash
# Unset PROPERTY_ID to let script auto-select clean property
unset PROPERTY_ID
./backend/scripts/pms_preisplan_doppel_aktiv_smoke.sh
```

**Solution (Option 3 - DB Cleanup):**
```sql
-- Direct DB update (LAST RESORT, requires supabase_admin)
UPDATE rate_plans
SET archived_at = NOW()
WHERE property_id = '<property-uuid>'
  AND archived_at IS NULL
  AND name LIKE '%- Smoke%';
```

**Expected Behavior:**
- Smoke tests should clean up automatically on EXIT
- If script is killed (Ctrl+C), cleanup may not run
- Archive step requires PATCH method with `-d '{}'`, not DELETE

**Prevention:**
- Always let smoke scripts run to completion (trap EXIT cleanup)
- Use Ctrl+C sparingly (trap may not catch all signals)
- Verify cleanup succeeded by checking for "✓ Deleted" messages

### Archive Endpoint Returns 422

**Symptom:** Archive endpoint returns HTTP 422: "Plan muss zuerst archiviert werden, bevor er gelöscht werden kann."

**Root Cause:** Calling DELETE endpoint instead of PATCH /archive, or missing required JSON body.

**How to Debug:**
```bash
# Check archive call in script or manual test
# WRONG (returns 422):
curl -X DELETE "$HOST/api/v1/pricing/rate-plans/$PLAN_ID" ...

# CORRECT (returns 200/204):
curl -X PATCH "$HOST/api/v1/pricing/rate-plans/$PLAN_ID/archive" \
  -H "Authorization: Bearer $JWT_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{}'
```

**Solution:**
- Use PATCH method (not DELETE)
- Use `/archive` endpoint suffix
- Always include `-d '{}'` (empty JSON body required by FastAPI)
- Include `Content-Type: application/json` header

**Expected Behavior:**
- PATCH /archive returns 200 or 204 (success)
- Plan's `archived_at` is set to NOW()
- Plan is no longer returned in active listings
- DELETE endpoint can now be called (if archived_at is set)

### Quote Returns 404 for Gap Pricing Test

**Symptom:** Quote endpoint returns HTTP 404 "Rate plan not found or not applicable to property" when testing gap pricing (dates without season coverage).

**Root Cause:** Rate plan lookup fails BEFORE gap pricing validation runs. Common causes:
1. Rate plan created with `active = false`
2. Rate plan archived between creation and quote request
3. Rate plan belongs to different agency than quote request
4. Wrong rate_plan_id passed in quote request

**How to Debug:**
```bash
# Verify rate plan exists and is active
curl -X GET "$HOST/api/v1/pricing/rate-plans/$RATE_PLAN_ID" \
  -H "Authorization: Bearer $JWT_TOKEN" | python3 -c "
import sys, json
plan = json.load(sys.stdin)
print(f\"Active: {plan.get('active')}\")
print(f\"Archived: {plan.get('archived_at')}\")
print(f\"Property ID: {plan.get('property_id')}\")
print(f\"Agency ID: {plan.get('agency_id')}\")
"

# Expected:
# Active: True
# Archived: None
# Property ID: <matches-quote-property>
# Agency ID: <matches-x-agency-id-header>
```

**Solution:**
- Ensure rate plan has `"active": true` in create payload
- Verify rate plan exists before quote request (GET by ID returns 200)
- Check x-agency-id header matches rate plan's agency_id
- For smoke tests: Add verification step between creation and quote

**Expected Behavior:**
- Quote endpoint validates rate plan access first (404 if not found/applicable)
- Only then validates gap pricing (422 if no coverage and no fallback)
- This is correct HTTP semantics: auth/validation errors before business logic errors

### Quote Returns 500 (NameError: message Not Defined)

**Symptom:** Quote endpoint returns HTTP 500 Internal Server Error with traceback showing `NameError: name 'message' is not defined` in calculate_quote function (pricing.py line ~1629).

**Root Cause:** The `message` variable was referenced in QuoteResponse construction but was only defined in the "no rate plan found" code path. Successful quote calculations crashed when trying to build the response because `message` was never initialized.

**Fixed In:** This bug was fixed by initializing `message: str | None = None` at the start of calculate_quote function.

**How to Verify Fix:**
```bash
# 1. Deploy verification
export API_BASE_URL="https://api.fewo.kolibri-visions.de"
./backend/scripts/pms_verify_deploy.sh

# 2. Run quote gap smoke test
export HOST="https://api.fewo.kolibri-visions.de"
export MANAGER_JWT_TOKEN="<<<manager/admin JWT>>>"
export AGENCY_ID="<<<agency UUID>>>"
./backend/scripts/pms_quote_keine_saison_smoke.sh

# Expected: rc=0 (all tests pass, no 500 errors)
```

**Regression Test:**
- Location: `backend/tests/integration/test_pricing_quote_regression.py`
- Test: `test_quote_gap_no_fallback_returns_422`
- Validates: No NameError occurs, proper 422 returned for gap pricing

**Expected Behavior After Fix:**
- Gap + no fallback → HTTP 422 with "Keine Saison greift" message (not 500)
- Gap + fallback set → HTTP 200 with quote using fallback price
- Covered by season → HTTP 200 with quote using season price

### Quote Returns 200 Despite Missing Season & No Fallback

**Symptom:** Quote endpoint returns HTTP 200 with valid quote when requesting dates that have season gaps, even though `fallback_price_cents` is NULL. The nightly_cents values in nights_breakdown show base_nightly_cents (e.g., 10000) for gap nights.

**Root Cause:** Prior to fix, `base_nightly_cents` was used as a hidden fallback for gap nights in season-based rate plans. The fallback priority was:
1. Season price
2. base_nightly_cents (hidden fallback)
3. fallback_price_cents
4. Error

This violated the P2.13 invariant that season-based plans must use explicit fallback_price_cents for gap nights.

**Fixed In:** This bug was fixed by:
1. Adding has_seasons detection to distinguish season-based vs base-price-only plans
2. Reordering fallback priority: fallback_price_cents before base_nightly_cents
3. Gating base_nightly_cents usage: only allowed when rate plan has NO seasons

**How to Verify Fix:**
```bash
# 1. Deploy verification
export API_BASE_URL="https://api.fewo.kolibri-visions.de"
./backend/scripts/pms_verify_deploy.sh

# 2. Run quote gap smoke test
export HOST="https://api.fewo.kolibri-visions.de"
export MANAGER_JWT_TOKEN="<<<manager/admin JWT>>>"
export AGENCY_ID="<<<agency UUID>>>"
./backend/scripts/pms_quote_keine_saison_smoke.sh

# Expected: rc=0, STEP D returns 422 (not 200)
```

**Regression Tests:**
- Location: `backend/tests/integration/test_pricing_quote_regression.py`
- Primary test: `test_quote_gap_no_fallback_no_base_season_plan_returns_422`
- Backward compat: `test_quote_base_price_only_plan_returns_200`
- Validates: Season-based plans require explicit fallback; base-price-only plans still work

**Expected Behavior After Fix:**
- **Season-based plan + gap + NULL fallback** → HTTP 422 "Keine Saison greift..." (even if base_nightly_cents is set)
- **Season-based plan + gap + fallback set** → HTTP 200 using fallback_price_cents for gap nights
- **Base-price-only plan (no seasons)** → HTTP 200 using base_nightly_cents for all nights (backward compatible)

### PROD Verification (Verified: 2026-01-18)

**Status:** ✅ P2.13 Pricing Invariants Hardening VERIFIED in PROD

**Environment:**
- API Base: https://api.fewo.kolibri-visions.de
- Agency ID: ffd0123a-10b6-40cd-8ad5-66eee9757ab7
- Property ID: 23dd8fda-59ae-4b2f-8489-7a90f5d46c66
- Deploy: source_commit=68e4e6dadf427f869cd2af5139d8a1723ab5a6bd (2026-01-18T04:03:04+00:00)

**Smoke Tests (All Passed, rc=0):**
1. ✅ pms_preisplan_doppel_aktiv_smoke.sh - Duplicate active plan 409 enforcement
2. ✅ pms_saison_overlap_smoke.sh - Season overlap 422 enforcement
3. ✅ pms_quote_keine_saison_smoke.sh - Quote gap 422 enforcement (not 200, not 500)
4. ✅ pms_template_merge_konflikt_smoke.sh - Template merge conflict 422 enforcement

**DB Constraints Verified:**
- Partial unique index: `idx_rate_plans_one_active_per_property` (active)
- Exclusion constraint: `rate_plan_seasons_no_overlap_excl` (active)
- btree_gist extension: Enabled

**API Behavior Verified:**
- ✅ 409 Conflict on duplicate active plan with German error
- ✅ 422 on season overlap with conflicting season name/dates
- ✅ 422 on quote gap without fallback (enforces season-based vs base-price-only distinction)
- ✅ 422 on template merge conflicts with overlap count
- ✅ All error messages in German with actionable details

**Regression Tests:**
- All 7 tests in `test_pricing_quote_regression.py` pass
- Quote 500 NameError fixed (message variable initialized)
- Quote gap invariant enforced (base_nightly_cents gated by has_seasons)

**Known Warnings (Non-Blocking):**
- Quote smoke STEP G.2: `total_price_cents` vs `total_cents` field name mismatch (smoke script validation logic, not backend failure)
- Template preview: Structure warnings in logs (does not affect replace mode success or constraint enforcement)
- Season overlap during cleanup: Expected 422 (demonstrates constraint remains active)
- Planned cleanup in P2.14+ (smoke script improvements, not backend changes)

---

### Rate Plan DELETE Semantics (Soft Delete)

**Overview:** DELETE /api/v1/pricing/rate-plans/{id} implements soft delete using `deleted_at` column. Deleted plans are never returned by GET/LIST endpoints.

**Delete Flow:**
1. Rate plan must be archived first (422 if not archived)
2. DELETE sets `deleted_at = NOW()` and `updated_at = NOW()`
3. All GET/LIST/Quote endpoints filter by `deleted_at IS NULL`
4. Resource disappears from API immediately after DELETE

**Archive-Before-Delete Rule:**
- DB constraint: `rate_plans_deleted_must_be_archived` enforces `deleted_at IS NULL OR archived_at IS NOT NULL`
- Attempting to DELETE non-archived plan returns 422: "Cannot delete rate plan: must be archived first"

**Verification Commands:**

```bash
# Test delete semantics flow
export HOST="https://api.fewo.kolibri-visions.de"
export MANAGER_JWT_TOKEN="<<<manager/admin JWT>>>"
export AGENCY_ID="ffd0123a-10b6-40cd-8ad5-66eee9757ab7"  # Optional
export PROPERTY_ID="23dd8fda-59ae-4b2f-8489-7a90f5d46c66"  # Optional

# Run delete semantics smoke script
./backend/scripts/pms_preisplan_delete_semantics_smoke.sh
echo "rc=$?"

# Expected: All steps pass, rc=0
# STEP D: DELETE returns 204
# STEP E: GET-by-id returns 404 (deleted plan filtered out)
# STEP F: LIST excludes deleted plan
```

**Common Issues:**

**Symptom:** DELETE returns 422 "Cannot delete rate plan: must be archived first"

**Root Cause:** Rate plan has `archived_at IS NULL`. Delete requires archive first.

**Solution:**
```bash
# Archive before delete
curl -X PATCH "$HOST/api/v1/pricing/rate-plans/$RATE_PLAN_ID/archive" \
  -H "Authorization: Bearer $MANAGER_JWT_TOKEN"

# Then delete
curl -X DELETE "$HOST/api/v1/pricing/rate-plans/$RATE_PLAN_ID" \
  -H "Authorization: Bearer $MANAGER_JWT_TOKEN"
# Expected: 204 No Content
```

**Symptom:** After DELETE, GET-by-id still returns 200 (pre-P2.13 bug)

**Root Cause:** Running against old backend version without deleted_at column or filter.

**How to Debug:**
```bash
# Check if backend has deleted_at filter
psql $DATABASE_URL -c "\d rate_plans" | grep deleted_at
# Should show: deleted_at | timestamptz

# Check if plan is actually deleted
psql $DATABASE_URL -c "SELECT id, deleted_at, archived_at FROM rate_plans WHERE id = '$RATE_PLAN_ID';"
# Should show non-NULL deleted_at after DELETE
```

**Solution:**
- Deploy backend commit 7d1ae1d or later (includes migration + endpoint fix)
- Run migration `20260118110000_rate_plans_delete_semantics_archive_invariants.sql`
- Verify smoke script passes (all 6 steps)

**Migration Notes:**
- Migration adds `deleted_at` column if missing (idempotent)
- Adds CHECK constraints for archive invariants
- Backfills inconsistent data (archived plans with active=true)
- PROD-safe: Uses IF NOT EXISTS patterns

---

### Template Apply Validation Errors

**Symptom:** Apply-season-template endpoint returns HTTP 422 with validation errors like "field required: template_id" or "field required: body".

**Root Cause:** Request body missing required `template_id` field or has incorrect structure.

**Correct Request Body:**
```json
{
  "template_id": "UUID",        // REQUIRED - must be existing template
  "mode": "merge",              // Optional, default="replace"
  "dry_run": false,             // Optional, default=false
  "price_overrides": {}         // Optional
}
```

**Common Mistakes:**
- Passing `periods` array instead of `template_id` (wrong: endpoint doesn't accept inline periods)
- Using `merge_mode` instead of `mode` (wrong field name)
- Missing `template_id` entirely
- Passing invalid UUID format for `template_id`

**How to Debug:**
```bash
# Verify template exists and is active
curl -X GET "$HOST/api/v1/pricing/season-templates/$TEMPLATE_ID" \
  -H "Authorization: Bearer $JWT_TOKEN"

# Should return 200 with template details, not 404
```

**Solution:**
1. Create template first: `POST /api/v1/pricing/season-templates`
2. Extract template ID from response
3. Pass template_id in apply request (not periods)

**Template Cleanup:**
Templates don't have archive endpoint - delete directly:
```bash
curl -X DELETE "$HOST/api/v1/pricing/season-templates/$TEMPLATE_ID" \
  -H "Authorization: Bearer $JWT_TOKEN"
```

### Migration Constraint Failures (P2.13 DB Hardening)

**Symptom:** Migration `20260117214810_harden_rate_plans_invariants.sql` fails with:
- `duplicate key value violates unique constraint "idx_rate_plans_one_active_per_property"`
- `conflicting key value violates exclusion constraint "rate_plan_seasons_no_overlap_excl"`

**Root Cause:** Existing dirty data violates new constraints:
1. Multiple active rate plans for same property
2. Overlapping seasons within same rate plan

**How to Debug:**
```sql
-- Find properties with multiple active plans
SELECT property_id, COUNT(*) as active_count,
       array_agg(id) as plan_ids, array_agg(name) as plan_names
FROM rate_plans
WHERE archived_at IS NULL AND property_id IS NOT NULL
GROUP BY property_id
HAVING COUNT(*) > 1;

-- Find overlapping seasons
SELECT s1.rate_plan_id,
       s1.id as season1_id, s1.label as season1_label,
       s2.id as season2_id, s2.label as season2_label,
       s1.date_from as s1_from, s1.date_to as s1_to,
       s2.date_from as s2_from, s2.date_to as s2_to
FROM rate_plan_seasons s1
JOIN rate_plan_seasons s2
  ON s1.rate_plan_id = s2.rate_plan_id
  AND s1.id < s2.id
  AND s1.archived_at IS NULL
  AND s2.archived_at IS NULL
WHERE daterange(s1.date_from, s1.date_to, '[)') &&
      daterange(s2.date_from, s2.date_to, '[)');
```

**Solution (Clean Dirty Data Before Migration):**
```sql
-- Archive duplicate active plans (keep newest per property)
WITH ranked_plans AS (
  SELECT id, property_id, created_at,
         ROW_NUMBER() OVER (PARTITION BY property_id ORDER BY created_at DESC) as rn
  FROM rate_plans
  WHERE archived_at IS NULL AND property_id IS NOT NULL
)
UPDATE rate_plans
SET archived_at = NOW()
WHERE id IN (SELECT id FROM ranked_plans WHERE rn > 1);

-- Archive overlapping seasons (keep first created per overlap group)
WITH overlaps AS (
  SELECT DISTINCT s2.id as season_to_archive
  FROM rate_plan_seasons s1
  JOIN rate_plan_seasons s2
    ON s1.rate_plan_id = s2.rate_plan_id
    AND s1.created_at < s2.created_at
    AND s1.archived_at IS NULL
    AND s2.archived_at IS NULL
  WHERE daterange(s1.date_from, s1.date_to, '[)') &&
        daterange(s2.date_from, s2.date_to, '[)')
)
UPDATE rate_plan_seasons
SET archived_at = NOW()
WHERE id IN (SELECT season_to_archive FROM overlaps);

-- Re-run migration after cleanup
```

**Prevention:**
- Migration has graceful error handling (continues even if constraints fail)
- Logs warnings with cleanup instructions
- New data respects constraints immediately via DB-level enforcement

**Expected Behavior:**
- Clean PROD environments: Migration succeeds without issues
- Dirty data environments: Migration logs warnings, constraints added as NOT VALID, manual cleanup required before validation

---

## Rate Plans UI: Context-Sensitive Actions + Property Page Cleanup

**Overview:** Enhanced rate plans UI with context-sensitive actions based on archive status, restore functionality, and improved property detail page UX.

**Purpose:** Provide appropriate actions for archived vs non-archived rate plans, enable unarchiving via "Wiederherstellen" button, and eliminate duplicate navigation/summary blocks on property detail page.

**UI Behavior:**

1. **Context-Sensitive Actions** (`/properties/[id]/rate-plans`):
   - **Non-archived plans** (archived_at IS NULL):
     - Show "Bearbeiten" (Edit) button → Opens edit modal
     - Show "Archivieren" (Archive) button → Archives plan via PATCH /archive
     - Show "Löschen" (Delete) button **DISABLED** with tooltip "Zum Löschen zuerst archivieren"
   - **Archived plans** (archived_at IS NOT NULL):
     - Show "Wiederherstellen" (Restore) button → Unarchives plan via PATCH /restore
     - Show "Löschen" (Delete) button → Deletes plan via DELETE (sets deleted_at)
     - NO edit/archive actions (archived plans are read-only until restored)

2. **Restore Behavior**:
   - Endpoint: `PATCH /api/v1/pricing/rate-plans/{id}/restore`
   - Sets: `archived_at = NULL`, `active = false` (safe default), `updated_at = NOW()`
   - Respects one-active-plan-per-property invariant:
     - If restoring would create multiple active plans for same property → 409 Conflict
     - User must manually deactivate other plans before restoring to active state
     - After restore, plan is inactive (Entwurf) by default; user can activate via Edit modal

3. **Property Detail Page Cleanup** (`/properties/[id]`):
   - **Tab Rename**: Top navigation tab "Objekt-Preispläne" renamed to "Preiseinstellungen"
   - **Removed Duplicate Block**: Eliminated redundant "Preiseinstellungen" summary card (with "Aktive Tarifpläne 0" count and "Objekt-Preispläne öffnen" button)
   - Rationale: Tab navigation is primary access path; summary block was redundant and cluttered the overview page

**Code Locations:**

- Restore endpoint: `backend/app/api/routes/pricing.py:794-843`
- Restore handler (UI): `frontend/app/properties/[id]/rate-plans/page.tsx:175-190`
- Desktop actions (conditional): Lines 745-787
- Mobile actions (conditional): Lines 835-880
- Tab rename: `frontend/app/properties/[id]/layout.tsx:34` → "Preiseinstellungen"
- Removed block: `frontend/app/properties/[id]/page.tsx` (removed lines 262-281, cleaned up state/effects)

**API Endpoint:**

```http
PATCH /api/v1/pricing/rate-plans/{rate_plan_id}/restore
Authorization: Bearer {jwt_token}
x-agency-id: {agency_id}  # optional if JWT has agency_id claim

# Request Body: (empty)
{}

# Response: 204 No Content
# Side effects: archived_at = NULL, active = false, updated_at = NOW()
```

**Restore Logic:**

```typescript
// frontend/app/properties/[id]/rate-plans/page.tsx:175-190
const handleRestore = async (plan: RatePlan) => {
  if (!accessToken) return;
  try {
    await apiClient.patch(`/api/v1/pricing/rate-plans/${plan.id}/restore`, {}, accessToken);
    showToast(`${plan.name} erfolgreich wiederhergestellt`, "success");
    fetchRatePlans();  // Re-fetch to update UI
  } catch (error) {
    if (error instanceof ApiError && error.status === 409) {
      showToast("Wiederherstellen nicht möglich: Es existiert bereits ein aktiver Preisplan...", "error");
    } else {
      const message = error instanceof ApiError ? error.statusText : "Failed to restore rate plan";
      showToast(message, "error");
    }
  }
};
```

**Verification:**

```bash
# UI build check
cd /Users/khaled/Documents/KI/Claude/Claude\ Code/Projekte/PMS-Webapp/frontend
npm run build

# QA proofs
rg "handleRestore" frontend/app/properties/
rg "Wiederherstellen" frontend/app/properties/
rg "Preiseinstellungen" frontend/app/properties/
sed -n '34p' frontend/app/properties/[id]/layout.tsx  # Tab label
```

**Common Issues:**

### Restore Returns 409 Conflict

**Symptom:** Clicking "Wiederherstellen" shows error "Es existiert bereits ein aktiver Preisplan...".

**Root Cause:** Property already has another active plan. Restoring would violate one-active-plan-per-property invariant.

**How to Debug:**
```bash
# Check active plans for property
psql $DATABASE_URL -c "SELECT id, name, active, archived_at FROM rate_plans WHERE property_id = '{property_id}' AND deleted_at IS NULL;"

# Expected: At most ONE plan with active=true AND archived_at IS NULL
# If 2+ active: Manually deactivate one before restoring
```

**Solution:**
- Open currently active plan in Edit modal
- Uncheck "Aktiv" checkbox → save as inactive (Entwurf)
- Retry "Wiederherstellen" on archived plan
- After restore, plan is inactive by default; activate via Edit if needed

### Restore Returns 404 Not Found

**Symptom:** Clicking "Wiederherstellen" shows 404 error.

**Root Cause:** Plan is not archived (archived_at IS NULL) or is deleted (deleted_at IS NOT NULL).

**How to Debug:**
```bash
# Check plan status
psql $DATABASE_URL -c "SELECT id, name, archived_at, deleted_at FROM rate_plans WHERE id = '{plan_id}';"

# Restore requires: archived_at IS NOT NULL AND deleted_at IS NULL
```

**Solution:**
- Verify plan is archived (status badge shows "Archiviert")
- If deleted: Cannot restore deleted plans (hard delete from UI, soft delete in DB)
- If not archived: Use "Bearbeiten" instead (plan is already in editable state)

### Delete Button Still Enabled for Non-Archived Plans

**Symptom:** Delete button is clickable for non-archived plans (should be disabled).

**Root Cause:** Conditional rendering logic not working or frontend code not deployed.

**How to Debug:**
```bash
# Verify conditional actions logic
grep -A 10 "plan.archived_at.*Wiederherstellen" frontend/app/properties/[id]/rate-plans/page.tsx

# Should see:
# {plan.archived_at ? (
#   // Archived: Wiederherstellen + Löschen
# ) : (
#   // Non-archived: Bearbeiten + Archivieren + Löschen (disabled)
# )}
```

**Solution:**
- Verify frontend deployed with latest code (commit: "ui: archived rate plans actions + pricing tab cleanup")
- Hard refresh browser (Cmd+Shift+R / Ctrl+F5) to clear cached JS
- Check browser console for React render errors

### Property Detail Tab Still Shows "Objekt-Preispläne"

**Symptom:** Property detail page top tab shows "Objekt-Preispläne" instead of "Preiseinstellungen".

**Root Cause:** Frontend not deployed or browser cache.

**How to Debug:**
```bash
# Verify tab label in code
sed -n '34p' frontend/app/properties/[id]/layout.tsx
# Expected: label: "Preiseinstellungen",
```

**Solution:**
- Verify frontend deployed with latest code
- Hard refresh browser to clear cached layout component
- Check Coolify deployment logs for frontend build success

---

### Wiederherstellen-Konflikt (Aktiver Plan existiert)

**Symptom:** Clicking "Wiederherstellen" on an archived rate plan returns 409 Conflict when another active plan exists for the same property.

**Root Cause:** Backend enforces UX guard: only one active plan should exist per property at a time. Even though restore sets active=false, having multiple non-archived plans with one active creates confusion for staff.

**Expected Behavior:**
- Backend returns **409 Conflict** with German message including active plan name
- Frontend displays the error message in a toast (not generic "Database error")
- UI proactively disables "Wiederherstellen" button when active plan exists in current list

**Example Error Response:**

```json
{
  "detail": "Wiederherstellen nicht möglich: Für dieses Objekt ist bereits ein aktiver Tarifplan vorhanden ('Sommer 2026 Hauptsaison'). Bitte archivieren Sie den aktiven Tarifplan zuerst."
}
```

**How to Test (Manual):**
1. Navigate to property detail page → Preiseinstellungen tab
2. Ensure one rate plan is active (green "Aktiv" badge)
3. Create a second plan and archive it immediately
4. Enable "Archivierte anzeigen" toggle
5. Click "Wiederherstellen" on archived plan
6. Expect: German error toast with active plan name + "archivieren...zuerst" instruction

**How to Test (Automated):**
```bash
export HOST="https://api.fewo.kolibri-visions.de"
export JWT_TOKEN="..."
export AGENCY_ID="ffd0123a-10b6-40cd-8ad5-66eee9757ab7"  # optional
export PROPERTY_ID="23dd8fda-59ae-4b2f-8489-7a90f5d46c66"  # optional
./backend/scripts/pms_preisplan_restore_conflict_smoke.sh

# Expected output:
# ✅ STEP E.1 PASSED: Restore returned 409 Conflict
# ✅ STEP E.2 PASSED: Error message German + actionable + keywords present
```

**Staff Guidance:**
1. Identify which plan is currently active (green "Aktiv" badge)
2. Click "Bearbeiten" on the active plan
3. Uncheck "Aktiv" checkbox → Save (plan becomes "Entwurf / Inaktiv")
4. Retry "Wiederherstellen" on archived plan → should succeed
5. If needed, activate the restored plan via Edit modal

**API Endpoint:**

```bash
# Attempt restore with active plan existing
curl -X PATCH https://api.fewo.kolibri-visions.de/api/v1/pricing/rate-plans/{archived_plan_id}/restore \
  -H "Authorization: Bearer $JWT_TOKEN" \
  -H "x-agency-id: $AGENCY_ID"

# Expected response (409):
HTTP/1.1 409 Conflict
Content-Type: application/json

{
  "detail": "Wiederherstellen nicht möglich: Für dieses Objekt ist bereits ein aktiver Tarifplan vorhanden ('Plan Name'). Bitte archivieren Sie den aktiven Tarifplan zuerst."
}
```

**Code Locations:**
- Backend check: `backend/app/api/routes/pricing.py:826-849` (active plan query + 409 response)
- Frontend error handling: `frontend/app/properties/[id]/rate-plans/page.tsx:183-191` (uses backend detail message)
- Frontend UI guard: `frontend/app/properties/[id]/rate-plans/page.tsx:156-159` (hasActivePlan helper)
- Desktop button disable: Lines 757-760 (disabled + tooltip when hasActivePlan)
- Mobile button disable: Lines 850-855 (same logic for mobile)

**Troubleshooting:**

*UI shows generic error instead of German message:*
- Check browser DevTools Network tab for restore request
- Verify response has `detail` field with German text
- If detail missing, backend may not be deployed with latest code
- Frontend extracts `error.statusText` (populated from `detail` field)

*Restore button not disabled even when active plan exists:*
- Hard refresh browser (Cmd+Shift+R / Ctrl+F5) to clear component cache
- Verify `hasActivePlan()` function exists in page.tsx:156-159
- Check ratePlans state contains active plan with `active=true, archived_at=null`
- Ensure "Archivierte anzeigen" toggle is OFF (active plans only visible when not archived)

*Restore succeeds even with active plan (no 409):*
- Backend not deployed with latest code
- Verify backend route at pricing.py:826-849 includes active plan check
- Check logs: should show `SELECT id, name FROM rate_plans WHERE ... active = true`

---

### Restore-Konflikt Smoke Test schlägt fehl (STEP B 409)

**Symptom:** Smoke script `pms_preisplan_restore_conflict_smoke.sh` fails at STEP B with message "Active plan creation returned 409" or "409 conflict but no active plan found on retry".

**Root Cause:** Script attempts to create an active plan if none exists, but one already exists (race condition or API filtering issue). The 409 handling logic re-queries to find the existing plan.

**Expected Behavior (After Fix):**
- STEP B checks for active plan via GET /api/v1/pricing/rate-plans?property_id={id}
- If not found, creates one with active=true
- If creation returns 409, logs warning and re-queries
- If re-query finds active plan, continues test normally
- Only fails if 409 + re-query returns empty (rare edge case)

**How to Debug:**
```bash
# Check if property has active plans
PROPERTY_ID="..."
curl -sS "https://api.fewo.kolibri-visions.de/api/v1/pricing/rate-plans?property_id=${PROPERTY_ID}" \
  -H "Authorization: Bearer $JWT_TOKEN" | jq '.[] | select(.active == true and .archived_at == null)'

# If multiple active plans found: data corruption (violates constraint)
# If zero active plans: API filtering issue or wrong property_id

# Manually create inactive test plan (bypasses conflict)
curl -X POST "https://api.fewo.kolibri-visions.de/api/v1/pricing/rate-plans" \
  -H "Authorization: Bearer $JWT_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"property_id":"'$PROPERTY_ID'","name":"Test Inactive","active":false,"is_default":false,"currency":"EUR","base_nightly_cents":10000}'
```

**Solution:**
- If property has no active plans: Create one manually (active=true, is_default=false) before running smoke
- If property has multiple active plans: Fix data corruption (archive extras, keep only one active)
- If smoke script logic needs update: Check lines 200-231 in pms_preisplan_restore_conflict_smoke.sh

---

### 500 Creating Inactive Plan (Wrong Partial Index)

**Symptom:** POST /api/v1/pricing/rate-plans with `active=false` returns HTTP 500 with generic error:
```json
{"detail":{"error":"internal_server_error","message":"Database error occurred","path":null}}
```

Backend logs show:
```
asyncpg.exceptions.UniqueViolationError: duplicate key value violates unique constraint "idx_rate_plans_one_active_per_property"
Key (property_id)=(23dd8fda-59ae-4b2f-8489-7a90f5d46c66) already exists.
```

**Root Cause:** Database unique index `idx_rate_plans_one_active_per_property` is incorrectly defined (missing `AND active IS TRUE` in WHERE clause). This causes the constraint to apply to ALL non-archived plans instead of only ACTIVE plans.

**Introduced In:** Migration `20260117214810_harden_rate_plans_invariants.sql` created index with WHERE clause:
```sql
WHERE archived_at IS NULL AND property_id IS NOT NULL
```
Should have been:
```sql
WHERE active IS TRUE AND deleted_at IS NULL AND archived_at IS NULL AND property_id IS NOT NULL
```

**Fixed In:** Migration `20260118150000_fix_rate_plans_one_active_partial_index.sql` recreates index with correct partial WHERE clause.

**How to Debug:**
```bash
# Check if migration 20260118150000 has been applied
psql $DATABASE_URL -c "SELECT * FROM supabase_migrations.schema_migrations WHERE version = '20260118150000';"

# If not applied, check index definition
psql $DATABASE_URL -c "SELECT pg_get_indexdef(indexrelid) FROM pg_index JOIN pg_class ON pg_index.indexrelid = pg_class.oid WHERE relname = 'idx_rate_plans_one_active_per_property';"

# Expected (CORRECT):
# CREATE UNIQUE INDEX idx_rate_plans_one_active_per_property ON public.rate_plans USING btree (property_id) WHERE ((active IS TRUE) AND (deleted_at IS NULL) AND (archived_at IS NULL) AND (property_id IS NOT NULL))

# If missing "active IS TRUE", index is wrong
```

**Solution:**
```bash
# Apply migration 20260118150000
cd /data/repos/pms-webapp
git fetch origin main && git reset --hard origin/main
# Wait for Coolify to redeploy (auto-runs migrations)

# Or manually apply migration
psql $DATABASE_URL -f supabase/migrations/20260118150000_fix_rate_plans_one_active_partial_index.sql
```

**Expected Behavior After Fix:**
- Creating INACTIVE plan (`active=false`) when ACTIVE plan exists → **HTTP 201 OK** (allowed)
- Creating ACTIVE plan (`active=true`) when ACTIVE plan exists → **HTTP 409 Conflict** (blocked with German message)
- Smoke script `pms_preisplan_restore_conflict_smoke.sh` STEP C passes

**Defense-in-Depth:**
Backend (commit 86c059f+) includes try/except for `asyncpg.UniqueViolationError` that returns 409 with German message instead of 500, even if index definition is wrong. However, the index MUST be fixed for correct constraint semantics.

---

### Draft Rate Plans Blocked Alongside Active Plans (409/503)

**Symptom:** POST /api/v1/pricing/rate-plans with `active=false` (INACTIVE/draft plan) returns:
- **HTTP 409 Conflict** with message mentioning "Sie können zusätzliche Preispläne nur als ENTWURF (inaktiv) anlegen", OR
- **HTTP 503 Service Unavailable** with message "Datenbank-Schema veraltet: Unique-Index blockiert Entwurf-Preispläne"

**Root Cause:** Database unique index `idx_rate_plans_one_active_per_property` is missing the `active IS TRUE` predicate in its WHERE clause, causing it to block ALL non-archived plans (not just active ones).

**Introduced In:** Migration `20260117214810_harden_rate_plans_invariants.sql` line 39-41:
```sql
CREATE UNIQUE INDEX IF NOT EXISTS idx_rate_plans_one_active_per_property
    ON rate_plans(property_id)
    WHERE archived_at IS NULL AND property_id IS NOT NULL;  -- Missing: AND active IS TRUE
```

**Fixed In:** Migration `20260118160000_repair_rate_plans_one_active_index.sql` (drift-safe replacement)

**HTTP 409 vs 503:**
- **409:** Backend detected the conflict but couldn't determine if it's expected (active->active) or drift (inactive->conflict). Likely means migration hasn't been applied yet.
- **503:** Backend detected INACTIVE plan creation blocked by DB constraint → definitive schema drift. Apply migration immediately.

**How to Debug:**
```bash
# 1. Check if migration 20260118160000 has been applied
psql $DATABASE_URL -c "
  SELECT version, inserted_at
  FROM supabase_migrations.schema_migrations
  WHERE version = '20260118160000';
"
# Expected: 1 row if applied, 0 rows if not

# 2. Check current index definition
psql $DATABASE_URL -c "
  SELECT indexname, indexdef
  FROM pg_indexes
  WHERE schemaname = 'public'
    AND tablename = 'rate_plans'
    AND indexname = 'idx_rate_plans_one_active_per_property';
"
# Expected (CORRECT): indexdef contains "WHERE ((active IS TRUE) AND (deleted_at IS NULL) AND (archived_at IS NULL)..."
# Expected (WRONG): indexdef contains "WHERE ((archived_at IS NULL) AND (property_id IS NOT NULL))" (missing active predicate)

# 3. Test creating INACTIVE plan manually
curl -X POST "$HOST/api/v1/pricing/rate-plans" \
  -H "Authorization: Bearer $JWT_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "property_id": "'$PROPERTY_ID'",
    "name": "Test Draft Plan",
    "currency": "EUR",
    "base_nightly_cents": 10000,
    "active": false,
    "is_default": false
  }'
# Expected (CORRECT): HTTP 201 with response containing "active": false
# Expected (WRONG): HTTP 409 or 503 with error message
```

**Solution:**
```bash
# Option A: Wait for Coolify redeploy (auto-applies migrations)
cd /data/repos/pms-webapp
git fetch origin main && git reset --hard origin/main
# Wait for Coolify to detect changes and redeploy

# Option B: Manually apply migration (faster)
psql $DATABASE_URL -f supabase/migrations/20260118160000_repair_rate_plans_one_active_index.sql

# Verify migration applied successfully
psql $DATABASE_URL -c "
  SELECT indexdef
  FROM pg_indexes
  WHERE indexname = 'idx_rate_plans_one_active_per_property';
" | grep -q "active IS TRUE" && echo "✅ Index fixed" || echo "❌ Index still wrong"
```

**Expected Behavior After Fix:**
- Creating INACTIVE plan (`active=false`) when ACTIVE plan exists → **HTTP 201 OK** ✅
- Creating ACTIVE plan (`active=true`) when ACTIVE plan exists → **HTTP 409 Conflict** with German message ✅
- Smoke script `pms_preisplan_restore_conflict_smoke.sh` STEP C passes ✅

**Verification:**
```bash
# Run smoke script to verify fix
export HOST="https://api.fewo.kolibri-visions.de"
export JWT_TOKEN="<<<manager/admin JWT>>>"
bash backend/scripts/pms_preisplan_restore_conflict_smoke.sh
echo "rc=$?"
# Expected: rc=0, all steps pass (especially STEP C with HTTP 201)
```

---

### Index Creation Fails: Duplicate Active Plans per Property

**Symptom:** Migration `20260118160000_repair_rate_plans_one_active_index.sql` fails with error:
```
ERROR:  could not create unique index "idx_rate_plans_one_active_per_property"
DETAIL:  Key (property_id)=(...) is duplicated.
```

**Root Cause:** Database contains multiple ACTIVE plans for the same property_id. This violates the uniqueness constraint that the migration attempts to create. Likely caused by data corruption or race conditions before the constraint was in place.

**How to Debug:**
```bash
# Find properties with duplicate ACTIVE plans
psql $DATABASE_URL -c "
  SELECT property_id, COUNT(*) as active_plan_count,
         STRING_AGG(id::TEXT || ' (' || name || ')', ', ') as plan_details
  FROM public.rate_plans
  WHERE deleted_at IS NULL
    AND archived_at IS NULL
    AND active IS TRUE
    AND property_id IS NOT NULL
  GROUP BY property_id
  HAVING COUNT(*) > 1;
"
# Output shows property_id with count > 1 and list of conflicting plan IDs/names

# Inspect specific property's plans
psql $DATABASE_URL -c "
  SELECT id, name, active, is_default, archived_at, deleted_at, created_at
  FROM public.rate_plans
  WHERE property_id = '<property_id_from_above>'
    AND deleted_at IS NULL
    AND archived_at IS NULL
  ORDER BY created_at DESC;
"
```

**Remediation (Safe Approach):**

1. **Identify which plan should remain active:**
   - Newest plan (highest created_at)? OR
   - Plan currently set as default (is_default=true)? OR
   - Plan with most recent bookings?

2. **Archive or deactivate extra active plans:**
   ```bash
   # Option A: Archive older plans (preserves history)
   psql $DATABASE_URL -c "
     UPDATE public.rate_plans
     SET archived_at = NOW(), active = FALSE, updated_at = NOW()
     WHERE property_id = '<property_id>'
       AND id != '<plan_id_to_keep>'
       AND deleted_at IS NULL
       AND archived_at IS NULL
       AND active IS TRUE;
   "
   
   # Option B: Deactivate (keeps plans non-archived but inactive)
   psql $DATABASE_URL -c "
     UPDATE public.rate_plans
     SET active = FALSE, updated_at = NOW()
     WHERE property_id = '<property_id>'
       AND id != '<plan_id_to_keep>'
       AND deleted_at IS NULL
       AND archived_at IS NULL
       AND active IS TRUE;
   "
   ```

3. **Verify no duplicates remain:**
   ```bash
   psql $DATABASE_URL -c "
     SELECT property_id, COUNT(*) as active_plan_count
     FROM public.rate_plans
     WHERE deleted_at IS NULL
       AND archived_at IS NULL
       AND active IS TRUE
       AND property_id IS NOT NULL
     GROUP BY property_id
     HAVING COUNT(*) > 1;
   "
   # Expected: 0 rows
   ```

4. **Retry migration:**
   ```bash
   psql $DATABASE_URL -f supabase/migrations/20260118160000_repair_rate_plans_one_active_index.sql
   # Should succeed now
   ```

**Prevention:**
- Migration 20260118160000 creates the partial unique index to prevent future duplicates
- Backend error handling (commit e55815a+) returns HTTP 409 if attempting to create second active plan

**Verification After Fix:**
```bash
# Verify index exists with correct predicate
psql $DATABASE_URL -c "
  SELECT indexdef
  FROM pg_indexes
  WHERE schemaname = 'public'
    AND tablename = 'rate_plans'
    AND indexname = 'idx_rate_plans_one_active_per_property';
" | grep -q "active IS TRUE" && echo "✅ Index created successfully" || echo "❌ Index missing or wrong"

# Test constraint works
# Try creating second active plan for a property - should fail with 409
```

---

## P2.14 Rate Plan Templates — REMOVED

**Status:** Feature removed (commit cd2138c rollback)

**Reason:** "Vorlagen (Agentur)" auf Property-Ebene wurde entfernt; Vorlagen werden ausschließlich als **Saisonvorlagen** unter `/pricing/seasons` verwaltet (P2.1 Season Templates).

**Note:** The concept of agency-wide templates exists only as "Season Templates" (date-range templates with labels), not as standalone rate plan templates. Property pricing is managed via property-scoped rate plans with seasonal overrides.


## P2.15 Property Pricing — Season Schedule + Import + Gap Detection

**Overview:** Property-level season schedule view with year-by-year display, import from "Saisonvorlagen (Agentur)", gap detection, and category grouping.

**Purpose:** Provide property managers with clear season coverage visualization, quick template import, and warnings for pricing gaps in upcoming 24 months.

**Architecture:**
- **Frontend**: Year-by-year season schedule view with category grouping (Hauptsaison/Mittelsaison/Nebensaison/Sonstiges)
- **Import Source**: Existing "Saisonvorlagen (Agentur)" (season templates from P2.1)
- **Gap Detection**: Frontend algorithm checks next 730 days for coverage gaps
- **Idempotency**: Import skips duplicates based on matching date_from/date_to
- **UI Location**: Property detail page → "Preiseinstellungen" tab

**UI Routes:**
- `/properties/[id]/rate-plans` - Property pricing season schedule view (manager/admin)

**API Endpoints Used:**

Seasons:
- `GET /api/v1/pricing/rate-plans/{id}/seasons` - List property seasons
- `POST /api/v1/pricing/rate-plans/{id}/seasons` - Create season from template period
- `PATCH /api/v1/pricing/rate-plans/{id}/seasons/{season_id}` - Update season
- `DELETE /api/v1/pricing/rate-plans/{id}/seasons/{season_id}` - Archive season

Season Templates (import source):
- `GET /api/v1/pricing/season-templates` - List agency season templates
- `GET /api/v1/pricing/season-templates/{id}/periods` - Get template periods

Rate Plans:
- `GET /api/v1/pricing/rate-plans?property_id=` - Get property rate plan

**Frontend Features:**
1. **Year-by-Year Schedule**: Groups seasons by year (date_from year) with category chips
2. **Category Grouping**: Hauptsaison (red), Mittelsaison (orange), Nebensaison (blue), Sonstiges (gray)
3. **Gap Detection**: Warning banner shows missing date ranges in next 730 days
4. **Import Modal**: 
   - Select season template from agency library
   - Quick import buttons: "Dieses Jahr", "Nächstes Jahr", "2 Jahre voraus"
   - Shows template periods preview
5. **Idempotent Import**: Skips seasons with same date_from/date_to to prevent duplicates

**Categorization Heuristic:**
```typescript
const categorizeLabel = (label: string): "haupt" | "mittel" | "neben" | "sonstiges" => {
  const lower = label.toLowerCase();
  if (lower.includes("haupt")) return "haupt";
  if (lower.includes("mittel")) return "mittel";
  if (lower.includes("neben")) return "neben";
  return "sonstiges";
};
```

**Gap Detection Algorithm:**
```typescript
// Check next 730 days from today for coverage gaps
const horizon = 730; // days
const today = new Date();
const endDate = addDays(today, horizon);

// Sort seasons by date_from, identify gaps between consecutive seasons
// Return array of {start, end} gaps with > 1 day duration
```

**Verification Commands:**

```bash
# [HOST-SERVER-TERMINAL] Pull latest code
cd /data/repos/pms-webapp
git fetch origin main && git reset --hard origin/main

# [HOST-SERVER-TERMINAL] Optional: Verify deploy after Coolify redeploy
export API_BASE_URL="https://api.fewo.kolibri-visions.de"
./backend/scripts/pms_verify_deploy.sh

# [HOST-SERVER-TERMINAL] Run season import smoke test
export API_BASE_URL="https://api.fewo.kolibri-visions.de"
export JWT_TOKEN="<<<manager/admin JWT>>>"
export AGENCY_ID="ffd0123a-10b6-40cd-8ad5-66eee9757ab7"
# Optional:
# export PROPERTY_ID="23dd8fda-59ae-4b2f-8489-7a90f5d46c66"
# export SEASON_TEMPLATE_ID="..."
./backend/scripts/pms_objekt_saisonvorlage_import_gap_smoke.sh
echo "rc=$?"

# Expected output: All 5 tests pass, rc=0
```



### Verknüpfte Saisonzeiten + Synchronisierung

**Linked Seasons from Templates:**

When seasons are imported from season templates (Saisonvorlagen), they maintain a link to their source:
- `source_template_period_id`: UUID of the original template period
- `source_year`: Year the template was applied to (e.g., 2025, 2026)
- `source_is_overridden`: Boolean flag indicating manual edits

**Override Behavior:**

Seasons with `source_is_overridden=true` are protected from automatic updates:
- Set to `true` when a user manually edits date ranges, prices, or other season fields
- Prevents sync operations from overwriting user customizations
- User retains full control over manually adjusted seasons

**Sync from Template Endpoint:**

`POST /api/v1/pricing/rate-plans/{id}/seasons/sync-from-template`

This endpoint synchronizes property seasons with their source template:
- **Preview mode** (`dry_run=true`): Shows what would be created/updated without making changes
- **Apply mode** (`dry_run=false`): Creates missing seasons and updates non-overridden linked seasons
- **Idempotent**: Safe to run multiple times; skips overridden seasons
- **Gap closure**: Automatically adds new periods created in template since last import

**Request Body:**
```json
{
  "template_id": "550e8400-e29b-41d4-a716-446655440000",
  "target_year": 2026,
  "dry_run": true  // Preview mode
}
```

**Response (Preview):**
```json
{
  "changes": [
    {
      "action": "create",
      "period_id": "...",
      "label": "Hauptsaison",
      "date_from": "2026-06-01",
      "date_to": "2026-08-31"
    },
    {
      "action": "update",
      "season_id": "...",
      "period_id": "...",
      "label": "Nebensaison",
      "current_dates": {"from": "2026-01-01", "to": "2026-03-31"},
      "new_dates": {"from": "2026-01-01", "to": "2026-04-15"}
    },
    {
      "action": "skip_overridden",
      "season_id": "...",
      "label": "Mittelsaison (manuell bearbeitet)",
      "reason": "User has manually edited this season"
    }
  ]
}
```

**Restore/Purge Endpoints:**

Archived seasons can be restored or permanently deleted:

- `POST /api/v1/pricing/rate-plans/{id}/seasons/{season_id}/restore` - Unarchive a season (sets is_archived=false)
- `DELETE /api/v1/pricing/rate-plans/{id}/seasons/{season_id}/purge` - Permanently delete (requires is_archived=true)

**Troubleshooting:**

**Issue:** Template was edited (periods added/changed), but gaps don't close automatically

**Solution:** Run sync-from-template endpoint with `dry_run=false` to apply updates. Gaps occur when template periods are created/modified after initial import; sync closes these gaps by creating missing seasons.

**Issue:** Sync overwrites my manual price adjustments

**Expected Behavior:** This should NOT happen. Seasons with `source_is_overridden=true` are skipped during sync operations. If this occurs, check:
1. Verify season has `source_is_overridden=true` in database
2. Check sync response for `skip_overridden` actions
3. If overridden flag is missing, manually edit the season again to re-set the flag

**Issue:** Cannot purge active season (404 or validation error)

**Solution:** Archive the season first using `DELETE /api/v1/pricing/rate-plans/{id}/seasons/{season_id}` (soft delete), then call the purge endpoint. The purge endpoint requires `is_archived=true`.

**Verification Commands (Season Template Sync):**

```bash
# [HOST-SERVER-TERMINAL] Pull latest code
cd /data/repos/pms-webapp
git fetch origin main && git reset --hard origin/main

# [HOST-SERVER-TERMINAL] Optional: Verify deploy after Coolify redeploy
export API_BASE_URL="https://api.fewo.kolibri-visions.de"
./backend/scripts/pms_verify_deploy.sh

# [HOST-SERVER-TERMINAL] Run season template sync smoke test
export HOST="https://api.fewo.kolibri-visions.de"
export ADMIN_JWT_TOKEN="<<<manager/admin JWT>>>"
# Optional:
# export PROPERTY_ID="23dd8fda-59ae-4b2f-8489-7a90f5d46c66"
# export TEMPLATE_ID="550e8400-e29b-41d4-a716-446655440000"
# export YEARS="2025 2026"
# export AGENCY_ID="ffd0123a-10b6-40cd-8ad5-66eee9757ab7"
./backend/scripts/pms_season_template_sync_apply_smoke.sh
echo "rc=$?"

# Expected output: All 4 tests pass, rc=0
# Script handles both array and {items:[]} API response formats automatically
```

**PROD Behavior Notes:**
- Backend enforces ONE active rate plan per property (409 if violated)
- Smoke script creates an **INACTIVE** test rate plan (`active=false`) to avoid conflicts with existing active plans
- Script supports both array `[{...}]` and paginated `{items:[...]}` API responses
- Auto-selects property if PROPERTY_ID not provided; auto-creates template if TEMPLATE_ID not provided
- Cleanup via trap EXIT archives all test artifacts (template, seasons, test rate plan) even on failure


**Common Issues:**

### Preflight Health Check Returns 404

**Symptom:** Script fails immediately with "Health check failed (HTTP 404)" during preflight stage.

**Root Cause:** Script was checking health at wrong endpoint path (e.g., `/api/v1/ops/health` instead of `/health`).

**How to Debug:**
```bash
# Check which URL the script tried
# Script output shows: "URL checked: <url>"

# Manually verify health endpoints exist at root level
curl -sS https://api.fewo.kolibri-visions.de/health
curl -sS https://api.fewo.kolibri-visions.de/health/ready
# Both should return HTTP 200
```

**Solution:**
- Fixed in commit with message "fix(smoke): P2.15 preflight uses root /health + robust api base url"
- Script now uses ROOT_BASE for health checks (`/health`, `/health/ready`)
- Script now uses API_V1_BASE for API endpoints (`/api/v1/...`)
- Supports both API_BASE_URL formats:
  - `https://api.example.com` (host-only, recommended)
  - `https://api.example.com/api/v1` (full API path, also supported)

**Verification:**
```bash
# Test with host-only format
API_BASE_URL=https://api.fewo.kolibri-visions.de \
JWT_TOKEN="..." \
AGENCY_ID="..." \
./backend/scripts/pms_objekt_saisonvorlage_import_gap_smoke.sh

# Test with full API path format
API_BASE_URL=https://api.fewo.kolibri-visions.de/api/v1 \
JWT_TOKEN="..." \
AGENCY_ID="..." \
./backend/scripts/pms_objekt_saisonvorlage_import_gap_smoke.sh

# Both should pass preflight with:
# ✅ Preflight PASSED: /health OK (HTTP 200)
# ✅ Preflight PASSED: /health/ready OK (HTTP 200)
```



### HTTP 405 on GET /season-templates/{id}/periods

**Symptom:** Script Test 3 fails with "GET /periods returned HTTP 405 Method Not Allowed". UI import modal fails to load template periods.

**Root Cause:** GET endpoint for listing template periods was not implemented in API. Only POST/PATCH/DELETE existed for period management.

**How to Debug:**
```bash
# Test the periods endpoint directly
export TEMPLATE_ID="..."
curl -X GET "https://api.fewo.kolibri-visions.de/api/v1/pricing/season-templates/$TEMPLATE_ID/periods" \
  -H "Authorization: Bearer $JWT_TOKEN" \
  -H "x-agency-id: $AGENCY_ID"

# Expected before fix: HTTP 405 Method Not Allowed
# Expected after fix: HTTP 200 with JSON array of periods
```

**Solution:**
- Fixed in commit with message "fix(p2.15): add GET season-template periods + unblock import smoke/ui"
- Added GET /api/v1/pricing/season-templates/{template_id}/periods endpoint
- Returns list of SeasonTemplatePeriodResponse sorted by sort_order and date_from
- Uses same auth/agency scoping as other pricing endpoints
- Returns empty array if template has no periods
- Returns 404 if template not found or doesn't belong to agency

**API Endpoint Details:**
```
GET /api/v1/pricing/season-templates/{template_id}/periods
Authorization: Bearer <JWT>
x-agency-id: <agency_uuid>

Response 200:
[
  {
    "id": "uuid",
    "template_id": "uuid",
    "label": "Hauptsaison",
    "date_from": "2025-06-01",
    "date_to": "2025-08-31",
    "sort_order": 1,
    "active": true,
    "created_at": "...",
    "updated_at": "..."
  }
]

Response 404: Template not found or doesn't belong to agency
Response 403: User lacks manager/admin role
```

**Smoke Script Fallback:**
The P2.15 smoke script includes defense-in-depth fallback:
1. Primary: GET /season-templates/{id}/periods (direct endpoint)
2. Fallback (if 405): GET /season-templates/{id} (template detail with nested periods)

**Verification:**
```bash
# Test periods endpoint
export TEMPLATE_ID="..."
curl -sS "https://api.fewo.kolibri-visions.de/api/v1/pricing/season-templates/$TEMPLATE_ID/periods" \
  -H "Authorization: Bearer $JWT_TOKEN" \
  -H "x-agency-id: $AGENCY_ID" | jq '.'

# Expected: Array of periods with labels, date ranges, sort order

# Verify UI import preview works
# 1. Navigate to Property → Preiseinstellungen
# 2. Click "Import aus Vorlage"
# 3. Modal should show templates with period counts
# 4. Select template → periods should be visible in preview
```

### Season Import Creates Duplicates

**Symptom:** Importing same template multiple times creates duplicate seasons with identical date_from/date_to.

**Root Cause:** Frontend import logic not checking for existing seasons before creating new ones.

**How to Debug:**
```bash
# Check for duplicate date ranges in property seasons
export RATE_PLAN_ID="..."
curl -X GET "$API_BASE_URL/api/v1/pricing/rate-plans/$RATE_PLAN_ID/seasons" \
  -H "Authorization: Bearer $JWT_TOKEN" \
  -H "x-agency-id: $AGENCY_ID" | jq '.[] | {date_from, date_to}' | sort | uniq -d
```

**Solution:**
- Frontend skips import if season with same date_from AND date_to already exists
- Check frontend/app/properties/[id]/rate-plans/page.tsx line ~400-410: `existingSeasons.find()` logic
- Archive duplicate seasons manually if needed

### Gap Detection Shows False Positives

**Symptom:** Warning banner shows gaps that are actually covered by overlapping seasons.

**Root Cause:** Gap detection algorithm doesn't account for overlapping date ranges.

**How to Debug:**
```bash
# Verify gap algorithm in frontend
# Should merge overlapping/adjacent seasons before detecting gaps
# Check frontend/app/properties/[id]/rate-plans/page.tsx line ~250-300
```

**Solution:**
- Ensure gap detection sorts seasons and merges overlapping ranges
- Algorithm should: sort by date_from, merge if prev.date_to >= current.date_from
- Then detect gaps between merged ranges

### Season Template Import Returns 400

**Symptom:** POST /api/v1/pricing/rate-plans/{id}/seasons returns 400 "Invalid date range" or "Overlaps with existing season".

**Root Cause:** Backend validation rejects overlapping seasons or invalid date ranges.

**How to Debug:**
```bash
# Check existing seasons for overlaps
curl -X GET "$API_BASE_URL/api/v1/pricing/rate-plans/$RATE_PLAN_ID/seasons" \
  -H "Authorization: Bearer $JWT_TOKEN" \
  -H "x-agency-id: $AGENCY_ID" | jq '.[] | {id, label, date_from, date_to, active}' | grep -v '"active": false'

# Verify new season dates don't overlap with active seasons
```

**Solution:**
- Archive overlapping seasons before import (DELETE endpoint archives, doesn't delete)
- Or adjust template period dates to avoid overlap
- Check backend validation in app/services/pricing_service.py for overlap logic

### Category Chips Show Wrong Colors

**Symptom:** Season labeled "Hauptsaison" shows wrong category color (not red).

**Root Cause:** Label doesn't contain lowercase "haupt" keyword for heuristic match.

**How to Debug:**
```bash
# Check season label case-insensitive match
echo "Hauptsaison" | grep -i "haupt" && echo "✅ Match" || echo "❌ No match"
```

**Solution:**
- Heuristic uses lowercase match: `label.toLowerCase().includes("haupt")`
- Verify label contains keyword: "haupt", "mittel", or "neben"
- Update season label to match category keyword if needed

### Quick Import Buttons Create Wrong Years

**Symptom:** "Nächstes Jahr" button imports seasons for wrong year.

**Root Cause:** Frontend date calculation doesn't account for template period year placeholders or relative date offsets.

**How to Debug:**
```bash
# Check template periods for year placeholders
curl -X GET "$API_BASE_URL/api/v1/pricing/season-templates/$TEMPLATE_ID/periods" \
  -H "Authorization: Bearer $JWT_TOKEN" \
  -H "x-agency-id: $AGENCY_ID" | jq '.[] | {date_from, date_to}'

# Verify frontend adds correct year offset
# Check frontend/app/properties/[id]/rate-plans/page.tsx import logic
```

**Solution:**
- Template periods are date-range strings (e.g., "2025-06-01" to "2025-08-31")
- Quick import adjusts year: replaces year in date_from/date_to with target year
- For "Nächstes Jahr", target year = current year + 1
- Verify import logic correctly replaces year in date strings

### P2.15 UI-Layout: Jahres-Outline Ansicht (Objekt → Preiseinstellungen)

**UI Location:** `/properties/[id]/rate-plans` (Preiseinstellungen tab)

**Visual Structure (Hierarchical Outline):**
```
Objekt-Preiseinstellungen
├── PROMINENT Gap Warning (if applicable)
│   └── Red border, large 3xl warning icon, cannot be missed
│       - Lists all gap date ranges
│       - CTAs: "Aus Vorlage importieren", "Saisonzeit anlegen"
└── Year Sections (2026, 2027, ...)
    ├── Year Header with gap indicator badge (if year affected)
    └── Category Groups (outline style with left border)
        ├── Hauptsaison (red badge)
        ├── Mittelsaison (orange badge)
        ├── Nebensaison (blue badge)
        └── Sonstiges (gray badge, only if label doesn't match)
            └── Season Rows (bullets)
                - "von DD.MM bis DD.MM — €XX.XX/Nacht"
                - Actions: Bearbeiten, Archivieren
```

**Gap Detection Behavior:**
- Checks next 730 days from today
- **Archived seasons excluded** from coverage calculation
- Global warning banner: Shows all gaps (scrollable if many)
- Per-year indicator: Small badge on year header if that year has gaps

**Import Workflow:**
1. Click "Aus Saisonvorlage importieren" button (header OR gap warning CTA)
2. Modal opens with list of agency season templates
3. Select one or more templates (checkbox selection)
4. Periods preview shows: "X Saisonzeit(en)" or "Vorlage enthält keine Zeiträume"
5. Quick import buttons: "Dieses Jahr", "Nächstes Jahr", "2 Jahre voraus" (placeholder, not yet functional)
6. Click "Importieren" to create seasons
7. Idempotent: Skips duplicates (same date_from + date_to)
8. Success toast: "X Saison(en) importiert, Y übersprungen (Duplikate)"

**Key UI Messages:**
- Info callout: "ℹ️ Vorlagen sind Startpunkte: Saisonvorlagen helfen beim Erstellen von Preiszeiten. Nach dem Import können Sie die Saisonzeiten für jedes Objekt individuell anpassen."
- Gap warning: "Achtung: Saisonzeiten haben Lücken in den nächsten 2 Jahren!"
- Import modal: "Duplikate (gleiche Zeiträume) werden automatisch übersprungen."

**Troubleshooting:**

**Symptom:** Gap warning shows false gaps (seasons are present)
**Root Cause:** Archived seasons are counted as coverage
**Solution:** Gap detection filters `!s.archived_at` - archived seasons should NOT count. Check frontend filter at line 108.

**Symptom:** Year gap indicator shows "Lücken vorhanden" but year has full coverage
**Root Cause:** Gap overlaps year boundaries (gap starts before year but ends in year)
**Solution:** `getYearGaps()` function checks if gap overlaps year range - this is intentional for visibility.

**Symptom:** Import creates duplicates despite "idempotent" claim
**Root Cause:** Duplicate detection checks `date_from` and `date_to` only, not label
**Solution:** This is intentional. Same dates with different labels are allowed (e.g., manual adjustments after import).

**Symptom:** Category colors don't match expectations
**Root Cause:** Label doesn't contain keyword "haupt"/"mittel"/"neben"
**Solution:** Categorization is case-insensitive label matching. Update season label to include keyword for correct categorization.

### Saisonzeit anlegen/bearbeiten/archivieren (Objekt)

**UI Location:** `/properties/[id]/rate-plans` (Preiseinstellungen tab)

**CRUD Operations:**

1. **Saisonzeit anlegen** (Create):
   - Header button: "Saisonzeit anlegen"
   - Modal with form fields: Bezeichnung (optional), Von (required), Bis (required), Preis pro Nacht in EUR (required), Mindestaufenthalt (optional)
   - POST `/api/v1/pricing/rate-plans/{rate_plan_id}/seasons`
   - Payload: `{label, date_from, date_to, nightly_cents, min_stay_nights, active}`
   - Success: Toast "Saisonzeit erstellt" + refresh list

2. **Bearbeiten** (Edit):
   - Click "Bearbeiten" button on season row
   - Same modal as create, prefilled with existing values
   - PATCH `/api/v1/pricing/rate-plans/{rate_plan_id}/seasons/{season_id}`
   - Success: Toast "Saisonzeit aktualisiert" + refresh list

3. **Archivieren** (Archive):
   - Click "Archivieren" button on season row
   - Confirm dialog: "Saisonzeit archivieren?"
   - DELETE `/api/v1/pricing/rate-plans/{rate_plan_id}/seasons/{season_id}`
   - Success: Toast "Saisonzeit archiviert" + refresh list
   - Archived seasons: shown with "Archiviert" badge when "Archivierte anzeigen" toggled

**Import CTA:**
- Single import button: "Aus Saisonvorlage importieren" (header only, no duplicate in gap warning)
- Gap warning panel shows hint text: "Nutzen Sie oben 'Aus Saisonvorlage importieren' oder 'Saisonzeit anlegen', um die Lücken zu schließen."

**Troubleshooting:**

**Symptom:** Season create/edit/archive fails with API error
**Root Cause:** JWT token expired, x-agency-id header missing, or user lacks manager/admin role
**Solution:** Check JWT validity, verify x-agency-id header present in request, confirm user role in JWT claims. Frontend shows API error toast with statusText.

**Symptom:** Edit/Archive buttons missing on archived seasons

### P2.16 Smoke Test: No rate plan found

**Symptom:** Script exits with "No rate plan found for property" during Test 1 (Auto-fetch rate plan).

**Root Cause:** Either response parsing issue or no rate plans exist for the selected property.

**Diagnosis:**
1. Check diagnostics output in script:
   - URL: Verify endpoint path is correct (`/api/v1/pricing/rate-plans?property_id=...`)
   - HTTP code: Should be 200 (if 404/401/403, different issue)
   - Response preview: First 200 chars of API response
   - Parsed count: Number of rate plans detected (should be >= 1)

2. Response shape validation:
   - Script supports both shapes: plain array `[{...}]` and envelope `{items:[...], total:N}`
   - If parsed count = 0 but response is non-empty, shape may be unexpected

**Solution:**
```bash
# Manual verification
curl -H "Authorization: Bearer $JWT_TOKEN" \
     -H "x-agency-id: $AGENCY_ID" \
     "$API_BASE_URL/api/v1/pricing/rate-plans?property_id=$PROPERTY_ID"

# Expected responses:
# Shape 1 (array): [{"id":"...", "property_id":"...", ...}]
# Shape 2 (envelope): {"items":[{"id":"...", ...}], "total":1}

# If response is 200 but empty/different shape:
# - Backend may have filtering bug (property_id mismatch)

**Test 2 fails with 422 overlap:** Script auto-computes non-overlapping dates from existing seasons. If this still fails, check that rate plan doesn't have dense season coverage spanning years.

# - Check backend logs for SQL errors
# - Verify property has at least one active rate plan via DB or UI
```

**Verification:** Re-run script with diagnostics enabled. Check URL, HTTP code, and response preview. If response is valid but parsing fails, update script or report API shape incompatibility.
**Root Cause:** Intentional design - archived seasons are read-only
**Solution:** To restore/edit archived season, implement restore endpoint (currently not available). Workaround: Create new season with same dates.


### Tarifplan anlegen (Backoffice)

**UI Location:** `/properties/[id]/rate-plans` (when no rate plan exists)

**Workflow:**
1. Navigate to Objekt → Preiseinstellungen
2. If "Kein Tarifplan vorhanden" shown, click "Tarifplan anlegen"
3. Modal opens with fields:
   - Name (prefilled: "Standardpreis <Jahr>")
   - Basispreis pro Nacht (EUR, z.B. 150.00)
   - Aktiv (default checked)
4. Click "Tarifplan erstellen"
5. Success: Toast "Tarifplan erstellt", page shows normal seasons view
6. Now can create seasons or import from templates

**API:** POST /api/v1/pricing/rate-plans
- Payload: {property_id, name, base_nightly_cents, currency, active}
- EUR input converted to cents (150.00 EUR → 15000 cents)

**Troubleshooting:**
- **409 "Ein aktiver Tarifplan existiert bereits":** Property already has active rate plan. Deactivate existing first or contact admin.
- **422 validation error:** Check base price is positive number (> 0).
- **401/403:** JWT expired or user lacks permissions (need manager/admin role).

### Vorlage aktualisieren (Lücken nachziehen)

**UI Location:** `/properties/[id]/rate-plans` (Preiseinstellungen tab → Import modal)

**Purpose:** After editing season templates (add/edit/delete periods), use "Aktualisieren (Lücken schließen)" to apply missing ranges to properties without overwriting existing seasons.

**How It Works:**
1. Click "Aus Saisonvorlage importieren" in header
2. Select template from dropdown
3. Click "Aktualisieren (Lücken schließen)" button (green, below template selector)
4. Backend projects template periods into current year + next 2 years
5. Finds uncovered sub-ranges (gaps between existing seasons and template coverage)
6. Creates seasons only for missing sub-ranges (no overwrites)
7. Partial fills labeled with "(Ergänzung)" suffix
8. Success toast: "X Saison(en) ergänzt"

**Safety:**
- Non-destructive: Only adds missing ranges, never overwrites existing seasons
- Idempotent: Safe to run multiple times (duplicate detection by date_from + date_to)
- Partial fills: If template period overlaps existing season, only uncovered sub-range created

**Use Case:**
- Template updated: added "Ostersaison" period to "Haupt+Nebensaison" template
- Property X already has seasons from old template (missing Ostersaison)
- Click "Aktualisieren (Lücken schließen)" → Ostersaison ranges added to Property X
- Existing Hauptsaison/Nebensaison seasons unchanged

**Troubleshooting:**

**Symptom:** "Aktualisieren (Lücken schließen)" creates no seasons (toast shows "0 Saison(en) ergänzt")
**Root Cause:** No uncovered ranges found (existing seasons already cover all template periods)
**Solution:** Check if property seasons already match template periods. If gaps still visible, template may not cover those date ranges - add period to template first.

**Symptom:** Partial fills created with "(Ergänzung)" label instead of full template period
**Root Cause:** Template period overlaps with existing season - only uncovered sub-range created
**Solution:** Expected behavior (non-destructive). To replace with full period: archive existing overlapping season first, then refresh from template.


---
---

---

### Saisonvorlagen Sync: Overlap/ExclusionViolation vermeiden

**Problem:**
Sync-from-template im apply-Modus kann mit HTTP 500 fehlschlagen:
`asyncpg.exceptions.ExclusionViolationError: conflicting key value violates exclusion constraint "rate_plan_seasons_no_overlap_excl"`

**Root Cause:**
Sync versucht neue Saisonzeiten anzulegen, bevor Legacy-Saisonzeiten (ohne Vorlagen-Verknüpfung)
verkürzt/verknüpft wurden. Resultat: Neue INSERT kollidiert mit noch-langer Legacy-Saison.

**Lösung (ab P2.16.1 follow-up):**
Apply-Modus führt Datenbankoperationen in sicherer Reihenfolge aus:

1. **Phase 1: Relink-Updates** (Legacy-Saisonzeiten verkürzen + template_period_id setzen)
   - Verkürzt date_to, sodass keine Überschneidung mehr besteht
   - Setzt template_period_id für Verknüpfung

2. **Phase 2: Linked-Updates** (bestehende verknüpfte Saisonzeiten aktualisieren)
   - Aktualisiert date_from/date_to für bereits verknüpfte Saisonzeiten

3. **Phase 3: Creates** (neue Saisonzeiten anlegen)
   - Legt neue Saisonzeiten an (aus Vorlagen-Perioden ohne Match)

**Savepoint-Wrapping:**
Jede einzelne UPDATE/INSERT-Operation läuft in eigener Transaktion (savepoint).
Bei ExclusionViolation wird Operation in conflicts[] Liste aufgenommen (kein 500-Fehler).

**Response-Struktur:**
```json
{
  "status": "success",
  "counts": {
    "create": 2,
    "update": 1,
    "relink": 1,
    "conflict": 0
  },
  "created": [...],
  "updated": [...],
  "relinked": [...],
  "conflicts": [
    {
      "label": "Hauptsaison 2027",
      "date_from": "2027-01-01",
      "date_to": "2027-09-30",
      "reason": "Überschneidung mit bestehender Saisonzeit",
      "hint": "Überschneidung verhindert automatische Anlage – bitte Saisonzeiten manuell anpassen oder archivieren."
    }
  ]
}
```

**UI-Verhalten:**
- Conflicts werden in orangefarbener Box angezeigt (wie bisher)
- Kein generischer "Database error occurred" mehr
- Modal bleibt offen, damit Staff Konflikte sehen kann

**Troubleshooting:**

**Symptom:** "conflicts" Liste enthält Einträge nach apply

**Root Cause:** Trotz Phase-1-Shrink konnte Saison nicht verkürzt werden (anderes Overlap).

**Lösung:**
1. Betroffene Saison manuell in UI bearbeiten (Zeitraum anpassen)
2. Oder: Saison archivieren
3. Sync erneut ausführen

**Verification:**
- Smoke script: `backend/scripts/pms_p2161_sync_apply_overlap_smoke.sh`
- Testet: Legacy-Saison 2026-12-01 bis 2027-12-31, Vorlage mit 2027-Perioden
- Erwartung: Apply gibt 200 zurück (kein 500), Legacy wird verkürzt oder als conflict gemeldet

---

### Saisonvorlagen: Zeitraum-Überlappungen vermeiden

**Problem:**
Saisonvorlagen-Zeiträume können sich überschneiden (z.B. "Hauptsaison" 01.01-30.06 und "Nebensaison" 01.05-30.09), was beim Import zu vielen Konflikten führt.

**Lösung (ab P2.16.2):**
Überlappende Zeiträume werden auf Vorlagen-Ebene verhindert.

**Validierung:**

1. **CREATE Zeitraum:**
   - Vor INSERT: Prüfung gegen alle bestehenden Zeiträume der Vorlage
   - Bei Überlappung: HTTP 422 mit Fehlertext:
     ```
     Überlappung mit bestehendem Zeitraum: {label} [{date_from} bis {date_to}].
     Saisonvorlagen-Zeiträume dürfen sich nicht überschneiden.
     ```

2. **UPDATE Zeitraum:**
   - Vor UPDATE: Prüfung gegen alle ANDEREN Zeiträume der Vorlage (id != self)
   - Bei Überlappung: HTTP 422 (gleicher Fehlertext)

**Sync-Import Preflight:**

Vor dem Import prüft das System, ob die Vorlage interne Überlappungen hat:

- **Preview-Modus:** Zeigt `template_conflicts[]` in der Antwort
- **Apply-Modus:** Bei `template_conflicts` → HTTP 422, Import blockiert

**UI-Verhalten:**

- Import-Dialog zeigt roten Warnbereich: "Vorlage enthält überlappende Zeiträume"
- Liste der überlappenden Perioden mit Labels und Daten
- Button "Vorlage bearbeiten" → Navigation zur Vorlagen-Bearbeitung
- Import-Button deaktiviert, solange template_conflicts vorhanden

**Troubleshooting:**

**Symptom:** "Überlappung mit bestehendem Zeitraum" beim Anlegen/Bearbeiten von Vorlagen-Zeiträumen

**Root Cause:** Zeiträume in der Vorlage überschneiden sich.

**Lösung:**
1. Vorhandene Zeiträume der Vorlage prüfen: GET /api/v1/pricing/season-templates/{id}/periods
2. Überlappende Zeiträume identifizieren (date_from < other.date_to AND date_to > other.date_from)
3. Zeitraum-Grenzen anpassen:
   - Option A: Periode verkürzen (z.B. 01.01-30.04 statt 01.01-30.06)
   - Option B: Periode verschieben (z.B. 01.07-30.09 statt 01.05-30.09)
   - Option C: Periode archivieren (DELETE /api/v1/pricing/season-templates/{id}/periods/{period_id})

**Symptom:** Import-Dialog zeigt "Vorlage enthält überlappende Zeiträume", Import-Button deaktiviert

**Root Cause:** Vorlage hat interne Überlappungen (bereits existierende Daten vor P2.16.2).

**Lösung:**
1. Auf "Vorlage bearbeiten" klicken (Navigation zu /season-templates/{id})
2. Überlappende Zeiträume identifizieren und korrigieren (siehe oben)
3. Zurück zur Objekt-Seite → Import erneut versuchen

**Verification:**
- Smoke script: `backend/scripts/pms_p2162_template_overlap_smoke.sh`
- Testet: CREATE overlap → 422, UPDATE overlap → 422
- EXIT rc=0 = alle Tests bestanden

---


### Saisonvorlagen: Synchronisieren ersetzt bestehende Zeiträume (P2.16.2 Fix)

**Problem:**
Vor P2.16.2 Fix wurden bestehende Saisonzeiten als Konflikte behandelt und übersprungen, statt aktualisiert zu werden. Perioden erschienen sowohl unter "Reparierte Saisonzeiten" als auch "Konflikte".

**Lösung (ab P2.16.2 Fix):**
"Vorlage synchronisieren" aktualisiert/verknüpft bestehende Saisonzeiten mit Vorlagen-Zeiträumen.

**Zwei Modi:**

1. **"Nur fehlende importieren"** (missing_only):
   - Legt NUR fehlende Saisonzeiten an
   - Bestehende Saisonzeiten werden NICHT aktualisiert
   - Überschneidungen mit bestehenden → Konflikt-Liste

2. **"Vorlage synchronisieren"** (sync):
   - Aktualisiert bestehende Saisonzeiten (Zeiträume anpassen, Verknüpfung setzen)
   - Legt fehlende Saisonzeiten an
   - Archiviert/ignoriert veraltete Zeiträume
   - Nur unauflösbare Überschneidungen → Konflikt-Liste

**Matching-Logik (Priorität):**

Für jeden Vorlagen-Zeitraum wird eine passende bestehende Saisonzeit gesucht:

1. **Priorität 1:** `source_template_period_id == template_period.id` (direkte Verknüpfung)
2. **Priorität 2:** `source_template_id == template.id` UND Label-Match (normalisiert) UND Jahr-konsistent
3. **Priorität 3:** `date_from == period.date_from` UND Label-Match UND Jahr-konsistent

**Label-Normalisierung:**
Vergleich erfolgt case-insensitive, ohne führende/folgende Leerzeichen, kollabierte Leerzeichen.
Beispiele: "Hauptsaison" == "hauptsaison" == " Haupt Saison "

**Update-Reihenfolge (Overlap-Safety):**

Updates werden sortiert ausgeführt um DB-Constraint-Verletzungen zu vermeiden:
1. Shrinks (gewünschter Zeitraum ist Teilmenge von bestehendem)
2. Gleiche Größe
3. Expansions (gewünschter Zeitraum ist Obermenge)

**Overlap-Check:**

Vor jedem UPDATE/CREATE:
- Prüfung gegen ANDERE aktive Saisonzeiten (id != self, archived_at IS NULL)
- Bei Überschneidung: Eintrag in Konflikt-Liste, Operation übersprungen (kein Fehler)
- Andere Operationen werden fortgesetzt

**UI-Verhalten:**

Nach Sync:
- **Erfolg (keine Konflikte):** Grüner Toast "Synchronisierung erfolgreich"
- **Teilerfolg (Konflikte vorhanden):** Gelber Toast "Teilweise erfolgreich: X Konflikt(e)"
- **Fehler (500, 422):** Roter Toast mit Fehlerdetails
- Modal bleibt bei Konflikten offen, damit Staff Details sehen kann

**Anzeige:**
- "Aktualisierte Saisonzeiten" (updated)
- "Reparierte Saisonzeiten" (relinked, mit "war bis ...")
- "Neue Saisonzeiten" (created)
- "Konflikte" (conflicts)

Keine Duplikate zwischen Listen.

**Troubleshooting:**

**Symptom:** Saisonzeiten erscheinen sowohl in "Reparierte" als auch "Konflikte"

**Root Cause:** Vor P2.16.2 Fix: Matching-Logik fehlte.

**Lösung:** Update auf P2.16.2 Fix deployen.

**Symptom:** "Vorlage synchronisieren" aktualisiert bestehende Saisonzeiten nicht

**Root Cause:** mode=missing_only statt mode=sync.

**Lösung:** Korrekte Schaltfläche verwenden ("Vorlage synchronisieren" nicht "Nur fehlende importieren").

**Verification:**
- Smoke script: `backend/scripts/pms_p2162_sync_updates_existing_smoke.sh`
- Testet: Initial sync → Template mutation → Re-sync updates existing season
- EXIT rc=0 = Sync aktualisiert bestehende Zeiträume korrekt

---


### Saisonvorlagen: Sync ersetzt auch geänderte Startdaten (P2.16.3 Label+Overlap)

**Problem:**
Nach P2.16.2 Fix wurden Saisonzeiten nur aktualisiert, wenn date_from exakt übereinstimmte.
Beispiel: Bestehend "Nebensaison" 01.11-31.12, Vorlage will 01.12-31.12 → wurde als NEUES
Anlegen behandelt → Overlap-Konflikt → übersprungen → alte Zeiträume blieben.

**Lösung (ab P2.16.3):**
Matching nutzt Label + Overlap statt nur exakte Datumsübereinstimmung.

**Erweiterte Matching-Prioritäten:**

Für jeden Vorlagen-Zeitraum wird die beste passende bestehende Saisonzeit gesucht:

1. **Priorität 1:** Direkte Verknüpfung (`source_template_period_id == template_period.id`)
2. **Priorität 2:** Template + Label (`source_template_id` + Label-Match + Jahr)
3. **Priorität 3:** Legacy Relink (date_from + Label + Jahr)
4. **Priorität 4 (NEU):** Label + Overlap
   - Label-Match (normalisiert, case-insensitive)
   - Gleiches Jahr
   - Berechnung Overlap-Tage mit Vorlagen-Zeitraum
   - Wahl: Kandidat mit meisten Overlap-Tagen
   - Falls kein Overlap aber nur ein Kandidat im Jahr mit passendem Label → auch Match (Shift-Fall)

**Beispiel-Szenario (Prod-Bug gefixt):**

Vorlage:
- "Nebensaison": 01.12-31.12

Bestehendes Objekt:
- "Nebensaison": 01.11-31.12 (ohne Verknüpfung)

**Vor P2.16.3:**
- Matching-Versuch:
  - Priorität 1: Keine direkte Verknüpfung ❌
  - Priorität 2: Keine Template-Verknüpfung ❌
  - Priorität 3: date_from unterschiedlich (01.11 != 01.12) ❌
- Resultat: Behandlung als NEUES Anlegen
- Overlap mit bestehendem → Konflikt
- Skip → Alte Zeiträume bleiben

**Ab P2.16.3:**
- Matching-Versuch:
  - Priorität 1-3: Wie oben ❌
  - **Priorität 4:** Label "Nebensaison" == "Nebensaison" ✅, Jahr 2027 == 2027 ✅,
    Overlap 01.12-31.12 ✅ (31 Tage)
  - Kandidaten: 1 (nur diese Nebensaison)
  - **Match gefunden** ✅
- Resultat: UPDATE
- date_from: 01.11 → 01.12 (verkürzt)
- date_to: 31.12 (unverändert)
- Verknüpfung gesetzt: source_template_period_id

**UI-Anzeige:**

In "Reparierte Saisonzeiten":
```
• Nebensaison: 01.12 bis 31.12 (war 01.11 bis 31.12)
```

**Request-Kompatibilität:**

Endpoint akzeptiert nun sowohl snake_case als auch camelCase:
- `template_id` oder `templateId`
- `template_ids` oder `templateIds`

Bei Validierungsfehlern wird HTTP 400 (nicht 422) mit verständlicher deutscher Nachricht zurückgegeben.

**Troubleshooting:**

**Symptom:** "Request validation failed" beim Sync-Aufruf

**Root Cause:** Vor P2.16.3: Payload-Feld-Namen inkonsistent oder fehlend.

**Lösung:** Backend akzeptiert nun beide Konventionen. Frontend aktualisieren falls noch camelCase verwendet.

**Symptom:** Saisonzeit mit ähnlichem Label + Overlap wird trotzdem als Konflikt behandelt

**Root Cause:** Mehrere Kandidaten mit gleichem Overlap-Wert (mehrdeutig).

**Lösung:**
1. Nur einen Kandidaten behalten (andere archivieren)
2. Oder: Unterschiedliche Labels vergeben zur Eindeutigkeit

**Verification:**
- Smoke script: `backend/scripts/pms_p2163_sync_updates_date_from_smoke.sh`
- Testet: Legacy "Nebensaison" 01.11-31.12 wird gematched mit Template 01.12-31.12
- EXIT rc=0 = Matching funktioniert korrekt

---

---

## PROD Restart Loop: NameError "model_validator" Not Defined

**Overview:** Backend container crashes on startup with Python NameError related to missing Pydantic imports.

**Symptom:** Container restart loop in PROD. Logs show:
```
NameError: name 'model_validator' is not defined
  File "/app/app/schemas/pricing.py", line 466, in SeasonSyncRequest
    @model_validator(mode='after')
```

**Root Cause:** Schema module uses `@model_validator` decorator but Pydantic import statement is missing `model_validator`.

**How It Happened:** 
- P2.16.3 commit added `@model_validator(mode='after')` decorator to `SeasonSyncRequest` class
- Forgot to update pydantic import: `from pydantic import BaseModel, Field` → missing `model_validator`
- Python can't find the decorator when loading the module → NameError at import time
- FastAPI can't start → container crashes → restart loop

**Resolution (Hotfix Applied 2026-01-20):**

Import fix in `backend/app/schemas/pricing.py`:
```python
# BEFORE (broken):
from pydantic import BaseModel, Field

# AFTER (fixed):
from pydantic import BaseModel, Field, model_validator
```

**Verification Commands:**

```bash
# [LOCAL-DEV] Check import is present
rg -n "from pydantic import.*model_validator" backend/app/schemas/pricing.py

# Expected output:
# 24:from pydantic import BaseModel, Field, model_validator

# [LOCAL-DEV] Run regression test
cd backend
python3 -m pytest tests/test_import_schemas.py -v -c /dev/null

# Expected: 2 passed
```

**Prevention:**
- Regression test added: `backend/tests/test_import_schemas.py`
- Test imports `app.schemas.pricing` and instantiates `SeasonSyncRequest`
- Fails fast if decorator imports are missing
- Run in CI before deploy

**Similar Issues:**
If you see `NameError: name 'X' is not defined` where X is a Pydantic decorator or validator:
1. Check import statement in schema file
2. Add missing import: `field_validator`, `model_validator`, `validator`, etc.
3. Run regression test to verify
4. Add test case if missing

**Hotfix Commit:** (hash provided after push)

---


---

## Pricing → Saisonvorlagen Sync (422 Debug)

**Symptom:**
- POST to `/api/v1/pricing/rate-plans/{id}/sync-from-template` returns 422 "Request validation failed"
- Error message: `Field required [type=missing, input=...]` or similar validation error
- UI shows sync button but fails when clicked

**Root Cause:**
Backend validation expects specific required fields in request body. Missing or incorrectly formatted fields trigger 422 validation errors.

**Required Fields (SeasonSyncRequest schema):**
```python
# backend/app/schemas/pricing.py
class SeasonSyncRequest(BaseModel):
    years: list[int]          # REQUIRED: Array of year integers [2025, 2026]
    mode: str                 # REQUIRED: "replace" or other mode
    strategy: str = "default" # OPTIONAL: Default is "default"
```

**How to Capture DevTools Payload:**

1. **Open Browser DevTools:**
   - Chrome/Safari: F12 or Cmd+Option+I
   - Navigate to "Network" tab
   - Filter by "Fetch/XHR"

2. **Trigger Sync Operation:**
   - Click "Synchronisieren" button in UI
   - Watch Network tab for POST request to `sync-from-template`

3. **Inspect Request:**
   - Click on the request in Network tab
   - Go to "Payload" or "Request" tab
   - Copy the JSON payload sent to backend

4. **Common Issues:**
   - `years` sent as strings instead of integers: `["2025"]` → should be `[2025]`
   - `years` field missing entirely
   - Incorrect field names (camelCase vs snake_case)

**Curl Reproduction Example:**

```bash
# Set variables
HOST="https://your-backend.com"
JWT_TOKEN="your-jwt-token"
RATE_PLAN_ID="123"

# Test sync request (adjust payload as needed)
curl -X POST "$HOST/api/v1/pricing/rate-plans/$RATE_PLAN_ID/sync-from-template" \
  -H "Authorization: Bearer $JWT_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "years": [2025, 2026],
    "mode": "replace",
    "strategy": "default"
  }'

# Expected: 200 OK with SeasonSyncResponse
# If 422: Check error details for missing/invalid fields
```

**Troubleshooting Steps for 422 Errors:**

1. **Verify Request Payload Format:**
   ```bash
   # Check if years is an array of integers
   echo '{"years": [2025, 2026], "mode": "replace"}' | jq '.years | type'
   # Should output: "array"

   echo '{"years": [2025, 2026], "mode": "replace"}' | jq '.years[0] | type'
   # Should output: "number"
   ```

2. **Check Backend Schema Definition:**
   ```bash
   # View current schema in backend
   rg -A 10 "class SeasonSyncRequest" backend/app/schemas/pricing.py

   # Verify required fields (no default value means required)
   # Look for: years: list[int] (no = sign → required)
   ```

3. **Test with Minimal Valid Payload:**
   ```bash
   # Minimal request (only required fields)
   curl -X POST "$HOST/api/v1/pricing/rate-plans/$RATE_PLAN_ID/sync-from-template" \
     -H "Authorization: Bearer $JWT_TOKEN" \
     -H "Content-Type: application/json" \
     -d '{"years": [2025], "mode": "replace"}'
   ```

4. **Check UI Payload Construction:**
   ```bash
   # Frontend code location (if applicable)
   # Look for where sync request is built
   rg -n "sync-from-template" frontend/

   # Verify years array is constructed as integers, not strings
   # Wrong: years: selectedYears.map(y => y.toString())
   # Right: years: selectedYears.map(y => parseInt(y, 10))
   ```

5. **Backend Logs Review:**
   ```bash
   # Check backend logs for validation error details
   docker logs pms-backend --tail 100 | grep -A 5 "422"

   # Or in production logs
   tail -100 /var/log/pms-backend/error.log | grep "validation"
   ```

**Common Fixes:**

1. **UI sends strings instead of integers:**
   ```typescript
   // BEFORE (broken):
   const payload = {
     years: selectedYears.map(String), // ["2025", "2026"]
     mode: "replace"
   };

   // AFTER (fixed):
   const payload = {
     years: selectedYears.map(Number), // [2025, 2026]
     mode: "replace"
   };
   ```

2. **Missing required field:**
   ```json
   // BEFORE (broken - missing years):
   {
     "mode": "replace"
   }

   // AFTER (fixed):
   {
     "years": [2025],
     "mode": "replace"
   }
   ```

3. **Incorrect mode value:**
   ```json
   // Check backend for accepted mode values
   # rg "mode.*replace|mode.*Literal" backend/app/schemas/pricing.py

   // Use accepted values only
   ```

**Verification After Fix:**

```bash
# [LOCAL-DEV] Test sync endpoint with valid payload
cd backend
python3 -c "
from app.schemas.pricing import SeasonSyncRequest
req = SeasonSyncRequest(years=[2025, 2026], mode='replace', strategy='default')
print('✓ Schema validation passed:', req.model_dump())
"

# Expected output: ✓ Schema validation passed: {'years': [2025, 2026], 'mode': 'replace', 'strategy': 'default'}
```

**Related Sections:**
- See "P2.16.3: Sync Label+Overlap Matching + Request Robustness" in project_status.md
- See "Saisonvorlagen: Synchronisieren ersetzt bestehende Zeiträume" section above

**Prevention:**
- Add frontend TypeScript types matching backend schema
- Add E2E test for sync operation
- Monitor 422 errors in production metrics
- Document payload format in API documentation

---

### 400 "Template has no active periods"

**Symptom:** Sync returns 400 with "Template has no active periods" even after creating periods via API.

**Cause:**
1. Template periods were created without active field, OR
2. API endpoint not writing active=true explicitly, OR
3. DB default not applied correctly, OR
4. Filter query using strict WHERE active=true (excludes NULLs)

**Fix:**
1. Ensure API endpoint explicitly sets active=true when creating periods:
```python
# In backend/app/api/routes/pricing.py (period creation)
await db.execute(
    """
    INSERT INTO pricing_season_template_periods (
        template_id, label, date_from, date_to, active
    ) VALUES ($1, $2, $3, $4, $5)
    """,
    template_id, label, date_from, date_to, True  # Explicit active=True
)
```

2. Ensure DB migration sets DEFAULT:
```sql
-- In migration 20260116000000_add_season_templates.sql
active BOOLEAN NOT NULL DEFAULT true
```

3. Verify frontend sends active field:
```json
{
  "label": "Hauptsaison",
  "date_from": "07-01",
  "date_to": "08-31",
  "active": true  // Explicitly set active
}
```

**Verification:**
```bash
# Check template periods are active
curl "$HOST/api/v1/pricing/season-templates/$TEMPLATE_ID" | \
  python3 -c "import sys, json; t = json.load(sys.stdin); print(f\"Active periods: {len([p for p in t.get('periods', []) if p.get('active', False)])}\")"

# Expected: Active periods: 2 (or more)
# If 0: periods are inactive or NULL
```

**Debug:**
```bash
# Check DB directly
psql $DATABASE_URL -c "SELECT id, label, active FROM pricing_season_template_periods WHERE template_id = '<template-id>';"

# If active column is NULL or false, backfill:
psql $DATABASE_URL -c "UPDATE pricing_season_template_periods SET active = true WHERE active IS NULL OR active = false;"
```


## Pricing → Saisonvorlagen Sync (Pattern vs Absolute Multi-Year)

**Overview:** Season template sync supports two modes based on template structure.

**PATTERN MODE (Single-Year Template)**:
- Template periods span exactly ONE source year (e.g., all periods in 2026)
- Behavior: Month/day patterns are rebased to each target year
- Example: Template 2026-12-01..2026-12-31, target years [2026,2027] → creates 2026-12-01..2026-12-31 AND 2027-12-01..2027-12-31
- Use case: Reusable seasonal patterns (e.g., "Hochsaison" always Dec 1-31)

**ABSOLUTE MULTI-YEAR MODE (Multi-Year Template)**:
- Template periods span TWO OR MORE source years (e.g., 2026 + 2027)
- Behavior: NO rebase, periods are synced to their absolute years
- Example: Template has 2026-12-01..2026-12-31 + 2027-01-01..2027-06-30, target years [2026,2027] → creates exactly those periods, NOT extra 2027-12-01..2027-12-31
- Use case: Specific multi-year plans (e.g., transition periods across year boundary)

**Mode Detection**:
- API automatically detects mode by analyzing template_source_years (unique years from all period date_from values)
- Response includes `template_mode: "pattern" | "absolute"` and `template_source_years: [...]`

**Common Issues**:

### Sync Creates Unexpected Periods in Next Year

**Symptom:** Template with 2026-12-01..2026-12-31 + 2027 periods creates extra 2027-12-01..2027-12-31 when syncing years [2026,2027].

**Root Cause:** Template is multi-year (spans 2+ years), but periods are being rebased as if single-year pattern.

**How to Debug:**
```bash
# Check template structure
curl -X GET "$HOST/api/v1/pricing/season-templates/<template_id>" \
  -H "Authorization: Bearer $JWT_TOKEN" | jq '.periods[] | {label, date_from, date_to}'

# Check sync preview mode
curl -X POST "$HOST/api/v1/pricing/rate-plans/<plan_id>/seasons/sync-from-template" \
  -H "Authorization: Bearer $JWT_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"template_id": "<template_id>", "years": [2026, 2027], "mode": "preview", "strategy": "sync"}' \
  | jq '{template_mode, template_source_years, totals}'
```

**Expected Behavior:**
- If template_source_years = [2026,2027] → template_mode = "absolute" → no cross-year rebase
- If template_source_years = [2026] → template_mode = "pattern" → periods rebased to all target years

**Solution:**
- Multi-year templates work correctly in absolute mode (P2.16.12+)
- For pattern behavior, create separate single-year templates for each year
- Verify template_source_years in API response matches intent

### Gap Warning Shows Previous/Next Year Dates

**Symptom:** Gap detection shows "31.12.2025" or "01.01.2028" as gaps within 2026 or 2027.

**Root Cause:** Gap computation not clamping to year boundaries (fixed in P2.16.12).

**Solution:**
- Gaps are now strictly clamped to [01.01.YYYY - 31.12.YYYY] per year
- Gaps outside year boundaries are filtered out
- Update to P2.16.12+ to get fix

---
---

## Pricing → Seasons Bulk Actions (Archive/Delete)

**Overview:** Bulk operations for archiving and deleting rate plan seasons.

**Purpose:** Efficiently manage large numbers of seasons through batch operations with safety guardrails.

**Architecture:**
- **Bulk Archive**: Archives multiple active seasons in one request (sets archived_at)
- **Bulk Delete**: Permanently deletes multiple archived seasons in one request
- **Safety**: Delete ONLY works for archived seasons; active seasons must be archived first
- **Limits**: Max 200 season_ids per request

**API Endpoints:**

Staff (manager/admin):
- `POST /api/v1/pricing/seasons/bulk-archive` - Archive multiple active seasons
- `POST /api/v1/pricing/seasons/bulk-delete` - Delete multiple archived seasons

**Request Format:**
```json
{
  "season_ids": ["uuid1", "uuid2", "uuid3"]
}
```

**Response Format:**
```json
{
  "requested_count": 10,
  "processed_count": 8,
  "archived_count": 8,  // For archive operation
  "deleted_count": 0,   // For delete operation
  "skipped_ids": ["uuid9", "uuid10"],
  "errors": []
}
```

**Safety Rules:**
1. **Archive**: Only archives active seasons, skips already-archived
2. **Delete**: ONLY deletes archived seasons, rejects request if ANY active season included
3. **Agency Scoping**: All operations respect agency boundaries (via rate_plan → property → agency_id)
4. **Limits**: Max 200 season_ids per request (422 if exceeded)

**Verification Commands:**

```bash
# [HOST-SERVER-TERMINAL] Test bulk archive
export HOST="https://api.fewo.kolibri-visions.de"
export JWT_TOKEN="<<<manager/admin JWT>>>"

# Create test seasons (or use existing)
SEASON_ID_1="..."
SEASON_ID_2="..."

# Archive multiple seasons
curl -X POST "${HOST}/api/v1/pricing/seasons/bulk-archive" \\
  -H "Authorization: Bearer ${JWT_TOKEN}" \\
  -H "Content-Type: application/json" \\
  -d "{\"season_ids\": [\"${SEASON_ID_1}\", \"${SEASON_ID_2}\"]}" | jq

# Verify response: archived_count should equal number of active seasons

# Delete archived seasons
curl -X POST "${HOST}/api/v1/pricing/seasons/bulk-delete" \\
  -H "Authorization: Bearer ${JWT_TOKEN}" \\
  -H "Content-Type: application/json" \\
  -d "{\"season_ids\": [\"${SEASON_ID_1}\", \"${SEASON_ID_2}\"]}" | jq

# Verify response: deleted_count should equal number of archived seasons
```

**Common Issues:**

### Bulk Delete Returns 409/400 (Active Seasons Included)

**Symptom:** POST /api/v1/pricing/seasons/bulk-delete returns 409 Conflict or 400 Bad Request with error about active seasons.

**Root Cause:** Request includes season IDs that are NOT archived (archived_at IS NULL).

**How to Debug:**
```bash
# Check season status
curl -X GET "${HOST}/api/v1/pricing/seasons?rate_plan_id=<plan_id>&limit=100" \\
  -H "Authorization: Bearer ${JWT_TOKEN}" | jq '.items[] | {id, label, archived_at}'

# Identify which seasons are active (archived_at: null)
```

**Solution:**
- Archive seasons first using bulk-archive or individual PATCH /seasons/{id}/archive
- Then retry bulk-delete with archived season IDs only
- UI enforces this by only showing delete action in archived view

### Bulk Operation Returns Skipped IDs

**Symptom:** Bulk-archive response includes skipped_ids array with some season IDs.

**Root Cause:** Those seasons were already archived when bulk-archive was called (idempotent operation).

**Solution:**
- Expected behavior, not an error
- Skipped seasons are already in desired state (archived)
- Check processed_count vs requested_count to see how many were actually modified

### Bulk Operation Exceeds Limit (422)

**Symptom:** Request returns 422 Unprocessable Entity with error about exceeding max season_ids.

**Root Cause:** Request contains more than 200 season IDs.

**Solution:**
- Split request into multiple batches of 200 or fewer

### Bulk Archive Verification Shows 0 Archived (Listing Hides Archived)

**Symptom:** Bulk-archive returns successful response (archived_count=3), but subsequent listing/verification shows 0 archived seasons.

**Root Cause:** Default seasons listing endpoint hides archived seasons (WHERE archived_at IS NULL). Verification requires explicit parameter to include archived seasons.

**How to Debug:**
```bash
# Bulk archive seasons
curl -X POST "${HOST}/api/v1/pricing/seasons/bulk-archive" \
  -H "Authorization: Bearer ${JWT_TOKEN}" \
  -H "Content-Type: application/json" \
  -d '{"season_ids": ["<uuid1>", "<uuid2>", "<uuid3>"]}' | jq

# Verify response shows archived_count=3

# Default listing (hides archived)
curl -X GET "${HOST}/api/v1/pricing/rate-plans/<plan_id>/seasons?limit=100" \
  -H "Authorization: Bearer ${JWT_TOKEN}" | jq '.items[] | {id, archived_at}'
# Result: Archived seasons NOT in list

# Listing WITH archived parameter
curl -X GET "${HOST}/api/v1/pricing/rate-plans/<plan_id>/seasons?show_archived=true&limit=100" \
  -H "Authorization: Bearer ${JWT_TOKEN}" | jq '.items[] | {id, archived_at}'
# Result: Archived seasons APPEAR with archived_at set
```

**Solution:**
- Use `show_archived=true` or `include_archived=true` query parameter to fetch archived seasons
- Verify `archived_at` field is not null for archived seasons
- Smoke script (pms_p2_seasons_bulk_ops_smoke.sh) implements this correctly as of P2.16.14

**Alternative Params (synonyms):**
- `?show_archived=true` (preferred)
- `?include_archived=true` (synonym)
- `?archived=true` (legacy)
- Example: For 500 seasons, make 3 requests (200 + 200 + 100)

---

### Smoke Script Crash: AttributeError 'list' object has no attribute 'get'

**Symptom:** Smoke script `pms_p2_seasons_bulk_ops_smoke.sh` crashes in Test 1 with Python error:
```
AttributeError: 'list' object has no attribute 'get'
```

**Root Cause:** Seasons listing endpoint returns a JSON array `[{season}, ...]`, but script expected a dict wrapper `{items: [...]}`.

**How to Debug:**
```bash
# Check seasons listing response shape
curl -X GET "${HOST}/api/v1/pricing/rate-plans/<plan_id>/seasons?show_archived=true&limit=10" \
  -H "Authorization: Bearer ${JWT_TOKEN}" | python3 -c "import sys, json; data=json.load(sys.stdin); print(type(data).__name__)"

# Output: "list" or "dict"
```

**Solution:**
- Smoke script (as of P2.16.14 parsing fix) handles both response shapes:
  - List: `[{season}, ...]` → uses directly
  - Dict: `{items: [...]}` → extracts items/data/seasons array
- Upgrade to latest smoke script version from origin/main
- No backend changes needed (both response shapes are valid)

**Fixed In:** P2.16.14 parsing fix commit


### Bulk Smoke Script: JSON Parse Error

**Symptom:** Smoke script `pms_p2_seasons_bulk_ops_smoke.sh` fails with:
```
JSON parse error: Expecting value: line 1 column 1 (char 0)
```

**Root Cause:** Script attempted to JSON-parse a non-JSON response. This occurs when:
- HTTP request returned non-200 status (401, 403, 404, 500, etc.)
- Response body is empty
- Response is HTML error page instead of JSON
- Response includes HTTP headers (curl -i was used)

**How to Debug:**

The script (as of P2.16.14 robust capture fix) now captures HTTP status and body separately, and only parses JSON if:
- HTTP status is 200
- Body is non-empty
- Body is valid JSON

On failure, the script prints:
- Exact URL that was fetched
- HTTP status code
- Body snippet (first 800 characters)
- Path to saved debug file (default: `/tmp/pms_bulk_ops_last_response.json`)

**Example Debug Output:**
```
ERROR: HTTP request failed with status 403
URL: https://api.example.com/api/v1/pricing/rate-plans/.../seasons?show_archived=true
HTTP Status: 403
Body:
{"error": "Access denied", "message": "User not authorized for this agency"}
Full response saved to: /tmp/pms_bulk_ops_last_response.json
```

**Solution:**
1. Check HTTP status code:
   - 401: JWT token expired or invalid (refresh token)
   - 403: User not authorized for agency (check agency_id in JWT)
   - 404: Resource not found (rate plan deleted or wrong ID)
   - 500: Server error (check backend logs)
2. Check debug file at `/tmp/pms_bulk_ops_last_response.json` for full response
3. Verify JWT token has correct claims:
   ```bash
   echo $JWT_TOKEN | cut -d'.' -f2 | base64 -d | jq
   # Check: role, agency_id, sub, exp
   ```
4. Test endpoint manually:
   ```bash
   curl -X GET "${HOST}/api/v1/pricing/rate-plans/<plan_id>/seasons?show_archived=true" \
     -H "Authorization: Bearer ${JWT_TOKEN}" \
     -H "x-agency-id: ${AGENCY_ID}" -v
   ```

**Fixed In:** P2.16.14 robust JSON capture commit

---
---

### Smoke Test: "Template has no active periods" (Period Creation Failed)

**Symptom:** Smoke script fails with "Template has no active periods" even though it claims "Added 2 periods to template"

**Root Cause:** Period creation requests are failing silently (4xx/5xx) but script doesn't check HTTP status

**Debug:**
```bash
# Check if periods exist for a template
TEMPLATE_ID="<uuid>"
curl "$HOST/api/v1/pricing/season-templates/$TEMPLATE_ID/periods" | python3 -m json.tool

# Expected: Array with 2+ period objects
# If empty []: Period creation failed

# Check template details
curl "$HOST/api/v1/pricing/season-templates/$TEMPLATE_ID" | python3 -m json.tool

# Look for "periods": [] vs "periods": [{...}, {...}]
```

**Fix:**
1. Verify ADMIN_JWT_TOKEN has correct permissions (manager/admin role)
2. Check period creation payload matches Pydantic schema exactly
3. Ensure endpoint URL is correct: POST /api/v1/pricing/season-templates/{id}/periods
4. Check response status code (should be 200/201, not 4xx)

**Required Payload Fields:**
```json
{
  "label": "Hauptsaison",
  "date_from": "07-01",
  "date_to": "08-31",
  "active": true
}
```

**Common Issues:**
- Wrong endpoint URL (e.g. /templates vs /season-templates)
- Missing Content-Type: application/json header
- Wrong field names (start_date vs date_from)
- Invalid JWT token (expired or insufficient permissions)

---

### Smoke Test: Preview Returns create=0 (No-Op Scenario)

**Symptom:** Smoke script fails with "Expected at least 1 season to create, got 0" even though preview returns HTTP 200

**Cause:** Auto-selected rate plan already has seasons linked to the template periods for the requested years, so sync preview returns zero changes needed (valid no-op)

**Fix (as of P2.16.x):**
Smoke script now creates a dedicated test rate plan for each run, ensuring deterministic behavior (new rate plan = zero seasons = create > 0).

**Manual Verification:**
```bash
# Check if rate plan already has seasons
curl "$HOST/api/v1/pricing/rate-plans/$RATE_PLAN_ID/seasons" | python3 -m json.tool

# If seasons exist for the template periods in the requested years:
# - Preview will show create=0 (no-op, valid)
# - Apply will show update=0 or relink=0 (also valid)
# This is CORRECT behavior, not an error
```

**Archived_at Drift (503 Error):**

If period creation fails with HTTP 503 or "null value in column violates not-null constraint":
1. Supabase migration drift: `archived_at` column added but not in local schema
2. Fix: Apply migrations as supabase_admin or pull latest schema
3. Verify: `\d pricing_season_template_periods` should show `archived_at` column
```

---

### Seasons Created But Linkage Missing (source_template_period_id null)

**Symptom:** Smoke Test 3 fails with "No seasons have source_template_period_id set"

**Cause:** Season sync apply mode didn't populate linkage field

**Fix:** Upgrade to commit with P2.16.9 fix

**Verification:**
```bash
# Check season linkage after sync
curl "$HOST/api/v1/pricing/rate-plans/$RATE_PLAN_ID/seasons" | \
  python3 -c "import sys, json; seasons = json.load(sys.stdin); print(f'Linked: {sum(1 for s in seasons if s.get(\"source_template_period_id\"))} / {len(seasons)}')"
```

---

### Season Template Smoke Blocked by 409 Active Smoke Plan

**Symptom:** Smoke script fails with HTTP 409 "An active rate plan already exists for this property"

**Cause:** Previous smoke run left test rate plan active (cleanup failed or manual termination)

**Automatic Fix:** Script now auto-detects and archives smoke plans (names containing "SMOKE") before creating new test plan

**Manual Fallback:**
```bash
# Find smoke plans
curl "$HOST/api/v1/pricing/rate-plans?property_id=$PROPERTY_ID" | \
  python3 -c "import sys,json; [print(p['id'], p['name']) for p in json.load(sys.stdin) if 'SMOKE' in p.get('name','').upper()]"

# Archive manually
curl -X PATCH "$HOST/api/v1/pricing/rate-plans/{smoke_plan_id}/archive" \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  -d '{}'
```

**What to check:**
- Script logs show "Cleanup: Archived existing smoke plan..."
- Only plans with "SMOKE" in name are archived
- Real customer plans untouched

---

### Import Modal Imports Wrong Year(s)

**Symptom:** When selecting template "2026", import creates seasons for both 2026 AND 2027 (or other unwanted years)

**Cause:** Frontend was auto-deriving years from currentYear instead of respecting template selection

**Fix:** Upgrade to P2.16.11 which uses explicit selectedYears based on template name pattern or period dates

**What to check:**
- Browser DevTools → Network tab → inspect sync-from-template request payload
- Should see `"years": [2026]` when template "2026" selected
- UI shows "Jahre: 2026" in import modal
- Template name patterns: if template.name matches `^\d{4}$` → uses that year exactly

**Verification:**
```bash
# Check network request payload in browser DevTools:
# POST /api/v1/pricing/rate-plans/{id}/seasons/sync-from-template
# Body should have: {"template_id": "...", "years": [2026], "mode": "preview"}
```

---

### Gap Banner Shows Wrong Years / Off-By-One Errors

**Symptom:** Gap warning shows "2025" when no 2025 seasons exist, or shows gaps when year is fully covered (Jan 1 - Dec 31)

**Cause:** Date parsing used local timezone instead of UTC, causing date shifts and boundary errors

**Fix:** Upgrade to P2.16.11 which uses UTC parsing throughout gap detection

**What to check:**
- Gap detection uses UTC: `new Date(dateStr + "T00:00:00Z")`
- Year boundaries: `${year}-01-01T00:00:00Z` to `${year}-12-31T00:00:00Z`
- Only analyzes years present in active seasons
- If year fully covered (no gaps) → banner hidden

**Verification:**
- Create seasons covering Jan 1 - Dec 31 for a year
- Gap banner should disappear completely
- No "2025" artifacts in gap dates

---

### Bulk Smoke Script: JSON Parse Error (P2.16.14 v2 Hardening)

**Symptom:** Smoke script `pms_p2_seasons_bulk_ops_smoke.sh` fails with "JSON parse error" even after v1 hardening.

**Root Cause (v2 Fix):** Log lines from `log_error()` mixing into captured JSON variables. The v1 hardening validated HTTP status and body, but diagnostics were printed to stdout and captured by command substitution (`VERIFY_BODY=$(safe_fetch_json ...)`).

**How v2 Fix Works:**

The script now uses `http_get_json()` helper that:
1. **Follows redirects** automatically (`curl --location`)
2. **Captures to temp files**: HTTP status, content-type, headers, body
3. **Validates JSON with Python3** try/except BEFORE returning
4. **Separates output streams**:
   - stdout: Only validated JSON (for capture)
   - stderr: All diagnostics (not captured)
5. **Creates debug bundle** on failure: `/tmp/pms_bulk_ops_debug_<timestamp>.txt`

**Debug Bundle Contents:**
```
=== DEBUG BUNDLE ===
Timestamp: 2026-01-21 14:32:15 UTC

=== URL ===
https://api.example.com/api/v1/pricing/rate-plans/.../seasons?show_archived=true

=== HTTP Status ===
200

=== Content-Type ===
application/json

=== JSON Validation Error ===
JSON validation failed: Expecting value: line 1 column 1 (char 0)

=== Response Headers ===
HTTP/2 200
content-type: application/json
...

=== Response Body (Invalid JSON) ===
<!DOCTYPE html>
<html>Error: Rate plan not found</html>
```

**How to Debug:**
1. Check stderr output for diagnostics (not mixed into variables)
2. Locate debug bundle: `/tmp/pms_bulk_ops_debug_<timestamp>.txt`
3. Inspect full response including headers, body, validation error
4. Common issues:
   - **Content-Type mismatch**: Headers say `application/json` but body is HTML
   - **Backend crash**: Returns 200 with error page instead of JSON
   - **Empty body**: HTTP 200 but content-length: 0
   - **Redirect loop**: `--location` flag prevents but logs redirects

**Solution:**
- **v2 script** (commit `fix(p2.16.14): bulk ops smoke json fetch hardening v2`):
  - All diagnostics to stderr (prevents capture pollution)
  - Python3 JSON validation before returning
  - Debug bundle for post-mortem analysis
- **If still failing**: Check debug bundle, test endpoint manually, verify backend logs

**Manual Endpoint Test:**
```bash
curl -v --location \
  -H "Authorization: Bearer $JWT_TOKEN" \
  -H "x-agency-id: $AGENCY_ID" \
  "https://api.example.com/api/v1/pricing/rate-plans/$RATE_PLAN_ID/seasons?show_archived=true&limit=100"

# Check:
# - HTTP status (expect 200)
# - Content-Type header (expect application/json)
# - Response body is valid JSON (pipe to jq)
```

#### 307 Redirect Due to Trailing Slash (P2.16.14 v3)

**Symptom:** JSON parse error occurs even with v2 hardening, debug bundle shows HTTP 307 redirect or unexpected HTML response.

**Root Cause:** API endpoint returns 307 Temporary Redirect when URL has trailing slash inconsistency. Without curl's `-L` flag, the redirect response (HTML) is captured instead of following to the actual JSON endpoint.

**How to Diagnose:**
1. Check debug bundle for HTTP status 307
2. Look for `Location:` header in response headers
3. Verify if request URL has trailing slash issue (e.g., `/seasons/` vs `/seasons`)

**Solution:**
- **v3 script enhancement**: Adds explicit 307/redirect diagnostics and verifies curl uses `--location` flag
- **Verification**: curl `-L` (or `--location`) flag automatically follows redirects to final destination
- **Prevention**: Ensure all API calls use consistent URL patterns (with or without trailing slash as API expects)

**Manual Test for Redirects:**
```bash
# Test WITHOUT following redirects (will show 307 if present)
curl -v \
  -H "Authorization: Bearer $JWT_TOKEN" \
  "https://api.example.com/api/v1/pricing/seasons/"

# Test WITH following redirects (should get JSON)
curl -v -L \
  -H "Authorization: Bearer $JWT_TOKEN" \
  "https://api.example.com/api/v1/pricing/seasons/"
```

**Note:** The `http_get_json()` helper already uses `--location` by default, but this diagnostic helps identify redirect issues in manual testing or legacy scripts.

#### Empty Body / Non-JSON Response (P2.16.14 v4)

**Symptom:** JSON parse error persists despite v1/v2/v3 hardening. Debug bundles show valid HTTP 200 responses, but JSON parsing still fails intermittently.

**Root Cause (v4 Fix):** Command substitution contamination and stream mixing. Even with stderr-only diagnostics, command substitution `$()` can capture stdout fragments from subshells, race conditions, or background processes. Additionally, curl's stdout can be corrupted by signal handlers, debug output, or terminal state changes.

**How v4 Fix Works:**

The script completely eliminates command substitution for HTTP capture:

1. **Pure Temp-File Capture** (`http_fetch()` function, line 165):
   - Uses curl with `-o tempfile` to write body directly to disk
   - Uses `-w` to write metadata (http_code, url_effective, content_type) to separate file
   - Captures headers to separate file with `-D`
   - Zero stream mixing - all data goes to files, not stdout/stderr

2. **6-Layer Validation** (`json_body_or_die()` function, line 244):
   - Layer 1: HTTP status code check (expect 200)
   - Layer 2: Content-Type validation (expect application/json)
   - Layer 3: File existence and readability
   - Layer 4: Non-empty body check
   - Layer 5: Python3 JSON syntax validation
   - Layer 6: Basic structure validation (expect dict or list)

3. **Debug Bundle Directory** (created on first HTTP request):
   - Path: `/tmp/pms_http_debug_<PID>_<timestamp>/`
   - Preserved on failure for forensic analysis
   - Per-request files (for each labeled request):
     - `${label}.body` - Response body (raw)
     - `${label}.headers` - HTTP response headers
     - `${label}.meta` - Metadata (http_code, url_effective, content_type)
     - `${label}.writeout` - Curl writeout template values

4. **Applied to All Verification Blocks**:
   - Test 1: Archive verification (lines 583, 590, 599)
   - Test 2: Idempotency verification (lines 707, 708)
   - Test 4: Delete verification (lines 839, 840)

**How to Debug:**

1. Locate debug bundle directory (path printed in script output on failure):
   ```
   /tmp/pms_http_debug_12345_20260121_143215/
   ```

2. Inspect request files:
   ```bash
   cd /tmp/pms_http_debug_12345_20260121_143215/

   # Check HTTP status
   cat test1_verify_archived.meta
   # Output: http_code: 200, url_effective: https://..., content_type: application/json

   # Check response body
   cat test1_verify_archived.body | jq .
   # Should be valid JSON

   # Check headers for redirects
   cat test1_verify_archived.headers
   # Look for Location: header if redirect occurred
   ```

3. Common issues to look for:
   - **http_code != 200**: Check `.meta` file, verify endpoint exists
   - **content_type != application/json**: Check `.headers` and `.meta`, backend returned HTML/XML
   - **Empty body**: Check `.body` file size, backend crashed or returned no content
   - **Redirect chain**: Check `.headers` for `Location:` headers, verify final URL in `.meta`
   - **Invalid JSON syntax**: Check `.body` for malformed JSON, HTML fragments, binary data

**Solution:**

- **v4 script** (complete rewrite for bulletproof HTTP capture):
  - Eliminates command substitution entirely
  - All HTTP data captured to temp files
  - 6-layer validation before trusting response
  - Debug bundle with per-request forensics
  - Zero chance of stream contamination

- **If still failing**:
  - Inspect debug bundle files
  - Compare `.body` with expected JSON structure
  - Check backend logs for crashes/errors
  - Verify API endpoint behavior with manual curl test


#### JSON Parse Error with Valid Body File (P2.16.14 v5)

**Symptom:** Smoke script fails with "JSON parse error" in verification steps, but debug bundle shows valid JSON in body files.

**Root Cause (v5 Fix):** Even with v4's temp-file HTTP capture, JSON parsing still passed through shell variables (`echo "$BODY" | python3 -c "..."`). Shell variable storage corrupts JSON containing special characters (quotes, newlines, unicode, etc).

**How v5 Fix Works:**

The script eliminates shell variable JSON storage entirely:

1. **New Validation Helpers** (parse JSON directly from body files):
   - `validate_json_response()` - Validates body file is parseable JSON
   - `count_archived_seasons()` - Counts archived seasons in body file
   - `count_present_ids()` - Checks if specific IDs exist in body file

2. **Direct File-Based Parsing**:
   - All Python JSON parsing reads directly from body files
   - No JSON data ever stored in shell variables
   - Pattern: `python3 -c "..." < "$DEBUG_DIR/test1_verify_archived.body"`
   - Eliminates corruption from shell quoting, escaping, expansion

3. **Applied to All Verification Blocks**:
   - Test 1: Archive verification (parse response, count archived)
   - Test 2: Idempotency verification (parse response, check skipped_ids)
   - Test 4: Delete verification (parse response, count deleted)

**How to Debug:**

1. Verify body file contains valid JSON:
   ```bash
   # Direct JSON validation (same method as script)
   cat $DEBUG_DIR/test1_verify_archived.body | python3 -m json.tool

   # Should output formatted JSON without errors
   ```

2. Check what script is trying to parse:
   ```bash
   # Count archived seasons manually
   python3 -c "import sys, json; data = json.load(sys.stdin); items = data.get('items', data if isinstance(data, list) else []); print(len([s for s in items if s.get('archived_at')]))" < $DEBUG_DIR/test1_verify_archived.body

   # Check for specific IDs
   python3 -c "import sys, json; data = json.load(sys.stdin); items = data.get('items', data if isinstance(data, list) else []); ids = {s.get('id') for s in items}; print('season-id-123' in ids)" < $DEBUG_DIR/test1_verify_archived.body
   ```

3. Compare with old method (if mixing v4/v5 code):
   ```bash
   # OLD (v4): Variable storage corrupts JSON
   BODY=$(cat $DEBUG_DIR/test1_verify_archived.body)
   echo "$BODY" | python3 -m json.tool  # May fail with parse error

   # NEW (v5): Direct file read preserves JSON
   cat $DEBUG_DIR/test1_verify_archived.body | python3 -m json.tool  # Always works if file is valid
   ```

**Solution:**

- **v5 script** (final hardening for JSON verification):
  - All verification parsing reads directly from body files
  - New helper functions encapsulate file-based parsing
  - Zero JSON data in shell variables
  - Prevents corruption from shell quoting/escaping

- **If still failing**:
  - Check if body file itself is corrupt: `cat $DEBUG_DIR/*.body | python3 -m json.tool`
  - Verify HTTP response was actually JSON (check Content-Type in `.meta` file)
  - Ensure no legacy code still using shell variable JSON storage

**Verification Pattern (v5):**
```bash
# After http_fetch(), parse directly from body file
http_fetch "test1_verify_archived" "$URL" "-H 'Authorization: Bearer $JWT_TOKEN'"
json_body_or_die "$DEBUG_DIR/test1_verify_archived"

# Count archived seasons (reads body file directly, no shell variable)
ARCHIVED_COUNT=$(count_archived_seasons "$DEBUG_DIR/test1_verify_archived.body")

# Check for specific IDs (reads body file directly)
if count_present_ids "$DEBUG_DIR/test1_verify_archived.body" "${SEASON_IDS[@]}"; then
    log_info "All expected IDs found"
fi
```

**Automatic Fallback (pms_quote_keine_saison_smoke.sh):**
The `pms_quote_keine_saison_smoke.sh` script implements automatic 409 handling:
- If rate plan creation fails with HTTP 409, script automatically creates temporary "Smoke Property"
- Test runs against smoke property (customer's active plan untouched)
- Cleanup deletes both rate plan and smoke property (even on failure via trap EXIT)
- No manual intervention required in PROD
- Property name: "Quote Gap Objekt YYYYMMDD-HHMMSS - Smoke"

**Expected Behavior:**
```
⚠️  STEP B: HTTP 409 - Aktiver Preisplan existiert bereits für Property <uuid>
ℹ️  Fallback: Erstelle temporäre Smoke-Property...
✅ Smoke Property erstellt: <new-uuid>
ℹ️  PROPERTY_ID gewechselt zu Smoke Property: <new-uuid>
ℹ️  Wiederhole Preisplan-Erstellung mit Smoke Property...
✅ STEP B PASSED: Preisplan erstellt mit Smoke Property (id=<plan-uuid>)
```


**Quote Totals Validation Failure (pms_quote_keine_saison_smoke.sh):**

**Symptom:** STEP G.2 fails with "Quote totals mismatch or incorrect" and creates debug bundle.

**Root Cause:** API quote response totals don't match computed values from nights_breakdown, or subtotal is incorrect.

**Canonical Fields (QuoteResponse schema):**
- `total_cents`: Grand total (subtotal + fees + taxes) - THIS is the canonical total field
- `subtotal_cents`: Accommodation subtotal (sum of all nights from nights_breakdown)
- `fees_total_cents`: Total of all fees
- `taxes_total_cents`: Total of all taxes
- `nights_breakdown[]`: Per-night pricing array

**How Validation Works:**
1. Computes `computed_subtotal = sum(nights_breakdown[].nightly_cents)`
2. Computes `computed_total = computed_subtotal + fees_total_cents + taxes_total_cents`
3. Validates: `total_cents == computed_total` AND `subtotal_cents == computed_subtotal`
4. On mismatch: FAILS test (rc=1) and saves debug bundle

**Debug Bundle Location:** `/tmp/pms_quote_gap_debug_<timestamp>/`
- `quote_with_fallback.json`: Full API response
- `validation_summary.txt`: Computed vs actual values

**How to Debug:**
```bash
# [HOST-SERVER-TERMINAL] Check debug bundle
DEBUG_DIR=$(ls -td /tmp/pms_quote_gap_debug_* 2>/dev/null | head -1)
cat "$DEBUG_DIR/validation_summary.txt"
cat "$DEBUG_DIR/quote_with_fallback.json" | python3 -m json.tool

# Expected behavior:
# total_cents = subtotal_cents + fees_total_cents + taxes_total_cents
# subtotal_cents = sum of all nights_breakdown[].nightly_cents
```

**Solution:**
- If `total_cents` is 0 or null: API bug in quote calculation (check backend/app/services/pricing_service.py)
- If `subtotal_cents` doesn't match computed_subtotal: nights_breakdown aggregation bug
- If script reads wrong field: Update script to use `total_cents` (NOT `total_price_cents`)


---

## P2.16 Template Sync: Year-Bound Correction Issues

**Symptom:** Template sync creates conflicts or duplicates instead of correcting out-of-year imported seasons.

**Example Scenario:**
- Template "2026" has "Nebensaison 2026-12-01 to 2026-12-31"
- Property has imported season "Nebensaison 2026-12-01 to 2027-01-15" (incorrectly extends to 2027)
- Sync fails with conflict or creates duplicate instead of correcting existing season

**Root Cause:** Prior to P2.16 bugfix, sync did not enforce year boundaries for pattern-mode templates and did not handle duplicate seasons.

**How It's Fixed (Commit: fix(p2.16): template sync corrects imported seasons)**:
1. **Year Boundary Enforcement**: Pattern mode (single-year templates) now clamps `date_to` to December 31 of `source_year`
2. **Duplicate Detection**: Finds ALL seasons matching (rate_plan_id, source_template_period_id, source_year) match key
3. **Duplicate Archiving**: If multiple matches found, keeps most recently updated, archives rest
4. **Correction Logic**: Updates existing season with corrected dates instead of skipping/conflicting

**Diagnostic Commands**:
```bash
# [HOST-SERVER-TERMINAL] Check for out-of-year seasons
RATE_PLAN_ID="<uuid>"
psql $DATABASE_URL -c "
SELECT id, label, date_from, date_to, source_year,
       EXTRACT(YEAR FROM date_to) as date_to_year
FROM rate_plan_seasons
WHERE rate_plan_id = '$RATE_PLAN_ID'
  AND archived_at IS NULL
  AND source_year IS NOT NULL
  AND EXTRACT(YEAR FROM date_to) != source_year
ORDER BY date_from;
"

# Expected: Empty result (no out-of-year seasons)
# If rows returned: These seasons extend beyond their source_year boundary

# Check for duplicate seasons (same template period + year)
psql $DATABASE_URL -c "
SELECT source_template_period_id, source_year, COUNT(*) as duplicates
FROM rate_plan_seasons
WHERE rate_plan_id = '$RATE_PLAN_ID'
  AND archived_at IS NULL
  AND source_template_period_id IS NOT NULL
GROUP BY source_template_period_id, source_year
HAVING COUNT(*) > 1;
"

# Expected: Empty result (no duplicates)
# If rows returned: Multiple active seasons for same template period + year
```

**Manual Correction (if sync doesn't fix)**:
```bash
# Archive duplicate seasons manually (keep most recent)
DUPLICATE_SEASON_ID="<older-duplicate-uuid>"
curl -X PATCH "https://api.fewo.kolibri-visions.de/api/v1/pricing/rate-plans/<rate-plan-id>/seasons/$DUPLICATE_SEASON_ID/archive" \
  -H "Authorization: Bearer $JWT_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{}'

# Correct out-of-year season manually
SEASON_ID="<season-uuid>"
curl -X PATCH "https://api.fewo.kolibri-visions.de/api/v1/pricing/rate-plans/<rate-plan-id>/seasons/$SEASON_ID" \
  -H "Authorization: Bearer $JWT_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"date_to": "2026-12-31"}'
```

**Verification (After Fix)**:
```bash
# Run smoke test
HOST=https://api.fewo.kolibri-visions.de \
MANAGER_JWT_TOKEN="<jwt>" \
AGENCY_ID="<agency-id>" \
./backend/scripts/pms_p216_template_sync_correction_smoke.sh

# Expected: rc=0, all tests pass
# STEP F should show: "Season corrected to 2026-12-31 ✅"
# STEP G should show: "Exactly 1 active season (no duplicates) ✅"
```

**Related Runbook Sections:**
- [P2.16 Template Sync](../docs/ops/runbook.md#p216-template-sync) for general sync troubleshooting
- [P2.13 Pricing Invariants](../docs/ops/runbook.md#p213-pricing-invariants-hardening) for season overlap constraints


---

### P2.16 BUGFIX v2: Sync Not Idempotent (Second Sync Returns update=1)

**Symptom:** Second template sync returns `counts.update=1` instead of `0` when no actual changes needed. Smoke script STEP H (idempotency check) fails.

**Example Scenario:**
- First sync corrects season: 2026-12-01 to 2026-12-31 (success)
- Second sync with same template returns: `counts.update=1` (should be 0)
- No actual field changes, but season marked for update anyway

**Root Cause:** Prior to P2.16 BUGFIX v2 (commit eef0d19), sync unconditionally added matched seasons to `to_update` list without checking if managed fields actually differ. Backend updated `source_synced_at` and `updated_at` on every sync run regardless of whether label/date_from/date_to/source_* fields changed.

**How It's Fixed (Commit eef0d19: fix(p2.16): make template sync idempotent)**:
1. **Strict Diff Check**: Before adding season to `to_update`, compares 6 managed fields:
   - label
   - date_from
   - date_to
   - source_template_id
   - source_template_period_id
   - source_year
2. **No-Op Skip**: If ALL fields match, season is skipped entirely (not added to `to_update`)
3. **No DB Update**: When no changes needed, DB update is not executed (source_synced_at NOT updated)
4. **Debug Logging**: Logs `changed_fields` list when update needed, or "already up-to-date" when skipped

**Expected Result:**
- First sync: `counts.update=N` (N seasons corrected)
- Second sync: `counts.update=0` (no changes needed - true idempotency)
- STEP H passes: sync2 returns `counts.update=0, counts.create=0`

**Diagnostic Commands**:
```bash
# [HOST-SERVER-TERMINAL] Run smoke script to verify idempotency
export HOST="https://api.fewo.kolibri-visions.de"
export MANAGER_JWT_TOKEN="<jwt>"
export AGENCY_ID="<agency-id>"
./backend/scripts/pms_p216_template_sync_correction_smoke.sh

# Expected: rc=0, all tests pass
# STEP H should show: "Sync2 counts.update=0 (idempotent) ✅"

# Manual verification: Check logs for debug messages
# Should see: "Season {id} already up-to-date, skipping update" on second sync
```

**Related Changes:**
- File: `backend/app/api/routes/pricing.py` (sync_seasons_from_template route, lines 2323-2369)
- Commit: eef0d19 (fix(p2.16): make template sync idempotent)
- Smoke script: `backend/scripts/pms_p216_template_sync_correction_smoke.sh` (STEP H validates idempotency)

**Related Runbook Sections:**
- [P2.16 Template Sync: Year-Bound Correction](#p216-template-sync-year-bound-correction-issues) for year boundary enforcement
- [Season Templates](#season-templates-agency-wide-reusable-periods) for general template documentation


---

### P2.16 Hotfix (2026-01-22): No-op Sync Returns 500 (UnboundLocalError is_relink)

**Symptom:** Second template sync (idempotency check) returns HTTP 500 with `UnboundLocalError: cannot access local variable 'is_relink'`.

**Traceback:**
```
UnboundLocalError: cannot access local variable 'is_relink' where it is not associated with a value
File "/app/app/api/routes/pricing.py", line 2375, in sync_seasons_from_template
  if is_relink:
```

**Example Scenario:**
- First sync corrects season successfully (HTTP 200, update=1, relink=1)
- Second sync with same template crashes with 500 (instead of HTTP 200, update=0)
- STEP H in smoke script fails with 500 instead of passing idempotency check

**Root Cause:** 
Variable `is_relink` was defined inside `if needs_update:` block (line 2357) but referenced outside it (line 2375). In no-op case where `needs_update=False`, the variable was never assigned, causing UnboundLocalError.

**How It's Fixed:**
Moved relink tracking (`if is_relink:` block) INSIDE the `if needs_update:` block where `is_relink` is defined. Now variable is only referenced in the same scope where it's assigned.

**Expected Result:**
- First sync: HTTP 200, `counts.update=1, counts.relink=1` (season corrected and linked)
- Second sync: HTTP 200, `counts.update=0, counts.create=0` (true no-op, no crash)
- STEP H passes with HTTP 200

**Diagnostic Commands:**
```bash
# [HOST-SERVER-TERMINAL] Verify deployment
export API_BASE_URL="https://api.fewo.kolibri-visions.de"
export EXPECT_COMMIT="<new-commit-sha>"
./backend/scripts/pms_verify_deploy.sh

# Run smoke test (STEP H should now pass with 200)
export HOST="https://api.fewo.kolibri-visions.de"
export MANAGER_JWT_TOKEN="<jwt>"
export AGENCY_ID="ffd0123a-10b6-40cd-8ad5-66eee9757ab7"
./backend/scripts/pms_p216_template_sync_correction_smoke.sh

# Expected: STEP H PASSED: Idempotent (no changes on re-sync) ✅
# SMOKE_RC=0
```

**Related Changes:**
- File: `backend/app/api/routes/pricing.py` (lines 2346-2388)
- Fix: Moved relink tracking inside `if needs_update:` block
- Prevents UnboundLocalError on no-op sync paths

**Related Runbook Sections:**
- [P2.16 BUGFIX v2: Sync Not Idempotent](#p216-bugfix-v2-sync-not-idempotent-second-sync-returns-update1) for idempotency implementation


**PROD Verification (2026-01-22):**
- Commit: 1f5d2d5616d0da0435905ae0bed9b94765b34554
- Deploy: pms_verify_deploy.sh rc=0
- Smoke: pms_p216_template_sync_correction_smoke.sh rc=0 (STEP H HTTP 200, idempotent)


---

## P2 Amenities Admin UI

**Overview:** Admin UI for managing amenities catalog and assigning amenities to properties.

**Purpose:** Allow staff (manager/admin) to create and manage amenity definitions (e.g., WiFi, Pool, Kitchen features) and assign them to properties. Amenities are displayed in property detail views and can be used for filtering/marketing.

**Architecture:**
- **Database**: `amenities` table stores amenity definitions (name, description, category, icon, sort_order) scoped by agency_id
- **Junction Table**: `property_amenities` maps properties to amenities (many-to-many relationship)
- **RBAC**: Admin/manager roles required for amenity management
- **Multi-tenant**: All queries scoped by x-agency-id header (JWT lacks agency_id claim)

**UI Routes:**
- `/amenities` - Amenities catalog page (list, create, edit, delete)
- `/properties/[id]` - Property detail page with "Ausstattung" section (assign/view amenities)

**API Endpoints:**

Amenities CRUD:
- `GET /api/v1/amenities` - List all amenities for agency
- `POST /api/v1/amenities` - Create amenity (409 if name exists)
- `PATCH /api/v1/amenities/{id}` - Update amenity
- `DELETE /api/v1/amenities/{id}` - Delete amenity (cascade removes property assignments)

Property Amenities:
- `GET /api/v1/amenities/property/{property_id}` - Get property amenities
- `PUT /api/v1/amenities/property/{property_id}` - Replace property amenities (atomic)

**Database Tables:**
- `amenities` - Amenity definitions with agency scoping
- `property_amenities` - Property-to-amenity mappings (ON DELETE CASCADE)

**Verification Commands:**

```bash
# [HOST-SERVER-TERMINAL] Run UI smoke test
export ADMIN_URL="https://admin.fewo.kolibri-visions.de"
export EXPECTED_COMMIT="<commit-sha>"
./frontend/scripts/pms_admin_amenities_ui_smoke.sh
echo "rc=$?"

# Expected output: 5 tests pass, rc=0
# Test 1: Docker container commit match (if EXPECTED_COMMIT set)
# Test 2: /amenities page loads (HTTP 200)
# Test 3: Page content verified (Ausstattung heading, Neu button, Next.js markers)
# Test 4: /properties page references amenities (soft check)
# Test 5: Navigation includes "Ausstattung" entry
```

**Common Issues:**

### Amenities Page Returns 401 (Session Expired)

**Symptom:** /amenities page returns 401 Unauthorized or shows "Session abgelaufen" error.

**Root Cause:** JWT token expired or not present in session/localStorage.

**How to Debug:**
```bash
# Check browser console for auth errors
# DevTools > Console > Look for "Session abgelaufen" or 401 errors

# Check localStorage for session token
# DevTools > Application > Local Storage > Check for auth token
```

**Solution:**
- User must login again at /login
- Check Supabase session persistence settings
- Verify JWT_SECRET matches between backend and Supabase

### Amenities Page Returns 403 (Access Denied)

**Symptom:** /amenities page returns 403 Forbidden or shows "Keine Berechtigung" error.

**Root Cause:** User role is not admin/manager. Amenities management requires elevated permissions.

**How to Debug:**
```bash
# Decode JWT to check role claim
echo $JWT_TOKEN | cut -d'.' -f2 | base64 -d | jq '.role'
# Should return "admin" or "manager", not "staff" or "owner"

# Check team_members table
psql $DATABASE_URL -c "SELECT user_id, role FROM team_members WHERE user_id = '<user-id>';"
```

**Solution:**
- Admin must upgrade user role via team_members table
- Update role: `UPDATE team_members SET role = 'manager' WHERE user_id = '<user-id>';`
- User must logout and login again to refresh JWT claims

### Create Amenity Returns 409 (Name Already Exists)

**Symptom:** POST /api/v1/amenities returns 409 Conflict with error "Amenity name already exists".

**Root Cause:** UNIQUE constraint on (agency_id, name). Cannot create duplicate amenity names within same agency.

**How to Debug:**
```bash
# Check existing amenities for agency
curl -X GET "https://api.fewo.kolibri-visions.de/api/v1/amenities" \
  -H "Authorization: Bearer $JWT_TOKEN" \
  -H "x-agency-id: $AGENCY_ID" | jq '.[] | {id, name}'
```

**Solution:**
- Use different amenity name (e.g., "WLAN" vs "WiFi", "Schwimmbad" vs "Pool")
- Or update existing amenity instead: PATCH /api/v1/amenities/{id}
- Or delete old amenity first (cascade deletes property assignments)

### Property Amenities Not Saving

**Symptom:** Clicking "Speichern" in amenities assignment modal shows success toast but amenities don't appear on property.

**Root Cause:** PUT /api/v1/amenities/property/{property_id} returned success but subsequent GET returns empty array.

**How to Debug:**
```bash
# Check property_amenities table directly
PROPERTY_ID="<property-uuid>"
psql $DATABASE_URL -c "
SELECT pa.property_id, pa.amenity_id, a.name
FROM property_amenities pa
JOIN amenities a ON pa.amenity_id = a.id
WHERE pa.property_id = '$PROPERTY_ID';
"

# Expected: Rows showing assigned amenities
# If empty: Check if PUT request actually sent amenity_ids array
```

**Solution:**
- Check browser DevTools Network tab: PUT request body should have `{"amenity_ids": ["uuid1", "uuid2"]}`
- Verify x-agency-id header matches property's agency_id
- Check property exists and user has permission to update it
- If table has rows but UI doesn't show: Check GET request uses same x-agency-id header

### Amenities Modal Shows "Keine Ausstattung verfügbar"

**Symptom:** Opening amenities assignment modal shows "Keine Ausstattung verfügbar" message.

**Root Cause:** GET /api/v1/amenities returns empty array - no amenities created for this agency yet.

**How to Debug:**
```bash
# Check amenities table for agency
AGENCY_ID="<agency-uuid>"
psql $DATABASE_URL -c "SELECT id, name, category FROM amenities WHERE agency_id = '$AGENCY_ID';"

# Expected: Rows showing amenities
# If empty: No amenities created yet
```

**Solution:**
- Go to /amenities page and create amenities first (click "Neu" button)
- Create amenities with names like "WLAN", "Pool", "Küche", "Parkplatz", etc.
- Then return to property detail and try assigning amenities again

### Delete Amenity Cascade Issues

**Symptom:** Deleting amenity fails or leaves orphaned property_amenities records.

**Root Cause:** ON DELETE CASCADE constraint not working or missing in migration.

**How to Debug:**
```bash
# Check FK constraint
psql $DATABASE_URL -c "
SELECT
  tc.constraint_name,
  tc.table_name,
  kcu.column_name,
  ccu.table_name AS foreign_table_name,
  ccu.column_name AS foreign_column_name,
  rc.delete_rule
FROM information_schema.table_constraints AS tc
JOIN information_schema.key_column_usage AS kcu
  ON tc.constraint_name = kcu.constraint_name
JOIN information_schema.constraint_column_usage AS ccu
  ON ccu.constraint_name = tc.constraint_name
JOIN information_schema.referential_constraints AS rc
  ON tc.constraint_name = rc.constraint_name
WHERE tc.table_name = 'property_amenities'
  AND tc.constraint_type = 'FOREIGN KEY'
  AND ccu.table_name = 'amenities';
"

# Expected: delete_rule = 'CASCADE'
```

**Solution:**
- If cascade missing: Run migration to add ON DELETE CASCADE
- If constraint exists but not working: Check Postgres version and RLS policies
- Manual cleanup if needed:
```sql
-- Delete orphaned property_amenities
DELETE FROM property_amenities WHERE amenity_id NOT IN (SELECT id FROM amenities);
```

### Admin UI Shows "Keine Agentur-ID gefunden" on Amenities Page

**Symptom:** Browser at /amenities shows red error banner: "Keine Agentur-ID gefunden. Bitte melden Sie sich erneut an." Page is not usable (dead-end, no functionality available).

**Root Cause:** Page required agency_id in user.user_metadata and showed hard error banner if missing. Did not redirect to login for unauthenticated users or handle agency context resolution gracefully.

**How It's Fixed (2026-01-22):**
- **Auth Redirect:** Unauthenticated users are now redirected to `/login?next=/amenities`
- **Agency Context Optional:** Page attempts to load amenities even without explicit agency_id in user metadata
- **Backend Auto-Resolve:** Backend can auto-resolve agency for single-tenant users via team_members table
- **No Hard Error Banner:** Removed dead-end error banner; errors now come from API calls with actionable messages

**What Changed:**
- `frontend/app/amenities/page.tsx`: Added redirect to login if no user, removed agency_id requirement for page render
- `frontend/scripts/pms_admin_amenities_ui_smoke.sh`: Now fails if error banner is detected (regression detection)

**How to Debug (if banner still appears):**
```bash
# Check if user is authenticated
# DevTools > Application > Local Storage > Check for access_token

# Check user metadata contains agency_id
# DevTools > Console:
# const user = JSON.parse(localStorage.getItem('user'));
# console.log(user.user_metadata?.agency_id);

# If agency_id is missing but user is authenticated:
# Check backend: user's team_members entry should have agency_id set
psql $DATABASE_URL -c "SELECT user_id, agency_id, role FROM team_members WHERE user_id = '<user-id>';"
```

**Solution:**
- If not authenticated: Login at /login (should auto-redirect)
- If authenticated but no agency_id in metadata: Contact admin to assign user to agency via team_members table
- If backend returns 400/403: Check team_members table and backend logs for agency resolution errors

**Verification:**
```bash
# After fix deployed, smoke script should pass without detecting error banner
ADMIN_URL="https://admin.fewo.kolibri-visions.de" \
EXPECTED_COMMIT="<commit-sha>" \
./frontend/scripts/pms_admin_amenities_ui_smoke.sh

# Test 3 should PASS without "Keine Agentur-ID gefunden" error
```

### Build Fix: TypeScript Arguments Mismatch (2026-01-22)

**Symptom:** Coolify deploy failing during `npm run build` with TypeScript error:
```
./app/properties/[id]/page.tsx:169:9
Type error: Expected 1-2 arguments, but got 3.
```

**Root Cause:** `apiClient.get()` and `apiClient.put()` signatures only accepted (endpoint, token) and (endpoint, body, token), but properties page was calling with additional headers parameter for x-agency-id injection.

**Fix Applied:** Extended API client signatures in `frontend/app/lib/api-client.ts` to accept optional headers as last parameter:
- `get(endpoint, token?, headers?)`
- `put(endpoint, body?, token?, headers?)`
- `post(endpoint, body?, token?, headers?)`
- `patch(endpoint, body?, token?, headers?)`
- `delete(endpoint, token?, headers?)`

**Backward Compatible:** All existing call sites continue to work (headers optional).

**Verification After Deploy:**
```bash
# Verify build succeeds in Coolify logs
# Expected: "Build completed successfully" (no TypeScript errors)

# Verify deployment
cd /data/repos/pms-webapp && git fetch origin main && git reset --hard origin/main
./backend/scripts/pms_verify_deploy.sh
echo "rc=$?"

# Verify amenities UI still works
ADMIN_URL="https://admin.fewo.kolibri-visions.de" ./frontend/scripts/pms_admin_amenities_ui_smoke.sh
echo "rc=$?"
```

### UI Smoke Script Hardening (2026-01-22): Next.js App Router + Redirects

**Issue:** Original smoke script expected SSR-visible strings ("Ausstattung" heading, nav text, buttons) in HTML but Next.js App Router uses client-side rendering. Script also failed on 307 redirects from auth middleware instead of treating them as PASS (protected routes working correctly).

**Fix Applied:** Rewrote smoke script to be PROD-safe and handle Next.js CSR + auth redirects:
- **Follow Redirects:** Uses `-L` flag to follow redirects and detect final landing page
- **Cache-Busting:** Adds `?cb=timestamp` and `Cache-Control: no-cache` to avoid CDN caching
- **Next.js Markers:** Checks for `id="__next"`, `/_next/static/`, `__NEXT_DATA__`, `<html>` (at least 2 required)
- **Auth Redirect PASS:** Treats 30x redirects to `/login`, `/auth`, `/sign-in` as PASS (protected route exists, middleware works)
- **Commit Verification:** Uses `/api/ops/version` instead of Docker inspect (more reliable)
- **Weak Route Identity:** Optionally checks for "amenities" text but doesn't fail if missing (CSR bundles may not have readable strings)

**PASS Criteria:**
- **HTTP 200** with at least 2 Next.js markers = App deployed correctly
- **HTTP 30x** to login/auth route = Protected route exists, middleware works
- **All other status codes** = FAIL

**Limitations (curl-only smoke without browser):**
- Cannot verify client-side JavaScript execution
- Cannot test login flow or authenticated navigation
- Cannot verify modals, forms, or interactive elements
- Can only confirm pages exist and are Next.js apps OR are properly protected

**Verification Commands:**
```bash
# Run smoke test (recommended with EXPECTED_COMMIT for PROD verification)
ADMIN_URL="https://admin.fewo.kolibri-visions.de" \
EXPECTED_COMMIT="14349b2" \
./frontend/scripts/pms_admin_amenities_ui_smoke.sh
echo "rc=$?"

# Expected output: 5 tests PASS, rc=0
# Test 1: /api/ops/version commit match (if EXPECTED_COMMIT set)
# Test 2: /amenities page loads OR redirects to auth
# Test 3: /amenities content markers (if HTTP 200)
# Test 4: /properties page loads OR redirects to auth
# Test 5: Admin root page loads OR redirects

# Debug with temp files kept
KEEP_TEMP=true \
ADMIN_URL="https://admin.fewo.kolibri-visions.de" \
./frontend/scripts/pms_admin_amenities_ui_smoke.sh

# Temp files location will be printed for inspection
```

**Troubleshooting:**

*Test 1 SKIPPED (EXPECTED_COMMIT not set):*
- Not a failure, just a warning
- For PROD verification, always set EXPECTED_COMMIT to verify correct code is deployed

*Test 2-4 PASS with "redirects to auth":*
- This is CORRECT for protected routes without login cookies
- Means middleware is working and routes exist

*Test 2-4 FAIL with HTTP 4xx/5xx:*
- Check Coolify logs for deployment errors
- Verify admin URL is correct
- Check if admin service is running: `curl -I $ADMIN_URL`

*Test 3 FAIL with "missing Next.js markers":*
- Page returned 200 but is not a valid Next.js app page
- Check if static HTML was accidentally deployed instead of Next.js build
- Verify Coolify build used `npm run build` and served `.next/` output

---

## Central Rate Plans Page: Archived Plans Toggle Not Working

**Symptom:** On legacy central rate plans page (`/pricing/rate-plans`), toggling "Archivierte anzeigen" does not show/hide archived plans, or toggle state resets after page refresh.

**Expected Behavior:**
- Default view (no URL param): Toggle unchecked, archived plans hidden
- Toggle checked: URL updates to `?include_archived=1`, archived plans visible
- Page refresh: Toggle state persists via URL parameter

**How to Debug:**

1. **Check URL Parameter:**
   ```
   # URL when toggle ON should have:
   /pricing/rate-plans?include_archived=1
   
   # URL when toggle OFF should have:
   /pricing/rate-plans
   ```

2. **Check Browser DevTools Network Tab:**
   - Verify API calls include correct query parameter:
     - Toggle OFF: `GET /api/v1/pricing/rate-plans?property_id=<id>&include_archived=false&limit=100`
     - Toggle ON: `GET /api/v1/pricing/rate-plans?property_id=<id>&include_archived=true&limit=100`

3. **Check API Response:**
   - Verify response does not include archived plans when `include_archived=false`
   - Verify response includes archived plans when `include_archived=true`
   - Archived plans have `archived_at` field set to timestamp (not null)

**Common Issues:**

### Toggle Changes But Plans Don't Update

**Root Cause:** `useEffect` dependency on `showArchived` not triggering re-fetch.

**Solution:**
- Verify `useEffect` includes `showArchived` in dependency array (line ~164-170)
- Check `fetchPropertyPlans()` and `fetchTemplatePlans()` are called inside effect
- Clear browser cache and hard refresh (Cmd+Shift+R / Ctrl+F5)

### Toggle State Resets After Page Refresh

**Root Cause:** State initialization not reading from URL parameter.

**Solution:**
- Verify `useState` initialization reads from `useSearchParams()` (line ~74-77):
  ```tsx
  const [showArchived, setShowArchived] = useState(() => {
    const param = searchParams?.get("include_archived");
    return param === "true" || param === "1";
  });
  ```
- Check `useSearchParams()` is imported from `next/navigation`

### Archived Plans Still Show When Toggle OFF

**Root Cause:** Client-side filter not applied or API returning incorrect data.

**Solution:**
- Verify client-side filter exists (line ~743-747):
  ```tsx
  const displayedPlans = showArchived
    ? basePlans
    : basePlans.filter((plan) => !plan.archived_at);
  ```
- Check API backend filters by `archived_at IS NULL` when `include_archived=false`
- Verify `plan.archived_at` is correctly null for non-archived plans

### URL Parameter Not Updating When Toggle Changes

**Root Cause:** `handleToggleArchived` not calling `window.history.replaceState`.

**Solution:**
- Verify `handleToggleArchived` function (line ~245-256) includes:
  ```tsx
  const params = new URLSearchParams(window.location.search);
  if (checked) {
    params.set("include_archived", "1");
  } else {
    params.delete("include_archived");
  }
  const newUrl = params.toString() ? `${window.location.pathname}?${params.toString()}` : window.location.pathname;
  window.history.replaceState({}, "", newUrl);
  ```
- Check toggle checkbox uses `onChange={(e) => handleToggleArchived(e.target.checked)}`

**Diagnostic Commands:**
```bash
# BROWSER CONSOLE
# 1. Check current URL param
console.log(window.location.search);
// Expected when toggle ON: "?include_archived=1"
// Expected when toggle OFF: ""

# 2. Check component state (React DevTools)
# Find RatePlansPage component → check showArchived state
# Should match URL param

# 3. Test toggle manually
document.querySelector('input[type="checkbox"]').click();
// Watch network tab for API re-fetch with updated param
```

**Related Features:**
- P2.11.1: Property-specific rate plans page (`/properties/[id]/rate-plans`) has similar toggle but WITHOUT URL persistence
- This feature (P2 UI Polish) adds URL persistence to legacy central page

**Implementation Details:**
- File: `frontend/app/pricing/rate-plans/page.tsx`
- State initialization: Line ~74-77 (reads `include_archived` URL param)
- Toggle handler: Line ~245-256 (updates URL on change)
- Client-side filter: Line ~743-747 (defensive filter)
- API calls: Line ~207, 225 (includes `include_archived=${showArchived}`)

**Expected Timeline:**
- Default load → 0 archived plans visible
- Toggle ON → URL updates → API re-fetch → archived plans appear
- Toggle OFF → URL updates → API re-fetch → archived plans disappear
- Page refresh → URL param preserved → toggle state restored

**PROD Verification (2026-01-22):**
- Commit: 7325d6c35cc4aa513becf2b2dca511ff639098c9
- Backend API /api/v1/ops/version: source_commit match, started_at 2026-01-22T10:03:04.225833+00:00
- Admin UI /api/ops/version: source_commit match, started_at 2026-01-22T10:04:17.569Z
- Deploy: pms_verify_deploy.sh rc=0
- Manual UI testing: All verification checks passed at https://admin.fewo.kolibri-visions.de/pricing/rate-plans


---

## Central Rate Plans Page: Restore Archived Plans

**Overview:** Legacy central rate plans page (`/pricing/rate-plans`) now supports restoring archived rate plans via "Wiederherstellen" button.

**Purpose:** Allow staff to unarchive rate plans that were previously archived, restoring them to active use.

**Architecture:**
- **Route:** `/pricing/rate-plans`
- **Restore Endpoint:** `PATCH /api/v1/pricing/rate-plans/{id}/restore`
- **UI Pattern:** Same as property-specific rate plans page (context-sensitive actions based on `archived_at`)

**Behavior:**
- **Non-archived plans:** Show normal actions (Als Standard, Seasons, Bearbeiten, Archivieren)
- **Archived plans:** Show only "Wiederherstellen" button (green, replaces all other actions)
- **After restore:** Plan is unarchived (archived_at set to NULL), state refreshes automatically, plan appears in normal list
- **Restore success:** Toast message "Tarifplan erfolgreich wiederhergestellt"
- **Restore failure:** Toast message with error details

**Implementation Details:**
- File: `frontend/app/pricing/rate-plans/page.tsx`
- Restore state: Line ~73 (`confirmRestore` state similar to `confirmArchive`)
- Restore handler: Line ~418-436 (calls PATCH /restore endpoint)
- Restore button (property plans): Line ~928-934 (conditional on `plan.archived_at`)
- Restore button (templates): Line ~956-962 (conditional on `plan.archived_at`)
- Restore confirmation dialog: Line ~1115-1137 (similar to archive dialog)

**UI Flow:**
1. User toggles "Archivierte anzeigen" to show archived plans
2. Archived plans display with "archiviert" badge and only "Wiederherstellen" action
3. User clicks "Wiederherstellen" → Confirmation dialog appears
4. User confirms → PATCH /restore called → Toast success → List refreshes → Plan appears unarchived

**API Endpoint:**
```http
PATCH /api/v1/pricing/rate-plans/{id}/restore
Authorization: Bearer <token>
```

**Expected Response:**
```http
HTTP 204 No Content
```

**Restore Handler (UI):**
```tsx
const handleRestore = async (plan: RatePlan) => {
  if (!accessToken) return;

  try {
    await apiClient.patch(`/api/v1/pricing/rate-plans/${plan.id}/restore`, {}, accessToken);
    showToast(`${plan.name} erfolgreich wiederhergestellt`, "success");
    setConfirmRestore(null);
    // Refresh the appropriate list based on plan type
    if (plan.property_id) {
      fetchPropertyPlans();
    } else {
      fetchTemplatePlans();
    }
  } catch (error) {
    const message = error instanceof ApiError ? error.statusText : "Failed to restore rate plan";
    showToast(message, "error");
    setConfirmRestore(null);
  }
};
```

**Verification Commands:**
```bash
# BROWSER MANUAL TEST
# 1. Navigate to https://admin.fewo.kolibri-visions.de/pricing/rate-plans
# 2. Toggle "Archivierte anzeigen" ON
# 3. Find an archived plan (has "archiviert" badge)
# 4. Verify only "Wiederherstellen" button is shown (no other actions)
# 5. Click "Wiederherstellen" → Confirmation dialog appears
# 6. Click "Wiederherstellen" in dialog → Toast success appears
# 7. Plan disappears from list (if toggle OFF) or shows without "archiviert" badge
# 8. Toggle OFF → Plan should appear in normal list with all actions

# API TEST
export HOST="https://api.fewo.kolibri-visions.de"
export JWT_TOKEN="<manager/admin JWT>"
export RATE_PLAN_ID="<uuid of archived plan>"

# Restore archived plan
curl -X PATCH "$HOST/api/v1/pricing/rate-plans/$RATE_PLAN_ID/restore" \
  -H "Authorization: Bearer $JWT_TOKEN" \
  -w "\nHTTP %{http_code}\n"

# Expected: HTTP 204 No Content
# Verify: Plan's archived_at field is now NULL
```

**Common Issues:**

### Restore Button Not Showing for Archived Plans

**Symptom:** Archived plans don't show "Wiederherstellen" button.

**Root Cause:** Conditional rendering logic not checking `plan.archived_at`.

**Solution:**
- Verify conditional rendering (line ~928 for property plans, ~956 for templates):
  ```tsx
  {plan.archived_at ? (
    <button onClick={() => setConfirmRestore(plan)} className="text-green-600">
      Wiederherstellen
    </button>
  ) : (
    // Normal actions...
  )}
  ```
- Check `plan.archived_at` is correctly set in API response (timestamp or null)

### Restore Returns 404 Not Found

**Symptom:** Clicking "Wiederherstellen" shows 404 error.

**Root Cause:** Plan not found or restore endpoint path incorrect.

**Solution:**
- Verify plan ID is correct (check browser DevTools Network tab)
- Verify endpoint path is `/api/v1/pricing/rate-plans/{id}/restore` (not `/archived/{id}/restore`)
- Check plan exists in database: `SELECT id, archived_at FROM rate_plans WHERE id = '<uuid>'`

### List Doesn't Refresh After Restore

**Symptom:** Plan still shows as archived after successful restore.

**Root Cause:** List not re-fetched after restore.

**Solution:**
- Verify restore handler calls `fetchPropertyPlans()` or `fetchTemplatePlans()` based on plan type
- Clear browser cache and hard refresh (Cmd+Shift+R / Ctrl+F5)

**Related Features:**
- Property-specific rate plans page (`/properties/[id]/rate-plans`) already has restore functionality (implemented in P2.11.2)
- This feature adds restore to legacy central page for consistency


---

## P2 UI: Property Rate Plans — Archivierte Toggle URL-persistiert

**Overview:** Property-specific rate plans page with URL-persisted archived toggle.

**Purpose:** Enable property-level seasonal pricing with archived season visibility controlled by URL parameter, supporting bookmarkable/shareable links and proper browser back/forward navigation.

**Architecture:**
- **Route:** `/properties/[id]/rate-plans`
- **Query Param:** `include_archived` (values: `1` for show, omitted for hide)
- **URL State:** Managed via Next.js `useSearchParams` + `usePathname` + `router.replace`
- **Back/Forward:** `useEffect` syncs state with URL changes (browser navigation)
- **Default:** Archived seasons hidden (param absent)

**Implementation Details:**
- File: `frontend/app/properties/[id]/rate-plans/page.tsx`
- Imports: Line 4 (added `useSearchParams`, `usePathname`)
- URL read helper: Line 147-150 (`getShowArchivedFromUrl`)
- State init: Line 160 (reads `include_archived` URL param on mount)
- URL sync effect: Line 247-252 (handles back/forward navigation)
- Toggle handler: Line 255-268 (`handleToggleArchived` - updates state + URL)
- Toggle UI: Line 1287 (onClick calls `handleToggleArchived`)

**Behavior:**
1. **Load without param:** `/properties/[id]/rate-plans` → archived hidden, showArchived=false
2. **Load with param:** `/properties/[id]/rate-plans?include_archived=1` → archived visible, showArchived=true
3. **Toggle ON:** User clicks → state updates → URL becomes `...?include_archived=1` (shallow navigation, no reload)
4. **Toggle OFF:** User clicks → state updates → URL param removed (clean URL)
5. **Browser refresh:** State restored from URL param
6. **Back/Forward:** useEffect detects URL change → syncs state
7. **Other params:** Preserved (e.g., `?foo=bar&include_archived=1` works correctly)

**PROD Verification Steps:**

```bash
# 1. Deploy to PROD (Coolify auto-deploy from main branch)

# 2. Verify deployment
export API_BASE_URL="https://api.fewo.kolibri-visions.de"
./backend/scripts/pms_verify_deploy.sh
echo "rc=$?"

# 3. Manual UI verification at https://admin.fewo.kolibri-visions.de
# Test Case 1: Default state (archived hidden)
#   - Navigate to /properties/[id]/rate-plans (pick any property ID)
#   - Expected: Archived seasons NOT visible, URL has NO include_archived param
#   - Expected: Toggle switch is OFF (gray)

# Test Case 2: Toggle ON
#   - Click "Archivierte" toggle
#   - Expected: URL updates to ?include_archived=1 (NO page reload)
#   - Expected: Archived seasons become visible
#   - Expected: Toggle switch is ON (primary color)

# Test Case 3: Page refresh with param
#   - Browser refresh (Cmd+R / Ctrl+R)
#   - Expected: URL still has ?include_archived=1
#   - Expected: Archived seasons still visible
#   - Expected: Toggle switch still ON

# Test Case 4: Toggle OFF
#   - Click "Archivierte" toggle again
#   - Expected: URL param removed (?include_archived=1 disappears)
#   - Expected: Archived seasons disappear
#   - Expected: Toggle switch is OFF

# Test Case 5: Browser back/forward
#   - Click browser back button
#   - Expected: URL gets ?include_archived=1 again
#   - Expected: Toggle switch turns ON
#   - Expected: Archived seasons reappear
#   - Click browser forward button
#   - Expected: URL param removed
#   - Expected: Toggle switch turns OFF
#   - Expected: Archived seasons disappear

# Test Case 6: Direct URL with param
#   - Copy URL with ?include_archived=1
#   - Open in new tab or share with colleague
#   - Expected: Page loads with archived visible
#   - Expected: Toggle switch is ON

# Test Case 7: Other query params preserved
#   - Add custom param: /properties/[id]/rate-plans?foo=bar
#   - Toggle ON
#   - Expected: URL becomes ?foo=bar&include_archived=1 (both params present)
#   - Toggle OFF
#   - Expected: URL becomes ?foo=bar (only custom param remains)
```

**Common Issues:**

### Toggle Works But URL Doesn't Update

**Symptom:** Archived toggle changes UI but URL remains unchanged after clicking.

**Root Cause:** `handleToggleArchived` not called or `router.replace` not working.

**How to Debug:**
```bash
# Check browser console for errors
# Verify onClick handler in DevTools Elements tab
# Should be: onClick={() => handleToggleArchived(!showArchived)}
# NOT: onClick={() => setShowArchived(!showArchived)}

# Check implementation
rg -n "handleToggleArchived" frontend/app/properties/[id]/rate-plans/page.tsx
```

**Solution:**
- Verify toggle button onClick uses `handleToggleArchived` (line ~1287)
- Ensure `router.replace` is called in handler (line ~267)
- Check Next.js router is available (not in SSR context issue)

### State Doesn't Restore After Refresh

**Symptom:** Archived toggle resets to hidden after page refresh, even with `?include_archived=1` in URL.

**Root Cause:** `showArchived` state not reading from `searchParams` on mount.

**How to Debug:**
```bash
# Check state initialization
sed -n '147,160p' frontend/app/properties/[id]/rate-plans/page.tsx

# Should see:
# const getShowArchivedFromUrl = () => { ... }
# const [showArchived, setShowArchived] = useState(getShowArchivedFromUrl);
```

**Solution:**
- Verify state initialization uses lazy initializer: `useState(getShowArchivedFromUrl)`
- Ensure `getShowArchivedFromUrl` reads `searchParams?.get("include_archived")`
- Check `useSearchParams` is imported and called (line 4, line 140)

### Back/Forward Navigation Doesn't Update Toggle

**Symptom:** Browser back/forward button changes URL but toggle switch doesn't update.

**Root Cause:** Missing `useEffect` that syncs state with URL changes.

**How to Debug:**
```bash
# Check for URL sync effect
sed -n '247,252p' frontend/app/properties/[id]/rate-plans/page.tsx

# Should see:
# useEffect(() => {
#   const fromUrl = getShowArchivedFromUrl();
#   if (fromUrl !== showArchived) {
#     setShowArchived(fromUrl);
#   }
# }, [searchParams]);
```

**Solution:**
- Verify useEffect with `[searchParams]` dependency exists (line ~247-252)
- Ensure effect calls `getShowArchivedFromUrl()` and updates state
- Check for infinite loops (effect should only update if state differs from URL)

**PROD Verification (2026-01-22):**
- Commit: `aaea1a852ce4bc85b20fc60fc5f45b9bfe5630cd`
- Backend `/api/v1/ops/version`: source_commit match, started_at `2026-01-22T11:49:05.400906+00:00`
- Admin `/api/ops/version`: source_commit match, started_at `2026-01-22T11:46:58.647Z`, environment `production`
- Deploy verify: `pms_verify_deploy.sh rc=0`
- Manual UI testing: All 7 test cases passed at https://admin.fewo.kolibri-visions.de/properties/[id]/rate-plans
  - Default state: archived hidden, no URL param ✅
  - Toggle ON: URL gets `?include_archived=1` without reload ✅
  - Refresh: state preserved from URL ✅
  - Toggle OFF: URL param removed ✅
  - Back/Forward: state syncs with URL ✅
  - Direct URL with param: loads with archived visible ✅
  - Other query params preserved correctly ✅

---

---

## OpenAPI Gap Map (Generate + Interpret)

**Overview:** Automated OpenAPI schema analysis for identifying missing/incomplete modules.

**Purpose:** Generate a comprehensive gap analysis report by fetching the live OpenAPI schema from PROD and categorizing modules by completeness.

**Script Location:** `backend/scripts/openapi_gap_map.py`

**Output Location:** `backend/docs/ops/openapi_gap_map.md`

**Usage:**
```bash
# Generate gap map (read-only, GET requests only)
python3 backend/scripts/openapi_gap_map.py

# Output: backend/docs/ops/openapi_gap_map.md
```

**Report Structure:**
- **Summary:** Total paths, tags, module counts by status
- **Tags Overview Table:** All tags with endpoint counts, HTTP methods, status, priority
- **Detailed Module Analysis:** Categorized by Minimal (1-2 endpoints), Partial (3-7), Complete (8+)
- **Implementation Candidates:** High/medium priority modules needing expansion
- **Missing Common PMS Modules:** Identified gaps (Amenities, Reviews, Payments, etc.)

**Categories:**
- 🔴 **Minimal** (1-2 endpoints): High priority for expansion
- 🟡 **Partial** (3-7 endpoints): Medium priority, review for missing operations
- ✅ **Complete** (8+ endpoints): Low priority, good coverage

**How to Interpret:**
1. **Minimal modules** are candidates for next implementation (highest value-to-effort ratio)
2. **Missing modules** represent greenfield opportunities
3. **Partial modules** may need specific operations added (e.g., DELETE, PATCH)

**Example Output:**
```
| Tag | Endpoints | GET | POST | PATCH | DELETE | PUT | Status | Priority |
|-----|-----------|-----|------|-------|--------|-----|--------|----------|
| Amenities | 0 | 0 | 0 | 0 | 0 | 0 | ⚪ Missing | Critical |
| Branding | 2 | 1 | 0 | 0 | 0 | 1 | 🔴 Minimal | High |
```

**Verification:**
- Script only performs read-only operations (GET/HEAD requests)
- Does not modify database or PROD state
- Safe to run anytime for current API state snapshot

---

## Amenities — Smoke / Verification

**Overview:** Property amenities/features management module.

**Purpose:** Enable property owners to define and assign amenities (WiFi, Pool, Parking, etc.) to properties for display on booking sites and guest information.

**Architecture:**
- **Tables:** `amenities` (catalog), `property_amenities` (many-to-many junction)
- **RLS:** Tenant-scoped (agency_id), admin/manager write access
- **Categories:** general, kitchen, bathroom, bedroom, outdoor, entertainment, safety, accessibility

**API Endpoints:**

Admin/Manager:
- `GET /api/v1/amenities?category=&limit=&offset=` - List amenities for agency
- `POST /api/v1/amenities` - Create amenity
- `GET /api/v1/amenities/{amenity_id}` - Get amenity by ID
- `PATCH /api/v1/amenities/{amenity_id}` - Update amenity
- `DELETE /api/v1/amenities/{amenity_id}` - Delete amenity (cascade removes property assignments)

Property Assignment:
- `GET /api/v1/amenities/property/{property_id}` - Get property amenities
- `PUT /api/v1/amenities/property/{property_id}` - Replace property amenities (admin/manager)

**Important - Backend Update Semantics:**
- Backend supports **PATCH-only** for amenity updates: `PATCH /api/v1/amenities/{id}` → 200 OK
- Backend does **NOT** support PUT for amenity updates: `PUT /api/v1/amenities/{id}` → 405 Method Not Allowed
- This is by design (verified via smoke Test 7c)
- Smoke Test 9 uses PATCH for the update step to match backend semantics
- Internal Next.js proxy (`/api/internal/amenities/{id}`) accepts both PUT and PATCH from clients and forwards as PATCH to backend

**Database Schema:**

```sql
-- Amenities catalog
CREATE TABLE amenities (
  id uuid PRIMARY KEY,
  agency_id uuid REFERENCES agencies(id),
  name text NOT NULL,
  description text,
  category text,  -- general, kitchen, bathroom, etc.
  icon text,      -- icon identifier (wifi, pool, parking)
  sort_order int DEFAULT 0,
  created_at timestamptz,
  updated_at timestamptz,
  CONSTRAINT amenities_unique_per_agency UNIQUE (agency_id, name)
);

-- Property assignments
CREATE TABLE property_amenities (
  property_id uuid REFERENCES properties(id) ON DELETE CASCADE,
  amenity_id uuid REFERENCES amenities(id) ON DELETE CASCADE,
  created_at timestamptz,
  PRIMARY KEY (property_id, amenity_id)
);
```

**Smoke Test Script:** `backend/scripts/pms_amenities_smoke.sh`

**Required Environment Variables:**
- `HOST` - Backend base URL (e.g., https://api.fewo.kolibri-visions.de)
- `ADMIN_TOKEN` - JWT token with admin/manager role
- `PROPERTY_ID` - Property UUID for assignment tests

**Optional Environment Variables:**
- `AGENCY_ID` - Agency UUID (for x-agency-id header in multi-tenant setups)

**Usage:**
```bash
HOST=https://api.fewo.kolibri-visions.de \
ADMIN_TOKEN="eyJ..." \
PROPERTY_ID="<property-uuid>" \
./backend/scripts/pms_amenities_smoke.sh
```

**Expected Output:**
```
ℹ Test 1: Create amenity 'WiFi' (category: general)
✅ Test 1 PASSED: Created WiFi amenity (ID: ...)
ℹ Test 2: Create amenity 'Pool' (category: outdoor)
✅ Test 2 PASSED: Created Pool amenity (ID: ...)
...
ℹ Test 10: Cleanup - Delete WiFi amenity
✅ Test 10 PASSED: Deleted WiFi amenity (cleanup complete)

================================================================================
✅ All Amenities smoke tests passed! 🎉
================================================================================
  Tests passed: 10
  Tests failed: 0
```

**Test Coverage:**
1. Create amenity (WiFi, category: general)
2. Create amenity (Pool, category: outdoor)
3. List amenities (verify count >= 2)
4. Get amenity by ID (verify name = "WiFi")
5. Update amenity (change description)
6. Assign amenities to property (WiFi + Pool)
7. Get property amenities (verify count = 2)
8. Delete amenity (Pool) - cascade removes from property
9. Verify cascade delete (property has only WiFi)
10. Cleanup (delete WiFi amenity)

**Common Issues:**

### Table Not Found (503 Service Unavailable)

**Symptom:** API returns 503 with message "Amenities service temporarily unavailable".

**Root Cause:** Migration `20260122000000_add_amenities.sql` not applied to database.

**How to Debug:**
```bash
# Check if amenities table exists
psql $DATABASE_URL -c "\dt amenities"

# Check migration status
# (Supabase: check supabase_migrations table or Supabase dashboard)
```

**Solution:**
- Apply migration: `supabase migration up` (local) or via Supabase dashboard (PROD)
- Verify table exists before running smoke test

### Amenity Name Conflict (409 Conflict)

**Symptom:** POST /api/v1/amenities returns 409 with "Amenity with name '...' already exists for this agency".

**Root Cause:** Unique constraint `amenities_unique_per_agency` violated (duplicate name within same agency).

**Solution:**
- Use different amenity name
- Or DELETE existing amenity first
- Or PATCH existing amenity instead of creating duplicate

### Property Not Found (404 Not Found)

**Symptom:** PUT /api/v1/amenities/property/{id} returns 404 "Property not found".

**Root Cause:** Property ID does not exist or belongs to different agency.

**How to Debug:**
```bash
# Verify property exists and agency_id
psql $DATABASE_URL -c "SELECT id, agency_id, name FROM properties WHERE id = '<property-uuid>';"

# Check user's agency_id from JWT
echo $ADMIN_TOKEN | cut -d'.' -f2 | base64 -d | jq '.agency_id'
```

**Solution:**
- Use correct property ID from same agency
- Verify JWT agency_id matches property.agency_id

### Agency Context Not Available (400 Bad Request)

**Symptom:** API returns 400 with message "Agency context not available in token" or "Cannot resolve agency: user has N agency memberships. Please set x-agency-id header."

**Root Cause:** Standard Supabase JWT lacks custom `agency_id` claim. Backend requires agency context via `x-agency-id` header for multi-tenant isolation.

**How to Debug:**
```bash
# Check JWT structure (standard Supabase JWT has aud=authenticated, no custom claims)
echo $ADMIN_TOKEN | cut -d'.' -f2 | base64 -d | jq '.'

# Check user's agency memberships
export USER_ID=$(echo $ADMIN_TOKEN | cut -d'.' -f2 | base64 -d | jq -r '.sub')
psql $DATABASE_URL -c "SELECT agency_id, role FROM team_members WHERE user_id = '$USER_ID';"
```

**Solution (Single Membership):**
If user has exactly one agency membership, backend auto-detects agency. No x-agency-id header needed.

**Solution (Multiple Memberships):**
User must specify agency via x-agency-id header:
```bash
# Specify agency explicitly
HOST=https://api.fewo.kolibri-visions.de \
ADMIN_TOKEN="eyJ..." \
AGENCY_ID="<agency-uuid>" \
PROPERTY_ID="<property-uuid>" \
./backend/scripts/pms_amenities_smoke.sh
```

**Architecture Note:**
- Amenities endpoints use `get_current_agency_id` dependency which accepts x-agency-id header and validates via team_members table
- Role enforcement uses `require_roles("admin", "manager")` which queries team_members for DB-driven role checking
- RLS policies are intentionally permissive (USING (true)) since JWT lacks custom claims - backend handles all validation

### Smoke Script 401 Invalid Token

**Symptom:** Smoke script returns 401 "Invalid token" even with valid JWT.

**Root Cause:** Broken header building. Script tried to use newline-separated HEADERS variable which curl interprets as malformed header.

**How to Debug:**
```bash
# Test curl command directly
curl -X GET "https://api.fewo.kolibri-visions.de/api/v1/amenities?limit=10" \
  -H "Authorization: Bearer $ADMIN_TOKEN" \
  -H "x-agency-id: $AGENCY_ID" \
  -w "\n%{http_code}\n"

# Should return 200, not 401
```

**Solution:**
Updated smoke script uses `curl_with_auth` helper function which correctly passes multiple -H flags:
```bash
curl_with_auth() {
  local method="$1"
  local url="$2"
  shift 2

  if [ -n "$AGENCY_ID" ]; then
    curl -sS -w "\n%{http_code}" -X "$method" "$url" \
      -H "Authorization: Bearer ${ADMIN_TOKEN}" \
      -H "x-agency-id: ${AGENCY_ID}" \
      "$@"
  else
    curl -sS -w "\n%{http_code}" -X "$method" "$url" \
      -H "Authorization: Bearer ${ADMIN_TOKEN}" \
      "$@"
  fi
}
```

**Verification:**
Re-run smoke script after pulling latest code (commit: fix(p2): amenities tenant auth via x-agency-id + rls align + smoke headers).

### Smoke Script Idempotency

**Overview:** Smoke script is fully idempotent and can be run multiple times without manual cleanup.

**Features:**
- **Unique names per run**: Uses timestamp suffix (e.g., "WLAN - Smoke 1737548123", "Pool - Smoke 1737548123")
- **Automatic cleanup**: Trap ensures created amenities are deleted on EXIT (success or failure)
- **409 self-healing**: If amenity name already exists (edge case), script searches by exact name and reuses the ID
- **Shape-robust parsing**: Handles both list `[]` and paginated `{"items":[...]}` API responses

**How it works:**
```bash
# Each run generates unique names
TIMESTAMP=$(date +%s)
WIFI_NAME="WLAN - Smoke ${TIMESTAMP}"
POOL_NAME="Pool - Smoke ${TIMESTAMP}"

# Track created IDs
CREATED_AMENITY_IDS=()
CREATED_AMENITY_IDS+=("$AMENITY_WIFI_ID")

# Cleanup on exit (success or failure)
trap cleanup EXIT
cleanup() {
  for amenity_id in "${CREATED_AMENITY_IDS[@]}"; do
    DELETE /api/v1/amenities/${amenity_id}
  done
}
```

**Why this matters:**
- Safe to rerun after failures (no leftover test data)
- No manual cleanup needed
- Parallel runs don't conflict (unique timestamps)
- Works with UNIQUE(agency_id, name) constraint

### 409 Conflict Handling (Idempotent Reruns)

**Symptom:** If smoke script is interrupted (Ctrl+C, network failure) before cleanup, rerunning might encounter 409 Conflict if timestamp hasn't changed (same second).

**Root Cause:** UNIQUE(agency_id, name) constraint violated when trying to create amenity with same name.

**How Script Self-Heals:**
1. POST /api/v1/amenities returns 409 Conflict
2. Script logs warning: "409 Conflict (amenity exists), searching by name..."
3. Script calls `find_amenity_by_name()` to search existing amenities by exact name
4. If found: Reuses existing amenity ID and continues tests
5. If not found: Logs failure (unexpected - conflict but amenity not found)

**Example:**
```bash
# First run (interrupted before cleanup)
WIFI_NAME="WLAN - Smoke 1737548123"  # Created successfully

# Second run (same second, before cleanup)
WIFI_NAME="WLAN - Smoke 1737548123"  # 409 Conflict
# Script searches and finds existing ID, continues with tests
```

**Resolution:**
Script automatically resolves 409 by reusing existing amenity. No manual intervention needed.

**Prevention:**
Run cleanup manually if interrupted:
```bash
# List amenities with "- Smoke" suffix
curl -X GET "$HOST/api/v1/amenities?limit=1000" \
  -H "Authorization: Bearer $ADMIN_TOKEN" | jq '.[] | select(.name | contains("- Smoke"))'

# Delete by ID
curl -X DELETE "$HOST/api/v1/amenities/<amenity-id>" \
  -H "Authorization: Bearer $ADMIN_TOKEN"
```

---

### AdminShell Missing from Amenities Page (Layout Fix - 2026-01-22)

**Symptom:** /amenities page renders without left navigation sidebar and top header bar. Page content appears naked without AdminShell wrapper.

**Root Cause:** `/amenities` directory had no `layout.tsx` file. Other admin pages (properties, dashboard) have layout files that wrap children with AdminShell component using server-side auth.

**How It's Fixed (2026-01-22):**
- **Created Layout:** Added `frontend/app/amenities/layout.tsx` that wraps children with AdminShell
- **Server-Side Auth:** Layout uses `getAuthenticatedUser('/amenities')` for consistent auth check
- **Auth Enforcement:** Unauthenticated users are redirected to `/login?next=/amenities` via layout

**What Changed:**
- `frontend/app/amenities/layout.tsx` - NEW FILE, wraps page with AdminShell + server auth
- Pattern matches `/properties/layout.tsx` and `/dashboard/layout.tsx`

**Architecture Note:**
Each admin page directory must have its own `layout.tsx` to apply AdminShell wrapper. Next.js App Router does NOT use route groups like `(admin)` for shared layouts - each route segment explicitly declares its layout.

**How to Debug (if AdminShell still missing):**
```bash
# Check if layout.tsx exists
ls -la frontend/app/amenities/layout.tsx

# Expected: File should exist and contain AdminShell import + getAuthenticatedUser

# Check file contents
cat frontend/app/amenities/layout.tsx | grep -E '(AdminShell|getAuthenticatedUser)'

# Expected output:
# import AdminShell from "../components/AdminShell";
# import { getAuthenticatedUser } from "../lib/server-auth";
# const userData = await getAuthenticatedUser('/amenities');
# return <AdminShell ...>{children}</AdminShell>
```

**Solution (if layout missing):**
Create `frontend/app/amenities/layout.tsx` following this pattern:
```typescript
import AdminShell from "../components/AdminShell";
import { getAuthenticatedUser } from "../lib/server-auth";

export const dynamic = 'force-dynamic';
export const revalidate = 0;
export const fetchCache = 'force-no-store';

export default async function AmenitiesLayout({ children }: { children: React.ReactNode }) {
  const userData = await getAuthenticatedUser('/amenities');
  return <AdminShell userRole={userData.role} userName={userData.name} agencyName={userData.agencyName}>{children}</AdminShell>;
}
```

---

### Property Assignment Modal Loads Empty (Internal Proxy Routes - 2026-01-22)

**Symptom:** Property detail page amenities assignment modal shows "Keine Ausstattung verfügbar" even when amenities exist in catalog. Browser DevTools may show CORS errors or auth failures when calling backend API directly.

**Root Cause:** Property detail page was calling backend API directly from client-side:
- `GET /api/v1/amenities` with bearer token + x-agency-id header
- This exposed JWT tokens in browser (security risk)
- Required x-agency-id in user metadata (fails for single-tenant users)
- Client-side API calls don't have access to httpOnly session cookies

**How It's Fixed (2026-01-22):**
- **Internal Proxy Routes:** Created Next.js API routes that proxy to backend:
  - `GET /api/internal/amenities` → `GET /api/v1/amenities`
  - `GET /api/internal/properties/{id}/amenities` → `GET /api/v1/amenities/property/{id}`
  - `PUT /api/internal/properties/{id}/amenities` → `PUT /api/v1/amenities/property/{id}`
- **Server-Side Auth:** Internal routes use `createSupabaseServerClient()` to get session from cookies
- **Agency Resolution:** Internal routes auto-resolve agency_id from user metadata or team_members table
- **Token Security:** JWT tokens stay server-side, not exposed in browser

**What Changed:**
- `frontend/app/api/internal/amenities/route.ts` - NEW FILE, GET/POST amenities proxy
- `frontend/app/api/internal/amenities/[id]/route.ts` - NEW FILE, PUT/DELETE amenity by ID proxy
- `frontend/app/api/internal/properties/[id]/amenities/route.ts` - NEW FILE, GET/PUT property amenities proxy
- `frontend/app/properties/[id]/page.tsx` - Updated to use internal routes instead of direct backend calls

**Architecture:**
```
Browser Client
  ↓ fetch("/api/internal/amenities") - session cookie
Next.js API Route (server-side)
  ↓ createSupabaseServerClient() → get JWT from session
  ↓ fetch("https://api.../api/v1/amenities", {Authorization: Bearer <jwt>, x-agency-id: <agency>})
Backend API
```

**How to Debug (if modal still empty):**
```bash
# Check internal route exists
ls -la frontend/app/api/internal/amenities/route.ts
ls -la frontend/app/api/internal/properties/[id]/amenities/route.ts

# Expected: Files should exist

# Test internal route directly (requires authenticated session cookie)
curl -X GET "https://admin.fewo.kolibri-visions.de/api/internal/amenities" \
  -H "Cookie: <session-cookie>" \
  -v

# Expected: 200 OK with amenities array OR 401 if not authenticated

# Check browser DevTools Network tab
# Should see: GET /api/internal/amenities (NOT /api/v1/amenities)
# Should NOT see: x-agency-id header in request (handled server-side)
# Should NOT see: Authorization bearer token in request (uses session cookie)
```

**Solution (if modal still fails):**
1. Verify internal routes deployed: Check Next.js build includes API routes
2. Check browser console for errors: Look for 401/403 from internal routes
3. Verify Supabase session valid: User must be authenticated with active session
4. Test backend API directly: Use smoke script with ADMIN_TOKEN to verify backend works
5. Check team_members table: User must have agency_id set for agency resolution

**Verification:**
```bash
# Run smoke script with backend API testing enabled
HOST="https://api.fewo.kolibri-visions.de" \
ADMIN_TOKEN="<jwt>" \
AGENCY_ID="<agency-uuid>" \
PROPERTY_ID="<property-uuid>" \
ADMIN_URL="https://admin.fewo.kolibri-visions.de" \
./frontend/scripts/pms_admin_amenities_ui_smoke.sh

# Expected: Test 6 (backend amenities API) and Test 7 (property amenities API) both PASS
```


---

### Known Issue: Amenities "Erstellen" Button Does Nothing (Fixed 2026-01-22)

**Symptom:** In admin UI at /amenities, clicking "Erstellen" button in "Neue Ausstattung" modal does nothing. No POST request fired, modal stays open, no error/success toast shown.

**Root Cause:** Amenities page was calling backend API directly instead of using internal proxy routes:
- Required `agencyId` from `user.user_metadata.agency_id` before attempting create/update/delete
- For single-tenant users without explicit agency_id in metadata, `agencyId` was null
- handleCreate early return: `if (!accessToken || !agencyId) return;` blocked create action
- Direct backend API calls exposed JWT tokens in browser DevTools Network tab (security risk)

**What Was Broken:**
1. `handleCreate`: Required agencyId, called `/api/v1/amenities` directly with Bearer token + x-agency-id header
2. `handleUpdate`: Required agencyId, called `/api/v1/amenities/{id}` directly
3. `handleDelete`: Required agencyId, called `/api/v1/amenities/{id}` directly
4. `fetchAmenities`: Called `/api/v1/amenities` directly with Bearer token

**How It's Fixed (2026-01-22):**
All CRUD operations now use internal Next.js API proxy routes:
- **Create:** POST `/api/internal/amenities` (no agencyId check, uses session cookies)
- **Read:** GET `/api/internal/amenities` (no agencyId check, uses session cookies)
- **Update:** PUT `/api/internal/amenities/{id}` (no agencyId check, uses session cookies)
- **Delete:** DELETE `/api/internal/amenities/{id}` (no agencyId check, uses session cookies)

Internal proxy routes handle:
- Server-side auth via Supabase session cookies (no JWT exposed in browser)
- Agency resolution from user metadata or team_members table (auto-detect for single-tenant users)
- Proper error responses with actionable messages

**Files Changed:**
- `frontend/app/amenities/page.tsx`:
  - Removed `agencyId` requirement checks
  - Changed all fetch calls to use `/api/internal/amenities` endpoints
  - Removed Authorization + x-agency-id headers (internal proxy handles this)
  - Updated handleUpdate to use PUT method (matches internal proxy route)

**Verification Steps:**

Manual Browser Test:
```bash
# 1. Login to admin UI
https://admin.fewo.kolibri-visions.de/login

# 2. Navigate to /amenities
# Expected: Page loads with AdminShell (left nav + top bar)

# 3. Click "Neu" button
# Expected: "Neue Ausstattung" modal opens

# 4. Fill in form:
#    - Name: "Sauna - Test"
#    - Beschreibung: "Optional description"
#    - Kategorie: Select from dropdown (e.g., "Wellness")
#    - Icon: "sauna" (optional)
#    - Reihenfolge: 0

# 5. Click "Erstellen" button
# Expected:
#   - Modal closes
#   - Success toast: "Ausstattung erfolgreich erstellt"
#   - New row appears in amenities list
#   - DevTools Network tab shows: POST /api/internal/amenities (NOT /api/v1/amenities)
#   - No JWT token visible in Network tab request headers (uses session cookie)

# 6. Navigate to /properties, click a property
# 7. Scroll to "Ausstattung" section, click "Ausstattung zuweisen"
# Expected: Modal opens with list including "Sauna - Test"

# 8. Select "Sauna - Test", click "Speichern"
# Expected:
#   - Modal closes
#   - Success toast shown
#   - "Sauna - Test" appears as chip in property detail

# 9. Return to /amenities, delete "Sauna - Test"
# Expected: Row removed, success toast shown
```

Backend API Verification (curl):
```bash
# Verify internal proxy routes work
# (Requires authenticated session cookie - use browser DevTools to copy cookie)

# Create amenity via internal proxy
curl -X POST "https://admin.fewo.kolibri-visions.de/api/internal/amenities" \
  -H "Content-Type: application/json" \
  -H "Cookie: <session-cookie>" \
  -d '{"name":"Test Amenity","description":"Test","category":"general","sort_order":0}'

# Expected: 200 OK with amenity JSON

# List amenities via internal proxy
curl -X GET "https://admin.fewo.kolibri-visions.de/api/internal/amenities" \
  -H "Cookie: <session-cookie>"

# Expected: 200 OK with amenities array including "Test Amenity"
```

**Related Issues:**
- This fix also resolves "Property Assignment Modal Loads Empty" issue (same root cause)
- See: "Property Assignment Modal Loads Empty (Internal Proxy Routes - 2026-01-22)" section above

**Status:** ✅ Fixed in commit bffd54b (internal proxy routes) + follow-up fix (this section)



---

### Amenities Edit Returns 405 Method Not Allowed (Troubleshooting 2026-01-22)

**Symptom:** In admin UI at /amenities, clicking edit pencil icon, modifying fields, and clicking "Speichern" shows error toast. Browser DevTools Network tab shows:
```
PUT /api/internal/amenities/<uuid> → 405 Method Not Allowed
```

**Root Cause - Possible Scenarios:**

1. **Next.js dev server not restarted after route handler creation:**
   - Internal proxy route file exists but dev server cached old routing table
   - Hot reload doesn't always pick up new API route files

2. **Production build missing route handler:**
   - Route handler file exists locally but wasn't deployed to production
   - Build process may have excluded the file
   - Static export or caching issue

3. **Route handler not exporting PUT method:**
   - File `frontend/app/api/internal/amenities/[id]/route.ts` exists but missing `export async function PUT`
   - Typo in function name or incorrect export syntax

4. **Middleware blocking PUT requests:**
   - Next.js middleware (middleware.ts) may be rewriting or blocking PUT to `/api/internal/*`

5. **Browser caching old 405 response:**
   - Browser cached previous 405 error before route handler was fixed
   - Hard refresh not clearing cache

**How to Debug:**

```bash
# 1. Verify route handler file exists locally
ls -la frontend/app/api/internal/amenities/[id]/route.ts

# Expected: File should exist with PUT and DELETE exports

# 2. Check file content for PUT export
grep -n "export async function PUT" frontend/app/api/internal/amenities/[id]/route.ts

# Expected output:
# 54:export async function PUT(

# 3. Verify Next.js build recognizes route
cd frontend && npm run build 2>&1 | grep "api/internal/amenities/\[id\]"

# Expected output:
# ├ λ /api/internal/amenities/[id]             0 B                0 B

# 4. Test PUT request locally (requires dev server running + authenticated session)
curl -X PUT "http://localhost:3000/api/internal/amenities/test-uuid" \
  -H "Content-Type: application/json" \
  -H "Cookie: <session-cookie>" \
  -d '{"name":"Test","description":"Test","category":"general"}' \
  -v

# Expected: Should NOT return 405. May return 401 (no session) or other error.
# If returns 405: Route handler not loaded or not exporting PUT

# 5. Check production deployment
curl -X PUT "https://admin.fewo.kolibri-visions.de/api/internal/amenities/test-uuid" \
  -H "Content-Type: application/json" \
  -H "Cookie: <session-cookie>" \
  -d '{"name":"Test"}' \
  -v 2>&1 | grep "< HTTP"

# Expected: 401 (if not authenticated) or 404 (if amenity doesn't exist)
# If returns 405: Route not deployed or build issue
```

**Solution (Step-by-Step):**

**If Local Dev Server:**
```bash
# 1. Verify file content is correct
cat frontend/app/api/internal/amenities/[id]/route.ts | grep -A 5 "export async function PUT"

# Should show:
# export async function PUT(
#   request: NextRequest,
#   { params }: { params: { id: string } }
# ) {

# 2. Restart Next.js dev server
cd frontend
# Kill existing process (Ctrl+C or pkill -f "next dev")
npm run dev

# Wait for server to fully start, then test edit operation
```

**If Production Deployment:**
```bash
# 1. Verify latest commit includes route handler
git log --oneline --all -10 | grep -E "(internal proxy|amenities)"

# Should show commits:
# bffd54b fix(p2): amenities property assignment loads catalog + internal proxy ...
# 8f49f1e fix(p2): amenities create modal submit wiring + internal proxy ...

# 2. Verify route handler in git
git show HEAD:frontend/app/api/internal/amenities/[id]/route.ts | head -60

# Should show file with PUT export at line ~54

# 3. Redeploy to production (Coolify or manual)
# - In Coolify: Go to application → Redeploy
# - Or: git pull on server + rebuild

# 4. Verify deploy completed
API_BASE_URL="https://api.fewo.kolibri-visions.de" ./backend/scripts/pms_verify_deploy.sh

# 5. Test CRUD cycle with smoke script
HOST="https://api.fewo.kolibri-visions.de" \
ADMIN_TOKEN="<jwt>" \
AGENCY_ID="<agency-uuid>" \
ADMIN_URL="https://admin.fewo.kolibri-visions.de" \
./frontend/scripts/pms_admin_amenities_ui_smoke.sh

# Expected: Test 8 (CRUD cycle) PASSES - this specifically tests PUT endpoint
```

**If Route Handler Missing PUT Export (Code Fix):**
```bash
# Edit frontend/app/api/internal/amenities/[id]/route.ts
# Ensure it has:

export async function PUT(
  request: NextRequest,
  { params }: { params: { id: string } }
) {
  const supabase = await createSupabaseServerClient();

  const {
    data: { session },
    error: sessionError,
  } = await supabase.auth.getSession();

  if (!session || sessionError) {
    return NextResponse.json(
      { detail: 'Unauthorized: No valid session' },
      { status: 401 }
    );
  }

  const accessToken = session.access_token;
  const agencyId = await getAgencyIdFromSession();
  const amenityId = params.id;

  const body = await request.json();

  const headers: Record<string, string> = {
    'Authorization': `Bearer ${accessToken}`,
    'Content-Type': 'application/json',
  };

  if (agencyId) {
    headers['x-agency-id'] = agencyId;
  }

  try {
    const response = await fetch(`${API_BASE_URL}/api/v1/amenities/${amenityId}`, {
      method: 'PUT',
      headers,
      body: JSON.stringify(body),
    });

    const data = await response.json();

    return NextResponse.json(data, {
      status: response.status,
      headers: {
        'Content-Type': 'application/json',
      },
    });
  } catch (error) {
    console.error(`Internal API proxy error (PUT /amenities/${amenityId}):`, error);
    return NextResponse.json(
      { detail: 'Internal proxy error' },
      { status: 500 }
    );
  }
}

# After adding/fixing PUT export:
# - Restart dev server (if local)
# - Commit and redeploy (if production)
# - Run smoke script Test 8 to verify
```

**Browser Cache Clear:**
```bash
# 1. Open browser DevTools (F12)
# 2. Right-click Refresh button → "Empty Cache and Hard Reload"
# 3. Or: DevTools → Network tab → "Disable cache" checkbox → reload page
# 4. Try edit operation again
```

**Verification After Fix:**
```bash
# Run smoke script with CRUD test enabled
HOST="https://api.fewo.kolibri-visions.de" \
ADMIN_TOKEN="<jwt>" \
ADMIN_URL="https://admin.fewo.kolibri-visions.de" \
./frontend/scripts/pms_admin_amenities_ui_smoke.sh

# Expected output for Test 8:
# Test 8: Backend API CRUD cycle (create/update/delete amenity)...
#   Step 1/4: Creating test amenity 'SMOKE_TEST_...'...
#     ✓ Created amenity with ID: <uuid>
#   Step 2/4: Updating amenity <uuid> (verify PUT works, not 405)...
#     ✓ Update succeeded, description changed (HTTP 200)
#   Step 3/4: Verifying update persisted...
#     ✓ Update verified (description persisted)
#   Step 4/4: Cleaning up (deleting test amenity)...
#     ✓ Test amenity deleted (HTTP 204)
# ✅ Test 8 PASSED: CRUD cycle completed (create → update via PUT → verify → delete)

# If Test 8 still fails on Step 2 with "Update returned 405 Method Not Allowed":
# - Route handler still not deployed or dev server not restarted
# - Check Next.js build output again
# - Verify route file committed and pushed
```

**Related Files:**
- Route handler: `frontend/app/api/internal/amenities/[id]/route.ts` (must have PUT export)
- UI component: `frontend/app/amenities/page.tsx` (calls PUT /api/internal/amenities/{id})
- Smoke script: `frontend/scripts/pms_admin_amenities_ui_smoke.sh` (Test 8 verifies PUT works)

**Expected State After Fix:**
- PUT /api/internal/amenities/{id} returns 200 (success) or 400/404 (validation/not found)
- PUT /api/internal/amenities/{id} NEVER returns 405 (route handler loaded and PUT exported)
- Edit operation in /amenities page works: modal closes, toast shows success, row updates
- DevTools Network tab shows: PUT /api/internal/amenities/{uuid} → 200 OK

**If Issue Persists:**
- Verify Next.js version is 14.x: `cat frontend/package.json | grep '"next"'`
- Check for conflicting routes: `find frontend/app/api -name "route.ts" | grep amenities`
- Review server logs for errors: Check Coolify logs or `npm run dev` output for runtime errors
- Test backend API directly: Verify backend /api/v1/amenities/{id} PUT works (bypass frontend)

**Status:** Added troubleshooting documentation + smoke Test 8 (CRUD cycle) to detect 405 regressions early


---

### Amenities Edit 405 (Internal Proxy Missing PUT)

**Symptom:** In admin UI at /amenities, editing an existing amenity and clicking "Speichern" fails. Browser console shows:
```
PUT https://admin.fewo.kolibri-visions.de/api/internal/amenities/<uuid> → 405 (Method Not Allowed)
```

No toast message appears, modal stays open, changes not persisted.

**Root Cause:** Next.js internal API route handler missing PUT export, or route not deployed/loaded.

**Confirmation Steps:**

1. **Browser Console Check:**
   ```javascript
   // Open DevTools → Console → Network tab
   // Click edit pencil → modify amenity → click "Speichern"
   // Look for: PUT /api/internal/amenities/<uuid>
   // Status: 405 = bug, 200/400/404 = route works
   ```

2. **Local File Check:**
   ```bash
   # Verify route file exists
   ls -la frontend/app/api/internal/amenities/[id]/route.ts
   
   # Verify PUT export exists
   grep -n "export async function PUT" frontend/app/api/internal/amenities/[id]/route.ts
   
   # Expected: Line ~54 should show: export async function PUT(
   ```

3. **Next.js Build Check:**
   ```bash
   cd frontend && npm run build 2>&1 | grep "api/internal/amenities/\[id\]"
   
   # Expected output:
   # ├ λ /api/internal/amenities/[id]             0 B                0 B
   # 
   # If missing: Route not recognized by Next.js
   ```

4. **Smoke Test Check:**
   ```bash
   # Run Test 6 (internal route 405 check)
   ADMIN_URL="https://admin.fewo.kolibri-visions.de" \
   ./frontend/scripts/pms_admin_amenities_ui_smoke.sh
   
   # Expected: Test 6 PASSED (returned 401/400/404, not 405)
   # If Test 6 FAILED: Route missing or not deployed
   ```

**Fix Summary:**

The route handler must exist at: `frontend/app/api/internal/amenities/[id]/route.ts`

And must export:
```typescript
export async function PUT(
  request: NextRequest,
  { params }: { params: { id: string } }
) {
  const supabase = await createSupabaseServerClient();
  const { data: { session }, error: sessionError } = await supabase.auth.getSession();
  if (!session || sessionError) {
    return NextResponse.json({ detail: 'Unauthorized: No valid session' }, { status: 401 });
  }

  const accessToken = session.access_token;
  const agencyId = await getAgencyIdFromSession();  // Reuse existing helper
  const amenityId = params.id;
  const body = await request.json();

  const headers: Record<string, string> = {
    'Authorization': `Bearer ${accessToken}`,
    'Content-Type': 'application/json',
  };
  if (agencyId) {
    headers['x-agency-id'] = agencyId;
  }

  const response = await fetch(`${API_BASE_URL}/api/v1/amenities/${amenityId}`, {
    method: 'PUT',
    headers,
    body: JSON.stringify(body),
  });

  const data = await response.json();
  return NextResponse.json(data, { status: response.status });
}
```

**Solution (Step-by-Step):**

**If Route File Missing:**
```bash
# Create the file with PUT + DELETE exports
# (See fix summary above for full implementation)
```

**If Route Exists But Dev Server Not Restarted:**
```bash
# Restart Next.js dev server
cd frontend
# Kill existing: Ctrl+C or pkill -f "next dev"
npm run dev

# Wait for server to start, then test edit operation
```

**If Deployed But Not Working (Production):**
```bash
# 1. Verify commit includes route handler
git log --oneline -10 | grep -E "(internal proxy|amenities)"

# Should show commits like:
# bffd54b fix(p2): amenities property assignment loads catalog + internal proxy ...

# 2. Verify file in deployed commit
git show HEAD:frontend/app/api/internal/amenities/[id]/route.ts | head -60

# Should show PUT export at line ~54

# 3. Redeploy (Coolify or manual)
# - Coolify: Go to application → Redeploy
# - Manual: git pull && npm run build

# 4. Run smoke test after deploy
ADMIN_URL="https://admin.fewo.kolibri-visions.de" \
./frontend/scripts/pms_admin_amenities_ui_smoke.sh

# Expected: Test 6 PASSED
```

**If Browser Cache:**
```bash
# 1. Open DevTools (F12)
# 2. Right-click Refresh → "Empty Cache and Hard Reload"
# 3. Or: DevTools → Network tab → "Disable cache" checkbox → reload
# 4. Try edit operation again
```

**Verification After Fix:**

1. **Smoke Test:**
   ```bash
   ADMIN_URL="https://admin.fewo.kolibri-visions.de" \
   ./frontend/scripts/pms_admin_amenities_ui_smoke.sh
   
   # Expected output for Test 6:
   # ℹ Test 6: Checking internal route does NOT return 405 (regression check)...
   #   HTTP code: 401
   # ✅ Test 6 PASSED: Internal route exists (returned 401, not 405)
   #   Note: 401 is expected without authentication - route handler is present
   ```

2. **Manual Browser Test:**
   ```bash
   # 1. Login at https://admin.fewo.kolibri-visions.de/amenities
   # 2. Click edit pencil on any amenity
   # 3. Change name or description
   # 4. Click "Speichern"
   # Expected:
   #   - Modal closes
   #   - Success toast: "Ausstattung erfolgreich aktualisiert"
   #   - Row updates in list
   #   - DevTools: PUT /api/internal/amenities/<uuid> → 200 OK (NOT 405)
   ```

**Related Issues:**
- "Amenities Create Button Does Nothing" (fixed in earlier commits)
- "Property Assignment Modal Loads Empty" (fixed in bffd54b - same internal proxy fix)

**Note:** Test 6 in UI smoke script specifically detects this regression by calling PUT to internal route without auth and verifying it returns 401/400/404 (route exists), NOT 405 (route missing).

**Status:** Fixed in commit bffd54b (route handler created). If regression occurs, check deployment or dev server restart needed.


---

### Amenities EDIT Save → 405 Method Not Allowed (Admin)

**Symptom:** In Admin UI at /amenities, editing an existing amenity and clicking "Speichern" fails silently. Browser DevTools Network tab shows:
```
PUT https://admin.fewo.kolibri-visions.de/api/internal/amenities/<uuid> → 405 (Method Not Allowed)
```

Modal stays open, no success toast, changes not persisted.

**Quick Diagnosis (curl):**

```bash
# Test if internal route accepts PUT (unauthenticated)
# Expected: 401 (no auth), 400 (bad request), 404 (not found) - NOT 405
AMENITY_ID="<any-uuid>"  # Can be dummy UUID
curl -k -sS -o /dev/null -w "%{http_code}" -X PUT \
  -H "Content-Type: application/json" \
  --data-binary '{"name":"test"}' \
  "https://admin.fewo.kolibri-visions.de/api/internal/amenities/$AMENITY_ID"

# If returns 405: Route handler missing PUT export or not deployed
# If returns 401/400/404: Route handler exists and is working (auth/validation fails as expected)
```

**Root Causes:**

1. **Route file not deployed or missing:**
   - File: `frontend/app/api/internal/amenities/[id]/route.ts`
   - Missing from production build or not committed to git

2. **PUT method not exported:**
   - Route file exists but `export async function PUT(...)` missing
   - Or typo in function name (e.g., `PATCH` instead of `PUT`)

3. **File permissions too restrictive:**
   - Directory/file not readable by Next.js server process
   - Check: `ls -la frontend/app/api/internal/amenities/[id]/`
   - Required: Directory 755 (`drwxr-xr-x`), file 644 (`-rw-r--r--`)

4. **Next.js build not recognizing route:**
   - Dynamic segment mismatch (file has `[id]` but code references different param)
   - Route shadowed by conflicting route in pages/api (App Router vs Pages Router conflict)

5. **Browser caching old 405 response:**
   - Previous request failed with 405, browser cached the error response

**Fix Steps:**

**Step 1: Verify route file exists and has correct structure**

```bash
# Check file exists
ls -la frontend/app/api/internal/amenities/[id]/route.ts

# Expected output:
# -rw-r--r--  1 user  group  4319 Jan 22 20:49 route.ts

# Verify PUT export exists
grep -n "export async function PUT" frontend/app/api/internal/amenities/[id]/route.ts

# Expected output:
# 54:export async function PUT(

# If file missing or PUT export missing: Route handler not implemented correctly
```

**Step 2: Fix file permissions if too restrictive**

```bash
# Check directory permissions
ls -ld frontend/app/api/internal/amenities/[id]

# Should be: drwxr-xr-x (755)
# If shows: drwx------ (700), fix with:
chmod 755 frontend/app/api/internal/amenities/[id]
chmod 644 frontend/app/api/internal/amenities/[id]/route.ts

# Commit permission changes
git add frontend/app/api/internal/amenities/[id]/route.ts
git status  # Verify file shows as modified (permissions changed)
```

**Step 3: Verify Next.js build recognizes route**

```bash
cd frontend
npm run build 2>&1 | grep "api/internal/amenities"

# Expected output:
# ├ λ /api/internal/amenities                  0 B                0 B
# ├ λ /api/internal/amenities/[id]             0 B                0 B

# If [id] route missing: Next.js not recognizing the route
```

**Step 4: Deploy to production**

```bash
# Commit and push
git add frontend/app/api/internal/amenities/[id]/route.ts
git commit -m "fix(p2): amenities edit save internal proxy PUT (no 405)"
git push origin main

# Trigger deploy (Coolify or manual)
# Wait for deployment to complete

# Verify deployment
cd /data/repos/pms-webapp  # On production server
git fetch origin main && git reset --hard origin/main
./backend/scripts/pms_verify_deploy.sh
```

**Step 5: Run smoke test**

```bash
# On production server or CI
ADMIN_URL="https://admin.fewo.kolibri-visions.de" \
HOST="https://api.fewo.kolibri-visions.de" \
ADMIN_TOKEN="<jwt-token>" \
AGENCY_ID="<agency-uuid>" \
./frontend/scripts/pms_admin_amenities_ui_smoke.sh

# Look for Test 7b output:
# ℹ Test 7b: Checking internal route PUT does NOT return 405 (with real ID)...
#   HTTP code: 401
# ✅ Test 7b PASSED: Internal route PUT handler exists (returned 401, not 405)

# If Test 7b FAILS with 405: Route still not deployed or permissions issue
```

**Step 6: Clear browser cache and test manually**

```bash
# Browser steps:
# 1. Open DevTools (F12)
# 2. Go to Network tab → "Disable cache" checkbox
# 3. Right-click Refresh → "Empty Cache and Hard Reload"
# 4. Navigate to https://admin.fewo.kolibri-visions.de/amenities
# 5. Click edit pencil on any amenity
# 6. Change name or description
# 7. Click "Speichern"

# Expected result:
# - Modal closes
# - Success toast: "Ausstattung erfolgreich aktualisiert"
# - Row updates in list
# - DevTools Network: PUT /api/internal/amenities/<uuid> → 200 OK (NOT 405)
```

**Complete Route Handler Code (for reference):**

If route handler is missing or incomplete, create/update `frontend/app/api/internal/amenities/[id]/route.ts`:

```typescript
import { NextRequest, NextResponse } from 'next/server';
import { createSupabaseServerClient } from '@/app/lib/supabase-server';

export const dynamic = 'force-dynamic';
export const revalidate = 0;
export const fetchCache = 'force-no-store';

const API_BASE_URL = process.env.NEXT_PUBLIC_API_BASE_URL || 'http://localhost:8000';

async function getAgencyIdFromSession(): Promise<string | null> {
  const supabase = await createSupabaseServerClient();
  const { data: { session }, error: sessionError } = await supabase.auth.getSession();
  if (!session || sessionError) return null;

  const userId = session.user.id;
  const agencyId = (session.user.user_metadata as any)?.agency_id;
  if (agencyId) return agencyId;

  const { data: teamMembers } = await supabase
    .from('team_members')
    .select('agency_id')
    .eq('user_id', userId)
    .eq('is_active', true);

  if (teamMembers && teamMembers.length === 1) {
    return teamMembers[0].agency_id;
  }
  return null;
}

export async function PUT(
  request: NextRequest,
  { params }: { params: { id: string } }
) {
  const supabase = await createSupabaseServerClient();
  const { data: { session }, error: sessionError } = await supabase.auth.getSession();

  if (!session || sessionError) {
    return NextResponse.json({ detail: 'Unauthorized: No valid session' }, { status: 401 });
  }

  const accessToken = session.access_token;
  const agencyId = await getAgencyIdFromSession();
  const amenityId = params.id;
  const body = await request.json();

  const headers: Record<string, string> = {
    'Authorization': `Bearer ${accessToken}`,
    'Content-Type': 'application/json',
  };
  if (agencyId) {
    headers['x-agency-id'] = agencyId;
  }

  const response = await fetch(`${API_BASE_URL}/api/v1/amenities/${amenityId}`, {
    method: 'PUT',
    headers,
    body: JSON.stringify(body),
  });

  const data = await response.json();
  return NextResponse.json(data, { status: response.status });
}

export async function DELETE(
  request: NextRequest,
  { params }: { params: { id: string } }
) {
  const supabase = await createSupabaseServerClient();
  const { data: { session }, error: sessionError } = await supabase.auth.getSession();

  if (!session || sessionError) {
    return NextResponse.json({ detail: 'Unauthorized: No valid session' }, { status: 401 });
  }

  const accessToken = session.access_token;
  const agencyId = await getAgencyIdFromSession();
  const amenityId = params.id;

  const headers: Record<string, string> = {
    'Authorization': `Bearer ${accessToken}`,
    'Content-Type': 'application/json',
  };
  if (agencyId) {
    headers['x-agency-id'] = agencyId;
  }

  const response = await fetch(`${API_BASE_URL}/api/v1/amenities/${amenityId}`, {
    method: 'DELETE',
    headers,
  });

  if (response.status === 204) {
    return new NextResponse(null, { status: 204 });
  }

  const data = await response.json();
  return NextResponse.json(data, { status: response.status });
}
```

**Related Issues:**
- "Amenities Create Button Does Nothing" (fixed in commit 8f49f1e)
- "Property Assignment Modal Loads Empty" (fixed in commit bffd54b)

**Status:** Fixed by ensuring correct file permissions (755/644) and verifying route handler deployment. Test 7b in smoke script detects this regression.


---

### Amenities Edit 405 (Method Not Allowed) - Route Handler Missing Method

**Symptom:** In Admin UI at /amenities, editing an existing amenity and clicking "Speichern" returns 405 Method Not Allowed. Browser DevTools Network tab shows:
```
PUT https://admin.fewo.kolibri-visions.de/api/internal/amenities/<uuid> → 405 (Method Not Allowed)
```

Response headers contain Next.js/RSC markers. Modal stays open, no success toast, changes not persisted.

**What 405 Means:**
405 from Next.js indicates:
1. Route handler file exists BUT does not export the requested HTTP method (PUT/PATCH)
2. Route conflict: Pages Router `/pages/api/...` overrides App Router `/app/api/...` for same path
3. Method name typo (e.g., `put` instead of `PUT` - must be uppercase)

**Quick Diagnosis (curl):**

```bash
# Test if internal route accepts PUT (unauthenticated, no cookies)
# Expected: 401 (no auth), 400 (bad request), 302 (redirect) - NOT 405
AMENITY_ID="<any-uuid-from-backend>"
curl -k -sS -o /dev/null -w "%{http_code}\n" -X PUT \
  -H "Content-Type: application/json" \
  --data-binary '{"name":"test"}' \
  "https://admin.fewo.kolibri-visions.de/api/internal/amenities/$AMENITY_ID"

# Result interpretation:
# 405: Route handler missing PUT export → FIX REQUIRED
# 401/302: Route exists, auth required → WORKING (no fix needed, just auth missing)
# 400: Route exists, validation fails → WORKING (route present)
```

**Root Causes:**

1. **Missing PUT/PATCH export in route handler:**
   - File: `frontend/app/api/internal/amenities/[id]/route.ts`
   - Missing: `export async function PUT(...)` or `export async function PATCH(...)`
   - Uppercase required: `PUT` not `put`

2. **Pages Router conflict (legacy API routes):**
   - If `frontend/pages/api/internal/amenities/[id].ts` exists, it can override App Router
   - Check: `ls frontend/pages/api/ 2>/dev/null`
   - If exists: Remove legacy route or update to support PUT

3. **Method mismatch (UI vs route handler):**
   - UI sends PUT but route only exports PATCH (or vice versa)
   - Fix: Export both PUT and PATCH as aliases

**Fix Steps:**

**Step 1: Verify route file exists and check exports**

```bash
# Check file exists
ls -la frontend/app/api/internal/amenities/[id]/route.ts

# Verify PUT/PATCH exports
grep -n "export async function" frontend/app/api/internal/amenities/[id]/route.ts

# Expected output should include:
# 54:export async function PUT(
# 115:export async function PATCH(  (optional but recommended)
# 124:export async function DELETE(
```

**Step 2: Check for Pages Router conflicts**

```bash
# Check if legacy Pages Router API exists
ls -R frontend/pages/api/internal/amenities/ 2>/dev/null

# If files exist: This is the conflict!
# Solution: Remove or rename legacy route, prefer App Router
```

**Step 3: Ensure route handler exports PUT and PATCH**

If missing, update `frontend/app/api/internal/amenities/[id]/route.ts`:

```typescript
// PUT /api/internal/amenities/:id - Update amenity
export async function PUT(
  request: NextRequest,
  { params }: { params: { id: string } }
) {
  const supabase = await createSupabaseServerClient();
  const { data: { session }, error: sessionError } = await supabase.auth.getSession();

  if (!session || sessionError) {
    return NextResponse.json({ detail: 'Unauthorized: No valid session' }, { status: 401 });
  }

  const accessToken = session.access_token;
  const agencyId = await getAgencyIdFromSession();
  const amenityId = params.id;
  const body = await request.json();

  const headers: Record<string, string> = {
    'Authorization': `Bearer ${accessToken}`,
    'Content-Type': 'application/json',
  };
  if (agencyId) {
    headers['x-agency-id'] = agencyId;
  }

  const response = await fetch(`${API_BASE_URL}/api/v1/amenities/${amenityId}`, {
    method: 'PUT',
    headers,
    body: JSON.stringify(body),
  });

  const data = await response.json();
  return NextResponse.json(data, { status: response.status });
}

// PATCH /api/internal/amenities/:id - Update amenity (alias for PUT)
export async function PATCH(
  request: NextRequest,
  { params }: { params: { id: string } }
) {
  // PATCH is an alias for PUT - delegates to same handler logic
  return PUT(request, { params });
}

export async function DELETE(...) { ... }
```

**Step 4: Verify UI uses correct method and credentials**

In `frontend/app/amenities/page.tsx` handleUpdate function:

```typescript
const response = await fetch(`/api/internal/amenities/${editingAmenity.id}`, {
  method: "PUT",  // or "PATCH" - both should work now
  headers: {
    "Content-Type": "application/json",
  },
  body: JSON.stringify(formData),
  credentials: "include",  // Ensure session cookies are sent
});
```

**Step 5: Deploy and verify**

```bash
# Commit and push
git add frontend/app/api/internal/amenities/[id]/route.ts
git add frontend/app/amenities/page.tsx
git commit -m "fix(p2): amenities edit save internal route supports PUT/PATCH (no 405)"
git push origin main

# Deploy (Coolify or manual)
# On production server:
cd /data/repos/pms-webapp
git fetch origin main && git reset --hard origin/main
./backend/scripts/pms_verify_deploy.sh

# Run smoke test
ADMIN_URL="https://admin.fewo.kolibri-visions.de" \
HOST="https://api.fewo.kolibri-visions.de" \
ADMIN_TOKEN="<jwt>" \
AGENCY_ID="<agency-uuid>" \
./frontend/scripts/pms_admin_amenities_ui_smoke.sh

# Look for Test 7b:
# ✅ Test 7b PASSED: Internal route PUT handler exists (returned 401, not 405)
```

**Step 6: Manual browser verification**

```bash
# 1. Login at https://admin.fewo.kolibri-visions.de/amenities
# 2. Click edit pencil on any amenity
# 3. Change name or description
# 4. Click "Speichern"

# Expected results:
# - Modal closes
# - Success toast: "Ausstattung erfolgreich aktualisiert"
# - Row updates in list
# - DevTools Network: PUT /api/internal/amenities/<uuid> → 200/204 (NOT 405)
```

**Canonical Route File:**
- App Router: `frontend/app/api/internal/amenities/[id]/route.ts` (MUST export PUT/PATCH/DELETE)
- Pages Router: Should NOT exist for this path (conflict if present)

**Related Issues:**
- "Amenities Create Button Does Nothing" (fixed: POST to internal route)
- "Property Assignment Modal Loads Empty" (fixed: internal proxy routes)

**Smoke Test:**
Test 7b in `pms_admin_amenities_ui_smoke.sh` specifically detects this regression:
- Extracts real amenity ID from backend API
- Performs unauthenticated PUT to internal route
- FAIL on 405 (route missing method)
- PASS on 401/400/302 (route exists, auth/validation fails as expected)

**Status:** Fixed by ensuring route handler exports both PUT (primary) and PATCH (alias), and UI includes `credentials: "include"`.

---

### Amenities Edit Save 405 (Backend PATCH-Only Semantics)

**Symptom:** Clicking "Speichern" (Save) in amenities edit modal returns 405 Method Not Allowed. Browser DevTools shows `PUT /api/internal/amenities/{id}` or `PATCH /api/internal/amenities/{id}` returning 405.

**Root Cause:** Backend API only supports PATCH for amenity updates, not PUT:
- `PUT /api/v1/amenities/{id}` → **405 Method Not Allowed**
- `PATCH /api/v1/amenities/{id}` → **200 OK** (with valid auth + payload)

If the internal Next.js proxy forwards PUT as PUT to the backend, it will fail with 405. The proxy must map both PUT and PATCH client requests to PATCH when forwarding to the backend.

**How to Debug:**

```bash
# 1. Verify backend semantics (from HOST-SERVER-TERMINAL or locally)
export HOST="https://api.fewo.kolibri-visions.de"
export ADMIN_TOKEN="<your-jwt-token>"
export AGENCY_ID="<your-agency-id>"
export AMENITY_ID="<existing-amenity-id>"

# Test PUT (should return 405)
curl -X PUT "$HOST/api/v1/amenities/$AMENITY_ID" \
  -H "Authorization: Bearer $ADMIN_TOKEN" \
  -H "x-agency-id: $AGENCY_ID" \
  -H "Content-Type: application/json" \
  -d '{"name":"test","description":"test","category":"general","icon":"wifi","sort_order":10}' \
  -w "\nHTTP: %{http_code}\n"

# Expected: HTTP 405 (PUT not supported by backend)

# Test PATCH (should return 200 or 422)
curl -X PATCH "$HOST/api/v1/amenities/$AMENITY_ID" \
  -H "Authorization: Bearer $ADMIN_TOKEN" \
  -H "x-agency-id: $AGENCY_ID" \
  -H "Content-Type: application/json" \
  -d '{"name":"test","description":"test","category":"general","icon":"wifi","sort_order":10}' \
  -w "\nHTTP: %{http_code}\n"

# Expected: HTTP 200 (success) or 422 (validation error)

# 2. Check internal proxy route methods (should accept PUT and PATCH)
export ADMIN_URL="https://admin.fewo.kolibri-visions.de"

# Test PUT to internal route (unauthenticated, expect 401 not 405)
curl -X PUT "$ADMIN_URL/api/internal/amenities/$AMENITY_ID" \
  -H "Content-Type: application/json" \
  -d '{"name":"test"}' \
  -w "\nHTTP: %{http_code}\n"

# Expected: HTTP 401 (unauthorized) or 302 (redirect) - NOT 405

# Test PATCH to internal route (unauthenticated, expect 401 not 405)
curl -X PATCH "$ADMIN_URL/api/internal/amenities/$AMENITY_ID" \
  -H "Content-Type: application/json" \
  -d '{"name":"test"}' \
  -w "\nHTTP: %{http_code}\n"

# Expected: HTTP 401 (unauthorized) or 302 (redirect) - NOT 405

# If either returns 405: internal route handler missing method export or not deployed
```

**Solution:**

1. **Internal Proxy Route** (`frontend/app/api/internal/amenities/[id]/route.ts`):
   - Must export both `PUT` and `PATCH` functions
   - Both must forward to backend using **PATCH method**:

```typescript
// Shared helper that always uses PATCH when forwarding to backend
async function proxyUpdate(
  request: NextRequest,
  { params }: { params: { id: string } }
) {
  const supabase = await createSupabaseServerClient();
  const { data: { session } } = await supabase.auth.getSession();

  if (!session) {
    return NextResponse.json({ detail: 'Unauthorized' }, { status: 401 });
  }

  const accessToken = session.access_token;
  const agencyId = await getAgencyIdFromSession();
  const body = await request.json();

  const headers: Record<string, string> = {
    'Authorization': `Bearer ${accessToken}`,
    'Content-Type': 'application/json',
  };

  if (agencyId) {
    headers['x-agency-id'] = agencyId;
  }

  // Always use PATCH when forwarding to backend (backend only supports PATCH)
  const response = await fetch(`${API_BASE_URL}/api/v1/amenities/${params.id}`, {
    method: 'PATCH',  // <-- Critical: always PATCH, never PUT
    headers,
    body: JSON.stringify(body),
  });

  const data = await response.json();
  return NextResponse.json(data, { status: response.status });
}

// Export both PUT and PATCH - both delegate to proxyUpdate
export async function PUT(
  request: NextRequest,
  { params }: { params: { id: string } }
) {
  return proxyUpdate(request, { params });
}

export async function PATCH(
  request: NextRequest,
  { params }: { params: { id: string } }
) {
  return proxyUpdate(request, { params });
}
```

2. **UI Code** (`frontend/app/amenities/page.tsx`):
   - Use **PATCH** for edit save (cleaner, aligns with backend):

```typescript
const response = await fetch(`/api/internal/amenities/${editingAmenity.id}`, {
  method: "PATCH",  // <-- Use PATCH (backend semantics)
  headers: {
    "Content-Type": "application/json",
  },
  body: JSON.stringify(formData),
  credentials: "include",
});
```

3. **Deploy and Verify**:

```bash
# On HOST-SERVER-TERMINAL after deployment
cd /data/repos/pms-webapp
git fetch origin main && git reset --hard origin/main

# Run smoke script (Tests 7c + 7d verify semantics)
ADMIN_URL="https://admin.fewo.kolibri-visions.de" \
HOST="https://api.fewo.kolibri-visions.de" \
ADMIN_TOKEN="${MANAGER_JWT_TOKEN}" \
AGENCY_ID="ffd0123a-10b6-40cd-8ad5-66eee9757ab7" \
./frontend/scripts/pms_admin_amenities_ui_smoke.sh

# Expected output:
# ✅ Test 7c PASSED: Backend semantics verified (PUT=405, PATCH=200)
# ✅ Test 7d PASSED: Internal route PATCH handler exists (returned 401, not 405)
```

4. **Manual Browser Test**:
   - Login at https://admin.fewo.kolibri-visions.de/amenities
   - Open DevTools (F12) → Network tab
   - Hard refresh (Cmd+Shift+R / Ctrl+F5) to clear cached JS
   - Click edit pencil on any amenity
   - Change name or description
   - Click "Speichern"
   - **Expected:**
     - Network tab: `PATCH /api/internal/amenities/{id}` → **200 OK** (NOT 405)
     - Modal closes
     - Success toast: "Ausstattung erfolgreich aktualisiert"
     - Row updates in list

**Common Issues:**

- **405 persists after code fix**: Browser cached old JavaScript bundle. Hard refresh (Cmd+Shift+R / Ctrl+F5) or clear browser cache.
- **405 on internal route but backend works**: Route handler not deployed or file permissions issue (directory must be 755, file 644).
- **Backend returns 405 for PATCH**: Verify backend version supports PATCH (check `/api/ops/version` commit). Backend changed semantics at some point to PATCH-only.
- **Both PUT and PATCH return 405**: Internal route handler not exporting both methods or route path typo.

**Smoke Tests:**

- **Test 7c**: Verifies backend semantics (PUT=405, PATCH=200)
- **Test 7d**: Verifies internal route accepts PATCH (returns 401/400, not 405)
- **Test 7b**: Verifies internal route accepts PUT (returns 401/400, not 405)

**Related Issues:**
- "Amenities Edit Returns 405 Method Not Allowed" (multiple previous fixes)
- File permissions regression (commit 707b159)
- Missing PATCH alias (commit 5bb3b51)

**Fix Commit:** TBD (PUT→PATCH mapping in internal proxy)

**Verification Checklist:**
- [ ] Backend PUT returns 405, PATCH returns 200/422
- [ ] Internal route PUT returns 401/302 (not 405)
- [ ] Internal route PATCH returns 401/302 (not 405)
- [ ] Smoke Tests 7b, 7c, 7d all PASS
- [ ] Manual browser test: Edit → Speichern → 200 OK, modal closes, toast shows


---

## Admin UI: Buchungen - ROLLED BACK (Not Deployed)

**Status:** ❌ Feature rolled back on 2026-01-23

**Background:**
Admin Bookings UI with internal proxy routes was implemented in commit cd9680a but rolled back in commit 6ef53b6 before production deployment.

**Removed Components:**
- Internal proxy routes: `/api/internal/bookings` and `/api/internal/bookings/[id]`
- Enhanced UI pages with confirm/cancel actions
- Smoke test script: `frontend/scripts/pms_admin_bookings_ui_smoke.sh`

**Current State:**
- Bookings UI (`/bookings` and `/bookings/[id]`) uses direct backend API calls (pre-cd9680a behavior)
- No server-side session auth proxy for bookings endpoints
- No dedicated smoke test for bookings UI

**Documentation:**
- Full rollback details in `backend/docs/project_status.md` under "Admin UI: Buchungen - ROLLED BACK"
- Original implementation documentation was removed by rollback commit

**Note:** This section exists for historical tracking only. Feature may be re-implemented in the future.


---

## Admin UI: Buchungen (Bookings) - Smoke + Troubleshooting

**Status:** ✅ IMPLEMENTED (2026-01-23)

**Overview:**
The Admin Bookings UI provides a comprehensive interface for managing bookings with:
- **List View** (`/bookings`): Search/filter bookings by reference, property, guest, status
- **Detail View** (`/bookings/[id]`): View full booking details with confirm/cancel actions
- **Direct Backend API Integration**: Uses JWT-based auth from auth context (NOT internal proxy pattern)
- **Action Buttons**: Confirm (inquiry/pending → confirmed), Cancel (any non-terminal status → cancelled)

**Architecture Note:**
Unlike the Amenities UI which uses an internal Next.js API proxy (`/api/internal/*`), the Bookings UI makes **direct calls to the backend API** (`/api/v1/bookings/*`) using the JWT access token from the auth context. This architectural difference is intentional.

**Backend Endpoints:**
- `GET /api/v1/bookings` - List bookings with filters
- `GET /api/v1/bookings/{id}` - Get single booking details
- `PATCH /api/v1/bookings/{id}/status` - Update booking status (confirm)
- `POST /api/v1/bookings/{id}/cancel` - Cancel booking

**Smoke Test Commands:**

**1. Auth-Safe Mode (No Admin Token):**
```bash
# Verify UI pages are accessible (200 OK, not 404/500)
curl -s -o /dev/null -w "%{http_code}" https://admin.fewo.kolibri-visions.de/bookings
# Expected: 200 (page loads, shows auth-required message if not logged in)

curl -s -o /dev/null -w "%{http_code}" https://admin.fewo.kolibri-visions.de/bookings/00000000-0000-0000-0000-000000000000
# Expected: 200 (page loads, shows auth-required message if not logged in)
```

**2. Full Mode with Backend API (Requires Admin Token):**
```bash
# Set your admin JWT token
export ADMIN_TOKEN="eyJ..."

# Test 1: List bookings
curl -s -H "Authorization: Bearer $ADMIN_TOKEN" \
  https://api.fewo.kolibri-visions.de/api/v1/bookings?limit=10
# Expected: 200 OK with JSON array of bookings

# Test 2: Get single booking (replace with real booking ID)
BOOKING_ID="existing-booking-uuid"
curl -s -H "Authorization: Bearer $ADMIN_TOKEN" \
  https://api.fewo.kolibri-visions.de/api/v1/bookings/$BOOKING_ID
# Expected: 200 OK with booking JSON

# Test 3: Confirm action (status update)
curl -s -X PATCH -H "Authorization: Bearer $ADMIN_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"status": "confirmed"}' \
  https://api.fewo.kolibri-visions.de/api/v1/bookings/$BOOKING_ID/status
# Expected: 200 OK (if status is inquiry/pending) OR 400 (invalid transition)

# Test 4: Cancel action
curl -s -X POST -H "Authorization: Bearer $ADMIN_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"cancellation_reason": "Smoke test", "cancelled_by": "system"}' \
  https://api.fewo.kolibri-visions.de/api/v1/bookings/$BOOKING_ID/cancel
# Expected: 200 OK (if not already cancelled) OR 400 (already terminal status)
```

**Troubleshooting Common Issues:**

**Issue 1: 401 Unauthorized**
- **Symptom:** API returns 401 when listing or viewing bookings
- **Cause:** Session expired, JWT token invalid, or not logged in
- **Solution:**
  - Clear browser cookies and log in again
  - Check that JWT token in auth context is valid (not expired)
  - Verify backend `/api/ops/health` returns 200 (backend is running)

**Issue 2: 404 Not Found**
- **Symptom:** Detail page shows "Buchung nicht gefunden"
- **Cause:** Booking ID doesn't exist in database, or wrong agency
- **Solution:**
  - Verify booking ID exists: `SELECT id FROM bookings WHERE id = '<uuid>'`
  - Check agency multi-tenancy: Booking must belong to user's agency_id
  - Confirm booking wasn't deleted (soft delete check if applicable)

**Issue 3: 403 Forbidden (Insufficient Permissions)**
- **Symptom:** Cancel action returns "Keine Berechtigung. Nur Admins und Manager können stornieren."
- **Cause:** User role is 'staff', 'accountant', or 'owner' (not admin/manager)
- **Solution:**
  - Check user role in JWT: `SELECT role FROM users WHERE id = '<user_id>'`
  - Cancel action requires `admin` or `manager` role
  - Confirm action only requires `admin`, `manager`, or `staff` role

**Issue 4: Empty List (No Bookings in Database)**
- **Symptom:** List page shows "Keine Buchungen gefunden"
- **Cause:** No bookings exist for agency, or filters exclude all results
- **Solution:**
  - Check database: `SELECT COUNT(*) FROM bookings WHERE agency_id = '<uuid>'`
  - Clear search filters and status filter (set to "Alle Status")
  - Verify agency_id in JWT matches agency with bookings
  - Create test booking via Public Booking Request or Channel Manager sync

**Issue 5: Actions Not Available (Status Already Terminal)**
- **Symptom:** Confirm/Cancel buttons don't appear on detail page
- **Cause:** Booking status is terminal (cancelled, declined, checked_out)
- **Solution:**
  - Check current status: Terminal states = `cancelled`, `declined`, `checked_out`
  - Confirm button: Only shown for `inquiry` or `pending` status
  - Cancel button: Hidden for `cancelled`, `declined`, `checked_out`
  - This is expected behavior (cannot modify terminal bookings)

**Issue 6: 400 Bad Request (Invalid Status Transition)**
- **Symptom:** Confirm action fails with "Ungültiger Statusübergang"
- **Cause:** Status workflow doesn't allow transition (e.g., checked_out → confirmed)
- **Solution:**
  - Review status workflow in backend: `backend/app/api/routes/bookings.py`
  - Valid confirm transitions: `inquiry|pending → confirmed`
  - Check current booking status before attempting action

**Related Files:**

**Frontend (UI Pages):**
- `/Users/khaled/Documents/KI/Claude/Claude Code/Projekte/PMS-Webapp/frontend/app/bookings/page.tsx` - List view
- `/Users/khaled/Documents/KI/Claude/Claude Code/Projekte/PMS-Webapp/frontend/app/bookings/[id]/page.tsx` - Detail view with actions
- `/Users/khaled/Documents/KI/Claude/Claude Code/Projekte/PMS-Webapp/frontend/app/lib/api-client.ts` - Direct backend API client
- `/Users/khaled/Documents/KI/Claude/Claude Code/Projekte/PMS-Webapp/frontend/app/lib/auth-context.tsx` - JWT auth context provider

**Backend (API Endpoints):**
- `/Users/khaled/Documents/KI/Claude/Claude Code/Projekte/PMS-Webapp/backend/app/api/routes/bookings.py` - All booking endpoints
- `/Users/khaled/Documents/KI/Claude/Claude Code/Projekte/PMS-Webapp/backend/app/services/booking_service.py` - Business logic
- `/Users/khaled/Documents/KI/Claude/Claude Code/Projekte/PMS-Webapp/backend/app/schemas/bookings.py` - Request/response schemas

**Smoke Test (TBD):**
- Smoke script location: `frontend/scripts/pms_admin_bookings_ui_smoke.sh` (not yet created)
- When created, should test: List (200), Detail (200/404), Confirm (200/400), Cancel (200/400/403)

**Deployment Verification:**
```bash
# 1. Check backend API health
curl https://api.fewo.kolibri-visions.de/api/ops/health

# 2. Verify frontend pages load (200 OK)
curl -s -o /dev/null -w "%{http_code}" https://admin.fewo.kolibri-visions.de/bookings

# 3. Test backend endpoints (with admin token)
curl -H "Authorization: Bearer $ADMIN_TOKEN" \
  https://api.fewo.kolibri-visions.de/api/v1/bookings?limit=5

# 4. Manual browser test:
#    - Login at https://admin.fewo.kolibri-visions.de
#    - Navigate to /bookings
#    - Click on a booking to view details
#    - If status is inquiry/pending, test "Bestätigen" button
#    - If status is not terminal, test "Stornieren" button
```


---

## Admin UI: Buchungen - Manuell anlegen (Create Booking)

**Overview:**
Manual booking creation via Admin UI allows staff to create bookings directly through the frontend interface. This is typically used for phone bookings, walk-in reservations, or manual entry of bookings from external channels.

**Required Fields:**
- `property_id` (UUID) - Property being booked
- `check_in` (date) - Check-in date (YYYY-MM-DD format, cannot be in the past)
- `check_out` (date) - Check-out date (must be after check_in)
- `num_adults` (integer) - Number of adult guests (minimum: 1)
- `source` (string) - Booking source: direct, airbnb, booking_com, expedia, fewo_direkt, google, other

**Optional Fields:**
- `guest_id` (UUID) - Guest making the booking (defaults to null if not provided)
- `num_children` (integer) - Number of children (default: 0)
- `num_infants` (integer) - Number of infants (default: 0)
- `num_pets` (integer) - Number of pets (default: 0)
- `status` (string) - Booking status: inquiry, pending, confirmed, checked_in, checked_out, cancelled, declined, no_show (default: inquiry)
- `nightly_rate` (decimal) - Rate per night (defaults to property base_price if not provided)
- `subtotal` (decimal) - Subtotal before fees (computed from nightly_rate × num_nights if not provided)
- `cleaning_fee` (decimal) - Cleaning fee (default: 0)
- `service_fee` (decimal) - Service fee (default: 0)
- `extra_guest_fee` (decimal) - Extra guest fee (default: 0)
- `discount_amount` (decimal) - Discount amount (default: 0)
- `discount_code` (string) - Discount code applied
- `tax_amount` (decimal) - Tax amount (default: 0)
- `total_price` (decimal) - Total price (computed from subtotal + fees - discounts if not provided)
- `currency` (string) - Currency code (default: EUR)
- `cancellation_policy` (string) - flexible, moderate, strict, non_refundable
- `guest_message` (text) - Message from guest
- `special_requests` (text) - Special requests
- `internal_notes` (text) - Internal staff notes
- `booking_reference` (string) - Booking reference (auto-generated if not provided, format: PMS-YYYY-NNNNNN)
- `channel_booking_id` (string) - External booking ID from channel
- `channel_guest_id` (string) - External guest ID from channel

**API Endpoint:**
```
POST /api/v1/bookings
```

**Request Headers:**
```
Authorization: Bearer <JWT_TOKEN>
Content-Type: application/json
```

**Minimal Request Example:**
```json
{
  "property_id": "123e4567-e89b-12d3-a456-426614174000",
  "check_in": "2026-07-01",
  "check_out": "2026-07-08",
  "num_adults": 2,
  "source": "direct",
  "status": "inquiry"
}
```

**Full Request Example:**
```json
{
  "property_id": "123e4567-e89b-12d3-a456-426614174000",
  "guest_id": "123e4567-e89b-12d3-a456-426614174001",
  "check_in": "2026-07-01",
  "check_out": "2026-07-08",
  "num_adults": 2,
  "num_children": 1,
  "source": "direct",
  "status": "confirmed",
  "nightly_rate": "120.00",
  "subtotal": "840.00",
  "cleaning_fee": "50.00",
  "tax_amount": "62.30",
  "total_price": "952.30",
  "currency": "EUR",
  "cancellation_policy": "moderate",
  "guest_message": "Early check-in requested",
  "internal_notes": "VIP guest - prepare welcome basket"
}
```

**Success Response (201 Created):**
```json
{
  "id": "123e4567-e89b-12d3-a456-426614174002",
  "booking_reference": "PMS-2026-000123",
  "property_id": "123e4567-e89b-12d3-a456-426614174000",
  "guest_id": "123e4567-e89b-12d3-a456-426614174001",
  "check_in": "2026-07-01",
  "check_out": "2026-07-08",
  "status": "confirmed",
  "total_price": "952.30",
  "currency": "EUR",
  "created_at": "2026-01-23T10:30:00Z"
}
```

**Common Errors:**

**Error 400 - Bad Request (Validation Failed):**
- **Symptom:** HTTP 400 with validation error message
- **Common Causes:**
  - `check_in` date is in the past
  - `check_out` is not after `check_in`
  - `num_adults` is less than 1
  - Invalid `source` value (not one of: direct, airbnb, booking_com, expedia, fewo_direkt, google, other)
  - Invalid `status` value (not one of: inquiry, pending, confirmed, checked_in, checked_out, cancelled, declined, no_show)
  - Invalid UUID format for `property_id` or `guest_id`
  - Missing required fields
- **Solution:**
  - Check error message for specific validation failure
  - Ensure `check_in` is today or future date
  - Ensure `check_out` > `check_in`
  - Ensure `num_adults` >= 1
  - Use valid enum values for `source` and `status`
  - Verify UUIDs are properly formatted
  - Provide all required fields: property_id, check_in, check_out, num_adults, source

**Error 401 - Unauthorized:**
- **Symptom:** HTTP 401 "Invalid authentication token" or "Token expired"
- **Cause:** JWT token is missing, expired, or invalid
- **Solution:**
  - Re-login to get fresh JWT token
  - Check Authorization header: `Bearer <token>`
  - Verify token hasn't expired (default: 24h lifetime)

**Error 403 - Forbidden:**
- **Symptom:** HTTP 403 "Insufficient permissions"
- **Cause:** User role lacks permission to create bookings
- **RBAC Policy:** Only admin, manager, and staff roles can create bookings
- **Solution:**
  - Verify user role: `curl -H "Authorization: Bearer $TOKEN" $API_BASE/api/v1/users/me`
  - Owner, accountant roles cannot create bookings (by design)
  - Contact admin to upgrade role if needed

**Error 404 - Property Not Found:**
- **Symptom:** HTTP 404 "Property not found"
- **Cause:** `property_id` does not exist or belongs to different agency
- **Solution:**
  - Verify property exists: `GET /api/v1/properties/{property_id}`
  - Check property belongs to same agency as authenticated user
  - Confirm UUID is correct (no typos)

**Error 404 - Guest Not Found:**
- **Symptom:** HTTP 422 "guest_id does not reference an existing guest" (or HTTP 404)
- **Cause:** `guest_id` provided but does not exist in database
- **Solution:**
  - Omit `guest_id` field to create booking without guest (guest_id will be null)
  - Or create guest first: `POST /api/v1/guests`
  - Or verify guest exists: `GET /api/v1/guests/{guest_id}`
  - Confirm guest belongs to same agency as property

**Error 409 - Conflict (Double Booking):**
- **Symptom:** HTTP 409 with `"conflict_type": "inventory_overlap"` or `"conflict_type": "double_booking"`
- **Cause:** Property already has confirmed booking for overlapping dates
- **Root Cause:** Database exclusion constraint prevents overlapping bookings for same property
- **Solution:**
  - Check availability first: `GET /api/v1/availability?property_id=X&start_date=Y&end_date=Z`
  - If dates show as blocked, pick different dates
  - If status=inquiry bookings exist, they do NOT block (non-blocking status)
  - Only confirmed/checked_in bookings block availability
  - To override: Cancel conflicting booking first, then create new booking

**Error 503 - Service Unavailable:**
- **Symptom:** HTTP 503 "Database temporarily unavailable" or "Service unavailable"
- **Cause:** Backend database connection pool exhausted, maintenance mode, or backend crash
- **Solution:**
  - Wait 30 seconds and retry request
  - Check backend health: `GET /api/ops/health`
  - Check database connectivity from backend container
  - Review backend logs for connection errors: `docker logs pms-backend`
  - If persistent, restart backend: `docker-compose restart backend`

**Smoke Test Commands:**

Test booking creation + cancellation workflow:
```bash
# Set environment variables
export API_BASE="https://api.fewo.kolibri-visions.de"
export ADMIN_TOKEN="your-jwt-token-here"
export PROPERTY_ID="your-property-uuid"

# Step 1: Create booking
BOOKING_RESPONSE=$(curl -s -X POST "$API_BASE/api/v1/bookings" \
  -H "Authorization: Bearer $ADMIN_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "property_id": "'"$PROPERTY_ID"'",
    "check_in": "2026-12-01",
    "check_out": "2026-12-08",
    "num_adults": 2,
    "source": "direct",
    "status": "inquiry"
  }')

echo "Create Response: $BOOKING_RESPONSE"

# Extract booking ID (requires jq or python)
BOOKING_ID=$(echo "$BOOKING_RESPONSE" | python3 -c "import sys, json; print(json.load(sys.stdin)['id'])")
echo "Created Booking ID: $BOOKING_ID"

# Step 2: Verify booking was created
curl -s -H "Authorization: Bearer $ADMIN_TOKEN" \
  "$API_BASE/api/v1/bookings/$BOOKING_ID"

# Step 3: Cancel booking (cleanup)
curl -s -X POST "$API_BASE/api/v1/bookings/$BOOKING_ID/cancel" \
  -H "Authorization: Bearer $ADMIN_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{
    "cancellation_reason": "Smoke test cleanup"
  }'

echo "Booking created and cancelled successfully"
```

**Expected Results:**
- Step 1: HTTP 201 Created, response contains booking ID and reference
- Step 2: HTTP 200 OK, response shows booking details with status=inquiry
- Step 3: HTTP 200 OK, booking status updated to cancelled

**Related Sections:**
- See "Admin UI: Buchungen (Bookings) - Smoke + Troubleshooting" for list/detail/confirm/cancel operations
- See "Database: Exclusion Constraints (Booking Conflicts)" for double-booking prevention
- See "Booking Status Workflow: Inquiry Policy (Non-Blocking Status)" for inquiry vs confirmed blocking behavior

**BookingCancelRequest Payload:**
- `cancelled_by` (required): Enum literal "guest" | "host" | "platform" | "system"
- `cancellation_reason` (optional): String reason
- `refund_amount` (optional): Decimal refund amount
- `post_cancel_hold_hours` (optional): Integer hours to hold booking before allowing re-booking (default: 0)

Admin UI sends: `cancelled_by: "host"`

**Cancel Behavior Notes:**
- **Idempotent Operation**: Re-cancelling already-cancelled bookings is allowed ("first cancel wins")
- **Status Update**: Booking `status` changes to `"cancelled"` on successful cancellation
- **Cancellation Reason Persistence**: Already-cancelled bookings keep their original `cancellation_reason` (not overwritten)
- **Verification**: To verify cancellation, check `booking.status == "cancelled"` (NOT by comparing `cancellation_reason`)
- **Post-Cancel Hold**: Optional `post_cancel_hold_hours` field delays re-booking window (default 0 = immediate)
**Properties Dropdown (Neue Buchung Modal):**
- Frontend fetches properties via: `GET /api/v1/properties?offset=0&limit=100&is_active=true`
- **API Limit Validation**: Maximum limit=100 (backend enforces via OpenAPI spec)
- **422 Validation Error**: Requesting limit > 100 causes 422 Unprocessable Entity with validation error: `{"field": "limit", "message": "ensure this value is less than or equal to 100"}`
- **Solution**: Frontend MUST use limit <= 100 (current: limit=100, is_active=true to show only active properties)
- **Pagination**: If agency has > 100 properties, implement pagination or search filter in dropdown
- **Smoke Test**: Test 7 verifies properties endpoint returns valid data with limit=100


## Bookings UI: Filters + Cancel Modal + Property/Guest Display

**Overview:**
Bookings Admin UI completion round #2 adds server-side filters, enhanced cancel modal, and improved property/guest display.

**Features:**

**List Page (/bookings):**
1. **Server-Side Status Filter**:
   - Status dropdown sends `?status=confirmed` query parameter to backend
   - Backend-supported values: requested, under_review, inquiry, pending, confirmed, checked_in, checked_out, cancelled, declined
   - Reduces client-side data transfer (only matching bookings returned)

2. **Server-Side Property Filter**:
   - Property dropdown sends `?property_id=<uuid>` query parameter to backend
   - Properties fetched from `GET /api/v1/properties?offset=0&limit=100&is_active=true`
   - Shows property names in dropdown (not IDs)

3. **Property Name Display**:
   - List table shows property name instead of truncated UUID
   - Fetches properties on page load for both filter and display
   - Fallback: `Objekt <uuid>...` if property not found in cache

4. **Filter Reset Button**:
   - "Filter zurücksetzen" button clears all filters (search, status, property)
   - Only visible when at least one filter is active

**Detail Page (/bookings/[id]):**
1. **Enhanced Cancel Modal**:
   - `cancelled_by` dropdown: guest | host | platform | system (previously hardcoded to "host")
   - `post_cancel_hold_hours` field: 0-168 hours (default 0 = immediate availability)
   - `cancellation_reason` textarea (required)
   - `refund_amount` input (optional)

2. **Property Name Display**:
   - Fetches property via `GET /api/v1/properties/{id}` on page load
   - Displays property name instead of "Objekt-ID: <uuid>"
   - Fallback: `ID: <uuid>...` if fetch fails

3. **Guest Existence Check**:
   - Already implemented (checks `GET /api/v1/guests/{id}`)
   - Shows link to guest page if guest exists
   - Shows "guest_id: ..." if guest doesn't exist

**Backend API Endpoints Used:**
- `GET /api/v1/bookings?status=<status>&property_id=<uuid>&limit=<n>&offset=<n>` - List with filters
- `GET /api/v1/properties?offset=0&limit=100&is_active=true` - Properties for filter/display
- `GET /api/v1/properties/{id}` - Single property for detail page
- `POST /api/v1/bookings/{id}/cancel` - Cancel with enhanced payload
- `GET /api/v1/guests/{id}` - Guest existence check

**Verification:**
```bash
# Verify filters work (network tab shows query params)
# 1. Open /bookings in browser
# 2. Select status filter → URL should show ?status=confirmed
# 3. Select property filter → URL should append &property_id=<uuid>
# 4. Network tab: GET /api/v1/bookings?status=confirmed&property_id=<uuid>

# Verify cancel modal fields
# 1. Open /bookings/<id> detail page
# 2. Click "Stornieren" button
# 3. Modal should show:
#    - "Storniert von" dropdown (4 options)
#    - "Sperrfrist nach Stornierung" number input (0-168)
#    - "Stornierungsgrund" textarea
#    - "Rückerstattungsbetrag" number input

# Verify property names display
# 1. List page should show property names (not UUIDs)
# 2. Detail page should show "Objekt: <property name>"
```

**Troubleshooting:**

- **Filters don't work / still client-side**: Check browser DevTools Network tab - request should include query params `?status=...&property_id=...`
- **Property dropdown empty**: Properties fetch failed (401/403) or no active properties exist
- **Property names not showing**: Properties fetch returned 422 (limit > 100) or 401 (token expired)
- **Cancel modal missing fields**: Old frontend version cached - hard refresh (Cmd+Shift+R / Ctrl+F5)


## Next.js Build Fails: Block-Scoped Variable Used Before Declaration

**Symptom:** Coolify redeploy fails during `npm run build` with TypeScript error:
```
Type error: Block-scoped variable 'X' used before its declaration.
```

**Common Cause:** A `useEffect` hook references a function/variable in its dependency array before that function is declared in the file. This violates JavaScript's Temporal Dead Zone (TDZ) rules.

**Example Error:**
```
Type error: Block-scoped variable 'fetchProperties' used before its declaration
  in app/bookings/page.tsx:192:20
```

**Root Cause:**
```tsx
// WRONG ORDER - TDZ violation
useEffect(() => {
  fetchProperties();  // References fetchProperties
}, [accessToken, fetchProperties]);  // Line 192: Uses fetchProperties in deps

// ...other code...

const fetchProperties = useCallback(...);  // Line 239: Declared too late!
```

**Solution:**
Move the function declaration BEFORE the `useEffect` that references it:

```tsx
// CORRECT ORDER
const fetchProperties = useCallback(...);  // Declare first

useEffect(() => {
  fetchProperties();  // Now safe to reference
}, [accessToken, fetchProperties]);  // Now safe in deps
```

**If the function uses other helpers:**
Ensure all dependencies are also declared in the correct order (helper functions before callbacks that use them):

```tsx
// 1. Helper functions first (no deps)
const showToast = (...) => { ... };

// 2. Callbacks that use helpers (with deps)
const fetchProperties = useCallback(async () => {
  showToast(...);  // Safe - showToast declared above
}, [accessToken]);

// 3. Effects that use callbacks
useEffect(() => {
  fetchProperties();
}, [accessToken, fetchProperties]);
```

**Quick Fix Steps:**
1. Identify the function referenced in the error (e.g., `fetchProperties`)
2. Find where it's declared (search for `const fetchProperties = `)
3. Move its declaration (and any helpers it uses) to BEFORE the first `useEffect` that references it
4. Rebuild: `npm run build`

**Verification:**
```bash
cd frontend
npm run build
# Should complete without "used before declaration" errors
```


---

## Pricing UI: Navigation / Informationsarchitektur

**Overview:** Simplified pricing UI navigation with property-scoped rate plans as the canonical access path.

**Purpose:** Eliminate duplicate/legacy UI routes and establish clear, consistent navigation for pricing features.

**Navigation Structure:**

1. **Saisonvorlagen** (Season Templates):
   - Route: `/pricing/seasons`
   - Sidebar: Pricing → Saisonzeiten
   - Purpose: Agenturweite Saisonvorlagen erstellen und verwalten
   - Apply to: Property rate plans via Objekte → Objekt → Preiseinstellungen

2. **Objektpreise** (Property Rate Plans):
   - Route: `/properties/[id]/rate-plans`
   - Access: Objekte → Objekt öffnen → Tab "Preiseinstellungen"
   - Purpose: Objektspezifische Tarifpläne und Saisonzeiten verwalten
   - Features:
     - Rate plan CRUD (create, edit, archive, restore, delete)
     - Seasons CRUD with template sync
     - Archive/restore via PATCH /archive and /restore endpoints
     - Default plan selection (is_default)
     - Archived plans toggle (default: hidden)

3. **Quote Test** (Preisberechnung):
   - Route: `/pricing/quote`
   - Sidebar: (no link, direct URL access only)
   - Purpose: Test pricing calculation with per-night breakdown

**Legacy Route Removed:**
- `/pricing/rate-plans` (central rate plans page) has been REMOVED
- Redirects to `/properties` with message: "Preispläne werden jetzt pro Objekt verwaltet"
- Rationale: Duplicate of property-specific rate plans page; canonical is per-property management

**API Endpoints (Unchanged):**
- Rate plan endpoints remain at `/api/v1/pricing/rate-plans/*`
- Season endpoints remain at `/api/v1/pricing/rate-plans/{id}/seasons/*`
- Season template endpoints remain at `/api/v1/pricing/season-templates/*`

**User Flow:**
1. Create season templates: `/pricing/seasons`
2. Manage property rate plans: Objekte → pick property → Preiseinstellungen tab
3. Apply templates to rate plan: Within Preiseinstellungen → "Vorlage anwenden"
4. Test pricing: Navigate directly to `/pricing/quote`

**Code Locations:**
- Legacy redirect stub: `frontend/app/pricing/rate-plans/page.tsx` (minimal redirect component)
- Canonical property rate plans: `frontend/app/properties/[id]/rate-plans/page.tsx`
- Property tabs layout: `frontend/app/properties/[id]/layout.tsx` (Überblick + Preiseinstellungen)
- Seasons page: `frontend/app/pricing/seasons/page.tsx` (updated helper text)
- Sidebar nav: `frontend/app/components/AdminShell.tsx` (no link to /pricing/rate-plans)

**Verification:**
- Visit `/pricing/rate-plans` → Redirects to `/properties` with explanatory message
- Sidebar: No "Tarifpläne" menu item under Pricing section (only Saisonzeiten)
- `/pricing/seasons` helper text: References "Objekte → Objekt → Preiseinstellungen" (not /pricing/rate-plans)
- Property rate plans page: Archive uses PATCH /archive, restore uses PATCH /restore (not DELETE for soft-delete)


---

## Modal: Aus Saisonvorlage importieren (Objekt-Preiseinstellungen)

**Overview:** Polished import/sync modal for season templates on property rate plans page with clear button labels, proper enablement logic, and improved readability.

**Purpose:** Allow users to import or synchronize season templates to property rate plans with clear understanding of the difference between Import and Sync operations.

**Location:** Objekte → Objekt → Preiseinstellungen → Button "Aus Saisonvorlage importieren"

**UI Elements:**

1. **Buttons** (Clear, concise labels):
   - **Abbrechen**: Always enabled, closes modal
   - **Importieren**: Enabled only when template(s) selected AND years set
   - **Synchronisieren**: Enabled only when exactly 1 template selected AND years set

2. **Explanatory Text** (Displayed above buttons):
   - **Importieren**: "Erstellt nur fehlende Saisonzeiten aus der Vorlage (ändert bestehende nicht)."
   - **Synchronisieren**: "Aktualisiert bereits importierte Saisonzeiten aus der Vorlage und repariert Verknüpfungen (kann bestehende Einträge ändern)."

3. **Empty State** (When no templates exist):
   - Message: "Keine Saisonvorlagen vorhanden"
   - CTA Button: "Saisonvorlagen anlegen" → navigates to /pricing/seasons
   - Importieren/Synchronisieren buttons: disabled

**Button Enablement Logic:**

| Condition | Abbrechen | Importieren | Synchronisieren |
|-----------|-----------|-------------|-----------------|
| No templates exist | ✅ Enabled | ❌ Disabled | ❌ Disabled |
| Templates exist, none selected | ✅ Enabled | ❌ Disabled | ❌ Disabled |
| 1+ templates selected, no years | ✅ Enabled | ❌ Disabled | ❌ Disabled |
| 1+ templates selected, years set | ✅ Enabled | ✅ Enabled | ✅ Enabled (only if exactly 1 template) |
| Loading (import/sync in progress) | ❌ Disabled | ❌ Disabled | ❌ Disabled |

**Operations:**

1. **Importieren (Create Strategy)**:
   - Endpoint: `POST /api/v1/pricing/rate-plans/{id}/seasons/sync-from-template`
   - Strategy: `create`
   - Behavior: Creates only missing seasons that don't exist yet (based on date ranges)
   - Use Case: Initial import, adding new seasons without touching existing ones

2. **Synchronisieren (Sync Strategy)**:
   - Endpoint: `POST /api/v1/pricing/rate-plans/{id}/seasons/sync-from-template`
   - Strategy: `sync`
   - Behavior: Creates new seasons, updates linked seasons, relinks legacy seasons, skips overridden seasons
   - Use Case: Updating existing seasons after template changes, repairing broken links

**Info Box Styling (Improved Contrast):**
- Background: `bg-white` or `bg-slate-50` (light, readable)
- Border: `border-slate-200` or `border-blue-400` (subtle, visible)
- Text: `text-slate-900` / `text-slate-700` (dark, high contrast)
- Sync Preview: White background with blue border for clear readability

**Verification:**

```bash
# UI Manual Check (after deployment):
# 1. Navigate to: Objekte → pick property → Preiseinstellungen
# 2. Click "Aus Saisonvorlage importieren" button
# 3. Verify button labels: Abbrechen | Importieren | Synchronisieren
# 4. Verify explanatory text above buttons (2 lines explaining Import vs Sync)
# 5. Without selection: Importieren + Synchronisieren disabled
# 6. Select template(s): Buttons enable (if years also set)
# 7. Info boxes: Check light background, dark text, good readability
# 8. If no templates exist: Empty state shows "Saisonvorlagen anlegen" link
```

**Troubleshooting:**

### Buttons Always Disabled

**Symptom:** Importieren/Synchronisieren buttons remain disabled even after selecting templates.

**Root Cause:** `selectedYears` state is empty (years not computed from template selection).

**Solution:**
- Verify template selection triggers years computation (lines 216-242 in page.tsx)
- Check browser DevTools React state: selectedTemplateIds and selectedYears arrays
- Ensure template has valid periods with date ranges

### Empty State Not Showing Link

**Symptom:** When no templates exist, modal shows message but no "Saisonvorlagen anlegen" button.

**Root Cause:** Empty state condition not matching or link not rendered.

**Solution:**
- Verify `templates.length === 0` condition (line 1704)
- Check link element at lines 1708-1712 (href="/pricing/seasons")
- Clear browser cache and hard refresh

### Info Box Unreadable

**Symptom:** Explanatory text or sync preview box has poor contrast (gray on gray).

**Root Cause:** Dark mode or theme variables not applying correctly.

**Solution:**
- Verify info box uses `bg-white` or `bg-slate-50` (not dark bg)
- Verify text uses `text-slate-900` / `text-slate-700` (not muted colors)
- Check Sync Preview div at line 1604: should have `bg-white` and `border-blue-400`
- Check Explanatory Text div at line 1748: should have `bg-slate-50` and `text-slate-900`


---

## Pricing: Season Sync Atomicity & Advisory Locks

**Overview:** Season template sync operations use single-transaction atomicity with advisory locks to prevent race conditions and ensure data consistency.

**Purpose:** Prevent partial failures and race conditions when multiple users sync season templates to the same rate plan simultaneously.

**Implementation Date:** 2026-01-24

**Architecture:**

**Advisory Lock:**
- Endpoint: `POST /api/v1/pricing/rate-plans/{id}/seasons/sync-from-template`
- Lock: `pg_advisory_xact_lock(hashtextextended(rate_plan_id, 1))`  
- Scope: Rate plan level (serializes all syncs on same rate plan)
- Type: Transaction-scoped (auto-released on COMMIT/ROLLBACK)
- Hash seed: `1` (distinguishes from booking service which uses seed `0`)

**Single Transaction:**
- All sync operations (archive duplicates, updates, creates) wrapped in ONE transaction
- All-or-nothing guarantee: Either ALL seasons are synced successfully OR none
- Automatic rollback on error (DB timeout, network error, constraint violation that shouldn't happen)
- ExclusionViolationError exceptions are caught and added to conflicts (do NOT trigger rollback)

**Execution Flow:**
```python
async with db.transaction():
    # 1. Acquire advisory lock (blocks if another sync is running on same rate plan)
    await db.execute("SELECT pg_advisory_xact_lock(...)", rate_plan_id)
    
    # 2. Phase 0: Archive duplicates
    for dup in duplicates:
        await db.execute("UPDATE rate_plan_seasons SET archived_at = now() ...")
    
    # 3. Phase 1: Updates (shrinks → equals → expansions)
    for update in ordered_updates:
        await db.execute("UPDATE rate_plan_seasons SET ...")
    
    # 4. Phase 2: Creates
    for create in to_create:
        await db.fetchval("INSERT INTO rate_plan_seasons ...")
    
    # Transaction commits here (lock released)
```

**Benefits:**
- ✅ **Atomicity**: No partial failures (either all seasons synced or none)
- ✅ **Serialization**: Parallel syncs on same rate plan are queued (no race conditions)
- ✅ **Consistency**: DB exclusion constraint + advisory lock provide double protection
- ✅ **Rollback**: Errors trigger automatic transaction rollback
- ✅ **Performance**: Single transaction reduces overhead vs. multiple small transactions

**Common Issues:**

### Sync Hangs / Times Out

**Symptom:** Sync operation hangs for 30+ seconds, then returns timeout error.

**Root Cause:** Another user is already syncing the same rate plan. Advisory lock causes current sync to wait.

**How to Debug:**
```bash
# Check if advisory locks are held
psql $DATABASE_URL -c "
SELECT 
    pg_backend_pid() as current_pid,
    locktype, 
    objid, 
    mode, 
    granted,
    pg_blocking_pids(pid) as blocking_pids
FROM pg_locks 
WHERE locktype = 'advisory' 
  AND objid = hashtextextended('<rate_plan_id>'::text, 1);
"

# Check long-running transactions
psql $DATABASE_URL -c "
SELECT pid, now() - xact_start AS duration, query 
FROM pg_stat_activity 
WHERE state = 'active' 
  AND now() - xact_start > interval '10 seconds';
"
```

**Solution:**
- **Normal behavior**: Wait for first sync to complete (usually <10s)
- **Stuck sync**: If duration >30s, may indicate deadlock or slow query
  - Option 1: Wait for timeout
  - Option 2: Cancel stuck backend: `SELECT pg_cancel_backend(<pid>);`
  - Option 3: Terminate stuck backend: `SELECT pg_terminate_backend(<pid>);` (last resort)

### Partial Sync After Error

**Symptom:** User reports "some seasons created, but not all" after error message.

**Root Cause:** This should NOT happen anymore (single transaction guarantees rollback).

**How to Debug:**
```bash
# Check sync logs for transaction errors
grep "Sync applied" /var/log/pms-backend.log | tail -20

# Check if any seasons have source_synced_at in last 5 minutes
psql $DATABASE_URL -c "
SELECT id, label, source_synced_at, source_template_id, source_year
FROM rate_plan_seasons 
WHERE rate_plan_id = '<rate_plan_id>'
  AND source_synced_at > now() - interval '5 minutes'
ORDER BY source_synced_at DESC;
"
```

**Solution:**
- If partial sync detected: **BUG** in transaction implementation
  - File bug report with exact error message and logs
  - Manual cleanup: Archive partial seasons or complete sync manually
- If NO partial sync: Error occurred BEFORE transaction started (e.g., validation error)
  - Check error detail, fix issue, retry sync

### ExclusionViolationError Despite Advisory Lock

**Symptom:** Sync returns conflicts with "DB constraint" error, even though advisory lock should prevent this.

**Root Cause:** Advisory lock prevents parallel syncs on SAME rate plan, but does NOT prevent:
- Manual season edits during sync
- Syncs on DIFFERENT rate plans that share overlapping date ranges (if properties are related)

**How to Debug:**
```bash
# Check if manual edits happened during sync window
psql $DATABASE_URL -c "
SELECT id, label, date_from, date_to, updated_at, source_is_overridden
FROM rate_plan_seasons 
WHERE rate_plan_id = '<rate_plan_id>'
  AND updated_at > now() - interval '1 minute'
ORDER BY updated_at DESC;
"
```

**Solution:**
- **Manual edits**: Normal behavior. User should mark season as overridden to prevent future sync conflicts.
- **Sync retry**: If conflict is transient, retry sync after resolving overlaps.

---

### APPLY Returns 409 on Constraint Violations (Strict Atomic Behavior)

**Implementation Date:** 2026-01-24 (hotfix)

**Behavior:**
- APPLY mode now has strict atomic semantics: Either ALL seasons are synced OR none
- Any constraint violation (ExclusionViolationError, UniqueViolationError) during APPLY triggers:
  - Immediate transaction rollback (all changes discarded)
  - HTTP 409 Conflict response
  - Message: "No changes were applied. Re-run preview mode to identify conflicts."

**What Changed:**
- REMOVED: Inner try/except blocks that caught constraint violations and tried to continue
- ADDED: Outer try/except wrapper that catches constraint violations AFTER transaction completes
- Result: Transaction is never in aborted state; rollback is clean

**409 Response Structure:**
```json
{
  "detail": {
    "error": "conflict",
    "message": "Season sync apply failed due to a conflicting season overlap or duplicate. No changes were applied. Please re-run preview mode to identify conflicts and resolve them before retrying.",
    "constraint": "exclude_constraint_name or unique_constraint_name"
  }
}
```

**When 409 Occurs:**
- Parallel sync created overlapping season during our transaction (advisory lock missed due to timing)
- Manual season edit created overlap during our transaction
- Rare race condition despite advisory lock (e.g., different rate plan with shared constraint)

**User Action on 409:**
1. Run PREVIEW mode to see current conflicts
2. Resolve conflicts manually (archive/delete overlapping seasons)
3. Retry APPLY

**Verification:**
```bash
# Check that no partial changes were applied
psql $DATABASE_URL -c "
SELECT COUNT(*) as season_count, 
       MAX(source_synced_at) as last_sync
FROM rate_plan_seasons 
WHERE rate_plan_id = '<rate_plan_id>' 
  AND archived_at IS NULL;
"
# Expected: season_count unchanged from before APPLY attempt
# Expected: last_sync unchanged (NULL if never synced before)
```

---

### Concurrency + Atomic Rollback Smoke Test

**Script:** `pms_season_sync_concurrency_rollback_smoke.sh`

**Purpose:** Production-safe validation of season sync APPLY concurrency serialization (advisory lock) and strict atomic rollback on 409 constraint conflicts.

**What It Tests:**

**Test A: Concurrency Serialization via Advisory Lock**
- Creates test rate plan (INACTIVE) with 12 monthly template periods
- Uses multi-year workload (default: 2026-2030) for noticeable sync time
- Fires 2 concurrent APPLY requests on same rate plan + template + years
- Expects: Both HTTP 200, correct final season count (12 × 5 = 60 seasons), no duplicates
- Verifies: Advisory lock (`pg_advisory_xact_lock`) serializes parallel requests

**Test B: Strict Atomic Rollback on 409 Conflict**
- Creates single conflicting season that overlaps with template period (2026-01-15 to 2026-01-25)
- Runs APPLY with same years (should detect conflict on Januar 2026)
- Expects: HTTP 409 or HTTP 200 with conflict detection
- Verifies strict atomicity:
  - Season count unchanged from baseline (only conflicting season remains)
  - No partial changes applied despite multi-period APPLY
  - "No changes were applied" message in 409 response (if 409 path taken)

**PROD-Safe Features:**
- Creates INACTIVE test rate plan (active=false) to avoid 409 conflicts with existing active plans
- Uses German labels with "- Smoke" suffix for easy identification
- Auto-archives all created resources via trap EXIT (seasons, template, rate plan)
- Runs on real property data but isolated via dedicated test rate plan

**Required Environment Variables:**
- `HOST`: PMS backend base URL (e.g., https://api.fewo.kolibri-visions.de)
- `ADMIN_JWT_TOKEN`: JWT token with manager/admin role

**Optional Environment Variables:**
- `AGENCY_ID`: Agency UUID for x-agency-id header (multi-tenant setups)
- `PROPERTY_ID`: Property UUID (auto-selected if not provided)
- `YEARS`: Space-separated years list (default: "2026 2027 2028 2029 2030")

**Usage:**

```bash
# Production test (minimal)
HOST=https://api.fewo.kolibri-visions.de \
ADMIN_JWT_TOKEN="<jwt>" \
./backend/scripts/pms_season_sync_concurrency_rollback_smoke.sh

# With specific agency
HOST=https://api.fewo.kolibri-visions.de \
ADMIN_JWT_TOKEN="<jwt>" \
AGENCY_ID="ffd0123a-10b6-40cd-8ad5-66eee9757ab7" \
./backend/scripts/pms_season_sync_concurrency_rollback_smoke.sh

# With custom years (reduced workload for faster testing)
HOST=https://api.fewo.kolibri-visions.de \
ADMIN_JWT_TOKEN="<jwt>" \
YEARS="2026 2027" \
./backend/scripts/pms_season_sync_concurrency_rollback_smoke.sh
```

**Expected Output:**

```
ℹ ==========================================
ℹ Season Sync Concurrency + Rollback Test
ℹ ==========================================
ℹ Auto-selected property: 23dd8fda-59ae-4b2f-8489-7a90f5d46c66
✅ Created test rate plan: 660e8400-e29b-41d4-a716-446655440000
✅ Created test template: 550e8400-e29b-41d4-a716-446655440000
✅ Created 12 monthly periods
✅ Verified template has 12 active periods
ℹ ==========================================
ℹ Test A: Concurrency Serialization
ℹ ==========================================
ℹ Using years: 2026 2027 2028 2029 2030
ℹ Expected seasons: 12 periods × 5 years = 60 seasons
ℹ Firing 2 concurrent APPLY requests...
✅ Both requests completed
ℹ Request A duration: 3s
ℹ Request B duration: 5s
ℹ Request A: HTTP 200
ℹ Request B: HTTP 200
ℹ Request A: created=60, updated=0
ℹ Request B: created=0, updated=60
✅ Test A.1 PASSED: Both requests returned HTTP 200
✅ Test A.2 PASSED: Total created=60, updated=60
ℹ Verifying final season count and no duplicates...
ℹ Found 60 active seasons (expected 60)
✅ Test A.3 PASSED: Correct season count (60 seasons)
✅ Test A.4 PASSED: No duplicate seasons (advisory lock serialized requests)
ℹ ==========================================
ℹ Test B: Atomic Rollback (409 Conflict)
ℹ ==========================================
✅ Archived existing seasons
✅ Verified: Rate plan has 0 seasons (clean slate)
ℹ Creating conflicting season (2026-01-15 to 2026-01-25, overlaps Januar 2026)...
✅ Created conflicting season: 770e8400-e29b-41d4-a716-446655440000
✅ Baseline: 1 conflicting season exists
ℹ Attempting APPLY with overlapping dates (expect 409 strict rollback)...
ℹ APPLY response: HTTP 409
✅ Test B.1 PASSED: Received HTTP 409 (strict atomic rollback)
✅ Test B.2 PASSED: 409 response includes 'no changes were applied' message
ℹ Final season count: 1 (baseline: 1)
✅ Test B.3 PASSED: ROLLBACK VERIFIED - Season count unchanged (1 seasons)
✅ Test B.4 PASSED: Strict atomicity enforced (no partial writes)
ℹ ==========================================
✅ All concurrency + rollback tests passed!
ℹ ==========================================
ℹ Summary:
ℹ   Test A (Concurrency):
ℹ     - 2 parallel APPLY requests serialized via advisory lock ✓
ℹ     - 60 seasons created (expected 60) ✓
ℹ     - No duplicate seasons ✓
ℹ
ℹ   Test B (Atomic Rollback):
ℹ     - Conflict detected (HTTP 409) ✓
ℹ     - No partial changes applied ✓
ℹ     - Season count unchanged (1) ✓
ℹ ==========================================
```

**Exit Codes:**
- `0`: All tests passed
- `1`: Test failure or setup error

**Troubleshooting:**

- **Test A fails (both requests != 200):** Check JWT token valid, property exists, backend is healthy
- **Test A fails (wrong season count):** Advisory lock not working, check PostgreSQL logs for lock errors
- **Test A fails (duplicates found):** Advisory lock failed to serialize requests, check `pg_advisory_xact_lock` in backend code
- **Test B fails (wrong HTTP code):** Expected 409 or 200, verify conflict detection logic in backend
- **Test B fails (partial changes applied):** ATOMICITY VIOLATION! Check transaction boundaries and exception handling in sync endpoint
- **401 Unauthorized:** JWT token expired or invalid
- **403 Forbidden:** JWT lacks manager/admin role
- **No properties found:** Database empty or agency filter too restrictive

**Verification of Fixes:**

This smoke test verifies the fixes from 2026-01-24:
1. **Advisory Lock (commit 2dfcd02):** Prevents race conditions during concurrent APPLY requests
2. **Strict Atomic Rollback (commit 25ccf94):** Ensures 409 constraint conflicts trigger complete rollback with no partial changes

**Known Pitfall Fixed (2026-01-24):**

Prior version (commit 89bdeb9) had HTTP response parsing bug in Test B:
- **Bug:** Script used `grep "HTTP_CODE:" "$CONFLICT_RESPONSE"` where `$CONFLICT_RESPONSE` was a variable, not a file
- **Symptom:** `grep: {json} HTTP_CODE:409: File name too long` when backend returned long JSON response
- **Impact:** Test B failed even though backend behavior was correct (409 returned, rollback successful)
- **Fix:** Changed Test B to use temp file pattern (same as Test A): `CONFLICT_RESPONSE_FILE=$(mktemp)` + `curl > "$CONFLICT_RESPONSE_FILE"` + `grep "HTTP_CODE:" "$CONFLICT_RESPONSE_FILE"`
- **Expected in PROD:** Test B now correctly reports `HTTP 409` (strict atomic rollback mode) and verifies no partial changes applied

**Related Documentation:**
- See "APPLY Returns 409 on Constraint Violations" above for 409 response structure and user actions
- See `backend/docs/project_status.md` for implementation details of advisory lock and atomic rollback

---

### Season Template Import - Atomic "missing_only" Strategy

**Implementation Date:** 2026-01-24

**Purpose:** Server-side atomic import of missing seasons from template, replacing client-side per-season POST loops with single bulk call.

**Behavior:**
- Frontend "Importieren" button now uses `POST /api/v1/pricing/rate-plans/{id}/seasons/sync-from-template` with `strategy="missing_only"` and `mode="apply"`
- Backend uses same atomic approach as sync: `pg_advisory_xact_lock` + single transaction
- Existing seasons treated as conflicts (skipped), only missing seasons created
- Idempotent: Re-importing same template creates 0 new seasons (all skipped as conflicts)

**Architecture:**
- **Endpoint:** `POST /api/v1/pricing/rate-plans/{id}/seasons/sync-from-template`
- **Strategy:** `"missing_only"` (vs `"sync"` which updates existing seasons)
- **Mode:** `"apply"` (vs `"preview"` which dry-runs)
- **Advisory Lock:** `pg_advisory_xact_lock(hashtextextended(rate_plan_id, 1))`
- **Transaction:** All-or-nothing atomicity (same as sync)
- **Conflict Handling:** Existing seasons added to `conflicts[]` response, not treated as errors

**Frontend Changes:**
- File: `frontend/app/properties/[id]/rate-plans/page.tsx:610-645`
- Replaced: Client-side per-period POST loop with existence check
- Now: Single server-side bulk call to sync endpoint
- Validation: Only allows single template selection (backend limitation)
- Success message: Shows created count + conflicts (skipped) count

**Response Structure:**
```json
{
  "counts": {
    "created": 4,
    "updated": 0,
    "skipped_overridden": 0
  },
  "conflicts": [
    {
      "label": "Q1 2026",
      "date_from": "2026-01-01",
      "date_to": "2026-03-31",
      "reason": "Saisonzeit existiert bereits (missing_only Modus)",
      "hint": "Im 'missing_only' Modus werden bestehende Saisonzeiten nicht aktualisiert.",
      "conflicting_season_id": "...",
      "conflicting_range": "[2026-01-01, 2026-03-31)"
    }
  ],
  "to_create": [...],
  "to_update": [],
  "skipped_overridden": []
}
```

**Smoke Test:**

**Script:** `pms_season_template_import_missing_smoke.sh`

**Required Environment Variables:**
- `HOST`: PMS backend base URL (e.g., https://api.fewo.kolibri-visions.de)
- `ADMIN_JWT_TOKEN`: JWT token with manager/admin role

**Optional Environment Variables:**
- `AGENCY_ID`: Agency UUID for x-agency-id header
- `PROPERTY_ID`: Property UUID (auto-selected if not provided)
- `YEAR`: Import year (default: 2026)

**Usage:**
```bash
# Production test
HOST=https://api.fewo.kolibri-visions.de \
ADMIN_JWT_TOKEN="<jwt>" \
./backend/scripts/pms_season_template_import_missing_smoke.sh

# With specific agency and year
HOST=https://api.fewo.kolibri-visions.de \
ADMIN_JWT_TOKEN="<jwt>" \
AGENCY_ID="ffd0123a-10b6-40cd-8ad5-66eee9757ab7" \
YEAR=2027 \
./backend/scripts/pms_season_template_import_missing_smoke.sh
```

**What It Tests:**

**Test A: Import Missing (First Run)**
- Creates test template with 4 quarterly periods (Q1-Q4)
- Runs APPLY with `strategy="missing_only"`, `mode="apply"`
- Expects: 4 seasons created, 0 conflicts, season count increases by 4

**Test B: Re-import (Second Run, Idempotency)**
- Re-runs same APPLY request on same rate plan + template + year
- Expects: 0 seasons created, 4 conflicts (all periods skipped), season count unchanged
- Verifies: Idempotent behavior, no duplicates created

**Expected Output:**
```
ℹ ===================================================================
ℹ PMS Season Template Import - Atomic 'missing_only' Smoke Test
ℹ ===================================================================
✅ Created test rate plan: ... (INACTIVE)
✅ Created test season template: ... (4 periods)
ℹ ===================================================================
ℹ TEST A: Import missing (first run)
ℹ ===================================================================
ℹ Baseline season count: 0
ℹ Importing missing seasons (strategy=missing_only, mode=apply)...
✅ Import APPLY response: HTTP 200
ℹ Import result: 4 created, 0 conflicts (skipped)
ℹ Final season count: 4 (baseline: 0)
✅ Test A PASSED: 4 seasons created, season count correct (0 → 4)
ℹ ===================================================================
ℹ TEST B: Re-import (second run, idempotency check)
ℹ ===================================================================
ℹ Re-importing (strategy=missing_only, mode=apply)...
✅ Re-import APPLY response: HTTP 200
ℹ Re-import result: 0 created, 4 conflicts (skipped)
✅ Test B PASSED: 0 created, 4 skipped (conflicts), season count unchanged (idempotent)
ℹ ===================================================================
✅ All tests passed! 🎉
ℹ ===================================================================
ℹ Summary:
ℹ   - Test A: Import missing created 4 seasons correctly ✓
ℹ   - Test B: Re-import skipped all (idempotent, no duplicates) ✓
```

**Exit Codes:**
- `0`: All tests passed
- `1`: Test failure or setup error

**Troubleshooting:**

- **Test A fails (wrong created count):** Backend not creating all periods, check template has 4 active periods
- **Test A fails (season count wrong):** Partial failure or atomicity issue, check transaction boundaries
- **Test B fails (created > 0):** NOT idempotent, backend creating duplicates despite existing seasons
- **Test B fails (conflicts != 4):** Backend not detecting all existing seasons as conflicts in missing_only mode
- **401 Unauthorized:** JWT token expired or invalid
- **403 Forbidden:** JWT lacks manager/admin role
- **400 "At least one season template must be selected":** Payload missing template_id

**Verification:**

This smoke test verifies P2.17 implementation (2026-01-24):
- Frontend uses server-side bulk endpoint (no client-side loops)
- Backend `strategy="missing_only"` correctly skips existing seasons
- Atomicity via advisory lock + transaction (same safety as sync)
- Idempotent re-import behavior (conflicts, not errors)

**Common Issues:**

- **UI shows "0 importiert" but seasons were created**: Frontend parsing bug (fixed 2026-01-24). Frontend was using `response.counts?.created` instead of `response.counts?.create`. Backend returns `counts.create` (not `counts.created`). Upgrade to commit >5899929.
- **Smoke script Test A fails with "0 created"**: Smoke parsing bug (fixed 2026-01-24). Script was parsing `.get('created')` instead of `.get('create')`. Backend response schema: `counts: {create, update, skip, conflict}`. Upgrade to latest smoke script.
- **Re-import creates duplicates (not idempotent)**: Backend missing_only strategy not working. Check backend logs for strategy value, verify sync endpoint handles `strategy="missing_only"` correctly (should add existing seasons to conflicts, not create).
- **Import shows conflicts but UI says "0 übersprungen"**: Frontend skippedCount fallback chain incorrect. After fix, uses: `counts?.skip ?? counts?.conflict ?? conflicts?.length`. Verify response structure in network tab.

**Expected UI Behavior:**

- **First import**: Toast shows "Import abgeschlossen: 4 importiert" (no skipped)
- **Re-import (idempotent)**: Toast shows "Import abgeschlossen: 0 importiert, 4 übersprungen"
- **Conflict (409)**: Toast shows "Konflikt: Import abgebrochen (keine Änderungen). Bitte Vorschau/Synchronisieren nutzen und Konflikte lösen."

**UI Hardening (P2.18, 2026-01-24):**

- **Template selection**: Changed from checkboxes to radio buttons (single-select only)
- **Button validation**: Both Import and Sync buttons require exactly 1 template + years
- **Explanatory text**: Simplified from 5 lines to 2 compact lines
- **Effect**: Clearer UX, prevents multi-template confusion (backend limitation)
- **Period count fix (P2.18.1)**: Modal now shows only active periods (archived periods not counted); refetch on modal open ensures fresh data

**Related Documentation:**
- See "Pricing: Season Sync Atomicity & Advisory Locks" above for advisory lock and transaction details
- See `backend/scripts/README.md` for complete smoke script documentation
- See `backend/docs/project_status.md` for P2.17 implementation details

---

---

## Season Templates Archived Toggle (P2.18.4)

**Overview:** Fixed "Archivierte anzeigen" toggle to show ONLY archived templates (not active+archived).

**Purpose:** Allow admin to switch between viewing active templates (default) and ONLY archived templates (toggle ON).

**Architecture:**
- **Query Parameter**: `archived_only=true` replaces `include_archived=true` for "only archived" mode
- **SQL Filter**: `WHERE archived_at IS NOT NULL` (only archived) vs `WHERE archived_at IS NULL` (only active)
- **UI Toggle**: "Nur archivierte anzeigen" checkbox

**API Endpoints:**

- `GET /api/v1/pricing/season-templates?archived_only=true&limit=&offset=` - List ONLY archived templates

**Verification Commands:**

```bash
# [HOST-SERVER-TERMINAL] Pull latest code
cd /data/repos/pms-webapp
git fetch origin main && git reset --hard origin/main

# [HOST-SERVER-TERMINAL] Run archived-only toggle smoke test
export HOST="https://api.fewo.kolibri-visions.de"
export ADMIN_JWT_TOKEN="<<<manager/admin JWT>>>"
# Optional:
# export AGENCY_ID="ffd0123a-10b6-40cd-8ad5-66eee9757ab7"
./backend/scripts/pms_season_templates_archived_only_toggle_smoke.sh
echo "rc=$?"

# Expected output: All 6 tests pass, rc=0
```

**Common Issues:**

### Toggle Shows Both Active and Archived Templates

**Symptom:** When "Nur archivierte anzeigen" is checked, both active and archived templates appear in the list.

**Root Cause:** Frontend using `include_archived=true` instead of `archived_only=true` query parameter.

**How to Debug:**
```bash
# Check network tab (DevTools)
# Should see: GET /api/v1/pricing/season-templates?archived_only=true
# NOT: GET /api/v1/pricing/season-templates?include_archived=true

# Verify fetchTemplates in frontend/app/pricing/seasons/page.tsx line ~85-90
```

**Solution:**
- Verify page.tsx uses: `archived_only=true` when `showArchived` is true
- Check toggle state variable: `const [showArchived, setShowArchived] = useState(false);`
- Hard refresh frontend (Cmd+Shift+R / Ctrl+F5) to clear cache

### Backend Returns Active Templates with archived_only=true

**Symptom:** GET /api/v1/pricing/season-templates?archived_only=true returns active templates (archived_at IS NULL).

**Root Cause:** Backend not filtering by `archived_at IS NOT NULL` when `archived_only=true`.

**How to Debug:**
```bash
# Test archived_only filter
curl -X GET "$HOST/api/v1/pricing/season-templates?archived_only=true&limit=10" \
  -H "Authorization: Bearer $ADMIN_JWT_TOKEN" | jq '.[] | {id, name, archived_at}'

# All results should have archived_at != null
```

**Solution:**
- Verify pricing.py line ~3025-3030: should have `if archived_only: conditions.append("archived_at IS NOT NULL")`
- Check archived_only parameter is correctly parsed from query string
- Ensure archived_only takes precedence over include_archived (if archived_only: ... elif not include_archived: ...)

---

## Season Template Period Bulk Delete (P2.18.5)

**Overview:** Bulk delete endpoint for season template periods with multi-select UI.

**Purpose:** Allow admin to select multiple periods and delete them in a single operation.

**Architecture:**
- **Endpoint**: `POST /season-templates/{template_id}/periods/bulk-delete`
- **Request**: `{"period_ids": ["uuid1", "uuid2", ...]}`
- **Response**: `{"requested_count": 2, "deleted_count": 2, "not_found_count": 0, "errors": []}`
- **Transaction Safety**: All deletes in a single DB transaction (atomic)
- **UI**: Checkboxes in periods table + "Ausgewählte löschen" button

**API Endpoints:**

- `POST /api/v1/pricing/season-templates/{id}/periods/bulk-delete` - Bulk delete periods (requires `period_ids` array, min 1, max 50)

**Verification Commands:**

```bash
# [HOST-SERVER-TERMINAL] Pull latest code
cd /data/repos/pms-webapp
git fetch origin main && git reset --hard origin/main

# [HOST-SERVER-TERMINAL] Run bulk delete smoke test
export HOST="https://api.fewo.kolibri-visions.de"
export ADMIN_JWT_TOKEN="<<<manager/admin JWT>>>"
# Optional:
# export AGENCY_ID="ffd0123a-10b6-40cd-8ad5-66eee9757ab7"
./backend/scripts/pms_season_template_period_bulk_delete_smoke.sh
echo "rc=$?"

# Expected output: All 7 tests pass, rc=0
```

**Common Issues:**

### Bulk Delete Returns 400 (Validation Error)

**Symptom:** POST /periods/bulk-delete returns 400 Bad Request with validation error.

**Root Cause:** Request body invalid (empty array, too many IDs, or invalid UUIDs).

**How to Debug:**
```bash
# Test with valid request
curl -X POST "$HOST/api/v1/pricing/season-templates/<template_id>/periods/bulk-delete" \
  -H "Authorization: Bearer $ADMIN_JWT_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"period_ids": ["<valid-uuid-1>", "<valid-uuid-2>"]}' -w "\n%{http_code}\n"

# Should return 200 with deleted_count response
```

**Solution:**
- Verify request has `period_ids` array with 1-50 valid UUIDs
- Check frontend sends correct payload: `{ period_ids: selectedPeriodIds.map(...) }`
- Ensure UUIDs are valid (hyphenated format: 8-4-4-4-12)

### Bulk Delete Deletes Wrong Periods

**Symptom:** Bulk delete removes periods from different template or all periods in template.

**Root Cause:** SQL query missing `WHERE template_id = $1` filter.

**How to Debug:**
```bash
# Check SQL query in pricing.py:bulk_delete_template_periods line ~3920-3925
# Should be:
# DELETE FROM pricing_season_template_periods
# WHERE template_id = $1 AND id = ANY($2::uuid[])
# RETURNING id
```

**Solution:**
- Verify SQL query includes `WHERE template_id = $1 AND id = ANY($2::uuid[])`
- Check parameters are bound correctly: `template_id, period_ids`
- Ensure `template_id` from URL path is used in query

### UI Checkboxes Don't Clear After Delete

**Symptom:** After bulk delete, checkboxes remain checked for deleted periods.

**Root Cause:** Frontend state `selectedPeriodIds` not cleared after successful delete.

**How to Debug:**
```bash
# Check handleBulkDeletePeriods in page.tsx line ~221-240
# Should clear selectedPeriodIds after successful delete:
# setSelectedPeriodIds(new Set());
```

**Solution:**
- Verify handler calls `setSelectedPeriodIds(new Set())` after successful delete
- Check template list is refetched: `fetchTemplates()` after delete
- Clear selection in finally block to ensure cleanup even on error

### Partial Success Not Reported

**Symptom:** Bulk delete returns 200 but `not_found_count` is always 0, even when some IDs don't exist.

**Root Cause:** Backend not calculating `not_found_count` correctly.

**How to Debug:**
```bash
# Test with 1 valid ID and 1 fake ID
curl -X POST "$HOST/api/v1/pricing/season-templates/<template_id>/periods/bulk-delete" \
  -H "Authorization: Bearer $ADMIN_JWT_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"period_ids": ["<valid-uuid>", "00000000-0000-0000-0000-000000000000"]}' \
  | jq '{deleted_count, not_found_count}'

# Expected: {"deleted_count": 1, "not_found_count": 1}
```

**Solution:**
- Verify backend calculates `not_found_count = requested_count - deleted_count`
- Check response includes all fields: requested_count, deleted_count, not_found_count, errors
- Ensure RETURNING clause counts actual deleted rows, not requested IDs

---

---

## Backend Restart Loop After P2.18.5 Deployment (NameError Hotfix)

**Incident Date:** 2026-01-25

**Symptom:** Backend fails to start after P2.18.5 deployment with restart loop. Error log shows:
```
NameError: name 'SeasonTemplatePeriodBulkDeleteResponse' is not defined
File "app/api/routes/pricing.py", line 3918, in <module>
  @router.post("/season-templates/{template_id}/periods/bulk-delete", response_model=SeasonTemplatePeriodBulkDeleteResponse)
```

**Root Cause:** P2.18.5 added bulk delete endpoint for season template periods but forgot to import the schemas:
- SeasonTemplatePeriodBulkDeleteRequest
- SeasonTemplatePeriodBulkDeleteResponse

Schemas exist in `backend/app/schemas/pricing.py` (lines 342, 352) but were not imported in `backend/app/api/routes/pricing.py`. Route decorator references undefined symbol at import time → NameError → startup crash.

**Fix Applied:** Added missing imports to `backend/app/api/routes/pricing.py` (lines 65-66):
```python
from ...schemas.pricing import (
    # ...
    SeasonTemplatePeriodBulkDeleteRequest,
    SeasonTemplatePeriodBulkDeleteResponse,
    # ...
)
```

**Verification Steps:**

```bash
# [HOST-SERVER-TERMINAL] Pull latest code with hotfix
cd /data/repos/pms-webapp
git fetch origin main && git reset --hard origin/main

# [HOST-SERVER-TERMINAL] Verify backend starts successfully
# Wait for Coolify redeploy or restart container manually
# Check /api/v1/ops/version endpoint returns 200

curl -sS https://api.fewo.kolibri-visions.de/api/v1/ops/version | jq '.'
# Expected: {"commit": "...", "version": "..."}

# [HOST-SERVER-TERMINAL] Run deploy verification script
export API_BASE_URL="https://api.fewo.kolibri-visions.de"
./backend/scripts/pms_verify_deploy.sh

# [HOST-SERVER-TERMINAL] Run P2.18.4 smoke test
export HOST="https://api.fewo.kolibri-visions.de"
export ADMIN_JWT_TOKEN="<<<manager/admin JWT>>>"
./backend/scripts/pms_season_templates_archived_only_toggle_smoke.sh
echo "rc=$?"

# [HOST-SERVER-TERMINAL] Run P2.18.5 smoke test
export HOST="https://api.fewo.kolibri-visions.de"
export ADMIN_JWT_TOKEN="<<<manager/admin JWT>>>"
./backend/scripts/pms_season_template_period_bulk_delete_smoke.sh
echo "rc=$?"
```

**Prevention:**
- Import sanity check: `python3 -m py_compile app/api/routes/pricing.py` before commit
- Full import test: `python3 -c "from app.api.routes import pricing"` (requires valid config)
- Code review: ensure all schemas used in decorators are imported

**Related Issues:**
- P2.18.4: Archived-only toggle (fixed in same commit)
- P2.18.5: Bulk delete periods (import-time crash from this feature)

---

---

## P2.19 Restore Archivierte Saisonvorlagen

**Overview:** Restore (unarchive) functionality for season templates.

**Purpose:** Allow admin to restore archived season templates back to active state.

**Architecture:**
- **Endpoint**: `POST /season-templates/{id}/restore`
- **Behavior**: Sets `archived_at = NULL`, `updated_at = NOW()`
- **Validation**: Template must be archived, name conflict check (409 if active template with same name exists)
- **Response**: 200 with full template payload (including periods)

**API Endpoints:**

- `POST /api/v1/pricing/season-templates/{id}/restore` - Restore archived template

**Verification Commands:**

```bash
# [HOST-SERVER-TERMINAL] Pull latest code
cd /data/repos/pms-webapp
git fetch origin main && git reset --hard origin/main

# [HOST-SERVER-TERMINAL] Run restore smoke test
export HOST="https://api.fewo.kolibri-visions.de"
export ADMIN_JWT_TOKEN="<<<manager/admin JWT>>>"
# Optional:
# export AGENCY_ID="ffd0123a-10b6-40cd-8ad5-66eee9757ab7"
./backend/scripts/pms_season_template_restore_smoke.sh
echo "rc=$?"

# Expected output: All 6 tests pass, rc=0
```

**Common Issues:**

### Restore Returns 404 (Template Not Found or Not Archived)

**Symptom:** POST /season-templates/{id}/restore returns 404 with message "Saisonvorlage {id} nicht gefunden oder nicht archiviert".

**Root Cause:** Template does not exist, belongs to different agency, or is not archived (archived_at IS NULL).

**How to Debug:**
```bash
# Check if template exists and is archived
psql $DATABASE_URL -c "SELECT id, name, archived_at FROM pricing_season_templates WHERE id = '<template_id>';"

# If archived_at is NULL, template is active (cannot restore)
# If row not found, template doesn't exist or wrong agency
```

**Solution:**
- Verify template ID is correct
- Ensure template is archived (archived_at IS NOT NULL)
- Check user has access to correct agency

### Restore Returns 409 (Name Conflict)

**Symptom:** POST /season-templates/{id}/restore returns 409 with message "Wiederherstellen nicht möglich: Es existiert bereits eine aktive Saisonvorlage mit dem Namen '...'".

**Root Cause:** Another active (non-archived) template with the same name exists in the same agency.

**How to Debug:**
```bash
# Check for name conflicts
psql $DATABASE_URL -c "SELECT id, name, archived_at FROM pricing_season_templates WHERE name = '<template_name>' AND archived_at IS NULL AND agency_id = '<agency_id>';"

# Should return at most 1 row (the one being restored would have archived_at NOT NULL)
```

**Solution:**
1. **Rename one of the templates**: Use PATCH /season-templates/{id} to rename either the archived template or the active one
2. **Archive the active template first**: If you want to replace the active template, archive it before restoring the other one
3. **Delete the active template**: If the active template is no longer needed, archive it then hard delete it

### UI Shows Restore Button But Not Working

**Symptom:** "Wiederherstellen" button appears in UI but clicking does nothing or shows error.

**Root Cause:** Frontend API call failing or not refreshing data after restore.

**How to Debug:**
```bash
# Check browser DevTools Network tab
# Should see: POST /api/v1/pricing/season-templates/{id}/restore
# Expected response: 200 with template payload
# Check Console for JavaScript errors
```

**Solution:**
- Verify JWT token is valid (not expired)
- Check network request succeeds (200 status)
- Ensure fetchTemplates() is called after successful restore
- Hard refresh browser (Cmd+Shift+R / Ctrl+F5) if UI state is stale

### Restored Template Still Shows in Archived List

**Symptom:** After restore, template still appears when "Nur archivierte anzeigen" toggle is ON.

**Root Cause:** Frontend not refetching template list after restore.

**How to Debug:**
```bash
# Check API directly
curl -X GET "$HOST/api/v1/pricing/season-templates?archived_only=true" \
  -H "Authorization: Bearer $ADMIN_JWT_TOKEN" | jq '.[] | select(.id=="<template_id>") | {id, name, archived_at}'

# If archived_at is null, backend correctly restored but frontend didn't refetch
```

**Solution:**
- Verify handleRestoreTemplate calls fetchTemplates() after successful restore
- Check showArchived state triggers useEffect to refetch
- Hard refresh UI to force reload

---

## P2.20 Properties CRUD, Filters, and Sorting

**Overview:** Complete Properties Admin UI with full CRUD operations, server-side filters, and sorting.

**Purpose:** Make Properties functionality fully production-ready with 1:1 parity between Backend API capabilities and Admin UI features.

**Architecture:**
- **Backend**: 5 fully functional REST endpoints (GET list, GET detail, POST create, PATCH update, DELETE soft-delete)
- **Filters**: 6 query parameters (is_active, property_type, city, min_guests, owner_id, assignable_for_owner_id)
- **Sorting**: Dynamic sort_by/sort_order parameters (name, city, max_guests, created_at, updated_at, base_price)
- **Frontend**: Complete Admin UI with Create modal, Edit modal, Delete confirmation, Filter bar, Sortable columns
- **RBAC**: admin/manager (full CRUD), owner (view/edit own), staff/accountant (read-only)

**UI Routes:**
- `/properties` - Properties list page with filters, sorting, pagination, Create button
- `/properties/[id]` - Property detail page with Edit/Delete buttons

**API Endpoints:**

All roles:
- `GET /api/v1/properties?limit=&offset=&sort_by=&sort_order=&is_active=&property_type=&city=&min_guests=` - List properties with filters and sorting
- `GET /api/v1/properties/{id}` - Get single property details

Admin/Manager only:
- `POST /api/v1/properties` - Create new property (requires: name, property_type, bedrooms, beds, bathrooms, max_guests, address_line1, city, postal_code, base_price)
- `DELETE /api/v1/properties/{id}` - Soft delete property (sets deleted_at timestamp)

Admin/Manager/Owner:
- `PATCH /api/v1/properties/{id}` - Update property (partial update, all fields optional)

**Database Tables:**
- `properties` - Main properties table with 40+ fields (capacity, location, pricing, booking rules, status, timestamps)

**Migrations:**
- None (properties table already exists from Phase 17B)
- Updated PaginationParams schema to support optional sort_by/sort_order (P2.20)

**Verification Commands:**

```bash
# [HOST-SERVER-TERMINAL] Pull latest code
cd /data/repos/pms-webapp
git fetch origin main && git reset --hard origin/main

# [HOST-SERVER-TERMINAL] Optional: Verify deploy after Coolify redeploy
export API_BASE_URL="https://api.fewo.kolibri-visions.de"
./backend/scripts/pms_verify_deploy.sh

# [HOST-SERVER-TERMINAL] Run CRUD smoke test
export HOST="https://api.fewo.kolibri-visions.de"
export ADMIN_JWT_TOKEN="<<<admin/manager JWT>>>"
# Optional:
# export AGENCY_ID="ffd0123a-10b6-40cd-8ad5-66eee9757ab7"
./backend/scripts/pms_properties_crud_smoke.sh
echo "rc=$?"

# [HOST-SERVER-TERMINAL] Run filters/sort smoke test
export HOST="https://api.fewo.kolibri-visions.de"
export ADMIN_JWT_TOKEN="<<<admin/manager JWT>>>"
./backend/scripts/pms_properties_filters_sort_smoke.sh
echo "rc=$?"

# Expected output: All 7+7=14 tests pass, both rc=0
```

**Common Issues:**

### Create Property Returns 400 (Missing Required Fields)

**Symptom:** POST /api/v1/properties returns 400 Bad Request with validation errors.

**Root Cause:** Missing required fields in request body.

**Required Fields:**
- name (string)
- property_type (apartment/house/villa/condo/room/studio/cabin/cottage/chalet)
- bedrooms (int >= 0)
- beds (int >= 1)
- bathrooms (number >= 0, can be 1.5)
- max_guests (int > 0)
- address_line1 (string)
- city (string)
- postal_code (string)
- base_price (decimal string, e.g. "120.00")

**Solution:**
```bash
curl -X POST "$HOST/api/v1/properties" \
  -H "Authorization: Bearer $JWT" \
  -H "Content-Type: application/json" \
  -d '{
    "name": "Test Property",
    "property_type": "apartment",
    "bedrooms": 2, "beds": 3, "bathrooms": 1,
    "max_guests": 4,
    "address_line1": "Test Street 123",
    "city": "Berlin", "postal_code": "10115",
    "base_price": "120.00"
  }'
```

### Filters Return No Results

**Symptom:** GET /api/v1/properties with filters returns empty array despite matching properties existing.

**Root Cause:** Filter values don't match database values exactly (case-sensitive for some fields).

**How to Debug:**
```bash
# Test each filter individually
curl -sS "$HOST/api/v1/properties?is_active=true" -H "Authorization: Bearer $JWT" | jq '.items | length'
curl -sS "$HOST/api/v1/properties?property_type=apartment" -H "Authorization: Bearer $JWT" | jq '.items | length'
curl -sS "$HOST/api/v1/properties?city=Berlin" -H "Authorization: Bearer $JWT" | jq '.items | length'
curl -sS "$HOST/api/v1/properties?min_guests=4" -H "Authorization: Bearer $JWT" | jq '.items | length'
```

**Solution:**
- `is_active`: Must be "true" or "false" (string, not boolean)
- `property_type`: Must be lowercase exact match (apartment, house, villa, etc.)
- `city`: Case-insensitive partial match (Berlin, berlin, BERLIN all work)
- `min_guests`: Numeric string (4, not "4")

### Sorting Not Working

**Symptom:** GET /api/v1/properties with sort_by/sort_order returns results in wrong order.

**Root Cause:** Invalid sort_by field name or properties.py doesn't recognize it.

**Allowed sort_by values (line 102 properties.py):**
- name
- created_at
- updated_at
- city
- max_guests
- base_price

**How to Debug:**
```bash
# Test sorting by name ascending
curl -sS "$HOST/api/v1/properties?sort_by=name&sort_order=asc&limit=5" \
  -H "Authorization: Bearer $JWT" | jq '.items[].name'

# Should return names in alphabetical order
```

**Solution:**
- Use one of the allowed sort_by values
- Ensure sort_order is "asc" or "desc" (lowercase)
- If sort_by is invalid, backend falls back to default (name/asc)

### Delete Property Returns 409 (Active Bookings)

**Symptom:** DELETE /api/v1/properties/{id} returns 409 Conflict with message "Cannot delete property with active bookings".

**Root Cause:** Property has bookings that aren't completed/cancelled.

**How to Debug:**
```bash
# Check if property has active bookings
psql $DATABASE_URL -c "SELECT COUNT(*) FROM bookings WHERE property_id = '<property_id>' AND status NOT IN ('cancelled', 'completed');"
```

**Solution:**
- Cancel or complete all active bookings first
- Or mark property as inactive instead of deleting: `PATCH /api/v1/properties/{id}` with `{"is_active": false}`
- Soft delete preserves historical booking data

### Owner Can't Edit Property

**Symptom:** Owner user gets 403 Forbidden when trying to PATCH /api/v1/properties/{id}.

**Root Cause:** Property is not owned by this owner (owner_id doesn't match JWT sub claim).

**How to Debug:**
```bash
# Get property owner_id
curl -sS "$HOST/api/v1/properties/{id}" -H "Authorization: Bearer $JWT" | jq '.owner_id'

# Get JWT sub claim (user ID)
echo $JWT | cut -d'.' -f2 | base64 -d | jq '.sub'

# These must match for owner to have edit permission
```

**Solution:**
- Verify property.owner_id matches JWT sub claim
- If property is agency-owned (owner_id = null), only admin/manager can edit
- To assign property to owner: `PATCH /api/v1/properties/{id}/owner` as admin/manager

---

### Admin Deploy Fails: Property.title Does Not Exist (P2.20 Hotfix)

**Symptom:** Coolify admin UI build fails during `next build` with TypeScript error:
```
./app/properties/[id]/page.tsx:562:23 Type error: Property 'title' does not exist on type 'Property'.
```

**Root Cause:** Frontend Property interface had leftover references to `property.title` and `property.status` fields that don't exist in backend PropertyResponse schema. The backend only provides:
- `name` (public property name)
- `internal_name` (internal name for staff)
- `is_active` (boolean status, not a `status` string field)

**Fix Strategy:**
- Remove all references to non-existent `property.title` and `property.status` fields
- Ensure frontend Property interface matches backend PropertyResponse schema exactly
- Use `is_active` for status display (already rendered via getStatusDisplay() in header)

**How to Debug:**
```bash
# Find Property interface fields
rg -n "interface Property" frontend/app/properties/

# Find invalid field usages
rg -n "property\.(title|status)" frontend/app/properties/

# Check backend schema fields
rg -n "class PropertyResponse" backend/app/schemas/properties.py
sed -n '402,532p' backend/app/schemas/properties.py
```

**Solution:**
Removed the invalid conditional sections from detail page:
```typescript
// REMOVED - title field doesn't exist in backend
{property.title && (
  <div>
    <dt>Titel</dt>
    <dd>{property.title}</dd>
  </div>
)}

// REMOVED - status field doesn't exist (use is_active instead)
{property.status && (
  <div>
    <dt>Status</dt>
    <dd>{property.status}</dd>
  </div>
)}
```

**Prevention:**
- Always audit backend schema before adding frontend type fields
- Run `npm run build` locally before pushing to catch TypeScript errors
- Use ANTI-KREISREGEL: check actual code/schema first, don't guess field names

---

### CRUD Smoke Test Fails: deleted_at Not Set (P2.20.1 Bugfix)

**Symptom:** `pms_properties_crud_smoke.sh` fails at TEST 7 with error:
```
✗ TEST 7 FAILED: deleted_at not set (got '')
```

After DELETE, cleanup reports "Property not found" when attempting second delete.

**Root Cause (Before Fix):** DELETE endpoint returned `SuccessResponse` without property data. The soft delete worked (deleted_at was set in database), but the response didn't include the deleted property, so smoke script couldn't verify deleted_at timestamp. GET by ID filtered out deleted properties (WHERE deleted_at IS NULL), causing 404.

**Fix (P2.20.1):** DELETE response now includes the deleted property with deleted_at timestamp:
- New `PropertyDeleteResponse` schema with `success`, `message`, and `property` fields
- Service `delete_property()` returns full property data with deleted_at set
- DELETE route returns PropertyDeleteResponse instead of SuccessResponse
- Backward compatible: existing clients checking `.success` or `.message` still work

**Expected DELETE Response (After Fix):**
```json
{
  "success": true,
  "message": "Property <uuid> deleted successfully",
  "property": {
    "id": "...",
    "name": "...",
    "deleted_at": "2026-01-25T12:34:56.789Z",
    "is_active": false,
    ...
  }
}
```

**How to Debug:**
```bash
# Test DELETE returns property with deleted_at
DELETE_RESPONSE=$(curl -sS -X DELETE "$HOST/api/v1/properties/$PROPERTY_ID" \
  -H "Authorization: Bearer $ADMIN_JWT")

# Check response structure
echo "$DELETE_RESPONSE" | jq '.property.deleted_at'
# Should output: "2026-01-25T12:34:56.789Z" (not null or empty)

# Verify soft delete in database
psql $DATABASE_URL -c "SELECT id, deleted_at, is_active FROM properties WHERE id = '$PROPERTY_ID';"
# Should show: deleted_at IS NOT NULL, is_active = false
```

**Solution:**
- Pull latest code (commit includes PropertyDeleteResponse schema)
- Redeploy backend
- Rerun smoke script: `./backend/scripts/pms_properties_crud_smoke.sh`
- TEST 7 should now pass with message: "Property has deleted_at timestamp (<timestamp>)"

**Smoke Script Update (P2.20.1):**
- TEST 7 now verifies `.property.deleted_at` from DELETE response
- Contract-agnostic parsing: tries `.property.deleted_at`, fallback to `.deleted_at`
- Cleanup already best-effort with `|| true`, ignores 404 on second delete

**Prevention:**
- For soft delete operations, always return the deleted entity with timestamp in response
- Allows deterministic verification without requiring special include_deleted params
- Follows pattern: DELETE response should confirm what was actually deleted

---

## P2.21 Properties Nice-to-haves Pack

**Overview:** Comprehensive enhancement pack for properties including media management, listing status, location display, bulk operations, and CSV export.

**Purpose:** Provide complete property management capabilities with gallery uploads, public listing controls, batch operations for efficiency, and data export functionality.

**Architecture:**
- **Database**: `property_media` table for photos/videos with agency scoping, soft delete, cover image constraint
- **Backend**: Media CRUD endpoints, bulk action endpoint (activate/deactivate/archive/restore), CSV streaming export
- **Frontend**: 3-tab detail UI (Übersicht, Preiseinstellungen, Media), bulk selection on list page, export button
- **Derived Fields**: `is_listed` computed from `listed_at IS NOT NULL`
- **RBAC**: Manager/Admin for destructive operations (archive, restore, delete media)

**UI Routes:**
- `/properties` - List page with bulk actions and export (P2.20 + P2.21 enhancements)
- `/properties/{id}?tab=overview` - Overview tab (general info, owner, listing status, location, amenities)
- `/properties/{id}?tab=pricing` - Pricing tab (links to rate plans, season templates, quote calculator)
- `/properties/{id}?tab=media` - Media tab (photo gallery, upload, set cover, delete)

**API Endpoints:**

Media Management:
- `GET /api/v1/properties/{id}/media` - List property media (sorted by sort_order)
- `POST /api/v1/properties/{id}/media` - Add media (url, sort_order, is_cover, mime_type, byte_size)
- `PATCH /api/v1/properties/{id}/media/{media_id}` - Update media (is_cover, sort_order)
- `DELETE /api/v1/properties/{id}/media/{media_id}` - Soft delete media (sets deleted_at)

Bulk Operations:
- `POST /api/v1/properties/bulk` - Bulk action on properties
  - Actions: "activate" (is_active=true), "deactivate" (is_active=false), "archive" (deleted_at=NOW), "restore" (deleted_at=NULL)
  - Request: `{"property_ids": ["uuid1", "uuid2"], "action": "activate"}`
  - Response: `{"requested_count": 2, "updated_count": 2, "not_found_count": 0, "failed": []}`

Export:
- `GET /api/v1/properties/export` - CSV export with current filters (respects all query params)
  - Streaming response with Content-Disposition: attachment
  - Includes all property fields (40+ columns)

Listing Toggle (via PATCH /api/v1/properties/{id}):
- `{"is_listed": true}` - Sets listed_at = NOW() (public listing)
- `{"is_listed": false}` - Sets listed_at = NULL (unlisted)

**Database Tables:**

property_media:
```sql
CREATE TABLE property_media (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  agency_id UUID NOT NULL REFERENCES agencies(id),
  property_id UUID NOT NULL REFERENCES properties(id),
  url TEXT NOT NULL,
  sort_order INTEGER DEFAULT 0,
  is_cover BOOLEAN DEFAULT false,
  mime_type TEXT,
  byte_size BIGINT,
  created_at TIMESTAMPTZ DEFAULT now(),
  updated_at TIMESTAMPTZ DEFAULT now(),
  deleted_at TIMESTAMPTZ
);
-- Unique cover constraint: only one is_cover=true per property (WHERE deleted_at IS NULL)
```

properties (enhanced fields):
- `latitude DOUBLE PRECISION` - extracted from location geometry (ST_Y)
- `longitude DOUBLE PRECISION` - extracted from location geometry (ST_X)
- `is_listed BOOLEAN` - derived field (listed_at IS NOT NULL)
- `listed_at TIMESTAMPTZ` - timestamp when property was listed publicly

**Migration:** `20260125150000_add_property_media_and_enhancements.sql`

**Verification Commands:**

```bash
# [HOST-SERVER-TERMINAL] Pull latest code
cd /data/repos/pms-webapp
git fetch origin main && git reset --hard origin/main

# [HOST-SERVER-TERMINAL] Run P2.21 smoke test
export HOST="https://api.fewo.kolibri-visions.de"
export ADMIN_JWT_TOKEN="<<<admin JWT>>>"
# Optional:
# export AGENCY_ID="ffd0123a-10b6-40cd-8ad5-66eee9757ab7"
./backend/scripts/pms_properties_nice_to_haves_smoke.sh
echo "rc=$?"

# Expected output: All 10 tests pass, rc=0
```

**Smoke Test Coverage (pms_properties_nice_to_haves_smoke.sh):**
1. **Create property** - POST with lat/lng
2. **Add media** - POST /media with URL
3. **List media** - GET /media returns added media
4. **Set cover** - PATCH /media/{id} is_cover=true
5. **List toggle** - PATCH property is_listed=true, verify listed_at set
6. **Bulk activate** - POST /bulk action=activate
7. **Bulk archive** - POST /bulk action=archive, verify deleted_at set
8. **Bulk restore** - POST /bulk action=restore, verify deleted_at=NULL
9. **Export CSV** - GET /export returns CSV with property
10. **Cleanup** - Delete media and property

**Common Issues:**

### Media Upload Returns 400 (URL Required)

**Symptom:** POST /api/v1/properties/{id}/media returns 400 with "Field required" for url.

**Root Cause:** Request body missing `url` field or url is empty string.

**How to Debug:**
```bash
# Test media creation
curl -X POST "$HOST/api/v1/properties/$PROPERTY_ID/media" \
  -H "Authorization: Bearer $ADMIN_JWT" \
  -H "Content-Type: application/json" \
  -d '{"url": "https://example.com/image.jpg", "sort_order": 0}'

# Should return 201 with media object
```

**Solution:**
- Ensure request includes `{"url": "https://...", ...}`
- URL must be valid HTTP/HTTPS string
- Optional fields: sort_order (default 0), is_cover (default false), mime_type, byte_size

### Bulk Action Updates 0 Properties

**Symptom:** POST /api/v1/properties/bulk returns `{"updated_count": 0, "not_found_count": N}`.

**Root Cause:** Property IDs do not exist in agency or are already in target state.

**How to Debug:**
```bash
# Check property exists and belongs to agency
psql $DATABASE_URL -c "SELECT id, deleted_at, is_active FROM properties WHERE id IN ('uuid1', 'uuid2');"

# Verify bulk request
curl -X POST "$HOST/api/v1/properties/bulk" \
  -H "Authorization: Bearer $ADMIN_JWT" \
  -H "Content-Type: application/json" \
  -d '{"property_ids": ["uuid1", "uuid2"], "action": "activate"}' | jq

# Check response.updated_count and response.failed for error details
```

**Solution:**
- Verify property IDs exist: GET /api/v1/properties?limit=100
- Check agency_id matches JWT claim or x-agency-id header
- For "activate": properties must have deleted_at IS NULL
- For "archive": properties must have deleted_at IS NULL (can't archive twice)
- For "restore": properties must have deleted_at IS NOT NULL

### Export CSV Returns Empty File

**Symptom:** GET /api/v1/properties/export downloads CSV with headers only, no rows.

**Root Cause:** Query filters exclude all properties (e.g., owner_id not found, deleted properties hidden).

**How to Debug:**
```bash
# Test export with no filters
curl -X GET "$HOST/api/v1/properties/export" \
  -H "Authorization: Bearer $ADMIN_JWT" > /tmp/export.csv

# Check row count (should be > 1 if properties exist)
wc -l /tmp/export.csv

# Compare with list API
curl "$HOST/api/v1/properties?limit=100" \
  -H "Authorization: Bearer $ADMIN_JWT" | jq '.items | length'
```

**Solution:**
- Export uses same filters as list endpoint
- Check query params: `?owner_id=`, `?is_active=`, `?property_type=`, etc.
- Default: excludes deleted properties (deleted_at IS NULL)
- To include archived: backend does not support include_deleted param (archived properties never exported)

### Cover Image Constraint Violation

**Symptom:** PATCH /api/v1/properties/{id}/media/{media_id} with is_cover=true returns 409 Conflict.

**Root Cause:** Another media item already has is_cover=true for this property.

**How to Debug:**
```bash
# Check existing cover
psql $DATABASE_URL -c "SELECT id, url, is_cover FROM property_media WHERE property_id = '$PROPERTY_ID' AND deleted_at IS NULL;"

# Unique constraint ensures max 1 is_cover=true per property
```

**Solution:**
- Backend automatically unsets previous cover when setting new one (should not raise 409)
- If 409 occurs, check service layer property_service.py update_property_media method
- Expected behavior: Previous cover gets is_cover=false, new media gets is_cover=true (atomic transaction)

### Listed Toggle Does Not Update listed_at

**Symptom:** PATCH /api/v1/properties/{id} with `{"is_listed": true}` returns 200 but listed_at remains NULL.

**Root Cause:** Service layer not handling is_listed field to set/clear listed_at.

**How to Debug:**
```bash
# Test listing toggle
BEFORE=$(curl -sS "$HOST/api/v1/properties/$PROPERTY_ID" -H "Authorization: Bearer $ADMIN_JWT" | jq -r '.listed_at')
echo "Before: $BEFORE"

curl -X PATCH "$HOST/api/v1/properties/$PROPERTY_ID" \
  -H "Authorization: Bearer $ADMIN_JWT" \
  -H "Content-Type: application/json" \
  -d '{"is_listed": true}'

AFTER=$(curl -sS "$HOST/api/v1/properties/$PROPERTY_ID" -H "Authorization: Bearer $ADMIN_JWT" | jq -r '.listed_at')
echo "After: $AFTER"

# After should be ISO timestamp, not null
```

**Solution:**
- Verify property_service.py update_property method handles is_listed field
- Expected SQL: `SET listed_at = NOW()` when is_listed=true, `SET listed_at = NULL` when is_listed=false
- Check property response includes `is_listed` derived field: `(listed_at IS NOT NULL) as is_listed`

### Frontend Detail Page Tabs Not Working

**Symptom:** Clicking tabs on /properties/{id} does not change content, URL does not update.

**Root Cause:** useSearchParams or router.push not working correctly.

**How to Debug:**
```bash
# Check browser DevTools Network tab
# URL should update to /properties/{id}?tab=pricing when clicking Preiseinstellungen

# Verify query param in URL bar matches displayed content
```

**Solution:**
- Ensure Next.js app router is used (not pages router)
- Check imports: `import { useSearchParams, useRouter } from "next/navigation"`
- Verify handleTabChange calls `router.push(\`/properties/\${propertyId}?tab=\${tab}\`)`
- Hard refresh page (Cmd+Shift+R / Ctrl+F5) to clear Next.js cache

---

### CRUD Smoke Test Fails: Duplicate Property Tabs (P2.21.2 Bug)

**Symptom:** Property detail page shows TWO sets of tabs:
1. TOP tabs (correct): "Überblick | Preiseinstellungen"
2. INNER tabs (within page content): "Übersicht | Preiseinstellungen | Media"

Users see duplicate tab navigation. Clicking inner tabs changes URL query params (?tab=overview/pricing/media) but content stays within overview page.

**Root Cause (d4bb088):** P2.21 implementation added inner tabs with ?tab= query params inside the overview page, but the correct architecture requires TOP-level tabs in layout.tsx with separate routes for each tab.

**Fix (P2.21.2):**
- Extended TOP navigation (layout.tsx) to include "Media" tab → /properties/{id}/media
- Created dedicated media/page.tsx with media gallery UI (add, set cover, delete)
- Removed INNER tab navigation and conditional rendering from overview page.tsx
- Removed ?tab= query param logic (useSearchParams, currentTab state, handleTabChange)
- Removed pricing tab content (links to /rate-plans exist as separate route)
- Removed media tab content (now in media/page.tsx)
- Kept overview content: listing toggle, location, basic info cards, amenities

**Architecture (After Fix):**
- /properties/{id} → Overview page (general info, owner, listing, location, amenities)
- /properties/{id}/rate-plans → Pricing settings (existing route)
- /properties/{id}/media → Media gallery (new route with dedicated page)

**Verification Steps:**

UI Verification:
1. Navigate to /properties/{id}
2. Verify only ONE tab bar at top: "Überblick | Preiseinstellungen | Media"
3. Verify NO inner tabs within overview content
4. Verify "Überblick" tab shows: owner reference, listing toggle, location, basic info, amenities
5. Click "Preiseinstellungen" tab → redirects to /properties/{id}/rate-plans
6. Click "Media" tab → redirects to /properties/{id}/media
7. Verify media page shows gallery with add/set cover/delete functionality
8. Verify URL does NOT contain ?tab= query params

Deploy Verification:
```bash
# HOST-SERVER-TERMINAL
cd /data/repos/pms-webapp
git fetch origin main && git reset --hard origin/main

# Verify commit includes fix
git log --oneline -1
# Should show: fix(admin): remove duplicate property tabs + add media top tab (P2.21.2)

# Redeploy via Coolify (trigger redeploy for frontend + backend)

# Manual UI verification after deploy
# 1. Login as admin/manager
# 2. Navigate to /properties
# 3. Click any property row → should open /properties/{id}
# 4. Verify only ONE tab bar (no duplicate tabs)
# 5. Verify "Überblick" tab active by default
# 6. Click "Preiseinstellungen" → should route to /properties/{id}/rate-plans
# 7. Click "Media" → should route to /properties/{id}/media
# 8. Verify no ?tab= in URL
```

**Prevention:**
- For multi-section detail pages, always use top-level tabs in layout.tsx with separate routes
- Never implement tabs with query params (?tab=) inside page content
- Each tab should be a separate route (/ for overview, /rate-plans, /media, etc.)
- Query params should only be used for filters, pagination, search - not for navigation state

---

### Admin Build Fails: TSX Syntax Error in Properties Detail Page (P2.21.2.1)

**Symptom:** Coolify admin deploy fails with Next.js compile error:
```
./app/properties/[id]/page.tsx
Expected ',', got '{'
```

Error occurs around line ~852 at `{/* Amenities Assignment Modal */}` comment.

**Root Cause:** Extra closing `</div>` at line 849 (8-space indent) had no matching opening div. This caused the OUTER return div to close prematurely, leaving all modals (Amenities, Delete Confirmation, Edit, Toast) outside the JSX tree.

**TSX Structure (Before Fix - BROKEN):**
```tsx
return (
  <div className="space-y-6">  // line 493 - OUTER div opens
    {/* Header */}
    <div>...</div>  // line 495-554 - Header section

    {/* Content */}
    <div className="space-y-6">  // line 557 - Content div opens
      ...
      <div className="bg-bo-surface...">  // line 813 - Ausstattung card
        ...
      </div>  // line 848 - closes Ausstattung card
    </div>  // line 849 - closes Content div
  </div>  // line 850 - closes OUTER div - WRONG! Too early!
);  // line 851

{/* Amenities Assignment Modal */}  // line 852 - OUTSIDE return!
{showAmenitiesModal && (...)}  // JSX in non-JSX context = syntax error
```

**Issue:** Lines 848-850 had THREE closing `</div>` tags, but only TWO divs should close:
- Line 848: Close Ausstattung card (10-space indent) ✓
- Line 849: Close Content div (should be 6-space indent, was 8-space) ✗ EXTRA!
- Line 850: Close OUTER div (6-space indent) - became Content close after fix ✓

**Fix (P2.21.2.1):**
- Removed extra closing `</div>` at line 849 (8-space indent with no matching opening)
- Content div now closes at line 849 (was line 850)
- OUTER div now closes at line 1119 (end of return)
- All modals now correctly inside OUTER div

**TSX Structure (After Fix - CORRECT):**
```tsx
return (
  <div className="space-y-6">  // line 493 - OUTER div (stays open)
    {/* Header */}
    <div>...</div>  // line 495-554 - Header section

    {/* Content */}
    <div className="space-y-6">  // line 557 - Content div
      ...
      <div className="bg-bo-surface...">  // line 813 - Ausstattung card
        ...
      </div>  // line 848 - closes Ausstattung card
    </div>  // line 849 - closes Content div (FIXED)

    {/* Amenities Assignment Modal */}  // line 851 - INSIDE OUTER div
    {showAmenitiesModal && (...)}
    {/* other modals */}
  </div>  // line 1119 - closes OUTER div
);
```

**Verification Steps:**

Build Verification (MANDATORY):
```bash
# Build must succeed
cd frontend
npm run build

# Expected output:
# ✓ Compiled successfully
# ✓ Generating static pages (38/38)
```

Grep Proof (Modals Inside Return):
```bash
# Verify modal location
rg -n "Amenities Assignment Modal" frontend/app/properties/[id]/page.tsx
# Should show line 851 (inside return, after content close at 849)

# Verify no extra 8-space closing divs
python3 << 'EOF'
with open('frontend/app/properties/[id]/page.tsx') as f:
    lines = f.readlines()
for i in range(556, 851):
    line = lines[i]
    indent = len(line) - len(line.lstrip())
    if indent == 8 and '</div>' in line:
        print(f"Line {i+1}: {line.strip()}")
# Should output nothing (no 8-space closing divs)
EOF

# Verify no ?tab= query params (P2.21.2 architecture intact)
rg '\?tab=' frontend/app/properties/[id]/page.tsx
# Should return nothing

# Verify Media tab in TOP navigation
rg 'label.*Media' frontend/app/properties/[id]/layout.tsx
# Should show Media tab exists
```

Deploy Verification:
```bash
# HOST-SERVER-TERMINAL
cd /data/repos/pms-webapp
git fetch origin main && git reset --hard origin/main

# Verify commit
git log --oneline -1
# Should show: fix(admin): build fix for properties detail TSX (P2.21.2.1)

# Trigger Coolify redeploy for admin
# Monitor build logs - should succeed without TSX syntax errors

# After deploy, verify frontend compiles and runs
curl -sS https://admin.fewo.kolibri-visions.de/api/ops/version | jq
# Check commit matches latest

# Manual UI check
# 1. Navigate to /properties/{id}
# 2. Verify only ONE tab bar at top (no duplicate tabs)
# 3. Verify tabs work: Überblick, Preiseinstellungen, Media
# 4. Verify no ?tab= in URL
```

**Prevention:**
- When removing nested conditional rendering (`{condition && <div>...</div>}`), always verify outer div structure
- Count opening/closing tags carefully - use IDE bracket matching
- Test TSX compilation locally before pushing: `npx tsc --noEmit` in frontend dir
- For complex component restructuring, use git diff to review exactly what divs were added/removed

---

---

### Property Media: 500 Failed to list property media

**Symptom:** GET /api/v1/properties/{property_id}/media returns 500 Internal Server Error with message "Failed to list property media".

**Error in Logs:**
```
column pm.mime_type does not exist
column pm.byte_size does not exist (Hint: pm.file_size exists)
column pm.storage_provider does not exist
column pm.deleted_at does not exist
```

**Root Cause:** Schema drift. Production database has older property_media table schema with different column names (e.g., `file_size` instead of `byte_size`) or missing columns (`mime_type`, `storage_provider`, `deleted_at`). This occurs when:
1. property_media table was created manually or via older migration
2. Migration 20260125150000_add_property_media_and_enhancements.sql hasn't been applied
3. Migration 20260125160000_align_property_media_schema.sql (alignment fix) hasn't been applied

**Expected Behavior (After Fix):** Endpoint returns 503 Service Unavailable with actionable error message "Database schema not installed or out of date. Run DB migrations." This guides users to apply migrations instead of generic 500 error.

**Fix:**

**Option 1: Apply Migration (Recommended)**

1. **Pull latest code** on production server:
```bash
# HOST-SERVER-TERMINAL
cd /data/repos/pms-webapp
git fetch origin main && git reset --hard origin/main
```

2. **Apply migration** via Supabase CLI or SQL Editor:
```bash
# Using Supabase CLI (if configured)
supabase db push

# OR via SQL Editor in Supabase Dashboard:
# Copy contents of supabase/migrations/20260125160000_align_property_media_schema.sql
# Paste and execute in SQL Editor
```

3. **Verify columns exist**:
```sql
-- Check property_media schema
SELECT column_name, data_type
FROM information_schema.columns
WHERE table_schema = 'public'
  AND table_name = 'property_media'
ORDER BY ordinal_position;

-- Should include: mime_type (text), byte_size (bigint), storage_provider (text)
```

4. **Test endpoint**:
```bash
# HOST-SERVER-TERMINAL
export HOST="https://api.fewo.kolibri-visions.de"
export JWT_TOKEN="<<<your-jwt-token>>>"
export PROPERTY_ID="<<<property-uuid>>>"

./backend/scripts/pms_property_media_list_smoke.sh
# Expected: rc=0, HTTP 200, valid JSON array
```

**Option 2: Emergency SQL Patch (If migration fails)**

If migration cannot be applied immediately, run this SQL directly in Supabase SQL Editor:

```sql
-- Add missing columns if they don't exist
DO $$
BEGIN
  -- Add mime_type
  IF NOT EXISTS (
    SELECT 1 FROM information_schema.columns
    WHERE table_name = 'property_media' AND column_name = 'mime_type'
  ) THEN
    ALTER TABLE public.property_media ADD COLUMN mime_type TEXT;
  END IF;

  -- Add byte_size
  IF NOT EXISTS (
    SELECT 1 FROM information_schema.columns
    WHERE table_name = 'property_media' AND column_name = 'byte_size'
  ) THEN
    ALTER TABLE public.property_media ADD COLUMN byte_size BIGINT;
  END IF;

  -- Add storage_provider
  IF NOT EXISTS (
    SELECT 1 FROM information_schema.columns
    WHERE table_name = 'property_media' AND column_name = 'storage_provider'
  ) THEN
    ALTER TABLE public.property_media ADD COLUMN storage_provider TEXT;
  END IF;
END $$;

-- Backfill byte_size from file_size (if file_size column exists)
DO $$
BEGIN
  IF EXISTS (
    SELECT 1 FROM information_schema.columns
    WHERE table_name = 'property_media' AND column_name = 'file_size'
  ) THEN
    UPDATE public.property_media
    SET byte_size = file_size
    WHERE byte_size IS NULL AND file_size IS NOT NULL;
  END IF;
END $$;

-- Set default storage_provider
UPDATE public.property_media
SET storage_provider = 'supabase'
WHERE storage_provider IS NULL;
```

**Verification Commands:**

```bash
# HOST-SERVER-TERMINAL
cd /data/repos/pms-webapp
git fetch origin main && git reset --hard origin/main

# Run smoke test
export HOST="https://api.fewo.kolibri-visions.de"
export JWT_TOKEN="<<<jwt-token>>>"
export PROPERTY_ID="<<<property-uuid>>>"
# Optional:
# export AGENCY_ID="ffd0123a-10b6-40cd-8ad5-66eee9757ab7"

./backend/scripts/pms_property_media_list_smoke.sh
echo "rc=$?"

# Expected output:
# ✅ GET /api/v1/properties/{id}/media returned 200 OK
# ✅ Response is valid JSON
# ℹ Media count: N item(s)
# ✅ All property media smoke tests passed! 🎉
# rc=0
```

**Manual API Test:**
```bash
# Test with curl
curl -X GET "$HOST/api/v1/properties/$PROPERTY_ID/media" \
  -H "Authorization: Bearer $JWT_TOKEN" \
  -H "x-agency-id: $AGENCY_ID" \
  -w "\nHTTP %{http_code}\n"

# Expected: HTTP 200, JSON array with media objects
# Before fix: HTTP 500 or HTTP 503 with schema error message
```

**Prevention:**
- Always apply migrations to production before deploying code that depends on new schema
- Run `pms_verify_deploy.sh` + smoke scripts after deployment to catch schema drift early
- Monitor backend logs for "column does not exist" errors (indicates schema drift)

---

---

## Property Media Upload

**Overview:** Property media file upload feature allows admin/manager users to upload image files directly from the Admin UI instead of entering URLs.

**Purpose:** Enable drag-and-drop/file picker upload of property images to Supabase Storage with automatic media record creation.

**Architecture:**
- **Storage Backend**: Supabase Storage bucket `property-media` (private bucket, exists in PROD)
- **Storage Path**: `agencies/{agency_id}/properties/{property_id}/{uuid}.{ext}`
- **Upload Endpoint**: POST /api/v1/properties/{id}/media/upload (multipart/form-data)
- **RBAC**: Requires manager/admin role
- **File Validation**: 
  - Allowed types: image/jpeg, image/png, image/webp
  - Max size: 10MB
  - MIME type validation on backend
- **Storage Module**: `backend/app/core/storage.py` (SupabaseStorage helper)

**Frontend:**
- **UI Route**: `/properties/:id/media` - Dual-mode modal (File Upload / URL)
- **Upload Flow**: File picker → Preview → Upload to backend → Backend uploads to Supabase → Media record created
- **Modes**: 
  - File Upload (default): Drag-and-drop zone with preview
  - URL (secondary): Direct URL entry (existing flow)

**API Endpoints:**
- `POST /api/v1/properties/{id}/media/upload` - Upload image file (multipart/form-data)
- `GET /api/v1/properties/{id}/media` - List property media (includes uploaded files)
- `DELETE /api/v1/properties/{id}/media/{media_id}` - Delete media (also deletes from Supabase Storage)

**Verification Commands:**

```bash
# [HOST-SERVER-TERMINAL] Pull latest code
cd /data/repos/pms-webapp
git fetch origin main && git reset --hard origin/main

# [HOST-SERVER-TERMINAL] Optional: Verify deploy after Coolify redeploy
export API_BASE_URL="https://api.fewo.kolibri-visions.de"
./backend/scripts/pms_verify_deploy.sh

# [HOST-SERVER-TERMINAL] Run upload smoke test
export HOST="https://api.fewo.kolibri-visions.de"
export MANAGER_JWT_TOKEN="<<<manager/admin JWT>>>"
# Optional:
# export PROPERTY_ID="660e8400-e29b-41d4-a716-446655440000"
# export AGENCY_ID="ffd0123a-10b6-40cd-8ad5-66eee9757ab7"
./backend/scripts/pms_property_media_upload_smoke.sh
echo "rc=$?"

# Expected output: All 3 tests pass (upload → list → delete), rc=0
```

**Common Issues:**

### Upload Returns 500 "Upload failed"

**Symptom:** POST /api/v1/properties/{id}/media/upload returns 500 with error message "Upload failed (HTTP XXX): ..."

**Root Cause:** Supabase Storage API call failed. Common reasons:
- SUPABASE_SERVICE_ROLE_KEY env var not set or invalid
- property-media bucket doesn't exist
- Service role lacks storage permissions
- Network/connectivity issues to Supabase

**How to Debug:**
```bash
# Check backend env vars
docker exec pms-backend env | grep SUPABASE

# Should see:
# SUPABASE_URL=https://<project>.supabase.co
# SUPABASE_SERVICE_ROLE_KEY=eyJ...
# SUPABASE_ANON_KEY=eyJ...

# Test Supabase Storage API directly with service role
SUPABASE_URL="https://<project>.supabase.co"
SERVICE_KEY="eyJ..."
curl -X GET "$SUPABASE_URL/storage/v1/bucket/property-media" \
  -H "Authorization: Bearer $SERVICE_KEY" \
  -H "apikey: $SERVICE_KEY"

# Should return bucket details (200 OK)
```

**Solution:**
- Ensure SUPABASE_SERVICE_ROLE_KEY is set in backend environment (check Coolify env vars)
- Verify property-media bucket exists in Supabase Dashboard → Storage
- Check service role has storage.objects.create permission
- Restart backend after env var changes

### Upload Returns 400 "Invalid file type"

**Symptom:** Upload fails with 400 Bad Request, error: "Only image/jpeg, image/png, and image/webp files are allowed"

**Root Cause:** User uploaded unsupported file type (e.g., GIF, BMP, TIFF, PDF)

**How to Debug:**
```bash
# Check file MIME type in request
# Browser DevTools → Network → Upload request → Request Headers
# Content-Type: multipart/form-data; boundary=...
# Form Data → file → type: image/xxx

# Backend validates against: ["image/jpeg", "image/png", "image/webp"]
```

**Solution:**
- Only upload JPG, PNG, or WebP files
- Convert other image formats (e.g., `convert input.gif output.png` with ImageMagick)
- If WebP needed for production, ensure browser support (all modern browsers OK)

### Upload Returns 413 "Payload too large"

**Symptom:** Upload fails with 413 error before reaching backend validation

**Root Cause:** File exceeds proxy/server limits (backend allows 10MB, but proxy may have lower limit)

**How to Debug:**
```bash
# Check file size
ls -lh /path/to/image.jpg
# If > 10MB, backend will reject with 400
# If < 10MB but still 413, proxy limit issue

# Check nginx/Coolify proxy config
# Default nginx client_max_body_size might be 1m or 10m
```

**Solution:**
- Compress images before upload (use tools like TinyPNG, ImageOptim)
- Increase proxy limit if needed (Coolify → Service → nginx client_max_body_size)
- Backend limit: 10MB (hardcoded in upload endpoint validation)

### Uploaded Image URL Doesn't Load (404 or 403)

**Symptom:** Upload succeeds, media appears in listing, but image URL returns 404 or 403 when accessed

**Root Cause:** property-media bucket is private (requires signed URLs), but public URL was returned

**How to Debug:**
```bash
# Check media record URL format
curl -X GET "$HOST/api/v1/properties/$PROPERTY_ID/media" \
  -H "Authorization: Bearer $JWT" | jq '.[0].url'

# Public URL format (won't work for private bucket):
# https://<project>.supabase.co/storage/v1/object/public/property-media/...

# Signed URL format (works for private bucket):
# https://<project>.supabase.co/storage/v1/object/sign/property-media/...?token=...

# Test URL directly
curl -I "<url from media record>"
# 403 Forbidden → private bucket, need signed URL
# 404 Not Found → file doesn't exist in storage
```

**Solution:**
- Update media listing endpoint to generate signed URLs for private bucket
- Or update storage.py to use signed URLs on upload
- Or make property-media bucket public (if appropriate for use case)
- For private bucket, frontend should request signed URLs from backend (not use URLs directly)

### Media Upload Succeeds But Doesn't Appear in Listing

**Symptom:** Upload returns 201 Created, but media doesn't show in GET /api/v1/properties/{id}/media

**Root Cause:** Media record created with deleted_at timestamp or transaction not committed

**How to Debug:**
```bash
# Check database for media record
psql $DATABASE_URL -c "
  SELECT id, property_id, url, deleted_at, created_at 
  FROM property_media 
  WHERE property_id = '$PROPERTY_ID' 
  ORDER BY created_at DESC 
  LIMIT 5;
"

# Check if deleted_at IS NOT NULL (soft deleted)
# Check if record exists but missing from API response

# Check backend logs for transaction errors
docker logs pms-backend --tail 100 | grep -i "property_media\|transaction\|rollback"
```

**Solution:**
- If deleted_at IS NOT NULL: Bug in upload endpoint, should set deleted_at = NULL
- If transaction rollback: Check backend logs for constraint violations or errors
- If record missing: Check agency_id scoping in listing query (media belongs to different agency)

### Delete Media Fails (Storage File Remains)

**Symptom:** DELETE /api/v1/properties/{id}/media/{media_id} returns 204 but file remains in Supabase Storage

**Root Cause:** Storage deletion failed but database deletion succeeded (orphaned file in bucket)

**How to Debug:**
```bash
# Check if file still exists in Supabase Storage
# Dashboard → Storage → property-media → Browse path

# Check backend logs for StorageError
docker logs pms-backend --tail 100 | grep -i "storage\|delete"

# Manually delete via Storage API
STORAGE_PATH="agencies/<agency_id>/properties/<property_id>/<uuid>.png"
curl -X DELETE "$SUPABASE_URL/storage/v1/object/property-media/$STORAGE_PATH" \
  -H "Authorization: Bearer $SERVICE_KEY" \
  -H "apikey: $SERVICE_KEY"
```

**Solution:**
- Orphaned files don't affect functionality (database is source of truth)
- Implement periodic cleanup job to delete orphaned storage files
- Update delete endpoint to log storage errors but not fail transaction
- Current implementation: storage deletion failure returns False but doesn't raise exception

---

### Media Thumbnails Broken / HTTP 400 on /object/public URLs

**Symptom:** Media gallery shows broken images or HTTP 400 errors. Browser console shows errors loading thumbnails from URLs like `https://sb-pms.kolibri-visions.de/storage/v1/object/public/property-media/...`

**Root Cause:** The property-media bucket is PRIVATE, not public. Public URLs (`/object/public/...`) return HTTP 400 for private buckets. The API must return signed URLs via `display_url` field.

**How to Debug:**
```bash
# Check if media listing returns display_url
curl -X GET "$HOST/api/v1/properties/$PROPERTY_ID/media" \
  -H "Authorization: Bearer $JWT" | jq '.[0] | {url, display_url, storage_provider, storage_path}'

# Expected output:
# {
#   "url": "https://...supabase.co/storage/v1/object/public/property-media/...",
#   "display_url": "https://...supabase.co/storage/v1/object/sign/property-media/...?token=...",
#   "storage_provider": "supabase",
#   "storage_path": "agencies/.../properties/.../uuid.png"
# }

# If display_url is null:
# - Check backend has app/core/storage.py with get_signed_url() method
# - Check property_service.py list_property_media() generates signed URLs
# - Check SUPABASE_SERVICE_ROLE_KEY env var is set

# Test signed URL accessibility
SIGNED_URL="<display_url from above>"
curl -k -I "$SIGNED_URL"
# Should return HTTP 200 with Content-Type: image/*
```

**Solution:**
- Backend: Ensure `list_property_media()` generates signed URLs for storage_provider="supabase"
- Backend: Ensure `add_property_media()` and `update_property_media()` also return display_url
- Frontend: Use `display_url || signed_url || url` for image src (fallback chain)
- Required env vars: SUPABASE_URL, SUPABASE_SERVICE_ROLE_KEY
- Bucket must exist: property-media (can be private or public, but private requires signed URLs)

**Production Verification:**
```bash
# Run updated smoke script with display_url validation
HOST="https://api.fewo.kolibri-visions.de" \
MANAGER_JWT_TOKEN="..." \
./backend/scripts/pms_property_media_upload_smoke.sh

# Test 2.5 should PASS (validates display_url is retrievable)
```


### InvalidJWT / jwt malformed on Signed Media URLs

**Symptom:** Media thumbnails show broken images (question marks). Browser console shows errors loading signed URLs with response: `{"statusCode":"400","error":"InvalidJWT","message":"jwt malformed"}`

**Root Cause:** Signed URLs must be generated by Supabase Storage API using service role key, not constructed manually. The JWT token embedded in signed URLs is created by Supabase and can only be validated by Supabase.

**How to Debug:**
```bash
# Check if display_url contains a valid Supabase signed URL
curl -X GET "$HOST/api/v1/properties/$PROPERTY_ID/media" \
  -H "Authorization: Bearer $JWT" | jq '.[0].display_url'

# Should return URL like:
# https://<project>.supabase.co/storage/v1/object/sign/property-media/<path>?token=<jwt>

# Test the signed URL directly
curl -H "Range: bytes=0-32" "<display_url>"
# Should return HTTP 200/206 with image data

# If InvalidJWT error:
# - Check backend calls POST /storage/v1/object/sign API (not manual JWT construction)
# - Check SUPABASE_SERVICE_ROLE_KEY env var is set correctly
# - Check response parsing handles signedURL/signedUrl field variations
```

**Solution:**
- Backend: `storage.py` must call `POST {SUPABASE_URL}/storage/v1/object/sign/{bucket}/{path}` with service role key
- Parse response robustly: try `signedURL`, `signedUrl`, or `signed_url` fields
- Return absolute URL: `SUPABASE_URL + signed_path`
- Never construct signed URLs manually - always use Supabase Storage API

**Affected Code:**
- `backend/app/core/storage.py`: `get_signed_url()` method
- `backend/app/services/property_service.py`: Media listing/add/update methods

---

### PATCH Media is_cover Returns 500 (Unique Constraint Violation)

**Symptom:** PATCH `/api/v1/properties/{id}/media/{media_id}` with `{"is_cover": true}` returns HTTP 500. Logs show: `duplicate key value violates unique constraint "ux_property_media_one_cover"` or `idx_property_media_unique_cover`.

**Root Cause:** Unique constraint `idx_property_media_unique_cover` ensures only one cover per property (WHERE is_cover=true AND deleted_at IS NULL). If the "unset existing cover" query doesn't clear ALL covers (including soft-deleted ones), a duplicate can occur.

**How to Debug:**
```bash
# Check for multiple covers (including soft-deleted)
psql $DATABASE_URL -c "
  SELECT id, property_id, is_cover, deleted_at
  FROM property_media
  WHERE property_id = '$PROPERTY_ID'
  AND is_cover = true
  ORDER BY deleted_at NULLS FIRST;
"

# Should show at most 1 row with deleted_at IS NULL
# If multiple rows with is_cover=true exist (even soft-deleted), constraint can fail

# Check the unique constraint definition
psql $DATABASE_URL -c "\d+ property_media" | grep -A2 "idx_property_media_unique_cover"
```

**Solution:**
- In `property_service.py` `update_property_media()` and `add_property_media()`:
  - When setting is_cover=true, first run:
    ```sql
    UPDATE property_media
    SET is_cover = false, updated_at = NOW()
    WHERE property_id = $property_id
    -- IMPORTANT: No filters on id or deleted_at
    -- This clears ALL covers including soft-deleted ones
    ```
  - Then set is_cover=true on the target media item
- This prevents race conditions and soft-deleted cover conflicts

**Fixed In:** P2.21.4.2 (2026-01-26)

---

### Media Thumbnails Broken / 401 Basic realm=kong / Missing /storage/v1

**Symptom:** Media gallery shows broken images (question marks). Browser console shows HTTP 401 errors when loading thumbnails. Error response: `401 Basic realm="kong"`. URLs look like `https://sb-pms.kolibri-visions.de/object/sign/property-media/...?token=...` (missing `/storage/v1` prefix).

**Root Cause:** Self-hosted Supabase (via Kong gateway) returns signed URL paths starting with `/object/sign/...` instead of `/storage/v1/object/sign/...`. The backend must normalize these paths to include the correct `/storage/v1` prefix for Kong routing.

**How to Debug:**
```bash
# Check display_url returned by media listing API
curl -sS -H "Authorization: Bearer $JWT_TOKEN" \
  "$HOST/api/v1/properties/$PROPERTY_ID/media" | jq '.[0].display_url'

# Expected (correct): https://sb-pms.kolibri-visions.de/storage/v1/object/sign/property-media/...?token=...
# Broken (missing prefix): https://sb-pms.kolibri-visions.de/object/sign/property-media/...?token=...

# Test if display_url is accessible
DISPLAY_URL=$(curl -sS -H "Authorization: Bearer $JWT_TOKEN" \
  "$HOST/api/v1/properties/$PROPERTY_ID/media" | jq -r '.[0].display_url')
curl -I "$DISPLAY_URL"

# Should return HTTP 200/206 with Content-Type: image/*
# NOT 401 Basic realm="kong"
```

**Solution:**
- Backend: Ensure `storage.py` includes `_normalize_storage_url()` helper that handles:
  - Paths starting with `/object/...` → prefix with `/storage/v1`
  - Paths starting with `/storage/v1/...` → no change
  - Absolute URLs → no change
- Verify `get_signed_url()` calls `_normalize_storage_url()` before returning
- All `list_property_media()`, `add_property_media()`, `update_property_media()` use `storage.get_signed_url()` for Supabase files

**Production Verification:**
```bash
# Run smoke script with display_url validation (Test 2.5)
HOST="https://api.fewo.kolibri-visions.de" \
MANAGER_JWT_TOKEN="..." \
./backend/scripts/pms_property_media_upload_smoke.sh

# Test 2.5 should PASS (validates display_url is HTTP 200/206 with image/* content-type)
# Admin UI should show real thumbnails (no "?" placeholders)
```

---

### Cover Toggle Returns 500 / Unique Constraint Violation (Transactional Fix)

**Symptom:** PATCH `/api/v1/properties/{id}/media/{media_id}` with `{"is_cover": true}` returns HTTP 500. Backend logs show:
```
duplicate key value violates unique constraint "idx_property_media_unique_cover"
Detail: Key (property_id, is_cover) where is_cover=true AND deleted_at IS NULL already exists.
```

**Root Cause:** The unique partial index `idx_property_media_unique_cover` ensures only ONE cover per property (WHERE is_cover=true AND deleted_at IS NULL). If the "clear all covers" UPDATE and "set new cover" UPDATE are not in the same transaction, a race condition or incomplete clear can violate the constraint.

**How to Debug:**
```bash
# Check how many covers exist for a property (including soft-deleted)
psql $DATABASE_URL -c "
SELECT id, is_cover, deleted_at
FROM property_media
WHERE property_id = '$PROPERTY_ID'
ORDER BY is_cover DESC, deleted_at NULLS FIRST;
"

# Should show at most 1 row with is_cover=true AND deleted_at IS NULL

# Try setting cover via PATCH
curl -X PATCH "$HOST/api/v1/properties/$PROPERTY_ID/media/$MEDIA_ID" \
  -H "Authorization: Bearer $JWT_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"is_cover": true}'

# Should return HTTP 200 with updated media object
# NOT 500 with unique constraint violation
```

**Solution:**
- Backend: Wrap cover toggle in explicit transaction in `property_service.py`:
  ```python
  if media_data.get("is_cover"):
      async with self.db.transaction():
          # Step 1: Clear ALL covers (including soft-deleted)
          await self.db.execute(
              "UPDATE property_media SET is_cover=false WHERE property_id=$1",
              property_id
          )
          # Step 2: Set this media as cover (INSERT or UPDATE)
          row = await self.db.fetchrow(query, *params)
  ```
- Apply same fix to both `add_property_media()` (INSERT) and `update_property_media()` (UPDATE)
- Clear query must NOT filter by `id` or `deleted_at` (clears even the row being updated + soft-deleted covers)

**Production Verification:**
```bash
# Run smoke script with cover toggle test (Test 2.7)
HOST="https://api.fewo.kolibri-visions.de" \
MANAGER_JWT_TOKEN="..." \
./backend/scripts/pms_property_media_upload_smoke.sh

# Test 2.7 should PASS:
#   - Set media 1 as cover → success
#   - Set media 2 as cover → success
#   - Verify only media 2 is cover (count=1, id=media_2)

# Manual test in Admin UI:
# 1. Go to /properties/{id}/media
# 2. Click "Als Titelbild" on image A → should work (no 500)
# 3. Click "Als Titelbild" on image B → should work (no 500)
# 4. Verify only ONE "Titelbild" badge shows (on image B)
```

---

### Admin UI Thumbnails Show "?" (Cached Signed URLs)

**Symptom:** Admin UI at https://admin.fewo.kolibri-visions.de shows "?" placeholders for media thumbnails even though backend returns correct signed URLs. curl -I on display_url returns HTTP 200 and Content-Type: image/*, but browser shows broken images.

**Root Cause:** Browser or Next.js caching keeps expired signed URLs (1-hour TTL). Once an image fails to load, the old behavior permanently replaced `<img src>` with a placeholder SVG, preventing recovery even after refetch.

**How to Debug:**
```bash
# Verify backend returns correct signed URLs
curl -sS -H "Authorization: Bearer $JWT_TOKEN" \
  "$HOST/api/v1/properties/$PROPERTY_ID/media" | jq '.[0].display_url'

# Verify display_url is accessible
DISPLAY_URL=$(curl -sS -H "Authorization: Bearer $JWT_TOKEN" \
  "$HOST/api/v1/properties/$PROPERTY_ID/media" | jq -r '.[0].display_url')
curl -I "$DISPLAY_URL"

# Should return HTTP 200 with Content-Type: image/*
# If not, backend normalization issue (see "401 Basic realm=kong" section above)

# If backend works but UI still shows "?":
# 1. Open browser DevTools (F12) → Network tab
# 2. Hard reload (Cmd+Shift+R / Ctrl+F5)
# 3. Check image requests - should be HTTP 200, NOT 304 (cached)
# 4. If still 304 or failing, clear browser cache + reload
```

**Solution (Applied in P2.21.4.4):**
- Frontend: Media fetch now uses `cache: "no-store"` + Cache-Control: no-store header + cache-bust query param `?cb=Date.now()`
- Frontend: Image onError handler auto-retries once by refetching media (gets fresh signed URLs), then shows placeholder with "Neu laden" button
- Frontend: Cover image (Überblick tab) has same retry logic
- API client: GET method accepts `noCache` parameter to force cache: "no-store"

**Production Verification:**
```bash
# Admin UI:
# 1. Login to https://admin.fewo.kolibri-visions.de
# 2. Navigate to /properties/{id}/media
# 3. Hard reload (Cmd+Shift+R / Ctrl+F5)
# 4. Thumbnails should load (no "?" placeholders)
# 5. Open DevTools → Network tab
# 6. Refresh page → image requests should be HTTP 200 (NOT 304 cached)
# 7. Check request URL includes ?cb= timestamp parameter
# 8. Check response headers: no Cache-Control: max-age (should be no-store or absent)

# If image fails to load:
# - Wait 1-2 seconds → should auto-retry (refetch media)
# - If still broken, click "Neu laden" button on thumbnail
# - Should refetch and load image

# Cover image (Überblick tab):
# 1. Navigate to /properties/{id} (Überblick tab)
# 2. Cover image should load correctly
# 3. If broken, should auto-retry once
```

**Manual Workaround (if fix not deployed):**
- Hard reload (Cmd+Shift+R / Ctrl+F5) to clear cache
- If still broken, clear browser cache completely
- Thumbnails will work for 1 hour (signed URL TTL), then need reload

**Related Issues:**
- If signed URLs return 401 Basic realm="kong", see "Media Thumbnails Broken / 401 Basic realm=kong" section above
- If cover toggle returns 500, see "Cover Toggle Returns 500 / Transactional Fix" section above

---

### Media Delete Removes Storage Object (P2.21.4.5)

**Overview:** DELETE /api/v1/properties/{id}/media/{media_id} now deletes BOTH the database record (soft delete) AND the Supabase Storage object from the property-media bucket.

**Behavior:**
- Fetches media record to get storage_provider and storage_path
- If storage_provider == "supabase": Deletes file from Supabase Storage using storage_path
- Then soft deletes DB record (sets deleted_at)
- Storage deletion is idempotent: If file already deleted/missing → treat as success
- If storage deletion fails due to auth/config → returns HTTP 409 Conflict with actionable message

**How to Debug:**
```bash
# Verify media is deleted from DB
psql $DATABASE_URL -c "SELECT id, deleted_at FROM property_media WHERE id = '$MEDIA_ID';"
# Should show deleted_at is NOT NULL

# Verify storage file is not accessible
# (Use display_url from media listing before deletion)
curl -I "$DISPLAY_URL"
# Should return HTTP 400/404/410 (NOT 200/206)
```

**Troubleshooting:**

**Issue: Delete returns 409 "Failed to delete media file from storage"**

**Cause:** Supabase Storage deletion failed (auth error, network, bucket not found).

**Solution:**
```bash
# Check backend env vars
# SUPABASE_URL, SUPABASE_SERVICE_ROLE_KEY must be set
# Service role key must have storage delete permissions

# Check backend logs for specific error
grep "Failed to delete storage file" /var/log/backend.log

# If file was already deleted manually:
# Re-delete via UI → should succeed (idempotent)

# If auth issue:
# Verify SUPABASE_SERVICE_ROLE_KEY is correct service role key (not anon key)
# Restart backend after fixing env vars
```

**Issue: Delete succeeds but file still accessible**

**Cause:** Eventual consistency lag or CDN caching.

**Solution:**
- Wait 1-2 minutes, retry accessing display_url
- Expected: HTTP 404/410 after lag
- If still HTTP 200 after 5 min → storage deletion did not work, check backend logs

---

### Admin Lightbox / Thumbnail Enlarge (P2.21.4.5)

**Overview:** Media gallery thumbnails in Admin UI (/properties/{id}/media) are now clickable. Clicking a thumbnail opens a lightbox modal with the image enlarged.

**Features:**
- Click thumbnail → Modal with full-size image (max 85vh)
- "Öffnen in neuem Tab" button → Opens image URL in new browser tab
- "Schließen" button or click outside modal → Closes lightbox
- Uses same signed URL + retry logic as thumbnails

**How to Use:**
```
# Admin UI:
1. Navigate to /properties/{id}/media
2. Click any thumbnail image
3. Lightbox modal opens with enlarged image
4. Click "Schließen" or press Esc (browser native) to close
5. Click "Öffnen in neuem Tab" to open image in new tab
```

**Troubleshooting:**

**Issue: Lightbox shows "?" placeholder**

**Cause:** Same as thumbnail issue - expired signed URL or caching.

**Solution:**
- Close lightbox, hard reload page (Cmd+Shift+R)
- Click thumbnail again → lightbox should load image
- If still broken, see "Admin UI Thumbnails Show '?'" section above

**Issue: Clicking thumbnail doesn't open lightbox**

**Cause:** JavaScript error or modal state issue.

**Solution:**
- Open browser console (F12) → Check for errors
- Hard reload page (Cmd+Shift+R)
- If persists, check frontend deployment is up-to-date

---

---

### Admin UI: Media Lightbox Thumbnails Not Clickable (P2.21.4.6)

**Symptom:** Thumbnails in Admin UI media gallery show hover overlay with action buttons, but clicking the thumbnail itself does not open the lightbox modal. Lightbox only opens when clicking outside the overlay area.

**Root Cause:** CSS pointer-events issue. The hover overlay (`absolute inset-0`) covers the entire thumbnail including the image, blocking the image's `onClick` handler. The overlay has no `pointer-events: none` CSS, so it intercepts all clicks.

**How It's Fixed (P2.21.4.6):**
- Overlay div: Added `pointer-events-none` class (overlay doesn't block clicks)
- Action buttons ("Als Titelbild", "Löschen"): Added `pointer-events-auto` class (buttons still clickable)
- Result: Clicking anywhere on thumbnail (including overlay area) triggers lightbox

**Verification:**
```bash
# HOST-SERVER-TERMINAL
# UI smoke test validates lightbox testid exists
export ADMIN_BASE_URL="https://admin.fewo.kolibri-visions.de"
export MANAGER_JWT_TOKEN="..."
export PROPERTY_ID="23dd8fda-59ae-4b2f-8489-7a90f5d46c66"
./backend/scripts/pms_admin_ui_overview_media_smoke.sh
echo "rc=$?"

# Manual verification:
# 1. Navigate to /properties/{id}/media
# 2. Click thumbnail image (anywhere, including hover area)
# 3. Lightbox modal should open immediately
# 4. Verify action buttons still work (Als Titelbild/Löschen)
```

---

### Admin UI: Overview Page Redesign (P2.21.4.6)

**Overview:** Improved property overview page layout for better UX - "stylish, inviting, summary character".

**Changes Implemented:**
1. **Hero Summary Section**: Combined cover image + key facts in single card
   - Left: Compact cover preview (aspect-video, max-w 400px, object-contain)
   - Right: Quick facts (status, listed, guests, bedrooms, base price, min stay)
   - Quick actions: Medien verwalten, Listen/Unlisten, Eigentümer anzeigen
2. **Organized Content Cards**: Added emoji icons to section headers
   - 📋 Objektinformationen (includes description, property type)
   - 📍 Adresse & Lage
   - 🛏️ Kapazität
   - 💰 Zeiten & Preise
   - 🔑 IDs und Referenzen (less prominent: opacity-75, smaller text)
   - ⏱️ Zeitstempel (less prominent: opacity-75, smaller text)
   - 🗺️ Koordinaten & Karte
   - ⭐ Ausstattung
3. **Edit Mode**: Added data-testid for automation (`overview-edit-toggle`, `edit-modal-overlay`)

**Verification:**
```bash
# HOST-SERVER-TERMINAL
# UI smoke test validates overview testids (P2.21.4.6 + P2.21.4.7)
export ADMIN_BASE_URL="https://admin.fewo.kolibri-visions.de"
export MANAGER_JWT_TOKEN="..."
export PROPERTY_ID="23dd8fda-59ae-4b2f-8489-7a90f5d46c66"
./backend/scripts/pms_admin_ui_overview_media_smoke.sh
# Expected: rc=0, 6/6 tests passed (includes P2.21.4.7 lightbox nav, coords editing)

# Manual verification:
# 1. Navigate to /properties/{id}
# 2. Verify Hero Summary section appears FIRST (cover left + facts right)
# 3. Verify all emoji icons render correctly
# 4. Click "Bearbeiten" button → modal opens with all editable fields
# 5. Edit any field → click "Speichern" → changes persist
# 6. Edit again → click "Abbrechen" → changes reverted (no API call)
```

**Troubleshooting:**
- **Cover image not showing**: Check media endpoint returns is_cover=true item, see "Media Thumbnails Broken" section
- **Edit modal fields empty**: Check property data loaded, ensure editData state initialized correctly
- **Hero section layout broken on mobile**: Check responsive grid: `grid-cols-1 lg:grid-cols-[400px_1fr]` collapses to single column on small screens

---

## Admin UI Polish: Lightbox Navigation + Overview Reordering + Coordinates Editing (P2.21.4.7)

**Overview:** Polish improvements for Admin UI property pages - lightbox navigation, card reordering, and editable coordinates.

**Changes Implemented:**

1. **Lightbox Prev/Next Navigation** (media/page.tsx):
   - Left/right arrow controls (‹ and ›) inside lightbox modal
   - Keyboard support: ArrowLeft/ArrowRight to navigate, ESC to close
   - Wrap-around navigation: next from last → first, prev from first → last
   - Arrows hidden when 0-1 images
   - Index indicator: "2 / 5" shown when multiple images
   - Implementation: lightboxIndex state, openLightbox/goToPrevious/goToNext functions, keyboard event listener
   - data-testids: lightbox-prev, lightbox-next, lightbox-index

2. **Overview Card Reordering** (properties/[id]/page.tsx):
   - Moved ⭐ Ausstattung card UP (after Zeiten & Preise, before IDs)
   - Moved 🗺️ Koordinaten & Karte card UP (directly under Ausstattung)
   - Final order: Hero → Objektinformationen → Adresse → Kapazität → Zeiten & Preise → **Ausstattung** → **Koordinaten & Karte** → IDs → Zeitstempel
   - Goal: Feature-rich content (amenities, map) visible before technical IDs/timestamps

3. **Coordinates Editing** (properties/[id]/page.tsx):
   - Added Breitengrad (latitude) and Längengrad (longitude) inputs to edit modal
   - Validation: lat -90 to 90, lng -180 to 180, step 0.000001, empty allowed → saves as null
   - data-testids: edit-lat, edit-lng (edit modal), overview-lat, overview-lng (coordinate display)
   - Backend: PropertyUpdate schema already supports latitude/longitude (ge=-90, le=90, ge=-180, le=180)
   - Saves via existing PATCH /api/v1/properties/{id} endpoint
   - Updated handleStartEdit to include lat/lng in editData initialization

**Verification:**
```bash
# HOST-SERVER-TERMINAL
# UI smoke test validates new testids
export ADMIN_BASE_URL="https://admin.fewo.kolibri-visions.de"
export MANAGER_JWT_TOKEN="..."
export PROPERTY_ID="23dd8fda-59ae-4b2f-8489-7a90f5d46c66"
./backend/scripts/pms_admin_ui_overview_media_smoke.sh
# Expected: rc=0, 6/6 tests passed (includes lightbox-prev/next, overview-lat/lng, edit-lat/lng)

# Manual verification - Lightbox Navigation:
# 1. Navigate to /properties/{id}/media
# 2. Click any thumbnail → lightbox opens
# 3. Click right arrow (›) → next image shows, index updates (e.g., 2/5 → 3/5)
# 4. Click left arrow (‹) → previous image shows
# 5. Press ArrowRight on keyboard → next image (same as clicking arrow)
# 6. Press ArrowLeft on keyboard → previous image
# 7. When on last image, click right arrow → wraps to first image
# 8. When on first image, click left arrow → wraps to last image
# 9. Press ESC → lightbox closes
# 10. If only 1 image: arrows not shown

# Manual verification - Card Order:
# 1. Navigate to /properties/{id}
# 2. Scroll down past Hero section
# 3. Verify card order: Zeiten & Preise → **Ausstattung** → **Koordinaten & Karte** → IDs → Zeitstempel
# 4. Ausstattung should appear before IDs (not after Koordinaten as before)

# Manual verification - Coordinates Editing:
# 1. Navigate to /properties/{id}
# 2. Click "Bearbeiten" button → edit modal opens
# 3. Scroll to Breitengrad and Längengrad inputs (near bottom, before "Objekt ist aktiv")
# 4. Enter latitude: 48.137154 (Munich)
# 5. Enter longitude: 11.576124 (Munich)
# 6. Click "Speichern" → modal closes, coordinates updated
# 7. Scroll to 🗺️ Koordinaten & Karte card → verify lat/lng display shows new values
# 8. Click "In Google Maps öffnen" → opens Munich location
# 9. Edit again → clear lat/lng inputs (empty) → Save → coordinates become null (show "—")
```

**Troubleshooting:**

### Lightbox Arrows Not Working
**Symptom:** Left/right arrows in lightbox don't navigate or don't appear.

**Possible Causes:**
1. Only 0-1 media items: Arrows intentionally hidden when `media.length <= 1`
2. JavaScript error in navigation functions
3. Browser console shows React error

**How to Debug:**
```bash
# Check browser console for errors
# Open DevTools → Console tab
# Click thumbnail → lightbox opens
# Look for errors like "Cannot read property 'id' of undefined"

# Verify media array populated
# DevTools → Components tab → find PropertyMediaPage component
# Check state.media array length
# If length > 1, arrows should be visible
```

**Solution:**
- If media.length > 1 but arrows missing: Check media/page.tsx line ~570-590 for conditional render `{media.length > 1 && ...}`
- If navigation broken: Check goToPrevious/goToNext functions (lines ~40-60) for index calculation errors
- If keyboard broken: Check useEffect keyboard listener (lines ~65-85) for event handler issues

### Card Order Wrong
**Symptom:** Ausstattung or Koordinaten cards appear in wrong position (e.g., after IDs instead of before).

**Possible Causes:**
1. Old frontend build cached by browser
2. Code not deployed to production
3. JSX structure changed by other developer

**How to Debug:**
```bash
# Check deployed frontend commit
curl -sS https://admin.fewo.kolibri-visions.de/api/ops/version | jq '.source_commit'
# Compare with local git log

# Hard refresh browser
# Cmd+Shift+R (Mac) or Ctrl+F5 (Windows)

# View page source (Cmd+U) and search for "Ausstattung"
# Should appear BEFORE "IDs und Referenzen" in HTML order
```

**Solution:**
- Hard refresh browser to clear cached JS bundle
- If still wrong: Redeploy frontend via Coolify
- Verify properties/[id]/page.tsx lines ~867-992 have correct card order

### Coordinate Inputs Not Saving
**Symptom:** Edit lat/lng, click "Speichern", but coordinates don't update on overview page.

**Possible Causes:**
1. Validation error: lat out of -90..90 or lng out of -180..180
2. Backend rejects update (PATCH endpoint error)
3. Frontend doesn't include lat/lng in PATCH payload

**How to Debug:**
```bash
# Open DevTools → Network tab
# Click "Bearbeiten" → enter lat/lng → click "Speichern"
# Find PATCH request to /api/v1/properties/{id}
# Check Request Payload for latitude/longitude fields
# Check Response: if 422, see validation error details

# Backend logs (if PATCH returns 500)
docker logs pms-backend-api | grep "PATCH /api/v1/properties"
```

**Solution:**
- If validation error: Ensure lat -90..90, lng -180..180, use decimal format (not 48° 8' 17")
- If payload missing lat/lng: Check handleSaveEdit function includes editData with lat/lng
- If backend error: Check PropertyUpdate schema in schemas/properties.py allows latitude/longitude

### data-testid Not Found in Smoke Script
**Symptom:** pms_admin_ui_overview_media_smoke.sh fails Test 4, 5, or 6 with "testid not found".

**Possible Causes:**
1. Frontend not deployed (old build)
2. HTML structure changed (data-testid attribute removed or renamed)
3. Edit modal not rendered in initial page load (lightbox only appears on click)

**How to Debug:**
```bash
# Fetch page HTML and search for testid
curl -sS "https://admin.fewo.kolibri-visions.de/properties/23dd8fda-59ae-4b2f-8489-7a90f5d46c66" | grep -i "lightbox-prev"

# Expected: should find data-testid="lightbox-prev" (even though not visible initially)
# If not found: frontend not deployed or code changed

# Check media page for lightbox nav
curl -sS "https://admin.fewo.kolibri-visions.de/properties/23dd8fda-59ae-4b2f-8489-7a90f5d46c66/media" | grep -c "lightbox-prev"
# Expected: 1 (lightbox modal rendered but hidden until clicked)
```

**Solution:**
- Redeploy frontend via Coolify
- Hard refresh browser and test manually before running smoke script
- If testid changed: Update smoke script to match new testid name

---

---

### Admin UI Smoke Script Gets 307 Redirect to /login (P2.21.4.7a)

**Symptom:** When running `pms_admin_ui_overview_media_smoke.sh`, curl requests to admin pages return HTTP 307 redirects to /login, so HTML doesn't contain expected data-testids and all tests fail 0/6.

**Root Cause:** Admin UI pages require authentication via Supabase session cookies (server-side). The layout component calls `getAuthenticatedUser()` which redirects to /login when no valid session is found. Smoke scripts using curl cannot maintain browser session cookies, causing auth failures.

**Solution (P2.21.4.7a):** Narrow smoke auth bypass in Next.js middleware:

1. **Middleware Logic** (frontend/middleware.ts):
   - Checks for BOTH headers: `x-pms-smoke: 1` AND `Authorization: Bearer <JWT>`
   - Only for GET/HEAD requests on `/properties/*` routes
   - Validates JWT online against Supabase `/auth/v1/user` endpoint (using anon key)
   - If validation succeeds: Injects JWT as Supabase session cookie (`sb-access-token`)
   - If validation fails: Falls through to normal redirect-to-login behavior

2. **Security Constraints**:
   - No bypass without BOTH headers
   - No bypass for POST/PUT/PATCH/DELETE methods
   - Only `/properties/*` routes (no other admin pages)
   - Token must be validated online (not just decoded)
   - If Supabase URL/anon key missing from env: bypass is disabled

3. **Smoke Script Updates** (pms_admin_ui_overview_media_smoke.sh):
   - Sends `Authorization: Bearer $MANAGER_JWT_TOKEN` header
   - Sends `x-pms-smoke: 1` header
   - Sends `Cache-Control: no-cache` header
   - Checks HTTP status before grepping HTML:
     - If not 200: Shows status, redirect location, first 40 lines of HTML, then fails
     - If 200: Proceeds to grep for data-testids

**How to Debug:**

```bash
# Test middleware bypass manually
export JWT_TOKEN="your-manager-jwt-token"
curl -I https://admin.fewo.kolibri-visions.de/properties/23dd8fda-59ae-4b2f-8489-7a90f5d46c66 \
  -H "Authorization: Bearer $JWT_TOKEN" \
  -H "x-pms-smoke: 1"

# Expected: HTTP/2 200 (with Set-Cookie: sb-access-token=...)
# NOT: HTTP/2 307 location: /login

# If still 307, check middleware logs:
docker logs pms-frontend | grep "Smoke auth"

# Verify JWT is valid:
curl -X GET "https://sb-pms.kolibri-visions.de/auth/v1/user" \
  -H "apikey: YOUR_SUPABASE_ANON_KEY" \
  -H "Authorization: Bearer $JWT_TOKEN"
# Should return 200 with user object
```

**Troubleshooting:**

1. **Still getting 307 despite headers:**
   - Check JWT is valid: `curl /auth/v1/user` as shown above
   - Check JWT not expired: Decode with `jwt.io` and verify `exp` claim
   - Check NEXT_PUBLIC_SB_URL and NEXT_PUBLIC_SUPABASE_ANON_KEY set in frontend env
   - Check middleware code deployed: `curl /api/ops/version` and verify commit hash

2. **Smoke script fails with HTTP 500:**
   - Check middleware validation error in logs: `docker logs pms-frontend`
   - Supabase /auth/v1/user endpoint might be down or rate-limited

3. **Bypass works but getAuthenticatedUser still fails:**
   - Cookie injection might not be working
   - Check cookie secure flag matches protocol (httpOnly + secure for HTTPS)
   - Check maxAge not too short (currently 1 hour = 3600s)

**Expected Behavior After Fix:**
- Smoke script: `./pms_admin_ui_overview_media_smoke.sh` returns 6/6 tests passed (rc=0)
- No 307 redirects to /login
- HTML contains expected data-testids
- Bypass only works with valid JWT + x-pms-smoke header
- Normal browser users still require proper login (no security degradation)

---

### Admin UI Smoke Bypass Still Returns 307 After P2.21.4.7a (P2.21.4.7b)

**Symptom:** After implementing P2.21.4.7a smoke auth bypass, curl requests with `x-pms-smoke: 1` and `Authorization: Bearer <JWT>` headers still return HTTP 307 redirects to /login. Smoke script fails 0/6 tests.

**Root Cause:** P2.21.4.7a set `sb-access-token` cookie only on the RESPONSE via `response.cookies.set()`. The server component `getAuthenticatedUser()` in layout runs DURING the same request and cannot see response cookies - it can only see REQUEST cookies. Therefore, the redirect still happens.

**Solution (P2.21.4.7b):** Inject the validated JWT token as a cookie into the DOWNSTREAM REQUEST headers, not just the response. This allows server components to see the cookie immediately during the same request.

**Implementation:**
```typescript
// After JWT validation succeeds (middleware.ts lines ~79-107):
if (userResponse.ok) {
  // Clone request headers and inject cookie
  const h = new Headers(request.headers);
  const existingCookies = request.headers.get('cookie') ?? '';
  const cookieValue = `sb-access-token=${token}`;
  const mergedCookies = existingCookies
    ? `${existingCookies}; ${cookieValue}`
    : cookieValue;
  h.set('cookie', mergedCookies);
  h.set('x-pathname', pathname);

  const response = NextResponse.next({
    request: { headers: h }, // Use modified headers with cookie
  });

  // Also set cookie on response for completeness
  response.cookies.set('sb-access-token', token, {...});
  
  // Debug header to confirm bypass activated
  response.headers.set('x-pms-smoke-auth', 'ok');

  return response;
}
```

**Key Difference from P2.21.4.7a:**
- **P2.21.4.7a**: Only set cookie on RESPONSE → server components can't see it → still 307
- **P2.21.4.7b**: Inject cookie into REQUEST headers → server components see it → 200 OK

**How to Debug:**
```bash
# Test smoke bypass with headers
curl -I https://admin.fewo.kolibri-visions.de/properties/23dd8fda-59ae-4b2f-8489-7a90f5d46c66 \
  -H "Authorization: Bearer $JWT_TOKEN" \
  -H "x-pms-smoke: 1"

# P2.21.4.7a (broken): Returns HTTP/1.1 307 Temporary Redirect
# P2.21.4.7b (fixed): Returns HTTP/1.1 200 OK

# Check for debug header
curl -I https://admin.fewo.kolibri-visions.de/properties/23dd8fda-59ae-4b2f-8489-7a90f5d46c66 \
  -H "Authorization: Bearer $JWT_TOKEN" \
  -H "x-pms-smoke: 1" | grep -i "x-pms-smoke-auth"

# Should output: x-pms-smoke-auth: ok
```

**Verification:**
```bash
# HOST-SERVER-TERMINAL
export ADMIN_BASE_URL="https://admin.fewo.kolibri-visions.de"
export MANAGER_JWT_TOKEN="..."
export PROPERTY_ID="23dd8fda-59ae-4b2f-8489-7a90f5d46c66"
./backend/scripts/pms_admin_ui_overview_media_smoke.sh

# Expected output: 6/6 tests passed, all ✅
# Expected: "ℹ Smoke bypass activated: x-pms-smoke-auth: ok" messages
```

**Common Issues:**

1. **Still getting 307 despite P2.21.4.7b:**
   - Check JWT is valid (not expired): Decode JWT and check `exp` claim
   - Check Supabase env vars in middleware: `NEXT_PUBLIC_SB_URL` and `NEXT_PUBLIC_SUPABASE_ANON_KEY` must be set
   - Check middleware matcher: Must include `/properties/*` routes
   - Check request method: Only GET/HEAD allowed, not POST/PUT/DELETE

2. **x-pms-smoke-auth header not present:**
   - Middleware bypass not activated - check BOTH headers present: `x-pms-smoke: 1` AND `Authorization: Bearer <token>`
   - JWT validation failed - check token against Supabase: `curl https://<supabase-url>/auth/v1/user -H "apikey: <anon-key>" -H "Authorization: Bearer <token>"`

3. **Deploy verification fails with commit mismatch:**
   - After P2.21.4.7b, `pms_verify_deploy.sh` is STRICT: auto-detects git HEAD and fails (rc=3) if deployed backend/admin commits don't match
   - To bypass strict mode: unset EXPECT_COMMIT env var before running script
   - To fix: Redeploy backend/admin via Coolify to match local git HEAD

---

### Admin UI Smoke Bypass Returns 307 Despite Middleware Activation (P2.21.4.7c)

**Symptom:** After deploying P2.21.4.7b, curl requests with smoke headers return HTTP 307 redirect to /login, BUT response includes `x-pms-smoke-auth: ok` header. This proves middleware is running and validating the JWT, but the server component still redirects in the same request.

**Root Cause:** P2.21.4.7b attempted to inject `sb-access-token` cookie into REQUEST headers (line 90 of middleware.ts), but this doesn't work because:
- Supabase SSR client reads cookies via Next.js `cookies()` helper
- Manually setting the `cookie` header doesn't affect how `cookies()` reads cookies
- `getAuthenticatedUser()` calls `supabase.auth.getSession()` which depends on cookies
- Session not found → redirect to /login in same request

**Solution (P2.21.4.7c):** Use internal request headers instead of cookie injection:

1. **Middleware** (frontend/middleware.ts):
   - After JWT validation succeeds, set internal REQUEST headers:
     - `x-pms-smoke-auth: ok` (signals smoke mode to server components)
     - `x-pms-smoke-token: <JWT>` (passes token for server-side validation)
   - These headers are INTERNAL to the request and NOT echoed in response headers
   - Response header `x-pms-smoke-auth: ok` is still set for debugging

2. **Auth Helper** (frontend/app/lib/server-auth.ts):
   - Check for `x-pms-smoke-auth === 'ok'` and `x-pms-smoke-token` headers
   - If present: Create minimal Supabase client with token in Authorization header
   - Call `supabase.auth.getUser(token)` directly (token-based, not session-based)
   - If user valid: Continue with normal auth flow (no redirect)
   - If user invalid: Fall back to redirect

**Key Difference from P2.21.4.7b:**
- **P2.21.4.7b**: Injected cookie into request headers → Supabase client didn't see it → still 307
- **P2.21.4.7c**: Pass internal headers + use token-based `getUser()` → bypasses session check → 200 OK

**How to Verify:**
```bash
# Test smoke bypass with curl
curl -I https://admin.fewo.kolibri-visions.de/properties/23dd8fda-59ae-4b2f-8489-7a90f5d46c66 \
  -H "Authorization: Bearer $MANAGER_JWT_TOKEN" \
  -H "x-pms-smoke: 1" \
  -H "Cache-Control: no-cache"

# P2.21.4.7b (broken): HTTP/2 307 + x-pms-smoke-auth: ok
# P2.21.4.7c (fixed): HTTP/2 200 + x-pms-smoke-auth: ok

# Run smoke script
export ADMIN_BASE_URL="https://admin.fewo.kolibri-visions.de"
export MANAGER_JWT_TOKEN="..."
export PROPERTY_ID="23dd8fda-59ae-4b2f-8489-7a90f5d46c66"
./backend/scripts/pms_admin_ui_overview_media_smoke.sh

# Expected: rc=0, 6/6 tests passed
# Expected: "ℹ Smoke bypass activated: x-pms-smoke-auth: ok" messages
```

**Security Notes:**
- `x-pms-smoke-token` header is INTERNAL (request only) and never echoed in response
- Bypass only activates when BOTH headers present: `x-pms-smoke: 1` AND `Authorization: Bearer <JWT>`
- JWT is still validated against Supabase `/auth/v1/user` in middleware
- Only works for GET/HEAD on `/properties/*` routes
- Without smoke headers, normal auth flow applies (redirect to /login)

---

### Admin UI Smoke Test Requires Playwright for DOM Validation (P2.21.4.7d)

**Problem:** Previous smoke script used `curl + grep` to validate HTML for data-testids, but this fails because:
- Property overview page (`frontend/app/properties/[id]/page.tsx`) has `"use client"` directive
- Media page (`frontend/app/properties/[id]/media/page.tsx`) has `"use client"` directive
- Client components render testids via React hydration on the client side
- Server HTML (`curl` response) doesn't include hydrated DOM elements
- Result: `grep` finds 0 matches for `data-testid="overview-cover"` etc.

**Solution (P2.21.4.7d):** Replace curl+grep with Playwright running in Docker container.

**How It Works:**
1. Bash script creates temporary Playwright JS file via heredoc
2. Runs Docker container: `mcr.microsoft.com/playwright:v1.48.0-jammy`
3. Playwright launches headless Chromium with smoke auth headers
4. Navigates to property pages, waits for client-side DOM hydration
5. Validates `data-testid` selectors exist in rendered DOM
6. Returns exit code 0 (pass) or >0 (fail count)

**Required Dependencies:**
- Docker installed and daemon running on host
- Internet access to pull Playwright Docker image (first run only)
- No node/npm required on host (Playwright runs in container)

**Verification:**
```bash
# HOST-SERVER-TERMINAL
export ADMIN_BASE_URL="https://admin.fewo.kolibri-visions.de"
export MANAGER_JWT_TOKEN="..."
export PROPERTY_ID="23dd8fda-59ae-4b2f-8489-7a90f5d46c66"
./backend/scripts/pms_admin_ui_overview_media_smoke.sh

# Expected output:
# ℹ Starting Admin UI Playwright smoke test...
# ℹ Running Playwright tests in Docker container...
# Test 1: Overview page contains cover section...
# ✓ Smoke bypass activated: x-pms-smoke-auth: ok
# ✅ Test 1 PASSED: Found data-testid="overview-cover" in DOM
# ...
# Test Results: 5/5 passed
# ✅ All Admin UI smoke tests passed! 🎉
```

**Common Issues:**

1. **Docker not found:**
   ```
   ❌ ERROR: docker command not found
   ```
   Solution: Install Docker Desktop or Docker Engine and ensure daemon is running.

2. **Playwright image pull fails:**
   ```
   Error: Unable to find image 'mcr.microsoft.com/playwright:v1.48.0-jammy' locally
   ```
   Solution: Ensure internet connection and Docker registry access. Image is ~1.5GB on first pull.

3. **HTTP 307 redirect to /login:**
   - Smoke auth bypass not working - check MANAGER_JWT_TOKEN is valid (not expired)
   - Check deployed code includes P2.21.4.7c middleware + auth helper fixes
   - Verify `x-pms-smoke-auth: ok` header missing in Playwright output

4. **Timeout waiting for selector:**
   ```
   ❌ Test 1 FAILED: data-testid="overview-cover" not found in DOM
   Error: Timeout 10000ms exceeded
   ```
   - Page loaded but testid missing: Check frontend deployment includes testids
   - Page didn't load: Check PROPERTY_ID exists and is accessible
   - Network issue: Check admin.fewo.kolibri-visions.de is reachable from Docker container

5. **No media thumbnails found (not a failure):**
   ```
   ⚠️  No media thumbnails found (property may have no media)
   ✅ Test 4 PASSED: Media page loaded, no media to test
   ```
   This is expected if the test property has no uploaded media. Test still passes.

**Comparison to Previous Approach:**
- **P2.21.4.7a-c**: `curl + grep` HTML for testids → fails (testids not in server HTML)
- **P2.21.4.7d**: Playwright → waits for client hydration → finds testids in DOM → passes

---

### Admin UI Smoke Script: Playwright Package Installation (P2.21.4.7e)

**Note:** The Playwright Docker image (`mcr.microsoft.com/playwright:v1.48.0-jammy`) includes browsers but not the Node.js playwright npm package. The smoke script automatically installs it inside the container on first run using cached Docker volumes.

**How It Works:**
- Script creates a `package.json` with `"playwright": "1.48.0"` dependency
- Docker volumes cache `node_modules` and npm cache between runs:
  - `pms_playwright_smoke_node_modules:/work/node_modules`
  - `pms_playwright_smoke_npm_cache:/root/.npm`
- On first run: `npm install` runs inside container (takes ~30-60 seconds)
- Subsequent runs: Skip npm install (node_modules already cached)
- Sets `PLAYWRIGHT_SKIP_BROWSER_DOWNLOAD=1` (browsers already in image)
- Sets `PLAYWRIGHT_BROWSERS_PATH=/ms-playwright` (use pre-installed browsers)

**First Run Behavior:**
```bash
$ ./backend/scripts/pms_admin_ui_overview_media_smoke.sh
ℹ Starting Admin UI Playwright smoke test...
ℹ Running Playwright tests in Docker container...
ℹ Installing Playwright npm package (cached volumes, first run only)...
# (npm install output - takes ~30-60 seconds)
Test 1: Overview page contains cover section...
...
```

**Subsequent Runs:**
```bash
$ ./backend/scripts/pms_admin_ui_overview_media_smoke.sh
ℹ Starting Admin UI Playwright smoke test...
ℹ Running Playwright tests in Docker container...
# (no npm install, uses cached node_modules)
Test 1: Overview page contains cover section...
...
```

**Troubleshooting:**

1. **npm install fails:**
   - Script shows last 50 lines of npm output and exits rc=1
   - Check internet connection and npm registry access
   - Try clearing volumes and re-running (see below)

2. **Corrupted cached volumes:**
   ```bash
   # Clear cached volumes and force fresh install
   docker volume rm pms_playwright_smoke_node_modules pms_playwright_smoke_npm_cache
   
   # Re-run script (will reinstall on next run)
   ./backend/scripts/pms_admin_ui_overview_media_smoke.sh
   ```

3. **playwright package not found after install:**
   - Script shows `ls -la node_modules/` output
   - Usually indicates npm install incomplete or corrupted
   - Solution: Clear volumes (see above)

---

### Admin UI Smoke Script: Docker Mount rc=125 Error (P2.21.4.7f)

**Symptom:** Smoke script fails with Docker exit code 125 and error message:
```
docker: Error response from daemon: failed to create shim task: OCI runtime create failed: 
runc create failed: unable to start container process: error during container init: 
error mounting "/var/lib/docker/volumes/pms_playwright_smoke_node_modules/_data" 
to rootfs at "/work/node_modules": mkdir /work/node_modules: read-only file system
```

**Root Cause:** Docker cannot create volume mountpoints (like `/work/node_modules`) under a parent directory that is mounted read-only. When the bind mount `-v "$TEMP_DIR:/work:ro"` uses the `:ro` flag, Docker cannot create the mountpoint for the volume `-v pms_playwright_smoke_node_modules:/work/node_modules`.

**How It's Fixed (P2.21.4.7f):**
The bind mount was changed from read-only to read-write:
- Before: `-v "$TEMP_DIR:/work:ro"`
- After: `-v "$TEMP_DIR:/work"` (default is read-write)

This allows Docker to create the `/work/node_modules` directory as a mountpoint for the volume.

**Security Note:**
The temp directory contains only `smoke.js` and `package.json`. These files are not modified by the bash script inside the container. The read-write mount is necessary only for Docker to create volume mountpoints, not for the script to modify files.

**Troubleshooting:**

1. **Script still fails with rc=125:**
   - Check the docker run command in the smoke script
   - Verify the bind mount does NOT have `:ro` flag: `rg '\-v.*TEMP_DIR.*work' backend/scripts/pms_admin_ui_overview_media_smoke.sh`
   - Expected: `-v "$TEMP_DIR:/work"` (no `:ro`)
   - NOT expected: `-v "$TEMP_DIR:/work:ro"`

2. **Volume mountpoints don't persist:**
   - Named volumes persist across runs: `docker volume ls | grep pms_playwright_smoke`
   - If volumes are missing, they will be recreated on next run (npm install happens again)

3. **Permission denied errors:**
   - Check Docker daemon has permission to create volumes
   - On Linux: Ensure user is in docker group or run with sudo

**Verification:**
```bash
# Clear volumes to test fresh mount
docker volume rm pms_playwright_smoke_node_modules pms_playwright_smoke_npm_cache 2>/dev/null || true

# Run smoke script
./backend/scripts/pms_admin_ui_overview_media_smoke.sh

# Expected: rc=0, 5/5 tests passed
# NOT expected: rc=125 with "read-only file system" error
```

---

### Next Build Failed: TSX Syntax Error After Style Migration (P2.21.4.7x)

**Symptom:** Coolify deploy fails at `npm run build` with TypeScript/TSX syntax error:
```
./app/properties/[id]/page.tsx
Error: Unexpected token `div`. Expected jsx identifier
```
Or similar JSX parsing errors in client components.

**Root Cause:** Unclosed JSX elements (like `<Card>`, `<div>`) cause the TypeScript parser to become confused, leading to syntax errors at seemingly unrelated lines. After migrating to new component libraries (like LuxeStay), missing closing tags are a common issue.

**How to Debug:**
```bash
# Run TypeScript compiler directly to get clearer error messages
cd frontend
npx tsc --noEmit --jsx preserve "app/properties/[id]/page.tsx"

# Look for errors like:
# "JSX element 'Card' has no corresponding closing tag"

# Check for missing closing tags
grep -n "<Card" app/properties/[id]/page.tsx
grep -n "</Card>" app/properties/[id]/page.tsx
# If opening tags > closing tags, you have an unclosed element
```

**Solution:**
1. Identify the unclosed JSX element from TypeScript error output
2. Trace through the component structure to find where it should close
3. Replace incorrect closing tag (e.g., `</div>`) with correct tag (e.g., `</Card>`)
4. Verify build: `npm run build`

**Common Patterns:**
- Component refactors that replace `<div>` wrappers with custom components
- Copy-paste errors where closing tags don't match opening tags
- Nested components where inner closing tags accidentally close outer components

**Prevention:**
- Use IDE with JSX bracket matching
- Run `npm run build` locally before committing style changes
- Use TypeScript strict mode to catch JSX errors early

**Related Issues:**
- Missing utility imports (e.g., `cn` function for className merging)
- TypeScript type errors in component props

---

### Media Gallery Design Issues (P2.21.4.7y)

**Overview:** LuxeStay design system applied to property media gallery with responsive grid, hover actions, and lightbox.

**Common Issues:**

1. **Lightbox not opening:**
   - Check browser console for JavaScript errors
   - Verify `data-testid="media-lightbox"` exists on grid container
   - Ensure media items have onClick handlers
   - Check if media array is populated: `console.log(media.length)`

2. **Keyboard navigation not working:**
   - Verify lightbox is open (lightboxIndex !== null)
   - Check browser DevTools for event listener conflicts
   - Test with different keyboard: Arrow Left/Right should navigate, Escape should close

3. **Cover badge not showing:**
   - Check if media item has `is_cover: true` property
   - Verify gold star SVG renders: `data-testid="cover-badge-{id}"`
   - Check CSS z-index layering (badge should be above image)

4. **Hover actions not appearing:**
   - Ensure mouse hover on thumbnail (not in gap between items)
   - Check `group` and `group-hover:` classes work together
   - Verify overlay div is not hidden by parent overflow

5. **Upload button not visible:**
   - Check if user has manager/admin role (required for upload)
   - Verify gold button renders in header section
   - Check CSS: `luxe-button-primary` class should apply gold background

6. **Grid layout broken:**
   - Check responsive breakpoints: 2 cols (mobile), 3 cols (md), 4 cols (lg)
   - Verify Tailwind CSS classes compiled correctly
   - Clear browser cache and hard refresh (Cmd+Shift+R)

7. **Breadcrumb navigation incorrect:**
   - Verify property name fetched correctly: `property?.name`
   - Check Link hrefs: `/properties` and `/properties/{id}`
   - Ensure ChevronRight icons render between breadcrumb items

**Verification Commands:**

```bash
# Check media page compiles
cd frontend
npx tsc --noEmit --jsx preserve "app/properties/[id]/media/page.tsx"

# Verify all smoke test hooks present
rg 'data-testid="(media-lightbox|lightbox-prev|lightbox-next)"' app/properties/[id]/media/page.tsx

# Build and check bundle size
npm run build | grep "properties/\[id\]/media"
# Expected: ~7-8 kB (within normal range)
```

**Manual UI Verification:**
1. Navigate to `/properties/{id}/media` in admin UI
2. Verify breadcrumb: Properties > {property_name} > Media
3. Verify tabs visible: General, Amenities, Media (active), Rates, Availability, Reviews
4. Verify gold "Upload Image" button top right
5. Hover over thumbnail → verify overlay with "Set as Cover" and "Delete" buttons appears
6. Click thumbnail → lightbox opens with image
7. Press Arrow Right → next image loads
8. Press Arrow Left → previous image loads
9. Press Escape → lightbox closes
10. Verify cover image has gold star badge in top-left corner
11. Verify footer shows "X images uploaded (Max 20)" and "Supported formats: JPG, PNG, WEBP"

**Design System Reference:**
- Primary (Navy): `#0F1F3A` → `luxe-primary`
- Accent (Gold): `#C9A24D` → `luxe-accent`
- Background (Cream): `#F2EFEA` → `luxe-surface`
- Text Muted (Gray): `#7A7D85` → `luxe-text-muted`

---

### Admin UI Property Pages: Tabs Duplication / i18n / Lightbox (P2.21.4.7g)

**Overview:** Hotfix for regressions after P2.21.4.7y LuxeStay redesign - duplicate tabs, mixed language, broken lightbox.

**Common Issues:**

1. **Duplicate Tabs Showing:**
   - Symptom: Two tab bars visible on property pages (top: "Überblick / Preiseinstellungen / Media", bottom: "General / Amenities / Media / Rates / Availability / Reviews")
   - Root cause: P2.21.4.7y added inline tab nav in media page content, duplicating layout.tsx tabs
   - Fix: Removed lines 305-342 from media/page.tsx (duplicate nav section)
   - Verification: Only ONE tab bar should be visible at top (from layout.tsx)

2. **Mixed Language (English/German):**
   - Symptom: UI shows English strings like "Upload Image", "Add Photos", "Set as Cover" when German selected
   - Root cause: No i18n system exists, hardcoded strings not translated
   - Fix: Direct string replacement - 22 English strings migrated to German
   - Verification: All UI strings in German when DE locale selected

3. **Lightbox Not Opening on Thumbnail Click:**
   - Symptom: Clicking media thumbnails doesn't open lightbox preview
   - Root cause: onClick on `<img>` tag blocked by absolute-positioned hover overlay
   - Fix: Moved onClick to parent `<div>`, added `pointer-events-none` to overlay, `pointer-events-auto` to buttons
   - Verification: Click thumbnail → lightbox opens, click overlay buttons → actions execute

4. **Smoke Test Failing (Missing Testids):**
   - Symptom: pms_admin_ui_overview_media_smoke.sh fails with "testid not found"
   - Root cause: Redesign removed or renamed data-testid attributes
   - Fix: All testids preserved during fixes (media-lightbox, media-thumb-{id}, lightbox-prev, lightbox-next, lightbox-index, overview-cover, overview-edit-toggle, edit-lat, edit-lng)
   - Verification: `rg 'data-testid="(media-lightbox|lightbox-prev)"' app/properties/[id]/media/page.tsx` shows all required testids

**Troubleshooting Commands:**

```bash
# Check for duplicate tabs
cd frontend
rg -n '"General"|"Amenities"|"Availability"|"Reviews"' app/properties/[id]/media/page.tsx
# Expected: no results (tabs only in layout.tsx)

# Check for English strings
rg -n '"Upload Image"|"Add Photos"|"Set as Cover"' app/properties/[id]/media/page.tsx
# Expected: no results (all translated to German)

# Check lightbox onClick location
rg -n 'onClick=.*openLightbox' app/properties/[id]/media/page.tsx
# Expected: onClick on parent div (line ~326), NOT on img tag

# Check pointer-events fix
rg -n 'pointer-events-none|pointer-events-auto' app/properties/[id]/media/page.tsx
# Expected: overlay has pointer-events-none, buttons have pointer-events-auto

# Verify all smoke testids present
rg 'data-testid="(media-lightbox|media-thumb-|lightbox-prev|lightbox-next|overview-cover)"' app/properties/[id]/*.tsx
# Expected: 9 matches across media and overview pages
```

**Manual Verification:**
1. Navigate to /properties/{id}/media
2. Verify only ONE tab bar at top (Überblick, Preiseinstellungen, Media)
3. Verify all text in German: "Bild hochladen", "Fotos hinzufügen", "Als Titelbild setzen", "Löschen"
4. Click any thumbnail → lightbox opens with image
5. Hover thumbnail → "Aktionen" overlay appears
6. Click "Als Titelbild setzen" button → action executes without opening lightbox
7. Press Arrow keys in lightbox → navigate images
8. Press ESC in lightbox → closes
9. Run smoke test: pms_admin_ui_overview_media_smoke.sh → expect 5/5 tests passed

---

### Admin UI Smoke Test: Overview Testids Not Found (P2.21.4.7h)

**Symptom:** Playwright smoke script fails on Overview page tests:
```
❌ Test 1 FAILED: data-testid="overview-cover" not found in DOM
❌ Test 2 FAILED: data-testid="overview-edit-toggle" not found in DOM
❌ Test 3 FAILED: Could not find coordinate inputs in edit modal
```

Despite smoke bypass header being present (`x-pms-smoke-auth: ok`), indicating server-side middleware bypass is working.

**Root Cause:** Client-side AuthProvider uses `supabase.auth.getSession()` which doesn't recognize manually-set `sb-access-token` cookie from middleware smoke bypass. When `accessToken` is null, the page renders "Session abgelaufen" error screen instead of the property overview content, so testids are not in the DOM.

**How It's Fixed (P2.21.4.7h):**

1. **Middleware** (`frontend/middleware.ts` lines 105-113):
   - Sets non-httpOnly `pms_smoke=1` cookie alongside `sb-access-token` cookie
   - Client JavaScript can read this cookie to detect smoke mode
   - Cookie has shorter expiry (5 minutes) than token (1 hour)

2. **AuthProvider** (`frontend/app/lib/auth-context.tsx` lines 22-41):
   - On mount, checks for `pms_smoke=1` cookie
   - If present, reads `sb-access-token` directly from cookie instead of calling `supabase.auth.getSession()`
   - Sets mock user for smoke mode: `{ id: 'smoke-user', email: 'smoke@test.local' }`
   - Bypasses normal Supabase session flow entirely

3. **Debug Artifacts** (Smoke Script lines 114-160):
   - On any failed selector wait, captures:
     - Current URL and document title
     - First visible `<h1>` text
     - DOM signature: total count of `[data-testid]` elements + first 30 testids
     - Full-page screenshot saved to `/tmp/smoke_testX_timestamp.png`
     - HTML dump saved to `/tmp/smoke_testX_timestamp.html`

**Verification Commands:**

```bash
# HOST-SERVER-TERMINAL
# Verify middleware sets pms_smoke cookie
curl -I "https://admin.fewo.kolibri-visions.de/properties/23dd8fda-59ae-4b2f-8489-7a90f5d46c66" \
  -H "Authorization: Bearer $JWT" \
  -H "x-pms-smoke: 1" | grep -i "set-cookie"

# Expected: Two cookies set
# set-cookie: sb-access-token=...; Path=/; HttpOnly; ...
# set-cookie: pms_smoke=1; Path=/; SameSite=Lax; Max-Age=300

# Run smoke test with debug artifacts
export ADMIN_BASE_URL="https://admin.fewo.kolibri-visions.de"
export MANAGER_JWT_TOKEN="..."
export PROPERTY_ID="23dd8fda-59ae-4b2f-8489-7a90f5d46c66"
./backend/scripts/pms_admin_ui_overview_media_smoke.sh

# Expected: rc=0, 5/5 tests passed
# If failed: Check /tmp/smoke_test*_*.png and /tmp/smoke_test*_*.html for debug artifacts
```

**Common Issues:**

1. **Test fails but no debug artifacts:**
   - Check Docker volume mounts allow writing to `/tmp`
   - Verify Playwright has fs write permissions inside container
   - Check stderr output for "Failed to capture debug artifacts" errors

2. **pms_smoke cookie not set:**
   - Verify middleware P2.21.4.7h code deployed to PROD
   - Check `x-pms-smoke: 1` header is sent by smoke script
   - Hard refresh browser and test manually (should see cookie in DevTools)

3. **Client still shows "Session abgelaufen":**
   - Verify AuthProvider P2.21.4.7h code deployed to PROD
   - Check browser DevTools → Application → Cookies for both `pms_smoke` and `sb-access-token`
   - If `pms_smoke` missing: middleware not setting cookie
   - If `sb-access-token` missing: server-side bypass not working (see P2.21.4.7c)

4. **Debug artifacts show 0 testids:**
   - Page loaded error screen instead of property overview
   - Check "First H1" in debug output - should be property name, not "Session abgelaufen"
   - Indicates client auth bypass not working

---

### Admin UI Smoke Script: smoke.js SyntaxError (P2.21.4.7i)

**Symptom:** Smoke script fails immediately with Node.js syntax error before tests run:
```
SyntaxError: Invalid or unexpected token
at /work/smoke.js:72
const screenshotPath = \`/tmp/smoke_${testName}_${timestamp}.png\`;
Node.js v20.18.0
```

**Root Cause:** The bash heredoc that generates smoke.js accidentally escaped backticks as `\`` in JavaScript template literals. These escaped backticks are invalid tokens outside strings in JavaScript.

**How It's Fixed (P2.21.4.7i):**

1. **Removed Template Literals** (smoke script lines 148-149):
   - Changed from: `const screenshotPath = \`/tmp/smoke_${testName}_${timestamp}.png\`;`
   - Changed to: `const screenshotPath = '/tmp/smoke_' + testName + '_' + timestamp + '.png';`
   - Uses string concatenation instead of template literals
   - Avoids heredoc quoting issues entirely

2. **Added Preflight Syntax Check** (smoke script lines 347-353):
   - Runs `node --check smoke.js` before executing tests
   - If syntax error found, displays smoke.js lines 60-90 with line numbers
   - Prevents cryptic Playwright failures from bad JS syntax

**Verification:**
```bash
# HOST-SERVER-TERMINAL
# Clear Docker volumes to get fresh smoke.js generation
docker volume rm pms_playwright_smoke_node_modules pms_playwright_smoke_npm_cache 2>/dev/null || true

# Run smoke test
export ADMIN_BASE_URL="https://admin.fewo.kolibri-visions.de"
export MANAGER_JWT_TOKEN="..."
export PROPERTY_ID="23dd8fda-59ae-4b2f-8489-7a90f5d46c66"
./backend/scripts/pms_admin_ui_overview_media_smoke.sh

# Expected: No SyntaxError, tests execute normally
# NOT expected: "Invalid or unexpected token" errors
```

**Troubleshooting:**

1. **Still getting SyntaxError after fix:**
   - Check heredoc in smoke script hasn't been corrupted
   - Verify no backticks remain: `grep -n '\\` backend/scripts/pms_admin_ui_overview_media_smoke.sh`
   - Should only find backticks in PLAYWRIGHT_EOF closing tag and bash substitutions

2. **node --check fails but error location unclear:**
   - The preflight check shows lines 60-90 by default
   - If error is outside this range, manually inspect smoke.js:
     ```bash
     # Inside Docker container (adjust docker run command to drop into shell)
     cat -n /work/smoke.js | less
     ```

---

### Admin UI Smoke Test: HttpOnly Cookie Limitation (P2.21.4.7j)

**Symptom:** Playwright smoke script fails on Overview page despite x-pms-smoke-auth: ok header. Page renders "Objekte" list with 0 data-testid elements. Title shows "PMS Channel Sync Console" instead of property details.

**Root Cause:** Client-side AuthProvider cannot read JWT from `sb-access-token` cookie because it's httpOnly: true. JavaScript document.cookie cannot access httpOnly cookies for security reasons. When client auth fails, page renders the wrong view (properties list instead of property detail).

**How It's Fixed (P2.21.4.7j):**

1. **Middleware** (`frontend/middleware.ts`):
   - Added `pms_smoke_token` cookie (non-httpOnly) for client-side JWT access
   - Cookie configuration:
     - Path: /
     - httpOnly: false (client JS can read)
     - Secure: true (HTTPS only, except localhost)
     - SameSite: Lax
     - Max-Age: 300 (5 minutes - short-lived for security)
   - Only set when: x-pms-smoke: 1 AND valid Authorization header AND GET/HEAD on /properties/*

2. **AuthProvider** (`frontend/app/lib/auth-context.tsx`):
   - Changed from reading `sb-access-token` (httpOnly) to reading `pms_smoke_token` (non-httpOnly)
   - When pms_smoke=1 and pms_smoke_token exists, uses token directly
   - Never logs token value

3. **Smoke Script Hardening** (`backend/scripts/pms_admin_ui_overview_media_smoke.sh`):
   - Debug artifacts now include cookie NAMES (not values) for auth troubleshooting
   - Media test ALWAYS asserts `[data-testid="media-lightbox"]` exists, even if 0 thumbnails
   - Catches "wrong page loaded" issues earlier with better diagnostics

**Security Notes:**
- `pms_smoke_token` is only set in smoke mode (requires both x-pms-smoke: 1 header AND valid JWT)
- Cookie limited to 5-minute TTL (shorter than main token's 1 hour)
- Only for GET/HEAD requests on /properties/* routes
- Token never logged in server responses, screenshots, or HTML dumps
- Middleware validates JWT against Supabase /auth/v1/user before setting any smoke cookies

**Verification:**
```bash
# HOST-SERVER-TERMINAL
export ADMIN_BASE_URL="https://admin.fewo.kolibri-visions.de"
export MANAGER_JWT_TOKEN="..."
export PROPERTY_ID="23dd8fda-59ae-4b2f-8489-7a90f5d46c66"
./backend/scripts/pms_admin_ui_overview_media_smoke.sh

# Expected: rc=0, 5/5 tests passed
# Overview tests find data-testid elements (not "Objekte" page)
# Media test asserts lightbox wrapper exists
```

**Common Issues:**

1. **Still seeing "Objekte" page with 0 testids:**
   - Check cookie names in debug artifacts: should include "pms_smoke" and "pms_smoke_token"
   - If pms_smoke_token missing: middleware not deployed or JWT validation failed
   - If both present but still fails: check browser console for JS errors

2. **"media-lightbox not found" error:**
   - Media page didn't load correctly or structure changed
   - Check screenshot artifact: should show media gallery grid
   - Verify frontend deployment includes media page with data-testid="media-lightbox"

---

## Admin UI Booking Details Redesign (Stitch Referenz)

**Overview:** Complete redesign of booking details page following Stitch reference layout with LuxeStay components and German language.

**Changes Implemented:**

1. **Layout Architecture** (frontend/app/bookings/[id]/page.tsx):
   - Breadcrumb navigation: Buchungen > Buchungs-Nummer
   - Header section: Booking number + status pill + action buttons (right-aligned)
   - Two-column layout: Left (guest info, property details, stay dates), Right (payment details, booked via, activity log)
   - All cards use LuxeStay Card component with icons

2. **German Language:**
   - All labels, buttons, status texts in German
   - Status translations: confirmed → Bestätigt, cancelled → Storniert, etc.
   - Inline translations (no i18n infrastructure)

3. **Data Integration:**
   - Guest data fetched: /api/v1/guests/{guest_id}
   - Property data fetched: /api/v1/properties/{property_id}
   - Graceful fallbacks when data unavailable

4. **Action Buttons:**
   - Rechnung herunterladen (Download invoice) - gold variant
   - Stornieren (Cancel) - danger variant
   - Bestätigen (Confirm) - primary variant
   - All buttons have data-testid attributes

**Verification Commands:**

```bash
# HOST-SERVER-TERMINAL
# Run booking details smoke test
export ADMIN_BASE_URL="https://admin.fewo.kolibri-visions.de"
export MANAGER_JWT_TOKEN="..."
export BOOKING_ID="550e8400-e29b-41d4-a716-446655440000"
./backend/scripts/pms_admin_ui_booking_details_smoke.sh
# Expected: rc=0, 5/5 tests passed

# Manual verification:
# 1. Navigate to /bookings/{id} in admin UI
# 2. Verify breadcrumb: Buchungen > Buchungs-Nummer
# 3. Verify status pill shows German status (e.g., "Bestätigt")
# 4. Verify left column: Gast, Objekt, Aufenthaltsdaten cards
# 5. Verify right column: Zahlungsdetails, Gebucht über, Aktivitätsprotokoll cards
# 6. Verify action buttons: Rechnung herunterladen, Stornieren, Bestätigen
# 7. Test confirm/cancel actions work
```

**Common Issues:**

### Booking Details Page Shows "Session abgelaufen"

**Symptom:** Page renders login error instead of booking details.

**Root Cause:** Client-side smoke auth bypass not working. Cookie pms_smoke_token missing or JWT expired.

**How to Debug:**
```bash
# Check JWT expiry
echo $MANAGER_JWT_TOKEN | cut -d'.' -f2 | base64 -d | jq '.exp'
# Compare with current timestamp: date +%s

# Check if smoke bypass activated (should see x-pms-smoke-auth: ok header)
curl -I https://admin.fewo.kolibri-visions.de/bookings/{id} \
  -H "Authorization: Bearer $MANAGER_JWT_TOKEN" \
  -H "x-pms-smoke: 1"
```

**Solution:**
- Refresh JWT token if expired
- Verify frontend middleware deployed (P2.21.4.7j pms_smoke_token cookie)
- Check browser DevTools → Application → Cookies for pms_smoke and pms_smoke_token

### Guest or Property Data Missing (Shows "Keine Informationen verfügbar")

**Symptom:** Guest card or Property card shows fallback message instead of data.

**Root Cause:** Guest or property API fetch failed or returned 404.

**How to Debug:**
```bash
# Check if guest exists
GUEST_ID=$(curl -sS "https://api.fewo.kolibri-visions.de/api/v1/bookings/{booking_id}" \
  -H "Authorization: Bearer $JWT" | jq -r '.guest_id')
curl -sS "https://api.fewo.kolibri-visions.de/api/v1/guests/$GUEST_ID" \
  -H "Authorization: Bearer $JWT"

# Check if property exists
PROPERTY_ID=$(curl -sS "https://api.fewo.kolibri-visions.de/api/v1/bookings/{booking_id}" \
  -H "Authorization: Bearer $JWT" | jq -r '.property_id')
curl -sS "https://api.fewo.kolibri-visions.de/api/v1/properties/$PROPERTY_ID" \
  -H "Authorization: Bearer $JWT"
```

**Solution:**
- Verify booking has valid guest_id and property_id references
- Create guest/property records if missing
- Check RBAC permissions for guest/property endpoints

### Action Buttons Not Working (No Response on Click)

**Symptom:** Clicking Bestätigen or Stornieren buttons does nothing.

**Root Cause:** API endpoints return error or booking status doesn't allow action.

**How to Debug:**
```bash
# Check browser console for API errors (F12 → Console)
# Look for 422 validation errors or 400 bad requests

# Check if booking status allows confirmation
# Only "requested", "under_review", "pending" can be confirmed
# Only "confirmed", "pending" can be cancelled
```

**Solution:**
- Verify booking status allows the action (see status mapping above)
- Check API logs for validation errors
- Verify JWT has correct role (manager/admin required)

### Smoke Test Fails: "booking-title not found"

**Symptom:** Playwright smoke test fails on Test 1 with timeout waiting for booking-title.

**Root Cause:** Page didn't render correctly or smoke auth bypass failed.

**How to Debug:**
```bash
# Check debug artifacts in /tmp/smoke_test1_*.png and /tmp/smoke_test1_*.html
# Look for "Session abgelaufen" in HTML (auth issue)
# Check if booking-title testid exists in HTML dump

# Verify testid in deployed code
curl -sS "https://admin.fewo.kolibri-visions.de/bookings/{id}" \
  -H "Authorization: Bearer $JWT" -H "x-pms-smoke: 1" | grep -c 'data-testid="booking-title"'
# Expected: 1
```

**Solution:**
- If 0: Frontend not deployed or testid removed
- If auth error: Follow "Session abgelaufen" troubleshooting above
- Hard refresh browser (Cmd+Shift+R) to clear cached JS bundle

---

### Booking Details Smoke: Redirect to /login (x-pms-smoke-auth missing) - P2.21.4.7k

**Symptom:** `pms_admin_ui_booking_details_smoke.sh` fails with HTTP 307 redirect to /login. Preflight check shows "Smoke bypass not active for /bookings/* routes".

**Root Cause:** frontend/middleware.ts smoke auth allowlist (line ~58) only includes `/properties/*` routes, not `/bookings/*`. When smoke script requests `/bookings/{id}` with smoke headers, middleware doesn't recognize it and falls through to normal auth flow which redirects to /login.

**How It's Fixed (P2.21.4.7k):**
Middleware condition extended to include both `/properties/*` and `/bookings/*`:
```typescript
// Before (P2.21.4.7a):
pathname.startsWith('/properties/') &&

// After (P2.21.4.7k):
(pathname.startsWith('/properties/') || pathname.startsWith('/bookings/')) &&
```

Security constraints remain unchanged:
- Requires BOTH headers: `Authorization: Bearer <jwt>` AND `x-pms-smoke: 1`
- Only GET/HEAD methods
- JWT validated against Supabase /auth/v1/user before setting cookies
- Cookies set: `pms_smoke=1`, `pms_smoke_token=<jwt>` (both non-httpOnly, 5-min TTL)
- Response header: `x-pms-smoke-auth: ok` when bypass active

**Preflight Check (Added in P2.21.4.7k):**
Booking smoke script now includes lightweight preflight HEAD request before running Playwright:
- Checks for HTTP 307/302 to /login → fails fast with actionable error
- Checks for x-pms-smoke-auth: ok header → warns if missing
- Prevents wasting time on Playwright tests when bypass not working

**Verification:**
```bash
# HOST-SERVER-TERMINAL
export ADMIN_BASE_URL="https://admin.fewo.kolibri-visions.de"
export MANAGER_JWT_TOKEN="..."
export BOOKING_ID="550e8400-e29b-41d4-a716-446655440000"

# Should pass preflight and all tests
./backend/scripts/pms_admin_ui_booking_details_smoke.sh
# Expected: "✅ Smoke bypass active: x-pms-smoke-auth: ok"
# Expected: rc=0, 5/5 tests passed
```

**Common Issues:**

- **Still getting 307 redirect after fix:** Frontend not deployed. Verify commit includes P2.21.4.7k fix: `git log -1 --oneline | grep P2.21.4.7k`. Redeploy frontend via Coolify.

- **x-pms-smoke-auth header present but still fails:** Different issue - check JWT expiry, Supabase env vars (NEXT_PUBLIC_SB_URL, NEXT_PUBLIC_SUPABASE_ANON_KEY), or network issues.

---

### Booking Details Smoke: Preflight Silent Fail (P2.21.4.7l)

**Symptom:** `pms_admin_ui_booking_details_smoke.sh` exits with ui_rc=1 immediately after "ℹ Checking smoke bypass activation..." without printing any error message. Manual curl to the same URL shows `x-pms-smoke-auth: ok` header, proving smoke bypass is working.

**Root Cause (P2.21.4.7k preflight):** Brittle header parsing with multiple issues:
- Used `curl -I -L` in command substitution instead of temp file → fragile
- Multiple `grep` commands in pipelines with `set -euo pipefail` → exit on grep fail (no match)
- CRLF line endings from curl not normalized → header extraction fails
- No diagnostic output on failure paths → silent exit

**How It's Fixed (P2.21.4.7l):**

1. **Temp File + CRLF Normalization:**
   - Use `curl -D tempfile` to write headers to file
   - Normalize CRLF → LF with `tr -d '\r'`
   - All parsing done on normalized file

2. **Robust Parsing with awk:**
   - Replace brittle `grep | cut` with `awk` using `IGNORECASE=1`
   - All awk commands use `|| true` to prevent pipeline exits
   - Extract status: `awk '/^HTTP\// {print $2; exit}' headers`
   - Extract x-pms-smoke-auth: `awk 'BEGIN{IGNORECASE=1} /^x-pms-smoke-auth:/ ...'`
   - Extract cookie names (not values): `awk 'BEGIN{IGNORECASE=1} /^set-cookie:/ {sub(/=.*/, ""); print}'`

3. **Comprehensive Diagnostics:**
   - On any failure path, print:
     - URL tested
     - HTTP status code
     - Location header (if redirect)
     - x-pms-smoke-auth header value
     - Cookie names (comma-separated, no values)
     - First 40 lines of headers for debugging
   - Never prints JWT token values

4. **Optional Features:**
   - `PMS_SMOKE_SKIP_PREFLIGHT=1` env var to bypass preflight (emergencies only)
   - Auto-derive `BOOKING_ID` if empty (fetches first booking from API)

**Verification:**
```bash
# HOST-SERVER-TERMINAL
export ADMIN_BASE_URL="https://admin.fewo.kolibri-visions.de"
export MANAGER_JWT_TOKEN="..."
export BOOKING_ID="550e8400-e29b-41d4-a716-446655440000"

# Should pass preflight with diagnostics
./backend/scripts/pms_admin_ui_booking_details_smoke.sh
# Expected output:
# ℹ Checking smoke bypass activation...
# ✅ Smoke bypass active: x-pms-smoke-auth: ok
# ℹ Running Playwright tests in Docker container...
# ...
# ✅ All Admin UI Booking Details smoke tests passed! 🎉

# If preflight fails, you'll see full diagnostics:
# ❌ ERROR: Preflight detected smoke bypass NOT active
#    URL: https://admin.fewo.kolibri-visions.de/bookings/...
#    Status: 307
#    Location: /login
#    x-pms-smoke-auth: <missing>
#    Cookie names: 
#    First 40 header lines:
#    ...
```

**Debug Commands:**

```bash
# Test preflight manually with same headers
BOOKING_ID="550e8400-e29b-41d4-a716-446655440000"
ADMIN_BASE_URL="https://admin.fewo.kolibri-visions.de"
BOOKING_URL="${ADMIN_BASE_URL}/bookings/${BOOKING_ID}"

# Dump headers to file
curl -k -sS -D /tmp/preflight_headers.txt -o /dev/null \
  -H "Authorization: Bearer $MANAGER_JWT_TOKEN" \
  -H "x-pms-smoke: 1" \
  -H "Cache-Control: no-cache" \
  "$BOOKING_URL"

# Normalize CRLF
tr -d '\r' < /tmp/preflight_headers.txt > /tmp/preflight_headers_norm.txt

# Extract status
awk '/^HTTP\// {print $2; exit}' /tmp/preflight_headers_norm.txt
# Expected: 200

# Extract x-pms-smoke-auth header
awk 'BEGIN{IGNORECASE=1} /^x-pms-smoke-auth:/ {sub(/^x-pms-smoke-auth:[[:space:]]*/, ""); print; exit}' /tmp/preflight_headers_norm.txt
# Expected: ok

# Extract cookie names
awk 'BEGIN{IGNORECASE=1} /^set-cookie:/ {sub(/^set-cookie:[[:space:]]*/, ""); sub(/=.*/, ""); print}' /tmp/preflight_headers_norm.txt
# Expected: pms_smoke, pms_smoke_token, sb-access-token (one per line)
```

**Common Issues:**

- **Still failing after P2.21.4.7l fix:** Check script deployed correctly. Verify preflight code includes `mktemp` and `tr -d '\r'`: `rg -n 'mktemp.*pms_booking_smoke_headers|tr -d' backend/scripts/pms_admin_ui_booking_details_smoke.sh`

- **Auto-derive BOOKING_ID fails:** Requires `API_BASE_URL` env var (backend API, not admin UI). If not set or API unreachable, provide `BOOKING_ID` explicitly.

- **Skip preflight for emergency deploy:** Set `PMS_SMOKE_SKIP_PREFLIGHT=1` to bypass preflight. Use only when middleware is known working but preflight has issues.

---

### Booking Details Smoke: Action Buttons Status-Conditional (P2.21.4.7m)

**Symptom:** `pms_admin_ui_booking_details_smoke.sh` Test 5 fails with "action buttons not found" even though smoke bypass is working. Debug artifacts show some action buttons missing from DOM (e.g., action-cancel absent).

**Root Cause:** Action buttons are conditionally rendered based on booking status in the UI:
- **action-invoice**: ALWAYS present (all statuses)
- **action-cancel**: Hidden for "cancelled", "declined", "checked_out" statuses
- **action-confirm**: Only shown for "inquiry", "pending" statuses

Previous smoke test (P2.21.4.7k/l) expected ALL buttons to exist, causing failures when testing cancelled bookings.

**How It's Fixed (P2.21.4.7m):**

Smoke Test 5 is now status-aware:
1. Always asserts `action-invoice` exists (required for all bookings)
2. Reads status from `booking-status-pill` testid
3. If status is "Storniert" (cancelled), "Abgelehnt" (declined), or "Ausgecheckt" (checked_out):
   - Does NOT require cancel/confirm buttons (pass)
4. Otherwise (active bookings):
   - Requires AT LEAST ONE of `action-cancel` or `action-confirm`

**Auto-Selection Enhancement:**
When `BOOKING_ID` is not provided and `API_BASE_URL` is set, the script now:
- Fetches up to 50 bookings from API
- Prefers a booking that is NOT in "cancelled", "declined", or "checked_out" status
- Falls back to any booking if no active ones found

**Verification:**
```bash
# HOST-SERVER-TERMINAL
export API_BASE_URL="https://api.fewo.kolibri-visions.de"
export ADMIN_BASE_URL="https://admin.fewo.kolibri-visions.de"
export MANAGER_JWT_TOKEN="..."

# Let script auto-select a non-cancelled booking
unset BOOKING_ID
./backend/scripts/pms_admin_ui_booking_details_smoke.sh
# Expected: Auto-derives non-cancelled booking, Test 5 passes

# Or explicitly test a cancelled booking
export BOOKING_ID="<uuid-of-cancelled-booking>"
./backend/scripts/pms_admin_ui_booking_details_smoke.sh
# Expected: Test 5 passes (status-aware, doesn't require cancel/confirm)
```

**Common Issues:**

- **Test 5 fails for active booking but no buttons:** Check frontend deployment includes action buttons with correct testids. Verify `canCancel()` and `canConfirm()` logic in `frontend/app/bookings/[id]/page.tsx`.

- **Auto-select always picks cancelled booking:** API endpoint may only return cancelled bookings. Check `GET /api/v1/bookings?limit=50` returns active bookings. Manually set `BOOKING_ID` to an active booking UUID.

- **Status pill text doesn't match:** German labels changed in UI. Check `getStatusLabel()` function in booking page. Update smoke script finished statuses list if needed.

---

## Admin UI Richtdesign: Properties List & Amenities Smoke Tests

**Overview:** Smoke tests for Admin UI Richtdesign redesign - properties list page and amenities page with LuxeStay design system.

**Changes Implemented:**

1. **Properties List Page** (frontend/app/properties/page.tsx):
   - Redesigned with LuxeStay components
   - Table layout with image thumbnails, type pills, capacity icons, status pills
   - Action menus per row
   - Required testids: properties-title, properties-search, properties-new-button, property-row, property-status-pill, property-actions

2. **Amenities Page** (frontend/app/amenities/page.tsx):
   - Redesigned with grouped/collapsible sections by category
   - Search input, toggle switches, action menus
   - Required testids: amenities-title, amenities-search, amenities-new-button, amenities-section-*, amenity-row, amenity-toggle, amenity-actions

3. **Sidebar Navigation** (frontend/app/components/AdminShell.tsx):
   - Removed unwanted line artifacts on hover/active states
   - Reduced vertical spacing between menu items
   - Simplified icon containers (smaller size, no shadows)

4. **Middleware Allowlist** (frontend/middleware.ts):
   - Extended smoke bypass to include /properties (exact match) and /amenities routes

**Verification Commands:**

```bash
# HOST-SERVER-TERMINAL
export ADMIN_BASE_URL="https://admin.fewo.kolibri-visions.de"
export MANAGER_JWT_TOKEN="..."

# Run properties list smoke test
./backend/scripts/pms_admin_ui_properties_list_smoke.sh
# Expected: rc=0, 6/6 tests passed

# Run amenities smoke test
./backend/scripts/pms_admin_ui_amenities_smoke.sh
# Expected: rc=0, 7/7 tests passed

# Manual verification:
# 1. Navigate to /properties - verify title, search, new button, table layout
# 2. Navigate to /amenities - verify title, search, grouped sections, toggles
# 3. Check sidebar navigation - no artifacts on hover, reduced spacing
```

**Common Issues:**

### Properties List Smoke: property-row not found

**Symptom:** Test 4 fails with "No property rows found" despite properties existing in database.

**Root Cause:** Frontend not deployed or testid attribute missing from property rows.

**How to Debug:**
```bash
# Check if properties API returns data
curl -sS "https://api.fewo.kolibri-visions.de/api/v1/properties?limit=10" \
  -H "Authorization: Bearer $JWT" | jq '.items | length'
# Expected: > 0

# Check debug artifacts
ls -lah /tmp/smoke_test4_*
# Screenshot should show property table or empty state
```

**Solution:**
- If API returns data but UI shows empty: Check frontend deployment includes property-row testids
- If API returns 0: Create test properties via admin UI
- Hard refresh browser (Cmd+Shift+R) to clear cached JS bundle

### Amenities Smoke: amenities-section-* not found

**Symptom:** Test 4 fails with "No category sections found" despite amenities existing.

**Root Cause:** Category grouping not working or testid attribute missing from section headers.

**How to Debug:**
```bash
# Check if amenities API returns data
curl -sS "https://api.fewo.kolibri-visions.de/api/v1/amenities?limit=50" \
  -H "Authorization: Bearer $JWT" | jq 'group_by(.category) | length'
# Expected: > 0

# Check debug artifacts screenshot
# Should show collapsible category sections or empty state
```

**Solution:**
- Verify frontend deployment includes amenities-section-{category} testids
- Check category grouping logic in amenities page component
- Create test amenities in multiple categories if database empty

### Smoke Bypass Not Active for /amenities or /properties

**Symptom:** Smoke tests fail with HTTP 307 redirect to /login despite x-pms-smoke header.

**Root Cause:** Middleware allowlist missing /amenities or exact /properties route.

**How to Debug:**
```bash
# Test middleware allowlist manually
curl -I "https://admin.fewo.kolibri-visions.de/properties" \
  -H "Authorization: Bearer $JWT" \
  -H "x-pms-smoke: 1"
# Expected: HTTP 200, x-pms-smoke-auth: ok header

curl -I "https://admin.fewo.kolibri-visions.de/amenities" \
  -H "Authorization: Bearer $JWT" \
  -H "x-pms-smoke: 1"
# Expected: HTTP 200, x-pms-smoke-auth: ok header
```

**Solution:**
- Verify middleware.ts line 58 includes:
  ```typescript
  (pathname === '/properties' || pathname.startsWith('/properties/') || 
   pathname === '/amenities' || pathname.startsWith('/bookings/'))
  ```
- Redeploy frontend if middleware not updated
- Check JWT not expired: `echo $JWT | cut -d. -f2 | base64 -d | jq .exp`

### Sidebar Navigation Still Shows Artifacts

**Symptom:** Unwanted lines or shadows visible on hover/active states in sidebar.

**Root Cause:** AdminShell.tsx not deployed with simplified styling.

**How to Debug:**
```bash
# Check deployed code includes simplified icon container
curl -sS "https://admin.fewo.kolibri-visions.de/_next/static/..." # check JS bundle

# Manual verification in browser DevTools
# Inspect nav item icon container
# Should have: w-8 h-8 (not w-9 h-9), no shadow classes
```

**Solution:**
- Verify AdminShell.tsx changes deployed
- Check icon container: `w-8 h-8` (not `w-9 h-9`)
- Check no `shadow-*` classes on icon container
- Check nav group spacing: `space-y-3` (not `space-y-5`)
- Hard refresh browser to clear cached components

---

## Admin UI Properties List: Functional Fixes & Polish (P2.21.4.7n)

**Overview:** Functional improvements and bug fixes for /properties list page following Admin UI Richtdesign deployment.

**Changes Implemented:**

1. **Search Functionality** (backend + frontend):
   - Backend: Added `search` parameter to PropertyFilter schema (backend/app/schemas/properties.py)
   - Backend: Implemented ILIKE search across name, internal_name, address_line1, city fields (backend/app/services/property_service.py)
   - Frontend: Search input now sends query to backend and filters results

2. **Cover Image Thumbnails** (frontend):
   - Fetches cover images from `/api/v1/properties/{id}/media` endpoint
   - Displays `is_cover: true` image in thumbnail column
   - Falls back to emoji placeholder if no cover image
   - Uses display_url > signed_url > url priority

3. **Removed Redundant Filter Button** (frontend):
   - Top "Filter" button removed (filter panel already accessible below)

4. **Actions Menu Differentiation** (frontend):
   - **Anzeigen**: Opens property detail in view mode (no change)
   - **Bearbeiten**: Opens property detail in edit mode (`?edit=1` query parameter)
   - **Archivieren**: Sets `is_active: false` via PATCH endpoint
   - **Löschen**: Soft delete via DELETE endpoint (sets `deleted_at`) with confirmation dialog

5. **Neues Objekt Modal Polish** (frontend):
   - Applied LuxeStay design system colors throughout
   - Updated typography (font-heading for headings, luxe-text-muted for labels)
   - Improved spacing (gap-5 between fields, space-y-8 between sections)
   - Enhanced input styling (border-bo-border, focus ring transitions)
   - Better error message styling (left border accent)
   - Polished footer buttons (improved padding and shadows)

**Verification Commands:**

```bash
# HOST-SERVER-TERMINAL
export ADMIN_BASE_URL="https://admin.fewo.kolibri-visions.de"
export MANAGER_JWT_TOKEN="..."

# Run properties list smoke test
./backend/scripts/pms_admin_ui_properties_list_smoke.sh
# Expected: rc=0, 6/6 tests passed

# Manual verification:
# 1. Navigate to /properties
# 2. Test search: Enter text in search box, verify results filter
# 3. Verify thumbnails show cover images (not just emojis)
# 4. Test "Bearbeiten" action: Opens detail page with ?edit=1
# 5. Test "Archivieren": Property status changes to "Inaktiv"
# 6. Test "Löschen": Confirmation dialog, then property marked deleted
# 7. Test "Neues Objekt" modal: Verify polished styling with LuxeStay colors
```

**Common Issues:**

### Search Returns No Results Despite Properties Existing

**Symptom:** Search input works but returns empty list even with valid search terms.

**Root Cause:** Backend search filter not deployed or database query incorrect.

**How to Debug:**
```bash
# Test backend search directly
curl -sS "https://api.fewo.kolibri-visions.de/api/v1/properties?search=berlin&limit=10" \
  -H "Authorization: Bearer $JWT" | jq '.items | length'
# Expected: > 0 if properties match search term

# Check backend logs for SQL errors
docker logs pms-backend-api | grep "list_properties"
```

**Solution:**
- Verify PropertyFilter schema includes `search` field (backend/app/schemas/properties.py)
- Verify PropertyService implements ILIKE search (backend/app/services/property_service.py)
- Redeploy backend if changes not deployed

### Cover Images Not Displaying (Still Shows Emoji)

**Symptom:** Properties list shows placeholder emoji instead of cover images despite images being uploaded.

**Root Cause:** Cover images not set (`is_cover: false` on all media) or media API failing.

**How to Debug:**
```bash
# Check if property has cover image
PROPERTY_ID="23dd8fda-59ae-4b2f-8489-7a90f5d46c66"
curl -sS "https://api.fewo.kolibri-visions.de/api/v1/properties/$PROPERTY_ID/media" \
  -H "Authorization: Bearer $JWT" | jq '.[] | select(.is_cover == true)'
# Expected: One media item with is_cover: true

# Check browser DevTools console for fetch errors
```

**Solution:**
- Set one media item as cover in Media tab (click "Als Titelbild setzen")
- Verify media API returns display_url or signed_url
- Check network tab for failed requests to /api/v1/properties/{id}/media

### Archivieren Action Shows Error Toast

**Symptom:** Clicking "Archivieren" shows "Fehler beim Archivieren" toast.

**Root Cause:** Backend PATCH endpoint failing or user lacks permissions.

**How to Debug:**
```bash
# Test PATCH endpoint directly
curl -X PATCH "https://api.fewo.kolibri-visions.de/api/v1/properties/$PROPERTY_ID" \
  -H "Authorization: Bearer $JWT" \
  -H "Content-Type: application/json" \
  -d '{"is_active": false}'
# Expected: 200 OK with updated property

# Check backend logs
docker logs pms-backend-api | grep "PATCH /api/v1/properties"
```

**Solution:**
- Verify user has manager/admin role
- Check PropertyUpdate schema allows `is_active` field
- Verify backend deployed with PATCH endpoint

### Löschen Action Doesn't Remove Property

**Symptom:** Confirmation dialog appears, but property still visible in list after deletion.

**Root Cause:** DELETE endpoint soft-deletes but frontend filter doesn't exclude deleted properties.

**How to Debug:**
```bash
# Check if property is soft-deleted
curl -sS "https://api.fewo.kolibri-visions.de/api/v1/properties/$PROPERTY_ID" \
  -H "Authorization: Bearer $JWT" | jq '.deleted_at'
# Expected: Non-null timestamp

# Check frontend filter
# Frontend should refetch properties after delete, which excludes deleted by default
```

**Solution:**
- Verify DELETE endpoint sets `deleted_at` timestamp
- Verify frontend calls `fetchProperties()` after successful delete
- Hard refresh browser (Cmd+Shift+R) to clear cache

### Bearbeiten Opens View Mode (Not Edit Mode)

**Symptom:** Clicking "Bearbeiten" opens property detail page but in view mode, not edit mode.

**Root Cause:** Detail page doesn't detect `?edit=1` query parameter yet (separate implementation needed).

**Note:** P2.21.4.7n adds the query parameter to the link. The detail page must be updated separately to detect this parameter and auto-enable edit mode on mount.

**Temporary Workaround:**
- Users can click "Bearbeiten" toggle button manually in detail page

---

## Admin UI Properties List: UX Fixes \(P2.21.4.7o\)

**Overview:** Functional fixes for properties list page after commit bd5008d - modal styling, export 404, search/filter caching issues.

**Changes Implemented:**

1. **"Neues Objekt" Modal Testids** \(frontend/app/properties/page.tsx\):
   - Added data-testid="new-property-open" to modal open button
   - Added data-testid="new-property-modal" to modal container
   - Added data-testid="new-property-name" to name input
   - Added data-testid="new-property-submit" to submit button

2. **Export Button 404 Fix** \(frontend/app/properties/page.tsx\):
   - Fixed handleExport function to use getApiBase\(\) for full API URL
   - Changed from relative `/api/v1/properties/export` to `${getApiBase()}/api/v1/properties/export`
   - Backend route exists at correct path, frontend just needed full URL

3. **Search/Filter Caching Fix** \(frontend/app/properties/page.tsx\):
   - Added noCache=true parameter to apiClient.get\(\) in fetchProperties
   - Prevents browser/CDN from serving stale filter results

**Verification Commands:**

```bash
# HOST-SERVER-TERMINAL
export ADMIN_BASE_URL="https://admin.fewo.kolibri-visions.de"
export MANAGER_JWT_TOKEN="..."

# Run updated properties list smoke test
./backend/scripts/pms_admin_ui_properties_list_smoke.sh
# Expected: rc=0, 9/9 tests passed

# Manual verification:
# 1. Navigate to /properties
# 2. Click "+ Neues Objekt" - verify modal opens with proper styling
# 3. Test search - type text, verify results filter correctly
# 4. Click Export button - verify CSV downloads \(no 404\)
```

**Common Issues:**

### Modal Opens But Looks Broken \(Transparent/Unstyled\)

**Symptom:** Clicking "+ Neues Objekt" opens modal but it has no background color or proper styling.

**Root Cause:** Frontend CSS not deployed or modal container missing LuxeStay classes.

**How to Debug:**
```bash
# Check if modal has proper classes
# DevTools → Elements → search for data-testid="new-property-modal"
# Should have: bg-luxe-surface rounded-xl shadow-bo-xl max-w-4xl

# Check deployed code
curl -sS "https://admin.fewo.kolibri-visions.de/_next/static/..." # check JS bundle
```

**Solution:**
- Verify P2.21.4.7n styling changes deployed \(modal already styled, just needed testids\)
- Hard refresh browser \(Cmd+Shift+R\)
- Check browser console for CSS loading errors

### Export Button Returns 404

**Symptom:** Clicking Export button shows "Fehler beim Export" toast, network tab shows 404 on `/api/v1/properties/export`.

**Root Cause:** Frontend using relative URL which Next.js doesn't proxy. Solution in P2.21.4.7o uses getApiBase\(\) for full URL.

**How to Debug:**
```bash
# Check if backend route exists
curl -sS "https://api.fewo.kolibri-visions.de/api/v1/properties/export" \
  -H "Authorization: Bearer $JWT"
# Expected: CSV download

# Check frontend code
rg "getApiBase.*export" frontend/app/properties/page.tsx
# Should show: ${getApiBase()}/api/v1/properties/export
```

**Solution:**
- Verify P2.21.4.7o changes deployed \(getApiBase import + usage in handleExport\)
- Verify NEXT_PUBLIC_API_BASE env var set correctly
- Check browser console for full error URL

### Search Doesn't Filter Results

**Symptom:** Typing in search box doesn't filter the property list.

**Root Cause:** Browser/CDN caching GET responses. Solution adds noCache=true parameter.

**How to Debug:**
```bash
# Check network tab in DevTools
# Search for "berlin"
# Should see new GET request to /api/v1/properties?search=berlin
# Check if Cache-Control: no-store header present

# Check frontend code
rg "apiClient.get.*noCache.*true" frontend/app/properties/page.tsx
# Should show: apiClient.get(..., accessToken, undefined, true)
```

**Solution:**
- Verify P2.21.4.7o noCache parameter deployed
- Clear browser cache
- Check backend returns fresh results \(not cached 200\)

### Smoke Test Fails on Test 7 \(Modal Background\)

**Symptom:** pms_admin_ui_properties_list_smoke.sh Test 7 fails with "Modal background is transparent".

**Root Cause:** Modal container doesn't have bg-luxe-surface class or CSS not loaded.

**How to Debug:**
```bash
# Check debug artifacts
ls -lah /tmp/smoke_test7_*.png
# Screenshot should show modal with proper white/cream background

# Manual test in browser
# Open /properties, click "+ Neues Objekt"
# Inspect modal element, check backgroundColor computed style
```

**Solution:**
- Verify modal has data-testid="new-property-modal" AND bg-luxe-surface class
- Redeploy frontend if P2.21.4.7n + P2.21.4.7o changes not deployed
- Check Tailwind CSS compiled correctly

### Smoke Test Fails on Test 8 \(Search Doesn't Reduce Results\)

**Symptom:** Test 8 shows "Search did not reduce results" despite typing search query.

**Root Cause:** Search not wired to backend, debounce timing too short, or caching issue.

**How to Debug:**
```bash
# Check if search query sent to backend
# DevTools → Network tab → filter XHR/Fetch
# Type in search box, wait 1.5s
# Should see: GET /api/v1/properties?search=zzz_nonexistent_property_xyz

# Check backend logs
docker logs pms-backend-api | grep "search="
```

**Solution:**
- Verify search param wired in fetchProperties \(debouncedSearchQuery appended to params\)
- Verify backend PropertyFilter schema has search field
- Increase debounce timeout if network slow

---

## Admin UI Properties List: Export/Search/Smoke Fixes (P2.21.4.7p)

**Overview:** Production fixes for properties list page after commit 5c152f0 - export API 422 error, search not filtering, smoke script failures.

**Changes Implemented:**

1. **Export API Route Collision Fix** (backend/app/api/routes/properties.py):
   - Root cause: `/properties/export` route declared AFTER `/{property_id}` parameterized route
   - FastAPI matched "export" as UUID parameter causing HTTP 422 validation error
   - Fix: Removed duplicate export function that was incorrectly placed after parameterized route
   - Export route now correctly declared BEFORE `/{property_id}` route (done in P2.21.4.7n)

2. **Export Search Parameter Fix** (frontend/app/properties/page.tsx):
   - Export handler was missing `debouncedSearchQuery` parameter
   - CSV export didn't respect active search filter
   - Fix: Added search parameter to export handler params collection

3. **Smoke Script Variable Scope Fix** (backend/scripts/pms_admin_ui_properties_list_smoke.sh):
   - `captureDebugArtifacts` function referenced `page` and `context` variables outside its scope
   - Caused "page is not defined" error when debug artifacts triggered
   - Fix: Updated function signature to accept `page, context, testName` parameters
   - Updated all 15+ call sites to pass page and context explicitly

**Verification Commands:**

```bash
# HOST-SERVER-TERMINAL
export ADMIN_BASE_URL="https://admin.fewo.kolibri-visions.de"
export MANAGER_JWT_TOKEN="..."

# Test export API directly
curl -X GET "https://api.fewo.kolibri-visions.de/api/v1/properties/export?search=berlin" \
  -H "Authorization: Bearer $JWT"
# Expected: CSV download with filtered results, NOT HTTP 422

# Run updated smoke test
./backend/scripts/pms_admin_ui_properties_list_smoke.sh
# Expected: rc=0, 9/9 tests passed, no "page is not defined" errors
```

**Common Issues:**

### Export API Returns HTTP 422 "validation error for path parameter property_id"

**Symptom:** GET /api/v1/properties/export returns HTTP 422 with message "validation error for path parameter property_id: Input should be a valid UUID".

**Root Cause (Pre-P2.21.4.7p):** Export route `/properties/export` was declared AFTER the parameterized route `/{property_id}`. FastAPI matched the literal string "export" against the UUID pattern first, causing validation error.

**How to Debug:**
```bash
# Test export endpoint
curl -I "https://api.fewo.kolibri-visions.de/api/v1/properties/export" \
  -H "Authorization: Bearer $JWT"

# If 422: Check route ordering in properties.py
rg -n "@router.get.*export" backend/app/api/routes/properties.py
rg -n "@router.get.*property_id.*UUID" backend/app/api/routes/properties.py

# Export route line number MUST be LESS than property_id route line number
```

**Solution:**
- Verify P2.21.4.7n + P2.21.4.7p changes deployed
- Export route should be declared before any `/{property_id}` routes
- Check for duplicate export function definitions (P2.21.4.7p removed duplicate)

### Export CSV Doesn't Respect Active Search Filter

**Symptom:** User types search query, clicks Export button, but CSV contains all properties (not filtered).

**Root Cause (Pre-P2.21.4.7p):** Export handler in frontend didn't pass `debouncedSearchQuery` to API params.

**How to Debug:**
```bash
# Check network tab in browser DevTools
# Filter by "export" request
# Check Request URL query parameters
# Should include: ?search=<query> if search box has text

# Check frontend code
rg "debouncedSearchQuery.*export" frontend/app/properties/page.tsx
# Should show: if (debouncedSearchQuery) params.append("search", debouncedSearchQuery);
```

**Solution:**
- Verify P2.21.4.7p frontend changes deployed
- Hard refresh browser (Cmd+Shift+R) to clear cached JS bundle
- Verify export handler includes all active filter parameters

### Smoke Script Fails with "page is not defined" Error

**Symptom:** `pms_admin_ui_properties_list_smoke.sh` fails with ReferenceError: page is not defined when trying to capture debug artifacts.

**Root Cause (Pre-P2.21.4.7p):** `captureDebugArtifacts` function defined outside async IIFE tried to reference `page` and `context` variables defined inside the IIFE scope.

**How to Debug:**
```bash
# Check function signature
rg -n "async function captureDebugArtifacts" backend/scripts/pms_admin_ui_properties_list_smoke.sh
# Should show: async function captureDebugArtifacts(page, context, testName)

# Check call sites
rg "captureDebugArtifacts\(" backend/scripts/pms_admin_ui_properties_list_smoke.sh | head -3
# All should show: await captureDebugArtifacts(page, context, 'testname');
```

**Solution:**
- Verify P2.21.4.7p smoke script changes deployed
- Function signature must accept page and context as parameters
- All call sites must pass page and context explicitly

### Smoke Test 7 Fails (Modal Background Transparent)

**Symptom:** Test 7 reports "Modal background is transparent" but modal looks correct in UI.

**Root Cause:** Test assertion checks backgroundColor computed style but may fail if CSS not fully loaded or class names changed.

**How to Debug:**
```bash
# Check modal testid and classes in UI code
rg 'data-testid="new-property-modal".*bg-luxe-surface' frontend/app/properties/page.tsx

# Check debug artifacts
ls -lah /tmp/smoke_test7_*.png
# Screenshot should show modal with proper styling
```

**Solution:**
- Verify modal has both testid AND bg-luxe-surface class
- Check if LuxeStay CSS variables defined correctly
- May need to increase wait time for CSS loading

### Smoke Test 8 Fails (Search Doesn't Reduce Results)

**Symptom:** Test 8 types nonexistent search query but property count doesn't decrease.

**Root Cause:** Search not wired to backend, debounce timing too short, or backend search filter not deployed.

**How to Debug:**
```bash
# Check if search query sent to backend
# DevTools → Network tab → Filter XHR/Fetch
# Type in search box, wait 1.5s
# Should see: GET /api/v1/properties?search=zzz_nonexistent_property_xyz

# Test backend search directly
curl -sS "https://api.fewo.kolibri-visions.de/api/v1/properties?search=zzz_nonexistent_property_xyz&limit=10" \
  -H "Authorization: Bearer $JWT" | jq '.items | length'
# Expected: 0
```

**Solution:**
- Verify backend PropertyFilter schema has search field (P2.21.4.7n)
- Verify frontend fetchProperties includes debouncedSearchQuery param
- Increase debounce wait time in smoke test if needed (currently 1.5s)

---
